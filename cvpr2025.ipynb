{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddba11c",
   "metadata": {},
   "source": [
    "## Scrape all CVPR 2026 BibTeX with abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76b0798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "684c7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openaccess.thecvf.com/CVPR2025?day=all\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28e7f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_paper(paper_dt):\n",
    "    try:\n",
    "        if (title_tag := paper_dt.find('a')):\n",
    "            paper_title = title_tag.text\n",
    "            paper_url = \"https://openaccess.thecvf.com\" + title_tag.get('href')\n",
    "\n",
    "            paper_response = requests.get(paper_url, timeout=20)\n",
    "            paper_soup = BeautifulSoup(paper_response.text, 'html.parser')\n",
    "\n",
    "            abstract_div = paper_soup.find('div', id='abstract')\n",
    "            paper_abstract = abstract_div.text.strip() if abstract_div else ''\n",
    "\n",
    "            paper_bibtex_div = paper_soup.find('div', class_='bibref')\n",
    "            paper_bibtex = paper_bibtex_div.text.strip() if paper_bibtex_div else ''\n",
    "\n",
    "            return paper_title, {\n",
    "                'url': paper_url,\n",
    "                'abstract': paper_abstract,\n",
    "                'bibtex': paper_bibtex,\n",
    "                'pub_type' : 'poster',\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {paper_dt}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32fea130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2871/2871 [02:02<00:00, 23.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "paper_dts = soup.find_all('dt', class_='ptitle')\n",
    "paper_dict = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    # submit all jobs\n",
    "    futures = [executor.submit(scrape_paper, dt) for dt in paper_dts]\n",
    "    # progress bar and collect results\n",
    "    for f in tqdm(as_completed(futures), total=len(futures)):\n",
    "        result = f.result()\n",
    "        if result:\n",
    "            paper_title, paper_info = result\n",
    "            paper_dict[paper_title] = paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c865b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@InProceedings{Liu_2025_CVPR,\n",
      "    author    = {Liu, Yuanpei and He, Zhenqi and Han, Kai},\n",
      "    title     = {Hyperbolic Category Discovery},\n",
      "    booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n",
      "    month     = {June},\n",
      "    year      = {2025},\n",
      "    pages     = {9891-9900}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(paper_dict['Hyperbolic Category Discovery']['bibtex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3b906",
   "metadata": {},
   "source": [
    "## Scrape oral paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "022d88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cvpr.thecvf.com/virtual/2025/calendar\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f57ddbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Oral Session 1A: Image and Video Synthesis': ['Motion Prompting: Controlling Video Generation with Motion Trajectories',\n",
       "  'Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise',\n",
       "  'LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping',\n",
       "  'Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space',\n",
       "  'RandAR: Decoder-only Autoregressive Visual Generation in Random Orders'],\n",
       " 'Oral Session 1B: Interpretability and Evaluation': ['OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation',\n",
       "  'LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions',\n",
       "  'Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild',\n",
       "  'Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models',\n",
       "  'Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector'],\n",
       " 'Oral Session 1C: Image Processing and Deep Architectures': ['CleanDIFT: Diffusion Features without Noise',\n",
       "  'OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels',\n",
       "  'Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather',\n",
       "  'DiffFNO: Diffusion Fourier Neural Operator',\n",
       "  'Removing Reflections from RAW Photos'],\n",
       " 'Oral Session 2A: 3D Computer Vision': ['FoundationStereo: Zero-Shot Stereo Matching',\n",
       "  'MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision',\n",
       "  'Multi-view Reconstruction via SfM-guided Monocular Depth Estimation',\n",
       "  'MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds',\n",
       "  'VGGT: Visual Geometry Grounded Transformer',\n",
       "  'CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner'],\n",
       " 'Oral Session 2B: Human Motion': ['CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models',\n",
       "  'Reanimating Images using Neural Representations of Dynamic Stimuli',\n",
       "  'EgoLM: Multi-Modal Language Model of Egocentric Motions',\n",
       "  'Reconstructing Humans with a Biomechanically Accurate Skeleton',\n",
       "  'MEGA: Masked Generative Autoencoder for Human Mesh Recovery',\n",
       "  'TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization'],\n",
       " 'Oral Session 2C: Temporal Modeling and Action Recognition': ['Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays',\n",
       "  'Temporally Consistent Object-Centric Learning by Contrasting Slots',\n",
       "  'Temporal Alignment-Free Video Matching for Few-shot Action Recognition',\n",
       "  'Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models',\n",
       "  'The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition',\n",
       "  'Rethinking Spiking Self-Attention Mechanism: Implementing α-XNOR Similarity Calculation in Spiking Transformers'],\n",
       " 'Oral Session 3A: 3D Computer Vision': ['MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos',\n",
       "  'Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos',\n",
       "  'Continuous 3D Perception Model with Persistent State',\n",
       "  'TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion',\n",
       "  'Neural Inverse Rendering from Propagating Light'],\n",
       " 'Oral Session 3B: Multimodal Computer Vision': ['SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images',\n",
       "  'Towards Universal Dataset Distillation via Task-Driven Diffusion',\n",
       "  'IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior',\n",
       "  'Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning',\n",
       "  'Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation'],\n",
       " 'Oral Session 3C: Vision and Language': ['Identifying and Mitigating Position Bias of Multi-image Vision-Language Models',\n",
       "  'Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key',\n",
       "  'Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content',\n",
       "  'Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces',\n",
       "  'From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons'],\n",
       " 'Oral Session 4A: Image and Video Synthesis': ['Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models',\n",
       "  'Language-Guided Image Tokenization for Generation',\n",
       "  'DreamRelation: Bridging Customization and Relation Generation',\n",
       "  'Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis',\n",
       "  'Autoregressive Distillation of Diffusion Transformers'],\n",
       " 'Oral Session 4B: Embodied Computer Vision': ['PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation',\n",
       "  'RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics',\n",
       "  'GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill',\n",
       "  'Navigation World Models',\n",
       "  'Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning'],\n",
       " 'Oral Session 4C: 3D Computer Vision': ['DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution',\n",
       "  'Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World',\n",
       "  'Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues',\n",
       "  'Camera Resection from Known Line Pencils and a Radially Distorted Scanline',\n",
       "  'Opportunistic Single-Photon Time of Flight'],\n",
       " 'Oral Session 5A: Generative AI': ['Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing',\n",
       "  'DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models',\n",
       "  'CustAny: Customizing Anything from A Single Example',\n",
       "  'Minority-Focused Text-to-Image Generation via Prompt Optimization',\n",
       "  'Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models'],\n",
       " 'Oral Session 5B: Learning Systems and Medical Applications': ['UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming',\n",
       "  'Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning',\n",
       "  'Enhancing Diversity for Data-free Quantization',\n",
       "  'TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model',\n",
       "  'Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation'],\n",
       " 'Oral Session 5C: Visual and Spatial Computing': ['Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks',\n",
       "  'Gromov–Wasserstein Problem with Cyclic Symmetry',\n",
       "  'Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields',\n",
       "  'Zero-Shot Monocular Scene Flow Estimation in the Wild',\n",
       "  '3D Student Splatting and Scooping'],\n",
       " 'Oral Session 6A: 3D from Single or Multi-View Sensors': ['DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models',\n",
       "  '3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting',\n",
       "  'DNF: Unconditional 4D Generation with Dictionary-based Neural Fields',\n",
       "  'CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models',\n",
       "  'Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models'],\n",
       " 'Oral Session 6B: Scene Understanding, Image Editing and Multimodal Learning': ['Effective SAM Combination for Open-Vocabulary Semantic Segmentation',\n",
       "  'FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video',\n",
       "  'Birth and Death of a Rose',\n",
       "  'Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining',\n",
       "  'AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea',\n",
       "  'Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens'],\n",
       " 'Oral Session 6C: Video, Action, and Language': ['Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding',\n",
       "  'Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding',\n",
       "  'LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models',\n",
       "  'VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection',\n",
       "  'SEAL: Semantic Attention Learning for Long Video Representation',\n",
       "  'Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oral_session_divs = soup.find_all('div', class_='oral-session')\n",
    "oral_dict = {}\n",
    "\n",
    "for session_div in oral_session_divs:\n",
    "    # scrape all oral session\n",
    "    if (title_tag := session_div.find('a')):\n",
    "        title_text = title_tag.text.replace('\\n', '').replace('  ', '').strip()\n",
    "        title_text = title_text.split('[')[0]\n",
    "        oral_dict[title_text] = []\n",
    "\n",
    "        # scrape papers in the oral session\n",
    "        papers = session_div.find_all('div', class_='content oral')\n",
    "        for paper in papers:\n",
    "            paper_title = paper.text.strip().replace('\\n', '').replace('  ', '').strip()\n",
    "            paper_title = paper_title.split(']')[-1].strip()\n",
    "            oral_dict[title_text].append(paper_title)\n",
    "\n",
    "oral_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d69bf242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Motion Prompting: Controlling Video Generation with Motion Trajectories',\n",
       " 'Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise',\n",
       " 'LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping',\n",
       " 'Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space',\n",
       " 'RandAR: Decoder-only Autoregressive Visual Generation in Random Orders']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oral_dict['Oral Session 1A: Image and Video Synthesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38ed522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oral Session 1A: Image and Video Synthesis -> 5 papers\n",
      "Oral Session 1B: Interpretability and Evaluation -> 5 papers\n",
      "Oral Session 1C: Image Processing and Deep Architectures -> 5 papers\n",
      "Oral Session 2A: 3D Computer Vision -> 6 papers\n",
      "Oral Session 2B: Human Motion -> 6 papers\n",
      "Oral Session 2C: Temporal Modeling and Action Recognition -> 6 papers\n",
      "Oral Session 3A: 3D Computer Vision -> 5 papers\n",
      "Oral Session 3B: Multimodal Computer Vision -> 5 papers\n",
      "Oral Session 3C: Vision and Language -> 5 papers\n",
      "Oral Session 4A: Image and Video Synthesis -> 5 papers\n",
      "Oral Session 4B: Embodied Computer Vision -> 5 papers\n",
      "Oral Session 4C: 3D Computer Vision -> 5 papers\n",
      "Oral Session 5A: Generative AI -> 5 papers\n",
      "Oral Session 5B: Learning Systems and Medical Applications -> 5 papers\n",
      "Oral Session 5C: Visual and Spatial Computing -> 5 papers\n",
      "Oral Session 6A: 3D from Single or Multi-View Sensors -> 5 papers\n",
      "Oral Session 6B: Scene Understanding, Image Editing and Multimodal Learning -> 6 papers\n",
      "Oral Session 6C: Video, Action, and Language -> 6 papers\n",
      "----------------------------------------------------------------------------------------------------\n",
      "total -> 95\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "for session, papers in oral_dict.items():\n",
    "    print(f\"{session} -> {len(papers)} papers\")\n",
    "    total += len(papers)\n",
    "\n",
    "print('-'*100)\n",
    "print(f\"total -> {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2b5c7",
   "metadata": {},
   "source": [
    "## Scrape highlight paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e26ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cvpr.thecvf.com/virtual/2025/awards_detail\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d8a3de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning Class Prototypes for Unified Sparse-Supervised 3D Object Detection',\n",
       " 'Scene-Centric Unsupervised Panoptic Segmentation',\n",
       " 'CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design',\n",
       " 'Multi-modal Vision Pre-training for Medical Image Analysis',\n",
       " 'SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting',\n",
       " 'High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model',\n",
       " 'End-to-End HOI Reconstruction Transformer with Graph-based Encoding',\n",
       " 'FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation',\n",
       " 'Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding',\n",
       " 'SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning',\n",
       " 'Structure from Collision',\n",
       " 'You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale',\n",
       " 'Volumetrically Consistent 3D Gaussian Rasterization',\n",
       " 'Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB',\n",
       " 'Active Hyperspectral Imaging Using an Event Camera',\n",
       " 'Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes',\n",
       " 'Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation',\n",
       " 'Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting',\n",
       " 'Revisiting MAE Pre-training for 3D Medical Image Segmentation',\n",
       " 'All-directional Disparity Estimation for Real-world QPD Images',\n",
       " 'Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition',\n",
       " 'CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image',\n",
       " 'World-consistent Video Diffusion with Explicit 3D Modeling',\n",
       " 'CrossOver: 3D Scene Cross-Modal Alignment',\n",
       " 'RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars',\n",
       " 'Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness',\n",
       " 'BADGR: Bundle Adjustment Diffusion Conditioned by Gradients for Wide-Baseline Floor Plan Reconstruction',\n",
       " 'Locally Orderless Images for Optimization in Differentiable Rendering',\n",
       " 'MLLM-as-a-Judge for Image Safety without Human Labeling',\n",
       " 'SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks',\n",
       " 'Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models',\n",
       " 'SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer',\n",
       " 'Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective',\n",
       " 'HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos',\n",
       " 'MonSter: Marry Monodepth to Stereo Unleashes Power',\n",
       " 'FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images',\n",
       " 'MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors',\n",
       " 'LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate',\n",
       " 'DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery',\n",
       " 'Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition',\n",
       " 'Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding',\n",
       " 'MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps',\n",
       " 'Towards In-the-wild 3D Plane Reconstruction from a Single Image',\n",
       " 'X-Dyna: Expressive Dynamic Human Image Animation',\n",
       " 'Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding',\n",
       " 'Type-R: Automatically Retouching Typos for Text-to-Image Generation',\n",
       " 'CoSER: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation',\n",
       " 'Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution',\n",
       " 'Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays',\n",
       " 'SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment',\n",
       " 'Reference-Based 3D-Aware Image Editing with Triplanes',\n",
       " 'Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval',\n",
       " 'Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding',\n",
       " 'EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events',\n",
       " 'Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces',\n",
       " 'BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance',\n",
       " 'Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation',\n",
       " 'Identity-Preserving Text-to-Video Generation by Frequency Decomposition',\n",
       " 'TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting',\n",
       " 'Structured 3D Latents for Scalable and Versatile 3D Generation',\n",
       " 'Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems',\n",
       " 'GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting',\n",
       " 'TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction',\n",
       " 'Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection',\n",
       " 'Structure-from-Motion with a Non-Parametric Camera Model',\n",
       " 'Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection',\n",
       " 'Shape Abstraction via Marching Differentiable Support Functions',\n",
       " 'ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping',\n",
       " 'MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond',\n",
       " 'CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation',\n",
       " 'Question-Aware Gaussian Experts for Audio-Visual Question Answering',\n",
       " 'HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation',\n",
       " 'CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching',\n",
       " 'A Unified Approach to Interpreting Self-supervised Pre-training Methods for 3D Point Clouds via Interactions',\n",
       " 'SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration',\n",
       " 'NTClick: Achieving Precise Interactive Segmentation With Noise-tolerant Clicks',\n",
       " 'Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion',\n",
       " 'Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality',\n",
       " 'TKG-DM: Training-free Chroma Key Content Generation Diffusion Model',\n",
       " 'Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways',\n",
       " 'Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene',\n",
       " 'StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer',\n",
       " 'TinyFusion: Diffusion Transformers Learned Shallow',\n",
       " 'FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis',\n",
       " 'Sonata: Self-Supervised Learning of Reliable Point Representations',\n",
       " 'Assessing and Learning Alignment of Unimodal Vision and Language Models',\n",
       " 'Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision',\n",
       " 'MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views',\n",
       " 'Simulator HC: Regression-based Online Simulation of Starting Problem-Solution Pairs for Homotopy Continuation in Geometric Vision',\n",
       " 'DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction',\n",
       " 'GenVDM: Generating Vector Displacement Maps From a Single Image',\n",
       " 'ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points',\n",
       " 'Functionality Understanding and Segmentation in 3D Scenes',\n",
       " 'SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos',\n",
       " 'Towards Scalable Human-aligned Benchmark for Text-guided Image Editing',\n",
       " 'Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction',\n",
       " 'From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech',\n",
       " 'SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts',\n",
       " 'One-Step Event-Driven High-Speed Autofocus',\n",
       " 'Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence Microscopy De-noising and Super-Resolution',\n",
       " 'Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs',\n",
       " 'v-CLR: View-Consistent Learning for Open-World Instance Segmentation',\n",
       " 'Doppelgängers and Adversarial Vulnerability',\n",
       " 'UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics',\n",
       " 'RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training',\n",
       " 'DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving',\n",
       " 'Understanding Multi-Task Activities from Single-Task Videos',\n",
       " 'Unlocking Generalization Power in LiDAR Point Cloud Registration',\n",
       " 'ICP: Immediate Compensation Pruning for Mid-to-high Sparsity',\n",
       " 'Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning',\n",
       " 'FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling',\n",
       " 'Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection',\n",
       " 'EventPSR: Surface Normal and Reflectance Estimation from Photometric Stereo Using an Event Camera',\n",
       " 'WonderWorld: Interactive 3D Scene Generation from a Single Image',\n",
       " 'Polarized Color Screen Matting',\n",
       " 'All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages',\n",
       " 'DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos',\n",
       " 'Unified Reconstruction of Static and Dynamic Scenes from Events',\n",
       " 'PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset',\n",
       " 'Distraction is All You Need for Multimodal Large Language Model Jailbreaking',\n",
       " 'Efficient Motion-Aware Video MLLM',\n",
       " 'SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer',\n",
       " 'O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models',\n",
       " 'Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models',\n",
       " 'Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis',\n",
       " 'Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis',\n",
       " 'DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving',\n",
       " 'GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model',\n",
       " 'SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models',\n",
       " 'MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation',\n",
       " 'ARM: Appearance Reconstruction Model for Relightable 3D Generation',\n",
       " 'CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction',\n",
       " 'XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?',\n",
       " 'Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity',\n",
       " 'NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction',\n",
       " '3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion',\n",
       " 'SkillMimic: Learning Basketball Interaction Skills from Demonstrations',\n",
       " 'Multitwine: Multi-Object Compositing with Text and Layout Control',\n",
       " 'InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions',\n",
       " 'Towards RAW Object Detection in Diverse Conditions',\n",
       " 'DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection',\n",
       " 'Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras',\n",
       " 'F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics',\n",
       " 'Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera',\n",
       " 'Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution',\n",
       " 'DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection',\n",
       " 'ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models',\n",
       " 'HotSpot: Signed Distance Function Optimization with an Asymptotically Sufficient Condition',\n",
       " 'Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling',\n",
       " 'Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking',\n",
       " 'UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models',\n",
       " 'Multimodal Autoregressive Pre-training of Large Vision Encoders',\n",
       " 'Deep Fair Multi-View Clustering with Attention KAN',\n",
       " 'ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate',\n",
       " 'Memories of Forgotten Concepts',\n",
       " 'DistinctAD: Distinctive Audio Description Generation in Contexts',\n",
       " 'Boost Your Human Image Generation Model via Direct Preference Optimization',\n",
       " 'Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling',\n",
       " 'The Scene Language: Representing Scenes with Programs, Words, and Embeddings',\n",
       " 'VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge',\n",
       " 'FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute',\n",
       " 'Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation',\n",
       " 'Interpreting Object-level Foundation Models via Visual Precision Search',\n",
       " 'Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models',\n",
       " 'Lifting Motion to the 3D World via 2D Diffusion',\n",
       " 'EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision',\n",
       " 'Augmented Deep Contexts for Spatially Embedded Video Coding',\n",
       " 'CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval',\n",
       " 'Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection',\n",
       " 'MATCHA: Towards Matching Anything',\n",
       " 'Free-viewpoint Human Animation with Pose-correlated Reference Selection',\n",
       " 'Compositional Caching for Training-free Open-vocabulary Attribute Detection',\n",
       " 'Order-One Rolling Shutter Cameras',\n",
       " 'Improving Personalized Search with Regularized Low-Rank Parameter Updates',\n",
       " 'Samba: A Unified Mamba-based Framework for General Salient Object Detection',\n",
       " 'MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds',\n",
       " 'Scaling Vision Pre-Training to 4K Resolution',\n",
       " \"Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model\",\n",
       " 'Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation',\n",
       " 'Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy',\n",
       " 'QuCOOP: A Versatile Framework for Solving Composite and Binary-Parametrised Problems on Quantum Annealers',\n",
       " 'UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning',\n",
       " 'Goku: Flow Based Video Generative Foundation Models',\n",
       " 'ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning',\n",
       " 'Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos',\n",
       " 'PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes',\n",
       " 'DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness',\n",
       " 'Event Ellipsometer: Event-based Mueller-Matrix Video Imaging',\n",
       " 'GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control',\n",
       " 'Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret',\n",
       " 'Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach',\n",
       " 'MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation',\n",
       " 'Generative Omnimatte: Learning to Decompose Video into Layers',\n",
       " 'ESC: Erasing Space Concept for Knowledge Deletion',\n",
       " 'IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera',\n",
       " 'SACB-Net: Spatial-awareness Convolutions for Medical Image Registration',\n",
       " 'Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning',\n",
       " 'Improving Gaussian Splatting with Localized Points Management',\n",
       " 'How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions',\n",
       " 'Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?',\n",
       " 'LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models',\n",
       " 'MUSt3R: Multi-view Network for Stereo 3D Reconstruction',\n",
       " 'Seeing More with Less: Human-like Representations in Vision Models',\n",
       " 'A Polarization-Aided Transformer for Image Deblurring via Motion Vector Decomposition',\n",
       " 'Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition',\n",
       " 'CASP: Compression of Large Multimodal Models Based on Attention Sparsity',\n",
       " 'Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation',\n",
       " 'ALIEN: Implicit Neural Representations for Human Motion Prediction under Arbitrary Latency',\n",
       " 'EBS-EKF: Accurate and High Frequency Event-based Star Tracking',\n",
       " 'HuPerFlow: A Comprehensive Benchmark for Human vs. Machine Motion Estimation Comparison',\n",
       " 'Few-shot Implicit Function Generation via Equivariance',\n",
       " 'Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers',\n",
       " 'HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos',\n",
       " 'Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs',\n",
       " 'Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models',\n",
       " 'ETAP: Event-based Tracking of Any Point',\n",
       " 'FIction: 4D Future Interaction Prediction from Video',\n",
       " 'Universal Scene Graph Generation',\n",
       " 'RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins',\n",
       " 'All-Optical Nonlinear Diffractive Deep Network for Ultrafast Image Denoising',\n",
       " 'Circumventing Shortcuts in Audio-visual Deepfake Detection Datasets with Unsupervised Learning',\n",
       " 'Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body',\n",
       " 'Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification',\n",
       " 'Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders',\n",
       " 'SmartCLIP: Modular Vision-language Alignment with Identification Guarantees',\n",
       " 'CRISP: Object Pose and Shape Estimation with Test-Time Adaptation',\n",
       " 'ImViD: Immersive Volumetric Videos for Enhanced VR Engagement',\n",
       " 'OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints',\n",
       " 'InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment',\n",
       " 'Rethinking Personalized Aesthetics Assessment: Employing Physique Aesthetics Assessment as An Exemplification',\n",
       " 'EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing',\n",
       " 'LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis',\n",
       " 'CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering',\n",
       " 'DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds',\n",
       " 'MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking',\n",
       " 'PGC: Physics-Based Gaussian Cloth from a Single Pose',\n",
       " 'EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation',\n",
       " 'DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos',\n",
       " 'Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models',\n",
       " 'Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach',\n",
       " 'WISH: Weakly Supervised Instance Segmentation using Heterogeneous Labels',\n",
       " 'BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer',\n",
       " 'From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing',\n",
       " 'Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think',\n",
       " 'Balanced Rate-Distortion Optimization in Learned Image Compression',\n",
       " 'SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation',\n",
       " 'NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery',\n",
       " 'FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement',\n",
       " 'Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers',\n",
       " 'Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding',\n",
       " 'Cross-View Completion Models are Zero-shot Correspondence Estimators',\n",
       " 'Supervising Sound Localization by In-the-wild Egomotion',\n",
       " 'HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction',\n",
       " 'MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations',\n",
       " 'ROLL: Robust Noisy Pseudo-label Learning for Multi-View Clustering with Noisy Correspondence',\n",
       " 'Learning Conditional Space-Time Prompt Distributions for Video Class-Incremental Learning',\n",
       " 'Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision',\n",
       " 'Can Generative Video Models Help Pose Estimation?',\n",
       " 'Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis',\n",
       " 'SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training',\n",
       " 'ReNeg: Learning Negative Embedding with Reward Guidance',\n",
       " 'Glossy Object Reconstruction with Cost-effective Polarized Acquisition',\n",
       " 'STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection',\n",
       " 'COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts',\n",
       " 'Material Anything: Generating Materials for Any 3D Object via Diffusion',\n",
       " 'Hyperbolic Safety-Aware Vision-Language Models',\n",
       " 'VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models',\n",
       " 'Panorama Generation From NFoV Image Done Right',\n",
       " 'UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion',\n",
       " 'CADDreamer: CAD Object Generation from Single-view Images',\n",
       " 'Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis',\n",
       " 'Realistic Test-Time Adaptation of Vision-Language Models',\n",
       " 'Context-Aware Multimodal Pretraining',\n",
       " 'RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness',\n",
       " 'TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance',\n",
       " 'Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning',\n",
       " 'Your ViT is Secretly an Image Segmentation Model',\n",
       " 'SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving',\n",
       " 'Image Quality Assessment: From Human to Machine Preference',\n",
       " 'LaTexBlend: Scaling Multi-concept Customized Generation with Latent Textual Blending',\n",
       " 'MITracker: Multi-View Integration for Visual Object Tracking',\n",
       " 'OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation',\n",
       " 'DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding',\n",
       " 'EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space',\n",
       " 'FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video',\n",
       " 'FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation',\n",
       " 'Generative Modeling of Class Probability for Multi-Modal Representation Learning',\n",
       " 'USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting',\n",
       " 'NLPrompt: Noise-Label Prompt Learning for Vision-Language Models',\n",
       " 'Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map',\n",
       " 'GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities',\n",
       " 'No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition',\n",
       " 'SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization',\n",
       " 'AIpparel: A Multimodal Foundation Model for Digital Garments',\n",
       " 'Style-Editor: Text-driven Object-centric Style Editing',\n",
       " 'Prior-free 3D Object Tracking',\n",
       " 'FineVQ: Fine-Grained User Generated Content Video Quality Assessment',\n",
       " 'Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection',\n",
       " 'Scaling Inference Time Compute for Diffusion Models',\n",
       " 'Olympus: A Universal Task Router for Computer Vision Tasks',\n",
       " 'HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis',\n",
       " 'Understanding Multi-layered Transmission Matrices',\n",
       " 'Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation',\n",
       " 'Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting',\n",
       " 'Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency',\n",
       " 'Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection',\n",
       " 'OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit',\n",
       " 'PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models',\n",
       " 'UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing',\n",
       " 'Learning to Filter Outlier Edges in Global SfM',\n",
       " 'MangaNinja: Line Art Colorization with Precise Reference Following',\n",
       " 'Reconstructing People, Places, and Cameras',\n",
       " 'Seurat: From Moving Points to Depth',\n",
       " 'Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation',\n",
       " 'Video Depth Anything: Consistent Depth Estimation for Super-Long Videos',\n",
       " 'DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences',\n",
       " 'AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities',\n",
       " 'Dataset Distillation with Neural Characteristic Function: A Minmax Perspective',\n",
       " 'Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation',\n",
       " 'NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting',\n",
       " 'Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models',\n",
       " 'Enduring, Efficient and Robust Trajectory Prediction Attack in Autonomous Driving via Optimization-Driven Multi-Frame Perturbation Framework',\n",
       " '3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes',\n",
       " 'Cross-modal Causal Relation Alignment for Video Question Grounding',\n",
       " 'Pippo: High-Resolution Multi-View Humans from a Single Image',\n",
       " 'Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration',\n",
       " 'Open-Canopy: Towards Very High Resolution Forest Monitoring',\n",
       " 'STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models',\n",
       " 'One-shot 3D Object Canonicalization based on Geometric and Semantic Consistency',\n",
       " 'Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters',\n",
       " 'Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video',\n",
       " 'Task-driven Image Fusion with Learnable Fusion Loss',\n",
       " 'Align3R: Aligned Monocular Depth Estimation for Dynamic Videos',\n",
       " 'UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior',\n",
       " 'CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation',\n",
       " 'Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks',\n",
       " 'SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons',\n",
       " 'High-Fidelity Lightweight Mesh Reconstruction from Point Clouds',\n",
       " 'InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing',\n",
       " 'Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks',\n",
       " 'UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units',\n",
       " 'DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction',\n",
       " 'Relative Pose Estimation through Affine Corrections of Monocular Depth Priors',\n",
       " 'Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better',\n",
       " 'Parallelized Autoregressive Visual Generation',\n",
       " 'Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics',\n",
       " 'Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging',\n",
       " 'Instruction-based Image Manipulation by Watching How Things Move',\n",
       " 'HumanRig: Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset',\n",
       " 'Light3R-SfM: Towards Feed-forward Structure-from-Motion',\n",
       " 'ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images',\n",
       " 'Doppelgangers++: Improved Visual Disambiguation with Geometric 3D Features',\n",
       " 'Towards Autonomous Micromobility through Scalable Urban Simulation',\n",
       " 'Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency',\n",
       " 'Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text',\n",
       " 'SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding',\n",
       " 'T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting',\n",
       " 'Implicit Correspondence Learning for Image-to-Point Cloud Registration',\n",
       " 'Visual Representation Learning through Causal Intervention for Controllable Image Editing',\n",
       " 'Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting',\n",
       " 'VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step',\n",
       " 'BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing',\n",
       " 'Optimizing for the Shortest Path in Denoising Diffusion Model',\n",
       " 'Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation',\n",
       " 'Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset',\n",
       " 'Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation',\n",
       " '4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion',\n",
       " 'VEU-Bench: Towards Comprehensive Understanding of Video Editing',\n",
       " 'MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving',\n",
       " 'H-MoRe: Learning Human-centric Motion Representation for Action Analysis',\n",
       " 'KAC: Kolmogorov-Arnold Classifier for Continual Learning',\n",
       " 'MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation',\n",
       " 'OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation',\n",
       " 'Estimating Body and Hand Motion in an Ego‑sensed World',\n",
       " 'Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures',\n",
       " 'Hardware-Rasterized Ray-Based Gaussian Splatting',\n",
       " 'AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction',\n",
       " 'Matrix3D: Large Photogrammetry Model All-in-One',\n",
       " 'Cubify Anything: Scaling Indoor 3D Object Detection',\n",
       " 'Gradient-Guided Annealing for Domain Generalization',\n",
       " 'Towards Explainable and Unprecedented Accuracy in Matching Challenging Finger Crease Patterns',\n",
       " 'OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities',\n",
       " 'Multirate Neural Image Compression with Adaptive Lattice Vector Quantization',\n",
       " 'Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility',\n",
       " 'SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity',\n",
       " 'Structure-Aware Correspondence Learning for Relative Pose Estimation',\n",
       " 'Event Fields: Capturing Light Fields at High Speed, Resolution, and Dynamic Range',\n",
       " 'Less is More: Efficient Model Merging with Binary Task Switch']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlight_divs = soup.find_all('div', class_='virtual-card')\n",
    "highlight_ls = []\n",
    "\n",
    "for highlight_div in highlight_divs:\n",
    "    # scrape all oral session\n",
    "    if (title_tag := highlight_div.find('a')):\n",
    "        title_text = title_tag.text.strip()\n",
    "        highlight_ls.append(title_text)\n",
    "\n",
    "highlight_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1404b93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(highlight_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf0cf2",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c99094",
   "metadata": {},
   "source": [
    "### BibTeX utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4efebdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'inproceedings',\n",
       " 'id': 'YuanpeiLiu2025CVPR',\n",
       " 'author': 'Liu, Yuanpei and He, Zhenqi and Han, Kai',\n",
       " 'title': 'Hyperbolic Category Discovery',\n",
       " 'booktitle': 'Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)',\n",
       " 'month': 'June',\n",
       " 'year': '2025',\n",
       " 'pages': '9891-9900'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_bitex(bibtex_data, gen_id=False, id_surfix='', lower_case_type=True):\n",
    "    entry_regex = re.compile(r'@([a-zA-Z]+){([^,]+),(.*)}', re.DOTALL)\n",
    "    match = entry_regex.search(bibtex_data)\n",
    "\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid BibTeX data\")\n",
    "\n",
    "    fields = {\n",
    "        'type': match.group(1),\n",
    "        'id': match.group(2)\n",
    "    }\n",
    "    fields_str = match.group(3)\n",
    "\n",
    "    mode = 'key'\n",
    "    store = ''\n",
    "    max_layer = 0\n",
    "    stack = []\n",
    "    keys = []\n",
    "    values = []\n",
    "\n",
    "    for idx, char in enumerate(fields_str):\n",
    "        if mode == 'key':\n",
    "            if char == '=':\n",
    "                keys.append(store.strip())\n",
    "                store = ''\n",
    "                mode = 'value'\n",
    "            else:\n",
    "                store += char\n",
    "        elif mode == 'value':\n",
    "            store += char\n",
    "\n",
    "            if char == '{':\n",
    "                stack.append(char)\n",
    "                max_layer += 1\n",
    "                if max_layer == 1:\n",
    "                    store = ''\n",
    "            elif char == '}':\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    store = store[:-1]\n",
    "\n",
    "            if (max_layer > 0 and not stack) or (max_layer == 0 and (char in ',}' or idx == len(fields_str) - 1)):\n",
    "                value = store.strip().replace(\": \", \"{:} \")\n",
    "                if value.startswith('{') or value.endswith('}'):\n",
    "                    value = f'\"{value}\"'\n",
    "                values.append(value)\n",
    "                store = ''\n",
    "                max_layer = 0\n",
    "                mode = 'key'\n",
    "\n",
    "    for key, value in zip(keys, values):\n",
    "        fields[key.lower().replace('\\n', '').replace(',', '').strip()] = value\n",
    "\n",
    "    if gen_id and 'author' in fields:\n",
    "        authors = fields['author'].split(' and ')\n",
    "        first_author = authors[0]\n",
    "        if ',' in first_author:\n",
    "            last_name, first_name = map(str.strip, first_author.split(','))\n",
    "        else:\n",
    "            name_parts = list(map(str.strip, first_author.split()))\n",
    "            last_name = name_parts.pop()\n",
    "            first_name = ' '.join(name_parts)\n",
    "\n",
    "        fields['id'] = re.sub(r'[^a-zA-Z0-9]', '', f\"{first_name}{last_name}{fields.get('year', '')}\") + id_surfix\n",
    "\n",
    "    if lower_case_type:\n",
    "        fields['type'] = fields['type'].lower()\n",
    "\n",
    "    return fields\n",
    "\n",
    "fields = parse_bitex(paper_dict['Hyperbolic Category Discovery']['bibtex'], gen_id=True, id_surfix='CVPR')\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b95cdf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@inproceedings{YuanpeiLiu2025CVPR,\n",
      "  author = {Liu, Yuanpei and He, Zhenqi and Han, Kai},\n",
      "  title = {Hyperbolic Category Discovery},\n",
      "  booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},\n",
      "  month = {June},\n",
      "  year = {2025},\n",
      "  pages = {9891-9900},\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_bibtex(fields):\n",
    "    # generate bibtex from a dict\n",
    "    bibtex = '@' + fields['type'] + '{' + fields['id'] + ',\\n'\n",
    "\n",
    "    for key, value in fields.items():\n",
    "        if key not in ['type', 'id']:\n",
    "            bibtex += '  ' + key + ' = ' + '{' + value + '},\\n'\n",
    "    bibtex += \"}\\n\"\n",
    "    return bibtex\n",
    "\n",
    "print(make_bibtex(fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a30d2a",
   "metadata": {},
   "source": [
    "### Oral\n",
    "\n",
    "_P.S. The unmatched papers need to be manually placed to the oral section from the poster section._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "516c641e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatched: Oral Session 2C: Temporal Modeling and Action Recognition -> Rethinking Spiking Self-Attention Mechanism: Implementing α-XNOR Similarity Calculation in Spiking Transformers'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "unmatched: Oral Session 4A: Image and Video Synthesis -> Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "unmatched: Oral Session 5C: Visual and Spatial Computing -> Gromov–Wasserstein Problem with Cyclic Symmetry'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bib = []\n",
    "mdbib = []\n",
    "bibkey = set()\n",
    "\n",
    "mdbib.append('## CVPR 2025 Oral\\n')\n",
    "\n",
    "bib.append('% ---------------------------')\n",
    "bib.append(f'% CVPR 2025 Oral')\n",
    "bib.append('% ---------------------------\\n')\n",
    "\n",
    "for session, papers in oral_dict.items():\n",
    "    mdbib.append(f\"### {session}\\n\")\n",
    "    mdbib.append(f'```bibtex')\n",
    "\n",
    "    for paper in papers:\n",
    "        try:\n",
    "            # get bibtex from paper_dict & mark publication type\n",
    "            bibtex = paper_dict[paper]['bibtex']\n",
    "            paper_dict[paper]['pub_type'] = 'oral'\n",
    "\n",
    "            # parse bibtex\n",
    "            fields = parse_bitex(bibtex, gen_id=True, id_surfix='CVPR')\n",
    "            fields['url'] = paper_dict[paper]['url']\n",
    "            fields['abstract'] = paper_dict[paper]['abstract']\n",
    "            \n",
    "            # resolve duplicated bibkey\n",
    "            while(fields['id'] in bibkey):\n",
    "                fields['id'] += '+'\n",
    "            bibkey.add(fields['id'])\n",
    "\n",
    "            # append to mdbib\n",
    "            mdbib.append(make_bibtex(fields))\n",
    "\n",
    "            # append to bib (w/o abstract)\n",
    "            bib.append(make_bibtex({\n",
    "                key: fields[key]\n",
    "                for key in fields.keys() if key != 'abstract'\n",
    "            }))\n",
    "            \n",
    "        except KeyError:\n",
    "            print(f\"unmatched: {session} -> {paper}'\")\n",
    "            print('-'*100)\n",
    "            continue\n",
    "\n",
    "    mdbib.append(f'```\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd6916",
   "metadata": {},
   "source": [
    "### Highlight\n",
    "\n",
    "_P.S. The unmatched papers need to be manually placed to the oral section from the poster section._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20bd2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unmatched: Doppelgängers and Adversarial Vulnerability'\n",
      "----------------------------------------------------------------------------------------------------\n",
      "unmatched: Estimating Body and Hand Motion in an Ego‑sensed World'\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mdbib.append('## CVPR 2025 Highlight\\n')\n",
    "\n",
    "bib.append('% ---------------------------')\n",
    "bib.append(f'% CVPR 2025 Highlight')\n",
    "bib.append('% ---------------------------\\n')\n",
    "\n",
    "for paper in highlight_ls:\n",
    "    try:\n",
    "        # get bibtex from paper_dict & mark publication type\n",
    "        bibtex = paper_dict[paper]['bibtex']\n",
    "        paper_dict[paper]['pub_type'] = 'highlight'\n",
    "        \n",
    "        # parse bibtex\n",
    "        fields = parse_bitex(bibtex, gen_id=True, id_surfix='CVPR')\n",
    "        fields['url'] = paper_dict[paper]['url']\n",
    "        fields['abstract'] = paper_dict[paper]['abstract']\n",
    "\n",
    "        # resolve duplicated bibkey\n",
    "        while(fields['id'] in bibkey):\n",
    "            fields['id'] += '+'\n",
    "        bibkey.add(fields['id'])\n",
    "\n",
    "        # append to mdbib\n",
    "        mdbib.append(f'```bibtex')\n",
    "        mdbib.append(make_bibtex(fields))\n",
    "        mdbib.append(f'```\\n')\n",
    "\n",
    "        # append to bib (w/o abstract)\n",
    "        bib.append(make_bibtex({\n",
    "            key: fields[key]\n",
    "            for key in fields.keys() if key != 'abstract'\n",
    "        }))\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"unmatched: {paper}'\")\n",
    "        print('-'*100)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85175351",
   "metadata": {},
   "source": [
    "### Poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7754151d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Less is More: Efficient Model Merging with Binary Task Switch'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aa6dda85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdbib.append('## CVPR 2025 Poster\\n')\n",
    "\n",
    "bib.append('% ---------------------------')\n",
    "bib.append(f'% CVPR 2025 Poster')\n",
    "bib.append('% ---------------------------\\n')\n",
    "\n",
    "for paper in paper_dict.values():\n",
    "    if paper['pub_type'] == 'poster':\n",
    "        # get bibtex from paper_dict\n",
    "        bibtex = paper['bibtex']\n",
    "        \n",
    "        # parse bibtex\n",
    "        fields = parse_bitex(bibtex, gen_id=True, id_surfix='CVPR')\n",
    "        fields['url'] = paper['url']\n",
    "        fields['abstract'] = paper['abstract']\n",
    "\n",
    "        # resolve duplicated bibkey\n",
    "        while(fields['id'] in bibkey):\n",
    "            fields['id'] += '+'\n",
    "        bibkey.add(fields['id'])\n",
    "\n",
    "        # append to mdbib\n",
    "        mdbib.append(f'```bibtex')\n",
    "        mdbib.append(make_bibtex(fields))\n",
    "        mdbib.append(f'```\\n')\n",
    "\n",
    "        # append to bib (w/o abstract)\n",
    "        bib.append(make_bibtex({\n",
    "            key: fields[key]\n",
    "            for key in fields.keys() if key != 'abstract'\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5d736",
   "metadata": {},
   "source": [
    "## Export to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d265d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'output/cvpr2025.md', 'w') as f:\n",
    "    f.write('\\n'.join(mdbib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5986841",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'output/cvpr2025.bib', 'w') as f:\n",
    "    f.write('\\n'.join(bib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c877f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top-conf-scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
