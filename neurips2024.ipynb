{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENUE_ID = 'NeurIPS.cc/2024/Conference'\n",
    "VENUE_LS = ['NeurIPS 2024 oral', 'NeurIPS 2024 spotlight', 'NeurIPS 2024 poster']\n",
    "CONFERENCE_NAME = 'NeurIPS'\n",
    "OUTPUT_PATH = 'output/NeurIPS 2024 Abstracts.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openreview\n",
    "\n",
    "load_dotenv('.env')\n",
    "client = openreview.api.OpenReviewClient(\n",
    "    baseurl='https://api2.openreview.net',\n",
    "    username=os.getenv(\"username\"),\n",
    "    password=os.getenv(\"password\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICLR.cc/2018/Conference',\n",
       " 'ICLR.cc/2019/Conference',\n",
       " 'ICLR.cc/2020/Conference',\n",
       " 'ICLR.cc/2021/Conference',\n",
       " 'ICLR.cc/2022/Conference',\n",
       " 'ICLR.cc/2023/Conference',\n",
       " 'ICLR.cc/2024/Conference',\n",
       " 'ICLR.cc/2025/Conference',\n",
       " 'ICML.cc/2020/Conference',\n",
       " 'ICML.cc/2023/Conference',\n",
       " 'ICML.cc/2024/Conference',\n",
       " 'ICML.cc/2025/Conference',\n",
       " 'NeurIPS.cc/2020/Conference',\n",
       " 'NeurIPS.cc/2021/Conference',\n",
       " 'NeurIPS.cc/2022/Conference',\n",
       " 'NeurIPS.cc/2023/Conference',\n",
       " 'NeurIPS.cc/2024/Conference',\n",
       " 'NeurIPS.cc/2025/Conference']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print matched venue ids\n",
    "top3 = ['ICML', 'ICLR', 'NeurIPS']\n",
    "venues = client.get_group(id='venues').members\n",
    "venue_ids = [ven for ven in venues if any(con in ven for con in top3) and 'Conference' in ven]\n",
    "venue_ids.sort()\n",
    "venue_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cdate': 1715802154333,\n",
      " 'content': {'_bibtex': {'value': '@inproceedings{\\n'\n",
      "                                  'he2024learning,\\n'\n",
      "                                  'title={Learning to grok: Emergence of '\n",
      "                                  'in-context learning and skill composition '\n",
      "                                  'in modular arithmetic tasks},\\n'\n",
      "                                  'author={Tianyu He and Darshil Doshi and '\n",
      "                                  'Aritra Das and Andrey Gromov},\\n'\n",
      "                                  'booktitle={The Thirty-eighth Annual '\n",
      "                                  'Conference on Neural Information Processing '\n",
      "                                  'Systems},\\n'\n",
      "                                  'year={2024},\\n'\n",
      "                                  'url={https://openreview.net/forum?id=aVh9KRZdRk}\\n'\n",
      "                                  '}'},\n",
      "             'abstract': {'value': 'Large language models can solve tasks that '\n",
      "                                   'were not present in the training set. This '\n",
      "                                   'capability is believed to be due to '\n",
      "                                   'in-context learning and skill composition. '\n",
      "                                   'In this work, we study the emergence of '\n",
      "                                   'in-context learning and skill composition '\n",
      "                                   'in a collection of modular arithmetic '\n",
      "                                   'tasks. Specifically, we consider a finite '\n",
      "                                   'collection of linear modular functions $z '\n",
      "                                   '= a  x + b  y \\\\text{ mod } p$ labeled by '\n",
      "                                   'the vector $(a, b) \\\\in \\\\mathbb{Z}_p^2$. '\n",
      "                                   'We use some of these tasks for '\n",
      "                                   'pre-training and the rest for '\n",
      "                                   'out-of-distribution testing. We '\n",
      "                                   'empirically show that a GPT-style '\n",
      "                                   'transformer exhibits a transition from '\n",
      "                                   'in-distribution to out-of-distribution '\n",
      "                                   'generalization as the number of '\n",
      "                                   'pre-training tasks increases. We find that '\n",
      "                                   'the smallest model capable of '\n",
      "                                   'out-of-distribution generalization '\n",
      "                                   'requires two transformer blocks, while for '\n",
      "                                   'deeper models, the out-of-distribution '\n",
      "                                   'generalization phase is *transient*, '\n",
      "                                   'necessitating early stopping. Finally, we '\n",
      "                                   'perform an interpretability study of the '\n",
      "                                   'pre-trained models, revealing highly '\n",
      "                                   'structured representations in both '\n",
      "                                   'attention heads and MLPs; and discuss the '\n",
      "                                   'learned algorithms. Notably, we find an '\n",
      "                                   'algorithmic shift in deeper models, as we '\n",
      "                                   'go from few to many in-context examples.'},\n",
      "             'authorids': {'value': ['~Tianyu_He2',\n",
      "                                     '~Darshil_Doshi1',\n",
      "                                     '~Aritra_Das1',\n",
      "                                     '~Andrey_Gromov1']},\n",
      "             'authors': {'value': ['Tianyu He',\n",
      "                                   'Darshil Doshi',\n",
      "                                   'Aritra Das',\n",
      "                                   'Andrey Gromov']},\n",
      "             'keywords': {'value': ['In-Context Learning',\n",
      "                                    'Grokking',\n",
      "                                    'Modular Arithmetic',\n",
      "                                    'Interpretability']},\n",
      "             'paperhash': {'value': 'he|learning_to_grok_emergence_of_incontext_learning_and_skill_composition_in_modular_arithmetic_tasks'},\n",
      "             'pdf': {'value': '/pdf/5737b58d308dafc16130635934df4276a7a574aa.pdf'},\n",
      "             'primary_area': {'value': 'interpretability_and_explainability'},\n",
      "             'supplementary_material': {'value': '/attachment/c28c40f21e731efeeeaac191191ae5a515ba2185.zip'},\n",
      "             'title': {'value': 'Learning to grok: Emergence of in-context '\n",
      "                                'learning and skill composition in modular '\n",
      "                                'arithmetic tasks'},\n",
      "             'venue': {'value': 'NeurIPS 2024 oral'},\n",
      "             'venueid': {'value': 'NeurIPS.cc/2024/Conference'}},\n",
      " 'ddate': None,\n",
      " 'details': {'writable': False},\n",
      " 'domain': 'NeurIPS.cc/2024/Conference',\n",
      " 'forum': 'aVh9KRZdRk',\n",
      " 'id': 'aVh9KRZdRk',\n",
      " 'invitations': ['NeurIPS.cc/2024/Conference/-/Submission',\n",
      "                 'NeurIPS.cc/2024/Conference/-/Post_Submission',\n",
      "                 'NeurIPS.cc/2024/Conference/Submission21497/-/Revision',\n",
      "                 'NeurIPS.cc/2024/Conference/-/Edit',\n",
      "                 'NeurIPS.cc/2024/Conference/Submission21497/-/Camera_Ready_Revision'],\n",
      " 'license': 'CC BY 4.0',\n",
      " 'mdate': 1730874005890,\n",
      " 'nonreaders': None,\n",
      " 'number': 21497,\n",
      " 'odate': 1730874005877,\n",
      " 'pdate': 1727288256361,\n",
      " 'readers': ['everyone'],\n",
      " 'replyto': None,\n",
      " 'signatures': ['NeurIPS.cc/2024/Conference/Submission21497/Authors'],\n",
      " 'tcdate': 1715802154333,\n",
      " 'tmdate': 1730874005890,\n",
      " 'writers': ['NeurIPS.cc/2024/Conference',\n",
      "             'NeurIPS.cc/2024/Conference/Submission21497/Authors']}\n"
     ]
    }
   ],
   "source": [
    "# print example paper note to examine the structure of content\n",
    "# p.s. the pattern will be used for client.get_notes() to match all papers\n",
    "sample_paper = client.get_notes(id='aVh9KRZdRk')[0]\n",
    "print(sample_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting V2 Notes: 100%|█████████▉| 4030/4035 [00:03<00:00, 1298.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Note(id = 'zzOOqD6R1b',number = 6188,cdate = 1715586496661,pdate = 1727287807547,odate = 1730873891234,mdate = 1730873891250,tcdate = 1715586496661,tmdate = 1730873891250,ddate = None,content = {'title': {'value': 'Stress-Testing Capability Elicitation With Password-Locked Models'}, 'authors': {'value': ['Ryan Greenblatt', 'Fabien Roger', 'Dmitrii Krasheninnikov', 'David Krueger']}, 'authorids': {'value': ['~Ryan_Greenblatt1', '~Fabien_Roger1', '~Dmitrii_Krasheninnikov1', '~David_Krueger1']}, 'keywords': {'value': ['LLMs', 'Elicitation', 'Fine-tuning', 'Sandbagging', 'Red-teaming', 'Safety']}, 'TLDR': {'value': 'We train models to behave poorly except when the prompt contains a password, and study when supervised fine-tuning and RL can recover high performance.'}, 'abstract': {'value': 'To determine the safety of large language models (LLMs), AI developers must be able to assess their dangerous capabilities. But simple prompting strategies often fail to elicit an LLM’s full capabilities. One way to elicit capabilities more robustly is to fine-tune the LLM to complete the task. In this paper, we investigate the conditions under which fine-tuning-based elicitation suffices to elicit capabilities. To do this, we introduce password-locked models, LLMs fine-tuned such that some of their capabilities are deliberately hidden. Specifically, these LLMs are trained to exhibit these capabilities only when a password is present in the prompt, and to imitate a much weaker LLM otherwise. Password-locked models enable a novel method of evaluating capabilities elicitation methods, by testing whether these password-locked capabilities can be elicited without using the password. We find that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. More surprisingly, fine-tuning can elicit other capabilities that have been locked using the same password, or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. Overall, our findings suggest that fine-tuning is an effective method of eliciting hidden capabilities of current models but may be unreliable when high-quality demonstrations are not available, e.g., as may be the case when models’ (hidden) capabilities exceed those of human demonstrators.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/060fc5a68cf9e8cd99067fa71d86b9b2407c68af.pdf'}, 'supplementary_material': {'value': '/attachment/da6619534d79b7246746540de2464bd411225e1e.zip'}, '_bibtex': {'value': '@inproceedings{\\ngreenblatt2024stresstesting,\\ntitle={Stress-Testing Capability Elicitation With Password-Locked Models},\\nauthor={Ryan Greenblatt and Fabien Roger and Dmitrii Krasheninnikov and David Krueger},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zzOOqD6R1b}\\n}'}, 'paperhash': {'value': 'greenblatt|stresstesting_capability_elicitation_with_passwordlocked_models'}},forum = 'zzOOqD6R1b',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6188/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6188/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6188/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6188/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zxSWIdyW3A',number = 2371,cdate = 1715053555126,pdate = 1727287688506,odate = 1730873857543,mdate = 1730873857562,tcdate = 1715053555126,tmdate = 1730873857562,ddate = None,content = {'title': {'value': 'Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging'}, 'authors': {'value': ['Jiamian Wang', 'Zongliang Wu', 'Yulun Zhang', 'Xin Yuan', 'Tao Lin', 'ZHIQIANG TAO']}, 'authorids': {'value': ['~Jiamian_Wang1', '~Zongliang_Wu1', '~Yulun_Zhang1', '~Xin_Yuan4', '~Tao_Lin1', '~ZHIQIANG_TAO2']}, 'keywords': {'value': ['snapshot compressive imaging', 'hyperpectral imaging', 'prompt learning', 'federated learning']}, 'abstract': {'value': 'Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their perfor- mance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9784818faf1b61e993e8c55556f64ad6c612ecad.pdf'}, 'supplementary_material': {'value': '/attachment/795e5ddcb1cd4711f49418eded4f618ffe25e568.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024cooperative,\\ntitle={Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging},\\nauthor={Jiamian Wang and Zongliang Wu and Yulun Zhang and Xin Yuan and Tao Lin and ZHIQIANG TAO},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zxSWIdyW3A}\\n}'}, 'paperhash': {'value': 'wang|cooperative_hardwareprompt_learning_for_snapshot_compressive_imaging'}},forum = 'zxSWIdyW3A',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2371/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2371/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2371/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2371/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zw2K6LfFI9',number = 274,cdate = 1713859057925,pdate = 1727287634209,odate = 1730873839667,mdate = 1730873839685,tcdate = 1713859057925,tmdate = 1730873839685,ddate = None,content = {'title': {'value': 'PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation'}, 'authors': {'value': ['Fei Ni', 'Jianye HAO', 'Shiguang Wu', 'Longxin Kou', 'Yifu Yuan', 'Zibin Dong', 'Jinyi Liu', 'MingZhi Li', 'Yuzheng Zhuang', 'YAN ZHENG']}, 'authorids': {'value': ['~Fei_Ni1', '~Jianye_HAO1', '~Shiguang_Wu4', '~Longxin_Kou1', '~Yifu_Yuan1', '~Zibin_Dong1', '~Jinyi_Liu1', '~MingZhi_Li1', '~Yuzheng_Zhuang1', '~YAN_ZHENG1']}, 'keywords': {'value': ['robotics manipulation', 'multi-modal LLM', 'image generation', 'instruction following']}, 'TLDR': {'value': 'We propose PERIA, a novel framework that leverages a MLLM to enable effective task decomposition via holistic language planning and vision planning for robotic manipulation in embodied scenarios.'}, 'abstract': {'value': 'Long-horizon manipulation tasks with general instructions often implicitly encapsulate multiple sub-tasks, posing significant challenges in instruction following.\\nWhile language planning is a common approach to decompose general instructions into stepwise sub-instructions, text-only guidance may lack expressiveness and lead to potential ambiguity. Considering that humans often imagine and visualize sub-instructions reasoning out before acting, the imagined subgoal images can provide more intuitive guidance and enhance the reliability of decomposition. Inspired by this, we propose **PERIA**(**PE**rceive, **R**eason, **I**magine, **A**ct), a novel framework that integrates holistic language planning and vision planning for long-horizon manipulation tasks with complex instructions, leveraging both logical and intuitive aspects of task decomposition.\\nSpecifically, we first perform a lightweight multimodal alignment on the encoding side to empower the MLLM to perceive visual details and language instructions. \\nThe MLLM is then jointly instruction-tuned with a pretrained image-editing model to unlock capabilities of simultaneous reasoning of language instructions and generation of imagined subgoals. Furthermore, we introduce a consistency alignment loss to encourage coherent subgoal images and align with their corresponding instructions, mitigating potential hallucinations and semantic conflicts between the two planning manners.\\nComprehensive evaluations across three task domains demonstrate that PERIA, benefiting from holistic language and vision planning, significantly outperforms competitive baselines in both instruction following accuracy and task success rate on complex manipulation tasks.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2a39fcbdd8617cd0a7fbe9312a20b9b51ea8ab74.pdf'}, '_bibtex': {'value': '@inproceedings{\\nni2024peria,\\ntitle={{PERIA}: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation},\\nauthor={Fei Ni and Jianye HAO and Shiguang Wu and Longxin Kou and Yifu Yuan and Zibin Dong and Jinyi Liu and MingZhi Li and Yuzheng Zhuang and YAN ZHENG},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zw2K6LfFI9}\\n}'}, 'paperhash': {'value': 'ni|peria_perceive_reason_imagine_act_via_holistic_language_and_vision_planning_for_manipulation'}},forum = 'zw2K6LfFI9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission274/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission274/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission274/-/Revision', 'NeurIPS.cc/2024/Conference/Submission274/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zv9gYC3xgF',number = 15161,cdate = 1715757734486,pdate = 1727288089828,odate = 1730873970975,mdate = 1730873970994,tcdate = 1715757734486,tmdate = 1730873970994,ddate = None,content = {'title': {'value': 'Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models'}, 'authors': {'value': ['Weihang Xu', 'Maryam Fazel', 'Simon Shaolei Du']}, 'authorids': {'value': ['~Weihang_Xu2', '~Maryam_Fazel1', '~Simon_Shaolei_Du1']}, 'keywords': {'value': ['over-parameterization', 'global convergence', 'non-convex optimization']}, 'abstract': {'value': 'We study the gradient Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM) in the over-parameterized setting, where a general GMM with $n>1$ components learns from data that are generated by a single ground truth Gaussian distribution. \\nWhile results for the special case of 2-Gaussian mixtures are well-known, a general global convergence analysis for arbitrary $n$ remains unresolved and faces several new technical barriers since the convergence becomes sub-linear and non-monotonic. To address these challenges, we construct a novel likelihood-based convergence analysis framework and rigorously prove that gradient EM converges globally with a sublinear rate $O(1/\\\\sqrt{t})$. This is the first global convergence result for Gaussian mixtures with more than $2$ components. The sublinear convergence rate is due to the algorithmic nature of learning over-parameterized GMM with gradient EM. We also identify a new emerging technical challenge for learning general over-parameterized GMM: the existence of bad local regions that can trap gradient EM for an exponential number of steps.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/01089f1b9d7a3757d7fe8abda681870c3db968be.pdf'}, 'TLDR': {'value': 'We give the first global convergence of gradient EM for over-parameterized Gaussian mixture models when the ground truth is single Gaussian.'}, '_bibtex': {'value': '@inproceedings{\\nxu2024toward,\\ntitle={Toward Global Convergence of Gradient {EM} for Over-Paramterized Gaussian Mixture Models},\\nauthor={Weihang Xu and Maryam Fazel and Simon Shaolei Du},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zv9gYC3xgF}\\n}'}, 'paperhash': {'value': 'xu|toward_global_convergence_of_gradient_em_for_overparamterized_gaussian_mixture_models'}},forum = 'zv9gYC3xgF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15161/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15161/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15161/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15161/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zv4UISZzp5',number = 9701,cdate = 1715684316577,pdate = 1727287918538,odate = 1730873922687,mdate = 1730873922706,tcdate = 1715684316577,tmdate = 1730873922706,ddate = None,content = {'title': {'value': 'IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation'}, 'authors': {'value': ['Fan Lin', 'Shuyi Xie', 'Yong Dai', 'Wenlin Yao', 'TianJiao Lang', 'Yu Zhang']}, 'authorids': {'value': ['~Fan_Lin2', '~Shuyi_Xie1', '~Yong_Dai1', '~Wenlin_Yao1', '~TianJiao_Lang1', '~Yu_Zhang21']}, 'keywords': {'value': ['LLM', 'Data Generalization', 'Discrimination Indexes']}, 'abstract': {'value': 'As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities. \\nOur data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains.\\nTo produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.\\nThe results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works.\\nWe will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/74ed0078ffe00fb63ba32cc447f4540054349fbb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024idgen,\\ntitle={{IDG}en: Item Discrimination Induced Prompt Generation for {LLM} Evaluation},\\nauthor={Fan Lin and Shuyi Xie and Yong Dai and Wenlin Yao and TianJiao Lang and Yu Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zv4UISZzp5}\\n}'}, 'paperhash': {'value': 'lin|idgen_item_discrimination_induced_prompt_generation_for_llm_evaluation'}},forum = 'zv4UISZzp5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9701/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9701/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9701/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9701/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'zuwpeRkJNH',number = 5623,cdate = 1715557934793,pdate = 1727287790116,odate = 1730873886628,mdate = 1730873886652,tcdate = 1715557934793,tmdate = 1730873886652,ddate = None,content = {'title': {'value': 'Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation'}, 'authors': {'value': ['Kun yuan', 'Vinkle Srivastav', 'Nassir Navab', 'Nicolas Padoy']}, 'authorids': {'value': ['~Kun_yuan6', '~Vinkle_Srivastav1', '~Nassir_Navab1', '~Nicolas_Padoy1']}, 'keywords': {'value': ['Surgical Data Science', 'Video-language Pretraining', 'Multi-modal', 'Surgical Foundation Model']}, 'abstract': {'value': 'Surgical video-language pretraining (VLP) faces unique challenges due to the knowledge domain gap and the scarcity of multi-modal data. This study aims to bridge the gap by addressing issues regarding textual information loss in surgical lecture videos and the spatial-temporal challenges of surgical VLP. To tackle these issues, we propose a hierarchical knowledge augmentation approach and a novel Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework. The proposed knowledge augmentation approach uses large language models (LLM) to refine and enrich surgical concepts, thus providing comprehensive language supervision and reducing the risk of overfitting. The PeskaVLP framework combines language supervision with visual self-supervision, constructing hard negative samples and employing a Dynamic Time Warping (DTW) based loss function to effectively comprehend the cross-modal procedural alignment. Extensive experiments on multiple public surgical scene understanding and cross-modal retrieval datasets show that our proposed method significantly improves zero-shot transferring performance and offers a generalist visual repre- sentation for further advancements in surgical scene understanding. The source code will be available at https://github.com/CAMMA-public/PeskaVLP.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b754552d7cad51cf70357809a56df08d88257ab9.pdf'}, 'supplementary_material': {'value': '/attachment/1231365d3818f4ad2ba8ea7068f8ec75882da969.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nyuan2024procedureaware,\\ntitle={Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation},\\nauthor={Kun yuan and Vinkle Srivastav and Nassir Navab and Nicolas Padoy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zuwpeRkJNH}\\n}'}, 'TLDR': {'value': 'Surgical Vision-language Foundation Model (CLIP-like)'}, 'paperhash': {'value': 'yuan|procedureaware_surgical_videolanguage_pretraining_with_hierarchical_knowledge_augmentation'}},forum = 'zuwpeRkJNH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5623/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5623/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5623/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission5623/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zuwLGhgxtQ',number = 11058,cdate = 1715700535402,pdate = 1727287959613,odate = 1730873934833,mdate = 1730873934854,tcdate = 1715700535402,tmdate = 1730873934854,ddate = None,content = {'title': {'value': 'A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers'}, 'authors': {'value': ['Ye He', 'Alireza Mousavi-Hosseini', 'Krishna Balasubramanian', 'Murat A Erdogdu']}, 'authorids': {'value': ['~Ye_He1', '~Alireza_Mousavi-Hosseini1', '~Krishna_Balasubramanian1', '~Murat_A_Erdogdu1']}, 'keywords': {'value': ['Proximal samplers', 'Complexity of heavy-tailed sampling', 'Restricted Gaussian oracle', 'Restricted Stable oracle']}, 'TLDR': {'value': 'Gaussian-based proximal samplers face accuracy limits sampling heavy-tailed targets. Stable-based samplers offer high-accuracy guarantees, surpassing this constraint.'}, 'abstract': {'value': 'We study the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees i.e., samplers that require only $\\\\mathcal{O}(\\\\log(1/\\\\varepsilon))$ versus $\\\\Omega(\\\\text{poly}(1/\\\\varepsilon))$ iterations to output a sample which is $\\\\varepsilon$-close to the target in $\\\\chi^2$-divergence. Our results are presented for proximal samplers that are based on Gaussian versus stable oracles. We show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. We also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bd86dfe1f5fac662f55df1bccfbb1134cf9043ed.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhe2024a,\\ntitle={A Separation in Heavy-Tailed Sampling: Gaussian vs. Stable Oracles for Proximal Samplers},\\nauthor={Ye He and Alireza Mousavi-Hosseini and Krishna Balasubramanian and Murat A Erdogdu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zuwLGhgxtQ}\\n}'}, 'paperhash': {'value': 'he|a_separation_in_heavytailed_sampling_gaussian_vs_stable_oracles_for_proximal_samplers'}},forum = 'zuwLGhgxtQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11058/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11058/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11058/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11058/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ztwl4ubnXV',number = 1456,cdate = 1714641795695,pdate = 1727287661531,odate = 1730873848966,mdate = 1736949645182,tcdate = 1714641795695,tmdate = 1736949645182,ddate = None,content = {'title': {'value': 'OxonFair: A Flexible Toolkit for Algorithmic Fairness'}, 'authors': {'value': ['Eoin D. Delaney', 'Zihao Fu', 'Sandra Wachter', 'Brent Mittelstadt', 'Chris Russell']}, 'authorids': {'value': ['~Eoin_D._Delaney1', '~Zihao_Fu1', '~Sandra_Wachter1', '~Brent_Mittelstadt1', '~Chris_Russell3']}, 'keywords': {'value': ['Fairness Toolkit', 'Algorithmic Fairness', 'Trustworthy AI']}, 'TLDR': {'value': 'We present a new toolkit for enforcing and measuring fairness with a focus on deep learning models.'}, 'abstract': {'value': 'We present OxonFair, a new open source toolkit for enforcing fairness in binary classification. Compared to existing toolkits: (i) We support NLP and Computer Vision classification as well as standard tabular problems. (ii) We support enforcing fairness on validation data, making us robust to a wide range of overfitting challenges. (iii) Our approach can optimize any measure based on True Positives, False Positive, False Negatives, and True Negatives. This makes it easily extensible and much more expressive than existing toolkits. It supports all 9 and all 10 of the decision-based group metrics of two popular review articles. (iv) We jointly optimize a performance objective alongside fairness constraints. This minimizes degradation while enforcing fairness, and even improves the performance of inadequately tuned unfair baselines. OxonFair is compatible with standard ML toolkits, including sklearn, Autogluon, and PyTorch and is available  at https://github.com/oxfordinternetinstitute/oxonfair.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1198c251b0f5664b73f1ec30b356982f81f81fc7.pdf'}, 'supplementary_material': {'value': '/attachment/f13b1845f29e53481fdce123227ac8b54be53db5.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ndelaney2024oxonfair,\\ntitle={OxonFair: A Flexible Toolkit for Algorithmic Fairness},\\nauthor={Eoin D. Delaney and Zihao Fu and Sandra Wachter and Brent Mittelstadt and Chris Russell},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ztwl4ubnXV}\\n}'}, 'paperhash': {'value': 'delaney|oxonfair_a_flexible_toolkit_for_algorithmic_fairness'}},forum = 'ztwl4ubnXV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1456/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1456/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1456/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission1456/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zsXbGJJ7Oo',number = 3476,cdate = 1715288417063,pdate = 1727287722008,odate = 1730873866631,mdate = 1731752302481,tcdate = 1715288417063,tmdate = 1731752302481,ddate = None,content = {'title': {'value': 'G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training'}, 'authors': {'value': ['Che Liu', 'Cheng Ouyang', 'Sibo Cheng', 'Anand Shah', 'Wenjia Bai', 'Rossella Arcucci']}, 'authorids': {'value': ['~Che_Liu3', '~Cheng_Ouyang2', '~Sibo_Cheng1', '~Anand_Shah1', '~Wenjia_Bai1', '~Rossella_Arcucci1']}, 'keywords': {'value': ['Medical Vision-Language Pre-training', 'dense visual learning']}, 'abstract': {'value': 'Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radiology reports, current medical VLP methods primarily focus on aligning images with entire reports. This focus hinders the learning of dense (pixel-level) visual features and is suboptimal for dense prediction tasks (e.g., medical image segmentation).\\n\\nTo address this challenge, we propose a novel medical VLP framework, named **Global to Dense level representation learning (G2D)**, which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations. In particular, G2D designs a **Pseudo Segmentation (PS)** task, which enables the model to learn dense visual features during VLP. Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters. With this simple yet effective idea, G2D achieves superior performance across 5 medical imaging tasks and 25 diseases. Particularly, in the segmentation task which requires dense visual features, **G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models**. The code can be found in https://github.com/cheliu-computation/G2D-NeurIPS24/tree/main.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose G2D, a novel medical VLP framework that learns global and dense visual features solely from image-text pairs, outperforming existing methods even with only 1% of the training data compared to their use of 100%.'}, 'pdf': {'value': '/pdf/266314e449f23eb30c332e9f0688da33556f643c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024gd,\\ntitle={G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training},\\nauthor={Che Liu and Cheng Ouyang and Sibo Cheng and Anand Shah and Wenjia Bai and Rossella Arcucci},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zsXbGJJ7Oo}\\n}'}, 'paperhash': {'value': 'liu|g2d_from_global_to_dense_radiography_representation_learning_via_visionlanguage_pretraining'}},forum = 'zsXbGJJ7Oo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3476/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3476/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3476/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3476/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zqLAMwVLkt',number = 10275,cdate = 1715692491986,pdate = 1727287934674,odate = 1730873927593,mdate = 1734567880570,tcdate = 1715692491986,tmdate = 1734567880570,ddate = None,content = {'title': {'value': 'Generative Semi-supervised Graph Anomaly Detection'}, 'authors': {'value': ['Hezhe Qiao', 'Qingsong Wen', 'Xiaoli Li', 'Ee-Peng Lim', 'Guansong Pang']}, 'authorids': {'value': ['~Hezhe_Qiao1', '~Qingsong_Wen2', '~Xiaoli_Li1', '~Ee-Peng_Lim1', '~Guansong_Pang1']}, 'keywords': {'value': ['Anomaly Detection', 'Graph Neural Network', 'Graph Anomaly Detection']}, 'TLDR': {'value': 'We propose a novel Generative Graph Anomaly Detection approach (GGAD) for an under-explored semi-supervised setting that has only labeled normal nodes and establish an evaluation benchmark for the setting.'}, 'abstract': {'value': \"This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the extensively explored unsupervised setting with a fully unlabeled graph. We reveal that having access to the normal nodes, even just a small percentage of normal nodes, helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (namely GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate pseudo anomaly nodes, referred to as 'outlier nodes', for providing effective negative node samples in training a discriminative one-class classifier. The main challenge here lies in the lack of ground truth information about real anomaly nodes. To address this challenge, GGAD is designed to leverage two important priors about the anomaly nodes -- asymmetric local affinity and egocentric closeness -- to generate reliable outlier nodes that assimilate anomaly nodes in both graph structure and feature representations. Comprehensive experiments on six real-world GAD datasets are performed to establish a benchmark for semi-supervised GAD and show that GGAD substantially outperforms state-of-the-art unsupervised and semi-supervised GAD methods with varying numbers of training normal nodes.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3c33b4f4c3c23708a8d12f3c6cbda3a20a9ca71e.pdf'}, 'supplementary_material': {'value': '/attachment/909949c36e711c24253839184c0f149002910c81.zip'}, '_bibtex': {'value': '@inproceedings{\\nqiao2024generative,\\ntitle={Generative Semi-supervised Graph Anomaly Detection},\\nauthor={Hezhe Qiao and Qingsong Wen and Xiaoli Li and Ee-Peng Lim and Guansong Pang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zqLAMwVLkt}\\n}'}, 'paperhash': {'value': 'qiao|generative_semisupervised_graph_anomaly_detection'}},forum = 'zqLAMwVLkt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10275/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10275/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10275/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10275/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zpw6NmhvKU',number = 14580,cdate = 1715751418510,pdate = 1727288073625,odate = 1730873966936,mdate = 1730873966957,tcdate = 1715751418510,tmdate = 1730873966957,ddate = None,content = {'title': {'value': 'RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting'}, 'authors': {'value': ['Hsiang Hsu', 'Ivan Brugere', 'Shubham Sharma', 'Freddy Lecue', 'Chun-Fu Chen']}, 'authorids': {'value': ['~Hsiang_Hsu1', '~Ivan_Brugere1', '~Shubham_Sharma4', '~Freddy_Lecue1', '~Chun-Fu_Chen1']}, 'keywords': {'value': ['Rashomon effects; predictive multiplicity; gradient boosting; tabular data']}, 'abstract': {'value': 'The Rashomon effect is a mixed blessing in responsible machine learning. It enhances the prospects of finding models that perform well in accuracy while adhering to ethical standards, such as fairness or interpretability. Conversely, it poses a risk to the credibility of machine decisions through predictive multiplicity. While recent studies have explored the Rashomon effect across various machine learning algorithms, its impact on gradient boosting---an algorithm widely applied to tabular datasets---remains unclear. This paper addresses this gap by systematically analyzing the Rashomon effect and predictive multiplicity in gradient boosting algorithms. We provide rigorous theoretical derivations to examine the Rashomon effect in the context of gradient boosting and offer an information-theoretic characterization of the Rashomon set. Additionally, we introduce a novel inference technique called RashomonGB to efficiently inspect the Rashomon effect in practice. On more than 20 datasets, our empirical results show that RashomonGB outperforms existing baselines in terms of improving the estimation of predictive multiplicity metrics and model selection with group fairness constraints. Lastly, we propose a framework to mitigate predictive multiplicity in gradient boosting and empirically demonstrate its effectiveness.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Theoretical and empirical analysis on the Rashomon effect and predictive multiplicity of gradient boosting, with a new methodology to audit and reduce predictive multiplicity.'}, 'pdf': {'value': '/pdf/838fbeed0eab05add105305af9fefdf722fe747f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhsu2024rashomongb,\\ntitle={Rashomon{GB}: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting},\\nauthor={Hsiang Hsu and Ivan Brugere and Shubham Sharma and Freddy Lecue and Chun-Fu Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zpw6NmhvKU}\\n}'}, 'paperhash': {'value': 'hsu|rashomongb_analyzing_the_rashomon_effect_and_mitigating_predictive_multiplicity_in_gradient_boosting'}},forum = 'zpw6NmhvKU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14580/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14580/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14580/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14580/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zn6s6VQYb0',number = 20228,cdate = 1715795208859,pdate = 1727288228421,odate = 1730873999950,mdate = 1735007008278,tcdate = 1715795208859,tmdate = 1735007008278,ddate = None,content = {'title': {'value': 'GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction'}, 'authors': {'value': ['Shijin Duan', 'Ruyi Ding', 'Jiaxing He', 'Aidong Adam Ding', 'Yunsi Fei', 'Xiaolin Xu']}, 'authorids': {'value': ['~Shijin_Duan1', '~Ruyi_Ding1', '~Jiaxing_He1', '~Aidong_Adam_Ding1', '~Yunsi_Fei1', '~Xiaolin_Xu3']}, 'keywords': {'value': ['Graph Neural Network', 'Graph Representation', 'Auto-Encoder']}, 'abstract': {'value': 'Graph-structured data is integral to many applications, prompting the development of various graph representation methods. Graph autoencoders (GAEs), in particular, reconstruct graph structures from node embeddings. Current GAE models primarily utilize self-correlation to represent graph structures and focus on node-level tasks, often overlooking multi-graph scenarios. Our theoretical analysis indicates that self-correlation generally falls short in accurately representing specific graph features such as islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts.To address these limitations, we introduce a cross-correlation mechanism that significantly enhances the GAE representational capabilities. Additionally, we propose the GraphCroc, a new GAE that supports flexible encoder architectures tailored for various downstream tasks and ensures robust structural reconstruction, through a mirrored encoding-decoding process. This model also tackles the challenge of representation bias during optimization by implementing a loss-balancing strategy. Both theoretical analysis and numerical evaluations demonstrate that our methodology significantly outperforms existing self-correlation-based GAEs in graph structure reconstruction.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/57096dd4679d0699198e3899786b24845b43c7a8.pdf'}, 'supplementary_material': {'value': '/attachment/11730d9a19e1603c2107c5a477f06920f0827738.zip'}, '_bibtex': {'value': '@inproceedings{\\nduan2024graphcroc,\\ntitle={GraphCroc: Cross-Correlation Autoencoder for Graph Structural Reconstruction},\\nauthor={Shijin Duan and Ruyi Ding and Jiaxing He and Aidong Adam Ding and Yunsi Fei and Xiaolin Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zn6s6VQYb0}\\n}'}, 'paperhash': {'value': 'duan|graphcroc_crosscorrelation_autoencoder_for_graph_structural_reconstruction'}},forum = 'zn6s6VQYb0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20228/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20228/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20228/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20228/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zm1LcgRpHm',number = 7262,cdate = 1715616479373,pdate = 1727287841046,odate = 1730873901010,mdate = 1730873901029,tcdate = 1715616479373,tmdate = 1730873901029,ddate = None,content = {'title': {'value': 'Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations'}, 'authors': {'value': ['Shivam Grover', 'Amin Jalali', 'Ali Etemad']}, 'authorids': {'value': ['~Shivam_Grover1', '~Amin_Jalali3', '~Ali_Etemad1']}, 'keywords': {'value': ['Time Series', 'Deep Learning', 'Representation Learning', 'Temporal Mechanism']}, 'TLDR': {'value': 'This paper presents S3, a modular neural network layer that is designed to enhance time-series representation learning by rearranging its segments, yielding improved results across various tasks with minimal computational overhead.'}, 'abstract': {'value': 'Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: Is there an alternative arrangement for time-series which could enable more effective representation learning? To address this, we propose a simple plug-and-play neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68\\\\%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d5ba68bdf83d04632580f0b9e7ac80199a8c19c5.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngrover2024segment,\\ntitle={Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations},\\nauthor={Shivam Grover and Amin Jalali and Ali Etemad},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zm1LcgRpHm}\\n}'}, 'paperhash': {'value': 'grover|segment_shuffle_and_stitch_a_simple_layer_for_improving_timeseries_representations'}},forum = 'zm1LcgRpHm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7262/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7262/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7262/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7262/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zlgfRk2CQa',number = 19341,cdate = 1715790419127,pdate = 1727288207227,odate = 1730873995056,mdate = 1730873995074,tcdate = 1715790419127,tmdate = 1730873995074,ddate = None,content = {'title': {'value': 'Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints'}, 'authors': {'value': ['Jay Bear', 'Adam Prugel-Bennett', 'Jonathon Hare']}, 'authorids': {'value': ['~Jay_Bear1', '~Adam_Prugel-Bennett1', '~Jonathon_Hare1']}, 'keywords': {'value': ['machine learning', 'iterative algorithms', 'deep thinking', 'lipschitz', 'traveling salesperson problem', 'contraction mapping']}, 'TLDR': {'value': 'Training machines to learn algorithms that are guaranteed to converge to a solution'}, 'abstract': {'value': 'Iterative algorithms solve problems by taking steps until a solution is reached. Models in the form of Deep Thinking (DT) networks have been demonstrated to learn iterative algorithms in a way that can scale to different sized problems at inference time using recurrent computation and convolutions. However, they are often unstable during training, and have no guarantees of convergence/termination at the solution. This paper addresses the problem of instability by analyzing the growth in intermediate representations, allowing us to build models (referred to as Deep Thinking with Lipschitz Constraints (DT-L)) with many fewer parameters and providing more reliable solutions. Additionally our DT-L formulation provides guarantees of convergence of the learned iterative procedure to a unique solution at inference time. We demonstrate DT-L is capable of robustly learning algorithms which extrapolate to harder problems than in the training set. We benchmark on the traveling salesperson problem to evaluate the capabilities of the modified system in an NP-hard problem where DT fails to learn.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0735617b982a5aca1dad5a07d887a2347d77d249.pdf'}, 'supplementary_material': {'value': '/attachment/1f64b053929cd11eefbd93bd1f5868d37f18f7bb.zip'}, '_bibtex': {'value': '@inproceedings{\\nbear2024rethinking,\\ntitle={Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints},\\nauthor={Jay Bear and Adam Prugel-Bennett and Jonathon Hare},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zlgfRk2CQa}\\n}'}, 'paperhash': {'value': 'bear|rethinking_deep_thinking_stable_learning_of_algorithms_using_lipschitz_constraints'}},forum = 'zlgfRk2CQa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19341/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19341/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19341/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19341/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zkhyrxlwqH',number = 6345,cdate = 1715590608043,pdate = 1727287811776,odate = 1730873892654,mdate = 1730873892674,tcdate = 1715590608043,tmdate = 1730873892674,ddate = None,content = {'title': {'value': 'Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization'}, 'authors': {'value': ['Sanghyeob Song', 'Jaihyun Lew', 'Hyemi Jang', 'Sungroh Yoon']}, 'authorids': {'value': ['~Sanghyeob_Song1', '~Jaihyun_Lew1', '~Hyemi_Jang1', '~Sungroh_Yoon1']}, 'keywords': {'value': ['Homography Estimation', 'Unsupervised Learning', 'Cross Domain', 'Image Registration', 'Image Alignment', 'Multimodal', 'Alternating Optimization']}, 'abstract': {'value': 'Estimating the homography between two images is crucial for mid- or high-level vision tasks, such as image stitching and fusion. However, using supervised learning methods is often challenging or costly due to the difficulty of collecting ground-truth data. In response, unsupervised learning approaches have emerged. Most early methods, though, assume that the given image pairs are from the same camera or have minor lighting differences. Consequently, while these methods perform effectively under such conditions, they generally fail when input image pairs come from different domains, referred to as multimodal image pairs.\\nTo address these limitations, we propose AltO, an unsupervised learning framework for estimating homography in multimodal image pairs. Our method employs a two-phase alternating optimization framework, similar to Expectation-Maximization (EM), where one phase reduces the geometry gap and the other addresses the modality gap. To handle these gaps, we use Barlow Twins loss for the modality gap and propose an extended version, Geometry Barlow Twins, for the geometry gap. As a result, we demonstrate that our method, AltO, can be trained on multimodal datasets without any ground-truth data. It not only outperforms other unsupervised methods but is also compatible with various architectures of homography estimators.\\nThe source code can be found at: https://github.com/songsang7/AltO'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce an unsupervised framework for estimating homography in multimodal images, improving performance by using a two-phase optimization and Barlow Twins loss.'}, 'pdf': {'value': '/pdf/dbd7c26b2dae2f1c86abaa70a60fb6e9e683d675.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsong2024unsupervised,\\ntitle={Unsupervised Homography Estimation on Multimodal Image Pair via Alternating Optimization},\\nauthor={Sanghyeob Song and Jaihyun Lew and Hyemi Jang and Sungroh Yoon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zkhyrxlwqH}\\n}'}, 'paperhash': {'value': 'song|unsupervised_homography_estimation_on_multimodal_image_pair_via_alternating_optimization'}},forum = 'zkhyrxlwqH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6345/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6345/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6345/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6345/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zkfCa4oESF',number = 199,cdate = 1713846115171,pdate = 1727287632329,odate = 1730873838965,mdate = 1730873838976,tcdate = 1713846115171,tmdate = 1730873838976,ddate = None,content = {'title': {'value': 'TPR: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning'}, 'authors': {'value': ['Hui Chen', 'Yanbin Liu', 'Yongqiang Ma', 'Nanning Zheng', 'Xin Yu']}, 'authorids': {'value': ['~Hui_Chen2', '~Yanbin_Liu1', '~Yongqiang_Ma3', '~Nanning_Zheng1', '~Xin_Yu1']}, 'keywords': {'value': ['Generalized Zero-shot Learning', 'Vision-Language Models', 'Contrastive Learning']}, 'TLDR': {'value': 'a model incorporating dual-space alignment and topology-preserving strategies for GZSL'}, 'abstract': {'value': \"Pre-trained vision-language models (VLMs) such as CLIP have shown excellent performance for zero-shot classification. Based on CLIP, recent methods design various learnable prompts to evaluate the zero-shot generalization capability on a base-to-novel setting. This setting assumes test samples are already divided into either base or novel classes, limiting its application to realistic scenarios. In this paper, we focus on a more challenging and practical setting: generalized zero-shot learning (GZSL), i.e., testing with no information about the base/novel division. To address this challenging zero-shot problem, we introduce two unique designs that enable us to classify an image without the need of knowing whether it comes from seen or unseen classes. Firstly, most existing methods only adopt a single latent space to align visual and linguistic features, which has a limited ability to represent complex visual-linguistic patterns, especially for fine-grained tasks. Instead, we propose a dual-space feature alignment module that effectively augments the latent space with a novel attribute space induced by a well-devised attribute reservoir. In particular, the attribute reservoir consists of a static vocabulary and learnable tokens complementing each other for flexible control over feature granularity. Secondly, finetuning CLIP models (e.g., prompt learning) on seen base classes usually sacrifices the model's original generalization capability on unseen novel classes. To mitigate this issue, we present a new topology-preserving objective that can enforce feature topology structures of the combined base and novel classes to resemble the topology of CLIP. In this manner, our model will inherit the generalization ability of CLIP through maintaining the pairwise class angles in the attribute space. Extensive experiments on twelve object recognition datasets demonstrate that our model, termed Topology-Preserving Reservoir (TPR), outperforms strong baselines including both prompt learning and conventional generative-based zero-shot methods.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e9ab97ad78449ecd4bb7169860020d90e331f252.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024tpr,\\ntitle={{TPR}: Topology-Preserving Reservoirs for Generalized Zero-Shot Learning},\\nauthor={Hui Chen and Yanbin Liu and Yongqiang Ma and Nanning Zheng and Xin Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zkfCa4oESF}\\n}'}, 'paperhash': {'value': 'chen|tpr_topologypreserving_reservoirs_for_generalized_zeroshot_learning'}},forum = 'zkfCa4oESF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission199/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission199/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission199/-/Revision', 'NeurIPS.cc/2024/Conference/Submission199/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ziehA15y8k',number = 14020,cdate = 1715745257269,pdate = 1727288057858,odate = 1730873962984,mdate = 1730873962996,tcdate = 1715745257269,tmdate = 1730873962996,ddate = None,content = {'title': {'value': 'Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning'}, 'authors': {'value': ['Yuefei Lyu', 'Chaozhuo Li', 'Sihong Xie', 'Xi Zhang']}, 'authorids': {'value': ['~Yuefei_Lyu1', '~Chaozhuo_Li1', '~Sihong_Xie1', '~Xi_Zhang12']}, 'keywords': {'value': ['graph adverisarial attack', 'reinforcement learning', 'inverse reinforcement learning', 'graph neural networks']}, 'abstract': {'value': 'Adversarial attacks against graph neural networks (GNNs) through perturbations of the graph structure are increasingly common in social network tasks like rumor detection. Social media platforms capture diverse attack sequence samples through both machine and manual screening processes. Investigating effective ways to leverage these adversarial samples to enhance robustness is imperative. We improve the maximum entropy inverse reinforcement learning (IRL) method with the mixture-of-experts approach to address multi-source graph adversarial attacks. This method reconstructs the attack policy, integrating various attack models and providing feature-level explanations, subsequently generating additional adversarial samples to fortify the robustness of detection models. We develop precise sample guidance and a bidirectional update mechanism to reduce the deviation caused by imprecise feature representation and negative sampling within the large action space of social graphs, while also accelerating policy learning. We take rumor detector as an example targeted GNN model on real-world rumor datasets. By utilizing a small subset of samples generated by various graph adversarial attack methods, we reconstruct the attack policy, closely approximating the performance of the original attack method. We validate that samples generated by the learned policy enhance model robustness through adversarial training and data augmentation.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8a54fe72827cc6095b89937198d8112de7d53f3d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlyu2024enhancing,\\ntitle={Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning},\\nauthor={Yuefei Lyu and Chaozhuo Li and Sihong Xie and Xi Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ziehA15y8k}\\n}'}, 'paperhash': {'value': 'lyu|enhancing_robustness_of_graph_neural_networks_on_social_media_with_explainable_inverse_reinforcement_learning'}},forum = 'ziehA15y8k',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14020/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14020/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14020/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14020/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'ziYC4FHRNr',number = 16187,cdate = 1715767548448,pdate = 1727288119171,odate = 1730873977603,mdate = 1730873977621,tcdate = 1715767548448,tmdate = 1730873977621,ddate = None,content = {'title': {'value': 'Entrywise error bounds for low-rank approximations of kernel matrices'}, 'authors': {'value': ['Alexander Modell']}, 'authorids': {'value': ['~Alexander_Modell1']}, 'keywords': {'value': ['low-rank approximation', 'kernel methods', 'SVD', 'theory', 'error bounds']}, 'TLDR': {'value': 'This paper proves an entrywise error bound on the low-rank approximation of a kernel matrix, obtained using the truncated eigen-decomposition (or singular value decomposition).'}, 'abstract': {'value': 'In this paper, we derive *entrywise* error bounds for low-rank approximations of kernel matrices obtained using the truncated eigen-decomposition (or singular value decomposition). While this approximation is well-known to be optimal with respect to the spectral and Frobenius norm error, little is known about the statistical behaviour of individual entries. Our error bounds fill this gap. A key technical innovation is a delocalisation result for the eigenvectors of the kernel matrix corresponding to small eigenvalues, which takes inspiration from the field of Random Matrix Theory. Finally, we validate our theory with an empirical study of a collection of synthetic and real-world datasets.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b7d1213d226d72a537e0292965c1e860cf9010e7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmodell2024entrywise,\\ntitle={Entrywise error bounds for low-rank approximations of kernel matrices},\\nauthor={Alexander Modell},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ziYC4FHRNr}\\n}'}, 'paperhash': {'value': 'modell|entrywise_error_bounds_for_lowrank_approximations_of_kernel_matrices'}},forum = 'ziYC4FHRNr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16187/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16187/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16187/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16187/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zgh0ChWocO',number = 17045,cdate = 1715776620257,pdate = 1727288143572,odate = 1730873982837,mdate = 1735386354823,tcdate = 1715776620257,tmdate = 1735386354823,ddate = None,content = {'title': {'value': 'Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards'}, 'authors': {'value': ['Qinwei Yang', 'Xueqing Liu', 'Yan Zeng', 'Ruocheng Guo', 'Yang Liu', 'Peng Wu']}, 'authorids': {'value': ['~Qinwei_Yang2', '~Xueqing_Liu6', '~Yan_Zeng2', '~Ruocheng_Guo1', '~Yang_Liu3', '~Peng_Wu5']}, 'keywords': {'value': ['Policy Learning', 'Short-Term and Long-Term Rewards', 'Causal Inference', 'Decision Making']}, 'TLDR': {'value': 'Learn the optimal policy that balances short-term and long-term rewards, especially in scenarios where the long-term outcomes are often missing due to data collection challenges over extended periods.'}, 'abstract': {'value': 'Learning the optimal policy to balance multiple short-term and long-term rewards has extensive applications across various domains. Yet, there is a noticeable scarcity of research addressing policy learning strategies in this context. In this paper, we aim to learn the optimal policy capable of effectively balancing multiple short-term and long-term rewards, especially in scenarios where the long-term outcomes are often missing due to data collection challenges over extended periods. Towards this goal, the conventional linear weighting method, which aggregates multiple rewards into a single surrogate reward through weighted summation, can only achieve sub-optimal policies when multiple rewards are related. Motivated by this, we propose a novel decomposition-based policy learning (DPPL) method that converts the whole problem into subproblems. The DPPL method is capable of obtaining optimal policies even when multiple rewards are interrelated. Nevertheless, the DPPL method requires a set of preference vectors specified in advance, posing challenges in practical applications where selecting suitable preferences is non-trivial. To mitigate this, we further theoretically transform the optimization problem in DPPL into an $\\\\varepsilon$-constraint problem, where $\\\\varepsilon$ represents the minimum acceptable levels of other rewards while maximizing one reward. This transformation provides intuitive into the selection of preference vectors. Extensive experiments are conducted on the proposed method and the results validate the effectiveness of the method.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4b0324785fb814e07ff3e021f3a6a8e3db3d00a6.pdf'}, 'supplementary_material': {'value': '/attachment/bae89354816bc00b1a6aa0b9272a42402039a7d1.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024learning,\\ntitle={Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards},\\nauthor={Qinwei Yang and Xueqing Liu and Yan Zeng and Ruocheng Guo and Yang Liu and Peng Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zgh0ChWocO}\\n}'}, 'paperhash': {'value': 'yang|learning_the_optimal_policy_for_balancing_shortterm_and_longterm_rewards'}},forum = 'zgh0ChWocO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17045/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17045/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17045/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17045/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zeaBrGv7Ll',number = 3917,cdate = 1715338282719,pdate = 1727287734505,odate = 1730873870362,mdate = 1734589871525,tcdate = 1715338282719,tmdate = 1734589871525,ddate = None,content = {'title': {'value': 'SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution'}, 'authors': {'value': ['Qi Tang', 'Yao Zhao', 'Meiqin Liu', 'Chao Yao']}, 'authorids': {'value': ['~Qi_Tang3', '~Yao_Zhao1', '~Meiqin_Liu1', '~Chao_Yao1']}, 'keywords': {'value': ['Video Super-Resolution', 'Diffusion Model']}, 'abstract': {'value': \"Diffusion-based Video Super-Resolution (VSR) is renowned for generating perceptually realistic videos, yet it grapples with maintaining detail consistency across frames due to stochastic fluctuations. The traditional approach of pixel-level alignment is ineffective for diffusion-processed frames because of iterative disruptions. To overcome this, we introduce SeeClear--a novel VSR framework leveraging conditional video generation, orchestrated by instance-centric and channel-wise semantic controls. This framework integrates a Semantic Distiller and a Pixel Condenser, which synergize to extract and upscale semantic details from low-resolution frames. The Instance-Centric Alignment Module (InCAM) utilizes video-clip-wise tokens to dynamically relate pixels within and across frames, enhancing coherency. Additionally, the Channel-wise Texture Aggregation Memory (CaTeGory) infuses extrinsic knowledge, capitalizing on long-standing semantic textures. Our method also innovates the blurring diffusion process with the ResShift mechanism, finely balancing between sharpness and diffusion effects. Comprehensive experiments confirm our framework's advantage over state-of-the-art diffusion-based VSR techniques.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/01c997cd807103ddbfa717a7445b4e1837ebeb53.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntang2024seeclear,\\ntitle={SeeClear: Semantic Distillation Enhances Pixel Condensation for Video Super-Resolution},\\nauthor={Qi Tang and Yao Zhao and Meiqin Liu and Chao Yao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zeaBrGv7Ll}\\n}'}, 'paperhash': {'value': 'tang|seeclear_semantic_distillation_enhances_pixel_condensation_for_video_superresolution'}},forum = 'zeaBrGv7Ll',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3917/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3917/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3917/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3917/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zeYyq0GpXO',number = 13571,cdate = 1715740852372,pdate = 1727288043517,odate = 1730873958823,mdate = 1730873958840,tcdate = 1715740852372,tmdate = 1730873958840,ddate = None,content = {'title': {'value': 'Exploring Context Window of Large Language Models via Decomposed Positional Vectors'}, 'authors': {'value': ['zican Dong', 'Junyi Li', 'Xin Men', 'Xin Zhao', 'Bingning Wang', 'Zhen Tian', 'weipeng chen', 'Ji-Rong Wen']}, 'authorids': {'value': ['~zican_Dong1', '~Junyi_Li4', '~Xin_Men1', '~Xin_Zhao10', '~Bingning_Wang3', '~Zhen_Tian1', '~weipeng_chen2', '~Ji-Rong_Wen1']}, 'keywords': {'value': ['Positional Vector', 'Context Window', 'Large Language Model', 'Length Extrapolation', 'Context Window Extension']}, 'abstract': {'value': 'Transformer-based large language models (LLMs) typically have a limited context window, resulting in significant performance degradation when processing text beyond the length of the context window. Extensive studies have been proposed to extend the context window and achieve length extrapolation of LLMs, but there is still a lack of in-depth interpretation of these approaches. In this study, we explore the positional information within and beyond the context window for deciphering the underlying mechanism of LLMs. By using a mean-based decomposition method, we disentangle positional vectors from hidden states of LLMs and analyze their formation and effect on attention. Furthermore, when texts exceed the context window, we analyze the change of positional vectors in two settings, i.e., direct extrapolation and context window extension. Based on our findings, we design two training-free context window extension methods, positional vector replacement and attention window extension. Experimental results show that our methods can effectively extend the context window length.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/93def3ab0f03ff754a0ecc781166fd37fd5b563c.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024exploring,\\ntitle={Exploring Context Window of Large Language Models via Decomposed Positional Vectors},\\nauthor={zican Dong and Junyi Li and Xin Men and Xin Zhao and Bingning Wang and Zhen Tian and weipeng chen and Ji-Rong Wen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zeYyq0GpXO}\\n}'}, 'paperhash': {'value': 'dong|exploring_context_window_of_large_language_models_via_decomposed_positional_vectors'}},forum = 'zeYyq0GpXO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13571/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13571/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13571/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13571/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zcEPOB9rCR',number = 15795,cdate = 1715763698784,pdate = 1727288107203,odate = 1730873974710,mdate = 1737116370281,tcdate = 1715763698784,tmdate = 1737116370281,ddate = None,content = {'title': {'value': 'Bridging Geometric States via Geometric Diffusion Bridge'}, 'authors': {'value': ['Shengjie Luo', 'Yixian Xu', 'Di He', 'Shuxin Zheng', 'Tie-Yan Liu', 'Liwei Wang']}, 'authorids': {'value': ['~Shengjie_Luo1', '~Yixian_Xu1', '~Di_He1', '~Shuxin_Zheng1', '~Tie-Yan_Liu1', '~Liwei_Wang1']}, 'keywords': {'value': ['Bridging Geometric States', 'Generative Modeling', 'Geometric Deep Learning', 'Diffusion Bridge', \"Doob's h-transform\", 'Equivariance']}, 'abstract': {'value': \"The accurate prediction of geometric state evolution in complex systems is critical for advancing scientific domains such as quantum chemistry and material modeling. Traditional experimental and computational methods face challenges in terms of environmental constraints and computational demands, while current deep learning approaches still fall short in terms of precision and generality. In this work, we introduce the Geometric Diffusion Bridge (GDB), a novel generative modeling framework that accurately bridges initial and target geometric states. GDB leverages a probabilistic approach to evolve geometric state distributions, employing an equivariant diffusion bridge derived by a modified version of Doob's $h$-transform for connecting geometric states. This tailored diffusion process is anchored by initial and target geometric states as fixed endpoints and governed by equivariant transition kernels. Moreover, trajectory data can be seamlessly leveraged in our GDB framework by using a chain of equivariant diffusion bridges, providing a more detailed and accurate characterization of evolution dynamics. Theoretically, we conduct a thorough examination to confirm our framework's ability to preserve joint distributions of geometric states and capability to completely model the underlying dynamics inducing trajectory distributions with negligible error. Experimental evaluations across various real-world scenarios show that GDB surpasses existing state-of-the-art approaches, opening up a new pathway for accurately bridging geometric states and tackling crucial scientific challenges with improved accuracy and applicability.\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f266817fbebd23596b1820451d7726da8a9cd128.pdf'}, '_bibtex': {'value': '@inproceedings{\\nluo2024bridging,\\ntitle={Bridging Geometric States via Geometric Diffusion Bridge},\\nauthor={Shengjie Luo and Yixian Xu and Di He and Shuxin Zheng and Tie-Yan Liu and Liwei Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zcEPOB9rCR}\\n}'}, 'paperhash': {'value': 'luo|bridging_geometric_states_via_geometric_diffusion_bridge'}},forum = 'zcEPOB9rCR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15795/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15795/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15795/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15795/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zb8jLAh2VN',number = 11943,cdate = 1715713568740,pdate = 1727287988914,odate = 1730873943851,mdate = 1730873943868,tcdate = 1715713568740,tmdate = 1730873943868,ddate = None,content = {'title': {'value': 'Inference of Neural Dynamics Using Switching Recurrent Neural Networks'}, 'authors': {'value': ['Yongxu Zhang', 'Shreya Saxena']}, 'authorids': {'value': ['~Yongxu_Zhang1', '~Shreya_Saxena2']}, 'keywords': {'value': ['Neural Dynamics', 'Variational Inference', 'Recurrent Neural Networks']}, 'abstract': {'value': 'Neural population activity often exhibits distinct dynamical features across time, which may correspond to distinct internal processes or behavior. Linear methods and variations thereof, such as Hidden Markov Model (HMM) and Switching Linear Dynamical System (SLDS), are often employed to identify discrete states with evolving neural dynamics. However, these techniques may not be able to capture the underlying nonlinear dynamics associated with neural propagation. Recurrent Neural Networks (RNNs) are commonly used to model neural dynamics thanks to their nonlinear characteristics. In our work, we develop Switching Recurrent Neural Networks (SRNN), RNNs with weights that switch across time, to reconstruct switching dynamics of neural time-series data. We apply these models to simulated data as well as cortical neural activity across mice and monkeys, which allows us to automatically detect discrete states that lead to the identification of varying neural dynamics. In a monkey reaching dataset with electrophysiology recordings, a mouse self-initiated lever pull dataset with widefield calcium recordings, and a mouse self-initiated decision making dataset with widefield calcium recording, SRNNs are able to automatically identify discrete states with distinct nonlinear neural dynamics. The inferred switches are aligned with the behavior, and the reconstructions show that the recovered neural dynamics are distinct across different stages of the behavior. We show that the neural dynamics have behaviorally-relevant switches across time and we are able to use SRNNs to successfully capture these switches and the corresponding dynamical features.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a72b19b658dbec5e7192f749e9871e5279caf5ab.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024inference,\\ntitle={Inference of Neural Dynamics Using Switching Recurrent Neural Networks},\\nauthor={Yongxu Zhang and Shreya Saxena},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zb8jLAh2VN}\\n}'}, 'paperhash': {'value': 'zhang|inference_of_neural_dynamics_using_switching_recurrent_neural_networks'}},forum = 'zb8jLAh2VN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11943/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11943/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11943/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11943/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zaXuMqOAF4',number = 16332,cdate = 1715769361913,pdate = 1727288123670,odate = 1730873978598,mdate = 1730873978618,tcdate = 1715769361913,tmdate = 1730873978618,ddate = None,content = {'title': {'value': 'Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs'}, 'authors': {'value': ['Xin Ma', 'Yang Liu', 'Jingjing Liu', 'Xiaoxu Ma']}, 'authorids': {'value': ['~Xin_Ma5', '~Yang_Liu59', '~Jingjing_Liu2', '~Xiaoxu_Ma1']}, 'keywords': {'value': ['Large language models', 'extrapolation', 'position encoding', 'transformers', 'self-attention']}, 'abstract': {'value': 'Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave PE can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair PE to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs’ applicative reach.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/aadea357c4db6663d79a4fd8524c3afe7c77650c.pdf'}, 'supplementary_material': {'value': '/attachment/5f3abdf1177202c764db483292a5af98703f34b1.zip'}, '_bibtex': {'value': '@inproceedings{\\nma2024mesaextrapolation,\\ntitle={Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in {LLM}s},\\nauthor={Xin Ma and Yang Liu and Jingjing Liu and Xiaoxu Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zaXuMqOAF4}\\n}'}, 'paperhash': {'value': 'ma|mesaextrapolation_a_weave_position_encoding_method_for_enhanced_extrapolation_in_llms'}},forum = 'zaXuMqOAF4',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16332/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16332/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16332/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16332/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'za9Jx8yqUA',number = 11013,cdate = 1715700118042,pdate = 1727287957883,odate = 1730873934141,mdate = 1730873934158,tcdate = 1715700118042,tmdate = 1730873934158,ddate = None,content = {'title': {'value': 'GenRL: Multimodal-foundation world models for generalization in embodied agents'}, 'authors': {'value': ['Pietro Mazzaglia', 'Tim Verbelen', 'Bart Dhoedt', 'Aaron Courville', 'Sai Rajeswar']}, 'authorids': {'value': ['~Pietro_Mazzaglia1', '~Tim_Verbelen1', '~Bart_Dhoedt1', '~Aaron_Courville3', '~Sai_Rajeswar2']}, 'keywords': {'value': ['world models', 'foundations models', 'reinforcement learning', 'multitask generalization']}, 'TLDR': {'value': \"Connecting multimodal foundation models' representations with world models' representations for RL enables specifying task by vision or language prompts and learning the corresponding embodied behaviors in imagination.\"}, 'abstract': {'value': 'Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be adopted in embodied contexts, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle to developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal-foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain’s dynamics, and learn the corresponding behaviors in imagination.\\nAs assessed through large-scale multi-task benchmarking in locomotion and manipulation domains, GenRL enables multi-task generalization from language and visual prompts. Furthermore, by introducing a data-free policy learning strategy, our approach lays the groundwork for foundational policy learning using generative world models. \\nWebsite, code and data: https://mazpie.github.io/genrl/'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/70954c88d0935070ce03152cf1eb67a2f2ac5a2e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmazzaglia2024genrl,\\ntitle={Gen{RL}: Multimodal-foundation world models for generalization in embodied agents},\\nauthor={Pietro Mazzaglia and Tim Verbelen and Bart Dhoedt and Aaron Courville and Sai Rajeswar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=za9Jx8yqUA}\\n}'}, 'paperhash': {'value': 'mazzaglia|genrl_multimodalfoundation_world_models_for_generalization_in_embodied_agents'}},forum = 'za9Jx8yqUA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11013/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zZVqZRXSao',number = 5055,cdate = 1715499940286,pdate = 1727287771891,odate = 1730873881561,mdate = 1730873881583,tcdate = 1715499940286,tmdate = 1730873881583,ddate = None,content = {'title': {'value': 'Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval'}, 'authors': {'value': ['Lixu Wang', 'Xinyu Du', 'Qi Zhu']}, 'authorids': {'value': ['~Lixu_Wang1', '~Xinyu_Du1', '~Qi_Zhu2']}, 'keywords': {'value': ['Image Retrieval', 'Domain Adaptation']}, 'TLDR': {'value': 'We identify an unexplored but important problem called Universal Unsupervised Cross-Domain Retrieval, and propose a two-stage semantic feature learning framework to address it.'}, 'abstract': {'value': 'Cross-domain retrieval (CDR) is finding increasingly broad applications across various domains. However, existing efforts have several major limitations, with the most critical being their reliance on accurate supervision. Recent studies thus focus on achieving unsupervised CDR, but they typically assume that the category spaces across domains are identical, an assumption that is often unrealistic in real-world scenarios. This is because only through dedicated and comprehensive analysis can the category composition of a data domain be obtained, which contradicts the premise of unsupervised scenarios. Therefore, in this work, we introduce the problem of **U**niversal **U**nsupervised **C**ross-**D**omain **R**etrieval (U^2CDR) for the first time and design a two-stage semantic feature learning framework to address it. In the first stage, a cross-domain unified prototypical structure is established under the guidance of an instance-prototype-mixed contrastive loss and a semantic-enhanced loss, to counteract category space differences. In the second stage, through a modified adversarial training mechanism, we ensure minimal changes for the established prototypical structure during domain alignment, enabling more accurate nearest-neighbor searching. Extensive experiments across multiple datasets and scenarios, including close-set, partial, and open-set CDR, demonstrate that our approach significantly outperforms existing state-of-the-art CDR methods and other related methods in solving U^2CDR challenges.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cb8530b37653420a6b121610aa8e7adce3e7940a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024semantic,\\ntitle={Semantic Feature Learning for Universal Unsupervised Cross-Domain Retrieval},\\nauthor={Lixu Wang and Xinyu Du and Qi Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zZVqZRXSao}\\n}'}, 'paperhash': {'value': 'wang|semantic_feature_learning_for_universal_unsupervised_crossdomain_retrieval'}},forum = 'zZVqZRXSao',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5055/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5055/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5055/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5055/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zXfhHJnMB2',number = 18932,cdate = 1715788209140,pdate = 1727288197497,odate = 1730873992672,mdate = 1736961258653,tcdate = 1715788209140,tmdate = 1736961258653,ddate = None,content = {'title': {'value': 'Neural Conditional Probability for Uncertainty Quantification'}, 'authors': {'value': ['Vladimir R Kostic', 'gregoire pacreau', 'Giacomo Turri', 'Pietro Novelli', 'Karim Lounici', 'Massimiliano Pontil']}, 'authorids': {'value': ['~Vladimir_R_Kostic1', '~gregoire_pacreau1', '~Giacomo_Turri1', '~Pietro_Novelli1', '~Karim_Lounici1', '~Massimiliano_Pontil3']}, 'keywords': {'value': ['Operator learning', 'conditional density estimation', 'statistical inference', 'deep learning', 'theoretical guarantees']}, 'TLDR': {'value': 'A novel operator approach for learning conditional probability distributions with deep learning'}, 'abstract': {'value': 'We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with \\na focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as \\nconditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing \\nefficient inference without the need for retraining even when conditioning changes. By leveraging the approximation \\ncapabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions. \\nWe provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. \\nIn experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. \\nThis demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve \\ncompetitive results,  even in the face of more complex architectures.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8fd1f3f77331bd6b69d3849ec940c067888f32dd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkostic2024neural,\\ntitle={Neural Conditional Probability for Uncertainty Quantification},\\nauthor={Vladimir R Kostic and gregoire pacreau and Giacomo Turri and Pietro Novelli and Karim Lounici and Massimiliano Pontil},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zXfhHJnMB2}\\n}'}, 'paperhash': {'value': 'kostic|neural_conditional_probability_for_uncertainty_quantification'}},forum = 'zXfhHJnMB2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18932/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18932/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18932/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18932/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zWuHSIALBh',number = 1553,cdate = 1714682903142,pdate = 1727287664464,odate = 1730873849785,mdate = 1730873849799,tcdate = 1714682903142,tmdate = 1730873849799,ddate = None,content = {'title': {'value': 'FLAME : Factuality-Aware Alignment for Large Language Models'}, 'authors': {'value': ['Sheng-Chieh Lin', 'Luyu Gao', 'Barlas Oguz', 'Wenhan Xiong', 'Jimmy Lin', 'Wen-tau Yih', 'Xilun Chen']}, 'authorids': {'value': ['~Sheng-Chieh_Lin1', '~Luyu_Gao1', '~Barlas_Oguz1', '~Wenhan_Xiong1', '~Jimmy_Lin2', '~Wen-tau_Yih1', '~Xilun_Chen1']}, 'keywords': {'value': ['large language models', 'factuality', 'alignment']}, 'TLDR': {'value': \"We find that the standard alignment process encourages hallucination, and propose factuality-aware alignment while maintaining the LLM's general instruction-following capability.\"}, 'abstract': {'value': 'Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. \\nWe have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., *hallucination*). \\nIn this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL).\\nIn particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination.\\nThis makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. \\nFurthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination.\\nBased on these observations, we propose *FactuaLity-aware AlignMEnt*, comprised of *factuality-aware SFT* and *factuality-aware RL* through direct preference optimization. \\nExperiments show that our proposed *FLAME* guides LLMs to output more factual responses while maintaining their instruction-following capability.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2f86bf598e6a6290e2144203a3b529f671fa6870.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024flame,\\ntitle={{FLAME} : Factuality-Aware Alignment for Large Language Models},\\nauthor={Sheng-Chieh Lin and Luyu Gao and Barlas Oguz and Wenhan Xiong and Jimmy Lin and Wen-tau Yih and Xilun Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zWuHSIALBh}\\n}'}, 'paperhash': {'value': 'lin|flame_factualityaware_alignment_for_large_language_models'}},forum = 'zWuHSIALBh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1553/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1553/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1553/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1553/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zWnW4zqkuM',number = 12638,cdate = 1715725593482,pdate = 1727288012976,odate = 1730873950615,mdate = 1730873950633,tcdate = 1715725593482,tmdate = 1730873950633,ddate = None,content = {'title': {'value': 'InstructG2I: Synthesizing Images from Multimodal Attributed Graphs'}, 'authors': {'value': ['Bowen Jin', 'Ziqi Pang', 'Bingjun Guo', 'Yu-Xiong Wang', 'Jiaxuan You', 'Jiawei Han']}, 'authorids': {'value': ['~Bowen_Jin1', '~Ziqi_Pang1', '~Bingjun_Guo1', '~Yu-Xiong_Wang1', '~Jiaxuan_You2', '~Jiawei_Han1']}, 'keywords': {'value': ['learning on graphs', 'image generation', 'diffusion model', 'graph neural network']}, 'abstract': {'value': 'In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs). This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions. To address these challenges, we propose a graph context-conditioned diffusion model called InstructG2I. InstructG2I first exploits the graph structure and multimodal information to conduct informative neighbor sampling by combining personalized page rank and re-ranking based on vision-language features. Then, a graph QFormer encoder adaptively encodes the graph nodes into an auxiliary set of graph prompts to guide the denoising process of diffusion. Finally, we propose graph classifier-free guidance, enabling controllable generation by varying the strength of graph guidance and multiple connected edges to a node. Extensive experiments conducted on three datasets from different domains demonstrate the effectiveness and controllability of our approach. The code is available at https://github.com/PeterGriffinJin/InstructG2I.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/14232e04d8524d77648d3c5ea135527ad4aef01a.pdf'}, '_bibtex': {'value': '@inproceedings{\\njin2024instructgi,\\ntitle={InstructG2I: Synthesizing Images from Multimodal Attributed Graphs},\\nauthor={Bowen Jin and Ziqi Pang and Bingjun Guo and Yu-Xiong Wang and Jiaxuan You and Jiawei Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zWnW4zqkuM}\\n}'}, 'paperhash': {'value': 'jin|instructg2i_synthesizing_images_from_multimodal_attributed_graphs'}},forum = 'zWnW4zqkuM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12638/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12638/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12638/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12638/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zVrQeoPIoQ',number = 2330,cdate = 1715044678313,pdate = 1727287687320,odate = 1730873857387,mdate = 1730873857404,tcdate = 1715044678313,tmdate = 1730873857404,ddate = None,content = {'title': {'value': 'Rethinking No-reference Image Exposure Assessment from Holism to Pixel:  Models, Datasets and Benchmarks'}, 'authors': {'value': ['Shuai He', 'Shuntian Zheng', 'Anlong Ming', 'Banyu Wu', 'Huadong Ma']}, 'authorids': {'value': ['~Shuai_He2', '~Shuntian_Zheng1', '~Anlong_Ming1', '~Banyu_Wu1', '~Huadong_Ma1']}, 'keywords': {'value': ['Image Exposure Assessment', 'Image Quality Assessment']}, 'TLDR': {'value': 'Pixel-level image exposure assessment from three perspectives: model, dataset, and benchmark.'}, 'abstract': {'value': 'The past decade has witnessed an increasing demand for enhancing image quality through exposure, and as a crucial prerequisite in this endeavor, Image Exposure Assessment (IEA) is now being accorded serious attention. However, IEA encounters two persistent challenges that remain unresolved over the long term: the accuracy and generalizability of No-reference IEA are inadequate for practical applications; the scope of IEA is confined to qualitative and quantitative analysis of the entire image or subimage, such as providing only a score to evaluate the exposure level, thereby lacking intuitive and precise fine-grained evaluation for complex exposure conditions.  The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark.  1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios. 2) Dataset-level: we elaborately build an exposure-oriented dataset, IEA40K, containing 40K  images, covering 17 typical lighting scenarios, 27 devices, and 50+ scenes, with each image densely annotated by more than 10 experts with pixel-level labels.  3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K. Our P-IEANet not only achieves state-of-the-art (SOTA) performance on all metrics but also seamlessly integrates with existing exposure correction and lighting enhancement methods. To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community. The code and dataset are available in \\\\href{https://github.com/mRobotit/Pixel-level-No-reference-Image-Exposure-Assessment}{\\\\textcolor{red} {here}}.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9d159e1b2a972b461ac4b69cc1ad301734642641.pdf'}, 'supplementary_material': {'value': '/attachment/4c033c87c975961e7194fda4f3ecc8ff4de81590.zip'}, '_bibtex': {'value': '@inproceedings{\\nhe2024rethinking,\\ntitle={Rethinking No-reference Image Exposure Assessment from Holism to Pixel:  Models, Datasets and Benchmarks},\\nauthor={Shuai He and Shuntian Zheng and Anlong Ming and Banyu Wu and Huadong Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zVrQeoPIoQ}\\n}'}, 'paperhash': {'value': 'he|rethinking_noreference_image_exposure_assessment_from_holism_to_pixel_models_datasets_and_benchmarks'}},forum = 'zVrQeoPIoQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2330/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2330/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2330/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2330/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'zV2GDsZb5a',number = 2007,cdate = 1714933943920,pdate = 1727287678307,odate = 1730873854129,mdate = 1730873854143,tcdate = 1714933943920,tmdate = 1730873854143,ddate = None,content = {'title': {'value': 'Neural Gaffer: Relighting Any Object via Diffusion'}, 'authors': {'value': ['Haian Jin', 'Yuan Li', 'Fujun Luan', 'Yuanbo Xiangli', 'Sai Bi', 'Kai Zhang', 'Zexiang Xu', 'Jin Sun', 'Noah Snavely']}, 'authorids': {'value': ['~Haian_Jin1', '~Yuan_Li13', '~Fujun_Luan2', '~Yuanbo_Xiangli1', '~Sai_Bi1', '~Kai_Zhang7', '~Zexiang_Xu1', '~Jin_Sun2', '~Noah_Snavely1']}, 'keywords': {'value': ['relighting', 'single-image relighting', 'diffusion models', '3D relighting.']}, 'TLDR': {'value': 'We proposed an 2D relighting diffusion model that can relight any object image under any environmental lighting, which can be used as a prior in many 2D and 3D tasks, such as text-based relighting, object insertion, and relighting 3D representation.'}, 'abstract': {'value': 'Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/6d6fe18c77b16819b0e040bdc736f39a1a1bbda1.zip'}, 'pdf': {'value': '/pdf/71a526cee2abe808ec8027770fd2ee1ce6e3a7fc.pdf'}, '_bibtex': {'value': '@inproceedings{\\njin2024neural,\\ntitle={Neural Gaffer: Relighting Any Object via Diffusion},\\nauthor={Haian Jin and Yuan Li and Fujun Luan and Yuanbo Xiangli and Sai Bi and Kai Zhang and Zexiang Xu and Jin Sun and Noah Snavely},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zV2GDsZb5a}\\n}'}, 'paperhash': {'value': 'jin|neural_gaffer_relighting_any_object_via_diffusion'}},forum = 'zV2GDsZb5a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2007/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2007/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2007/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2007/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zTu0QEpvtZ',number = 9432,cdate = 1715679685131,pdate = 1727287910912,odate = 1730873920660,mdate = 1730873920673,tcdate = 1715679685131,tmdate = 1730873920673,ddate = None,content = {'title': {'value': 'Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model'}, 'authors': {'value': ['Mingyang Yi', 'Aoxue Li', 'Yi Xin', 'Zhenguo Li']}, 'authorids': {'value': ['~Mingyang_Yi1', '~Aoxue_Li2', '~Yi_Xin1', '~Zhenguo_Li1']}, 'keywords': {'value': ['text-to-image generation; working mechanism;']}, 'TLDR': {'value': 'In this paper, we reveal the working mechanism of text-to-image diffusion model'}, 'abstract': {'value': 'Recently, the strong latent Diffusion Probabilistic Model (DPM) has been applied to high-quality Text-to-Image (T2I) generation (e.g., Stable Diffusion), by injecting the encoded target text prompt into the gradually denoised diffusion image generator. Despite the success of DPM in practice, the mechanism behind it remains to be explored. To fill this blank, we begin by examining the intermediate statuses during the gradual denoising generation process in DPM. The empirical observations indicate, the shape of image is reconstructed after the first few denoising steps, and then the image is filled with details (e.g., texture). The phenomenon is because the low-frequency signal (shape relevant) of the noisy image is not corrupted until the final stage in the forward process (initial stage of generation) of adding noise in DPM. Inspired by the observations, we proceed to explore the influence of each token in the text prompt during the two stages. After a series of experiments of T2I generations conditioned on a set of text prompts. We conclude that in the earlier generation stage, the image is mostly decided by the special token [\\\\texttt{EOS}] in the text prompt, and the information in the text prompt is already conveyed in this stage. After that, the diffusion model completes the details of generated images by information from themselves. Finally, we propose to apply this observation to accelerate the process of T2I generation by properly removing text guidance, which finally accelerates the sampling up to 25\\\\%+.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e5a980c943643aa24ee25b4d6f1f338b4cf65964.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyi2024towards,\\ntitle={Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model},\\nauthor={Mingyang Yi and Aoxue Li and Yi Xin and Zhenguo Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zTu0QEpvtZ}\\n}'}, 'paperhash': {'value': 'yi|towards_understanding_the_working_mechanism_of_texttoimage_diffusion_model'}},forum = 'zTu0QEpvtZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9432/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9432/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9432/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9432/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'zO55ovdLJw',number = 3631,cdate = 1715310952277,pdate = 1727287726601,odate = 1730873868209,mdate = 1730873868271,tcdate = 1715310952277,tmdate = 1730873868271,ddate = None,content = {'title': {'value': 'Deep Correlated Prompting for Visual Recognition with Missing Modalities'}, 'authors': {'value': ['Lianyu Hu', 'Tongkai Shi', 'Wei Feng', 'Fanhua Shang', 'Liang Wan']}, 'authorids': {'value': ['~Lianyu_Hu1', '~Tongkai_Shi1', '~Wei_Feng1', '~Fanhua_Shang2', '~Liang_Wan1']}, 'keywords': {'value': ['Multimodal prompting', 'Mutltimodal models', 'Missing modalities']}, 'TLDR': {'value': 'A deep correlated promp learning paradigm for visual recognition for missing-modality scenarios with minimal computational costs'}, 'abstract': {'value': 'Large-scale multimodal models have shown excellent performance over a series of tasks powered by the large corpus of paired multimodal training data. Generally, they are always assumed to receive modality-complete inputs. However, this simple assumption may not always hold in the real world due to privacy constraints or collection difficulty, where models pretrained on modality-complete data easily demonstrate degraded performance on missing-modality cases. To handle this issue, we refer to prompt learning to adapt large pretrained multimodal models to handle missing-modality scenarios by regarding different missing cases as different types of input. Instead of only prepending independent prompts to the intermediate layers, we present to leverage the correlations between prompts and input features and excavate the relationships between different layers of prompts to carefully design the instructions. We also incorporate the complementary semantics of different modalities to guide the prompting design for each modality. Extensive experiments on three commonly-used datasets consistently demonstrate the superiority of our method compared to the previous approaches upon different missing scenarios. Plentiful ablations are further given to show the generalizability and reliability of our method upon different modality-missing ratios and types.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e5eab82e91c827d97d0d74e6bfb40e12627a0fb3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024deep,\\ntitle={Deep Correlated Prompting for Visual Recognition with Missing Modalities},\\nauthor={Lianyu Hu and Tongkai Shi and Wei Feng and Fanhua Shang and Liang Wan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zO55ovdLJw}\\n}'}, 'paperhash': {'value': 'hu|deep_correlated_prompting_for_visual_recognition_with_missing_modalities'}},forum = 'zO55ovdLJw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3631/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3631/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3631/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3631/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zNiJZUAlxg',number = 4022,cdate = 1715350625418,pdate = 1727287737764,odate = 1730873871520,mdate = 1730873871537,tcdate = 1715350625418,tmdate = 1730873871537,ddate = None,content = {'title': {'value': 'ResAD: A Simple Framework for Class Generalizable Anomaly Detection'}, 'authors': {'value': ['Xincheng Yao', 'Zixin Chen', 'Chao Gao', 'Guangtao Zhai', 'Chongyang Zhang']}, 'authorids': {'value': ['~Xincheng_Yao2', '~Zixin_Chen1', '~Chao_Gao6', '~Guangtao_Zhai1', '~Chongyang_Zhang1']}, 'keywords': {'value': ['class-generalizable anomaly detection']}, 'TLDR': {'value': 'we propose a simple but effective class-generalizable AD framework, called ResAD, which can be applied to detect and localize anomalies in new classes.'}, 'abstract': {'value': 'This paper explores the problem of class-generalizable anomaly detection, where the objective is to train one unified AD model that can generalize to detect anomalies in diverse classes from different domains without any retraining or fine-tuning on the target data. Because normal feature representations vary significantly across classes, this will cause the widely studied one-for-one AD models to be poorly classgeneralizable (i.e., performance drops dramatically when used for new classes). In this work, we propose a simple but effective framework (called ResAD) that can be directly applied to detect anomalies in new classes. Our main insight is to learn the residual feature distribution rather than the initial feature distribution. In this way, we can significantly reduce feature variations. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. Therefore, the learned model can be directly adapted to new classes. ResAD consists of three components: (1) a Feature Converter that converts initial features into residual features; (2) a simple and shallow Feature Constraintor that constrains normal residual features into a spatial hypersphere for further reducing feature variations and maintaining consistency in feature scales among different classes; (3) a Feature Distribution Estimator that estimates the normal residual feature distribution, anomalies can be recognized as out-of-distribution. Despite the simplicity, ResAD can achieve remarkable anomaly detection results when directly used in new classes. The code is available at https://github.com/xcyao00/ResAD.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/597429127fac8d70a05f0ca884272186eeefa326.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyao2024resad,\\ntitle={Res{AD}: A Simple Framework for Class Generalizable Anomaly Detection},\\nauthor={Xincheng Yao and Zixin Chen and Chao Gao and Guangtao Zhai and Chongyang Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zNiJZUAlxg}\\n}'}, 'paperhash': {'value': 'yao|resad_a_simple_framework_for_class_generalizable_anomaly_detection'}},forum = 'zNiJZUAlxg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4022/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4022/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4022/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4022/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zNIhPZnqhh',number = 6495,cdate = 1715595609919,pdate = 1727287816382,odate = 1730873893885,mdate = 1737135317952,tcdate = 1715595609919,tmdate = 1737135317952,ddate = None,content = {'title': {'value': 'Continuous Spatiotemporal Events Decoupling through Spike-based Bayesian Computation'}, 'authors': {'value': ['Yajing Zheng', 'Jiyuan Zhang', 'Zhaofei Yu', 'Tiejun Huang']}, 'authorids': {'value': ['~Yajing_Zheng1', '~Jiyuan_Zhang3', '~Zhaofei_Yu1', '~Tiejun_Huang1']}, 'keywords': {'value': ['Bayesian Computation', 'Spiking Neural Network', 'Event Cameras', 'Motion Segmentation', 'Winner-Take-All', 'Spike-timing-dependent plasticity']}, 'abstract': {'value': 'Numerous studies have demonstrated that the cognitive processes of the human brain can be modeled using the Bayesian theorem for probabilistic inference of the external world. Spiking neural networks (SNNs), capable of performing Bayesian computation with greater physiological interpretability, offer a novel approach to distributed information processing in the cortex. However, applying these models to real-world scenarios to harness the advantages of brain-like computation remains a challenge. \\nRecently, bio-inspired sensors with high dynamic range and ultra-high temporal resolution have been widely used in extreme vision scenarios. Event streams, generated by various types of motion, represent spatiotemporal data. Inferring motion targets from these streams without prior knowledge remains a difficult task. The Bayesian inference-based Expectation-Maximization (EM) framework has proven effective for motion segmentation in event streams, allowing for decoupling without prior information about the motion or its source. \\nThis work demonstrates that Bayesian computation based on spiking neural networks can decouple event streams of different motions. The Winner-Take-All (WTA) circuits in the constructed network implement an equivalent E-step, while STDP achieves an equivalent optimization in M-step. Through theoretical analysis and experiments, we show that STDP-based learning can maximize the contrast of warped events under mixed motion models. Experimental results show that the constructed spiking network can effectively segment the motion contained in event streams.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7729d2d60eba37a6c173b695d96839c15c1f8704.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024continuous,\\ntitle={Continuous Spatiotemporal Events Decoupling through Spike-based Bayesian Computation},\\nauthor={Yajing Zheng and Jiyuan Zhang and Tiejun Huang and Zhaofei Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zNIhPZnqhh}\\n}'}, 'paperhash': {'value': 'zheng|continuous_spatiotemporal_events_decoupling_through_spikebased_bayesian_computation'}},forum = 'zNIhPZnqhh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6495/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6495/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6495/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6495/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zMNd0JuceF',number = 4762,cdate = 1715454194794,pdate = 1727287760330,odate = 1730873878876,mdate = 1730873878895,tcdate = 1715454194794,tmdate = 1730873878895,ddate = None,content = {'title': {'value': 'Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses'}, 'authors': {'value': ['Xiaosen Zheng', 'Tianyu Pang', 'Chao Du', 'Qian Liu', 'Jing Jiang', 'Min Lin']}, 'authorids': {'value': ['~Xiaosen_Zheng1', '~Tianyu_Pang1', '~Chao_Du1', '~Qian_Liu2', '~Jing_Jiang1', '~Min_Lin1']}, 'keywords': {'value': ['Jailbreaking Attacks', 'Large Language Models', 'Alignment', 'Jailbreaking Defenses']}, 'abstract': {'value': 'Recently, Anil et al. (2024) show that many-shot (up to hundreds of) demonstrations can jailbreak state-of-the-art LLMs by exploiting their long-context capability. Nevertheless, is it possible to use few-shot demonstrations to efficiently jailbreak LLMs within limited context sizes? While the vanilla few-shot jailbreaking may be inefficient, we propose improved techniques such as injecting special system tokens like [/INST] and employing demo-level random search from a collected demo pool. These simple techniques result in surprisingly effective jailbreaking against aligned LLMs (even with advanced defenses). For example, our method achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B without multiple restarts, even if the models are enhanced by strong defenses such as perplexity detection and/or SmoothLLM, which is challenging for suffix-based jailbreaking. In addition, we conduct comprehensive and elaborate (e.g., making sure to use correct system prompts) evaluations against other aligned LLMs and advanced defenses, where our method consistently achieves nearly 100% ASRs. Our code is available at https://github.com/sail-sg/I-FSJ.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': \"Improved few-shot jailbreaking composed by automated creation of the demonstration pool, the utilization of special tokens from the target LLM's system template, and demo-level random search, facilitate high ASRs.\"}, 'supplementary_material': {'value': '/attachment/69e4cd26f91ae3e0f17993b1780b4eb5b9998c1c.zip'}, 'pdf': {'value': '/pdf/3ed8249e86ca655d716f66c7d8b210570210f646.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024improved,\\ntitle={Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses},\\nauthor={Xiaosen Zheng and Tianyu Pang and Chao Du and Qian Liu and Jing Jiang and Min Lin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zMNd0JuceF}\\n}'}, 'paperhash': {'value': 'zheng|improved_fewshot_jailbreaking_can_circumvent_aligned_language_models_and_their_defenses'}},forum = 'zMNd0JuceF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4762/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4762/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4762/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4762/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zLU21oQjD5',number = 7781,cdate = 1715638177145,pdate = 1727287858326,odate = 1730873906392,mdate = 1730873906403,tcdate = 1715638177145,tmdate = 1730873906403,ddate = None,content = {'title': {'value': 'DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving'}, 'authors': {'value': ['Yuxuan Tong', 'Xiwen Zhang', 'Rui Wang', 'Ruidong Wu', 'Junxian He']}, 'authorids': {'value': ['~Yuxuan_Tong2', '~Xiwen_Zhang2', '~Rui_Wang1', '~Ruidong_Wu1', '~Junxian_He1']}, 'keywords': {'value': ['Large Language Models', 'Mathematical Reasoning', 'Synthetic Data']}, 'TLDR': {'value': 'To achieve the best performance (for mathematical reasoning), more correct responses for difficult queries are crucial.'}, 'abstract': {'value': 'Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.\\nHypothesizing that difficult queries are crucial to learning complex reasoning, we propose *Difficulty-Aware Rejection Tuning* (`DART`), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.\\nUtilizing `DART`, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.\\nWe fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called `DART-Math`.\\nIn comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, `DART-Math` outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/26d6bf8a231686aaa5faf9277e38c2b2d934ff28.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntong2024dartmath,\\ntitle={{DART}-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving},\\nauthor={Yuxuan Tong and Xiwen Zhang and Rui Wang and Ruidong Wu and Junxian He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zLU21oQjD5}\\n}'}, 'paperhash': {'value': 'tong|dartmath_difficultyaware_rejection_tuning_for_mathematical_problemsolving'}},forum = 'zLU21oQjD5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7781/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7781/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7781/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7781/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zLClygeRK8',number = 2564,cdate = 1715098797268,pdate = 1727287693745,odate = 1730873859048,mdate = 1730873859061,tcdate = 1715098797268,tmdate = 1730873859061,ddate = None,content = {'title': {'value': 'Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning'}, 'authors': {'value': ['Otmane Sakhi', 'Imad Aouali', 'Pierre Alquier', 'Nicolas Chopin']}, 'authorids': {'value': ['~Otmane_Sakhi1', '~Imad_Aouali2', '~Pierre_Alquier1', '~Nicolas_Chopin1']}, 'keywords': {'value': ['offline contextual bandit', 'off-policy evaluation', 'off-policy selection', 'off-policy learning', 'pessimism']}, 'abstract': {'value': \"This work investigates the offline formulation of the contextual bandit problem, where the goal is to leverage past interactions collected under a behavior policy to evaluate, select, and learn new, potentially better-performing, policies. Motivated by critical applications, we move beyond point estimators. Instead, we adopt the principle of _pessimism_ where we construct upper bounds that assess a policy's worst-case performance, enabling us to confidently select and learn improved policies. Precisely, we introduce novel, fully empirical concentration bounds for a broad class of importance weighting risk estimators. These bounds are general enough to cover most existing estimators and pave the way for the development of new ones. In particular, our pursuit of the tightest bound within this class motivates a novel estimator (LS), that _logarithmically smoothes_ large importance weights. The bound for LS is provably tighter than its competitors, and naturally results in improved policy selection and learning strategies. Extensive policy evaluation, selection, and learning experiments highlight the versatility and favorable performance of LS.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/176852333e6e4a1430f1fe58cab4bcb648cf96b2.pdf'}, 'supplementary_material': {'value': '/attachment/cba8c7c3041eaa14a49145883f724a603d37df7e.zip'}, '_bibtex': {'value': '@inproceedings{\\nsakhi2024logarithmic,\\ntitle={Logarithmic Smoothing for Pessimistic Off-Policy Evaluation, Selection and Learning},\\nauthor={Otmane Sakhi and Imad Aouali and Pierre Alquier and Nicolas Chopin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zLClygeRK8}\\n}'}, 'TLDR': {'value': 'We propose a novel, pessimistic off-policy estimator that logarithmically smoothes the importance weights, leading to improved policy evaluation, selection and learning strategies.'}, 'paperhash': {'value': 'sakhi|logarithmic_smoothing_for_pessimistic_offpolicy_evaluation_selection_and_learning'}},forum = 'zLClygeRK8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2564/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2564/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2564/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2564/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zLBlin2zvW',number = 9334,cdate = 1715678466464,pdate = 1727287907604,odate = 1730873919927,mdate = 1730873919946,tcdate = 1715678466464,tmdate = 1730873919946,ddate = None,content = {'title': {'value': 'Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders'}, 'authors': {'value': ['Senthooran Rajamanoharan', 'Arthur Conmy', 'Lewis Smith', 'Tom Lieberum', 'Vikrant Varma', 'Janos Kramar', 'Rohin Shah', 'Neel Nanda']}, 'authorids': {'value': ['~Senthooran_Rajamanoharan1', '~Arthur_Conmy1', '~Lewis_Smith3', '~Tom_Lieberum2', '~Vikrant_Varma1', '~Janos_Kramar1', '~Rohin_Shah1', '~Neel_Nanda1']}, 'keywords': {'value': ['Mechanistic Interpretability', 'Sparse Autoencoders', 'Science of Deep Learning']}, 'TLDR': {'value': 'We introduce Gated SAEs, a new SAE architecture that is a Pareto improvement over baseline approaches for training SAEs on LM activations.'}, 'abstract': {'value': \"Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of those activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/43584951381c20709a2cb6cf3ebc6ae1b2d501df.pdf'}, '_bibtex': {'value': '@inproceedings{\\nrajamanoharan2024improving,\\ntitle={Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders},\\nauthor={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and Janos Kramar and Rohin Shah and Neel Nanda},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zLBlin2zvW}\\n}'}, 'paperhash': {'value': 'rajamanoharan|improving_sparse_decomposition_of_language_model_activations_with_gated_sparse_autoencoders'}},forum = 'zLBlin2zvW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9334/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9334/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9334/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9334/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zJremsKVyh',number = 1488,cdate = 1714654906808,pdate = 1727287662307,odate = 1730873849161,mdate = 1737130480054,tcdate = 1714654906808,tmdate = 1737130480054,ddate = None,content = {'title': {'value': 'Marginal Causal Flows for Validation and Inference'}, 'authors': {'value': ['Daniel de Vassimon Manela', 'Laura Battaglia', 'Robin J. Evans']}, 'authorids': {'value': ['~Daniel_de_Vassimon_Manela1', '~Laura_Battaglia1', '~Robin_J._Evans2']}, 'keywords': {'value': ['Causal Inference', 'Normalising Flows', 'Synthetic Data', 'Marginal Structural Models']}, 'TLDR': {'value': 'We show how Normalising Flows can be used to explicitly parameterise marginal causal distributions, and illustrate its utility for both inference and synthetic data generation/benchmarking.'}, 'abstract': {'value': 'Investigating the marginal causal effect of an intervention on an outcome from complex data remains challenging due to the inflexibility of employed models and the lack of complexity in causal benchmark datasets, which often fail to reproduce intricate real-world data patterns. In this paper we introduce Frugal Flows, a likelihood-based machine learning model that uses normalising flows to flexibly learn the data-generating process, while also directly targeting the marginal causal quantities inferred from observational data. We provide a novel algorithm for fitting a model to observational data with a parametrically specified causal distribution, and propose that these models are exceptionally well suited for synthetic data generation to validate causal methods. Unlike existing data generation methods, Frugal Flows generate synthetic data that closely resembles the empirical dataset, while also automatically and exactly satisfying a user-defined average treatment effect. To our knowledge, Frugal Flows are the first generative model to both learn flexible data representations and also \\\\textit{exactly} parameterise quantities such as the average treatment effect and the degree of unobserved confounding. We demonstrate the above with experiments on  both simulated and real-world datasets.'}, 'pdf': {'value': '/pdf/5ca85bc9b90e258067e112db30bfa5eae96a4a2a.pdf'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/1df1843e8db7c9355b9ad25ef25954693c495f16.zip'}, '_bibtex': {'value': '@inproceedings{\\nmanela2024marginal,\\ntitle={Marginal Causal Flows for Validation and Inference},\\nauthor={Daniel de Vassimon Manela and Laura Battaglia and Robin J. Evans},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zJremsKVyh}\\n}'}, 'paperhash': {'value': 'manela|marginal_causal_flows_for_validation_and_inference'}},forum = 'zJremsKVyh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1488/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1488/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1488/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission1488/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zJNSbgl4UA',number = 5417,cdate = 1715530322086,pdate = 1727287783107,odate = 1730873884475,mdate = 1734550122072,tcdate = 1715530322086,tmdate = 1734550122072,ddate = None,content = {'title': {'value': 'Slicing Vision Transformer for Flexible Inference'}, 'authors': {'value': ['Yitian Zhang', 'Huseyin Coskun', 'Xu Ma', 'Huan Wang', 'Ke Ma', 'Stephen Xi Chen', 'Derek Hao Hu', 'Yun Fu']}, 'authorids': {'value': ['~Yitian_Zhang1', '~Huseyin_Coskun1', '~Xu_Ma2', '~Huan_Wang3', '~Ke_Ma3', '~Stephen_Xi_Chen1', '~Derek_Hao_Hu1', '~Yun_Fu1']}, 'keywords': {'value': ['Vision Transformer', 'Flexible Inference']}, 'TLDR': {'value': 'Use a single ViT to represent multiple smaller variants'}, 'abstract': {'value': 'Vision Transformers (ViT) is known for its scalability. In this work, we target to scale down a ViT to fit in an environment with dynamic-changing resource constraints. We observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths. Thus, we propose a general framework, named Scala, to enable a single network to represent multiple smaller ViTs with flexible inference capability, which aligns with the inherent design of ViT to vary from widths. Concretely, Scala activates several subnets during training, introduces Isolated Activation to disentangle the smallest sub-network from other subnets, and leverages Scale Coordination to ensure each sub-network receives simplified, steady, and accurate learning objectives. Comprehensive empirical validations on different tasks demonstrate that with only one-shot training, Scala learns slimmable representation without modifying the original ViT structure and matches the performance of Separate Training. Compared with the prior art, Scala achieves an average improvement of 1.6% on ImageNet-1K with fewer parameters.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d586f3321f7b5f7435024391bab2f98aeaac3132.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024slicing,\\ntitle={Slicing Vision Transformer for Flexibile Inference},\\nauthor={Yitian Zhang and Huseyin Coskun and Xu Ma and Huan Wang and Ke Ma and Stephen Xi Chen and Derek Hao Hu and Yun Fu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zJNSbgl4UA}\\n}'}, 'paperhash': {'value': 'zhang|slicing_vision_transformer_for_flexible_inference'}},forum = 'zJNSbgl4UA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5417/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5417/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5417/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5417/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zIr2QjU4hl',number = 12648,cdate = 1715725730593,pdate = 1727288013223,odate = 1730873950756,mdate = 1731570762196,tcdate = 1715725730593,tmdate = 1731570762196,ddate = None,content = {'title': {'value': 'Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models'}, 'authors': {'value': ['Masatoshi Uehara', 'Yulai Zhao', 'Ehsan Hajiramezanali', 'Gabriele Scalia', 'Gökcen Eraslan', 'Avantika Lal', 'Sergey Levine', 'Tommaso Biancalani']}, 'authorids': {'value': ['~Masatoshi_Uehara1', '~Yulai_Zhao1', '~Ehsan_Hajiramezanali1', '~Gabriele_Scalia1', '~Gökcen_Eraslan1', '~Avantika_Lal1', '~Sergey_Levine1', '~Tommaso_Biancalani1']}, 'keywords': {'value': ['Diffusion models', 'Reinforcement learning']}, 'TLDR': {'value': 'Conservative Fine-Tuning of Diffusion Models from Offline Data'}, 'abstract': {'value': 'AI-driven design problems, such as DNA/protein sequence design, are commonly tackled from two angles: generative modeling, which efficiently captures the feasible design space (e.g., natural images or biological sequences), and model-based optimization, which utilizes reward models for extrapolation. To combine the strengths of both approaches, we adopt a hybrid method that fine-tunes cutting-edge diffusion models by optimizing reward models through RL. Although prior work has explored similar avenues, they primarily focus on scenarios where accurate reward models are accessible. In contrast, we concentrate on an offline setting where a reward model is unknown, and we must learn from static offline datasets, a common scenario in scientific domains. In offline scenarios, existing approaches tend to suffer from overoptimization, as they may be misled by the reward model in out-of-distribution regions. To address this, we introduce a conservative fine-tuning approach, BRAID, by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.'}, 'pdf': {'value': '/pdf/208379a521961503552a6647a7533a7037e81262.pdf'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/285152ee7a6a7419534ed874d06583c7bb56f9f4.zip'}, '_bibtex': {'value': '@inproceedings{\\nuehara2024bridging,\\ntitle={Bridging Model-Based Optimization and Generative Modeling via Conservative Fine-Tuning of Diffusion Models},\\nauthor={Masatoshi Uehara and Yulai Zhao and Ehsan Hajiramezanali and Gabriele Scalia and G{\\\\\"o}kcen Eraslan and Avantika Lal and Sergey Levine and Tommaso Biancalani},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zIr2QjU4hl}\\n}'}, 'paperhash': {'value': 'uehara|bridging_modelbased_optimization_and_generative_modeling_via_conservative_finetuning_of_diffusion_models'}},forum = 'zIr2QjU4hl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12648/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12648/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12648/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12648/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zGN0YWy2he',number = 2335,cdate = 1715046669461,pdate = 1727287687424,odate = 1730873857426,mdate = 1730873857445,tcdate = 1715046669461,tmdate = 1730873857445,ddate = None,content = {'title': {'value': 'Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation'}, 'authors': {'value': ['Yunnan Wang', 'Ziqiang Li', 'Wenyao Zhang', 'Zequn Zhang', 'Baao Xie', 'Xihui Liu', 'Wenjun Zeng', 'Xin Jin']}, 'authorids': {'value': ['~Yunnan_Wang1', '~Ziqiang_Li3', '~Wenyao_Zhang1', '~Zequn_Zhang1', '~Baao_Xie3', '~Xihui_Liu1', '~Wenjun_Zeng3', '~Xin_Jin8']}, 'keywords': {'value': ['Scene Graph; Disentanglement; Diffusion Model; Compositional Image Generation']}, 'TLDR': {'value': 'In this paper, we leverage the scene graph, a powerful structured representation, for complex image generation.'}, 'abstract': {'value': 'There has been exciting progress in generating images from natural language or layout conditions. However, these methods struggle to faithfully reproduce complex scenes due to the insufficient modeling of multiple objects and their relationships. To address this issue, we leverage the scene graph, a powerful structured representation, for complex image generation. Different from the previous works that directly use scene graphs for generation, we employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. Specifically, we first propose a Semantics-Layout Variational AutoEncoder (SL-VAE) to jointly derive (layouts, semantics) from the input scene graph, which allows a more diverse and reasonable generation in a one-to-many mapping. We then develop a Compositional Masked Attention (CMA) integrated with a diffusion model, incorporating (layouts, semantics) with fine-grained attributes as generation guidance. To further achieve graph manipulation while keeping the visual content consistent, we introduce a Multi-Layered Sampler (MLS) for an \"isolated\" image editing effect. Extensive experiments demonstrate that our method outperforms recent competitors based on text, layout, or scene graph, in terms of generation rationality and controllability.'}, 'pdf': {'value': '/pdf/66bc4339c157f7e9cfc224307ac92ad79e98a4b8.pdf'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/4b7d2dd027fefb29263efc4e8b2ca9cc76f24486.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024scene,\\ntitle={Scene Graph Disentanglement and Composition for Generalizable Complex Image Generation},\\nauthor={Yunnan Wang and Ziqiang Li and Wenyao Zhang and Zequn Zhang and Baao Xie and Xihui Liu and Wenjun Zeng and Xin Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zGN0YWy2he}\\n}'}, 'paperhash': {'value': 'wang|scene_graph_disentanglement_and_composition_for_generalizable_complex_image_generation'}},forum = 'zGN0YWy2he',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2335/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2335/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2335/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2335/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zDaD8zv8tG',number = 20239,cdate = 1715795270835,pdate = 1727288228739,odate = 1730874000020,mdate = 1736943662167,tcdate = 1715795270835,tmdate = 1736943662167,ddate = None,content = {'title': {'value': 'A teacher-teacher framework for clinical language representation learning'}, 'authors': {'value': ['Feiqing Huang', 'Shenghan Zhang', 'Sara Morini Sweet', 'Tianxi Cai']}, 'authorids': {'value': ['~Feiqing_Huang1', '~Shenghan_Zhang2', '~Sara_Morini_Sweet1', '~Tianxi_Cai1']}, 'keywords': {'value': ['clinical language models', 'teacher-teacher framework', 'knowledge alignment']}, 'abstract': {'value': 'In recent years, there has been a proliferation of ready-to-use large language models (LLMs) designed for various applications, both general-purpose and domain-specific. Instead of advocating for the development of a new model or continuous pretraining of an existing one, this paper introduces a pragmatic teacher-teacher framework to facilitate mutual learning between two pre-existing models.\\nBy leveraging two teacher models possessing complementary knowledge, we introduce a LIghtweight kNowledge alignmEnt (LINE) module aimed at harmonizing their knowledge within a unified representation space. This framework is particularly valuable in clinical settings, where stringent regulations and privacy considerations dictate the handling of detailed clinical notes. Our trained LINE module excels in capturing critical information from clinical notes, leveraging highly de-identified data. Validation and downstream tasks further demonstrate the effectiveness of the proposed framework.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper introduces a teacher-teacher paradigm where two pretrained LLMs achieve knowledge exchange and alignment through a two-step, few-epoch training of the LINE module with a well-designed alignment objective.'}, 'pdf': {'value': '/pdf/897ef6718180dabcd9f755adb58c00cb26513df5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024a,\\ntitle={A teacher-teacher framework for clinical language representation learning},\\nauthor={Feiqing Huang and Shenghan Zhang and Sara Morini Sweet and Tianxi Cai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zDaD8zv8tG}\\n}'}, 'paperhash': {'value': 'huang|a_teacherteacher_framework_for_clinical_language_representation_learning'}},forum = 'zDaD8zv8tG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20239/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20239/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20239/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20239/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zBMKodNgKX',number = 12078,cdate = 1715715959328,pdate = 1727287993739,odate = 1730873945206,mdate = 1730873945224,tcdate = 1715715959328,tmdate = 1730873945224,ddate = None,content = {'title': {'value': 'FedNE: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction'}, 'authors': {'value': ['Ziwei Li', 'Xiaoqi Wang', 'Hong-You Chen', 'Han Wei Shen', 'Wei-Lun Chao']}, 'authorids': {'value': ['~Ziwei_Li3', '~Xiaoqi_Wang2', '~Hong-You_Chen1', '~Han_Wei_Shen1', '~Wei-Lun_Chao1']}, 'keywords': {'value': ['Federated Learning', 'Dimensionality Reduction', 'Unsupervised Learning', 'Representation Learning']}, 'abstract': {'value': 'Federated learning (FL) has rapidly evolved as a promising paradigm that enables collaborative model training across distributed participants without exchanging their local data. Despite its broad applications in fields such as computer vision, graph learning, and natural language processing, the development of a data projection model that can be effectively used to visualize data in the context of FL is crucial yet remains heavily under-explored. Neighbor embedding (NE) is an essential technique for visualizing complex high-dimensional data, but collaboratively learning a joint NE model is difficult. The key challenge lies in the objective function, as effective visualization algorithms like NE require computing loss functions among pairs of data. \\nIn this paper, we introduce \\\\textsc{FedNE}, a novel approach that integrates the \\\\textsc{FedAvg} framework with the contrastive NE technique, without any requirements of shareable data. To address the lack of inter-client repulsion which is crucial for the alignment in the global embedding space, we develop a surrogate loss function that each client learns and shares with each other. Additionally, we propose a data-mixing strategy to augment the local data, aiming to relax the problems of invisible neighbors and false neighbors constructed by the local $k$NN graphs. We conduct comprehensive experiments on both synthetic and real-world datasets. The results demonstrate that our \\\\textsc{FedNE} can effectively preserve the neighborhood data structures and enhance the alignment in the global embedding space compared to several baseline methods.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/713ead3a7d3c84218a49bae4d46cdf7a3a34d042.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024fedne,\\ntitle={Fed{NE}: Surrogate-Assisted Federated Neighbor Embedding for Dimensionality Reduction},\\nauthor={Ziwei Li and Xiaoqi Wang and Hong-You Chen and Han Wei Shen and Wei-Lun Chao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zBMKodNgKX}\\n}'}, 'paperhash': {'value': 'li|fedne_surrogateassisted_federated_neighbor_embedding_for_dimensionality_reduction'}},forum = 'zBMKodNgKX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12078/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12078/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12078/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12078/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zBG7WogAvm',number = 4055,cdate = 1715353002930,pdate = 1727287738721,odate = 1730873871819,mdate = 1735831662011,tcdate = 1715353002930,tmdate = 1735831662011,ddate = None,content = {'title': {'value': 'Amortized Bayesian Experimental Design for Decision-Making'}, 'authors': {'value': ['Daolang Huang', 'Yujia Guo', 'Luigi Acerbi', 'Samuel Kaski']}, 'authorids': {'value': ['~Daolang_Huang1', '~Yujia_Guo1', '~Luigi_Acerbi1', '~Samuel_Kaski1']}, 'keywords': {'value': ['Bayesian experimental design', 'amortized inference', 'Bayesian decision theory', 'neural processes']}, 'abstract': {'value': 'Many critical decisions, such as personalized medical diagnoses and product pricing, are made based on insights gained from designing, observing, and analyzing a series of experiments. This highlights the crucial role of experimental design, which goes beyond merely collecting information on system parameters as in traditional Bayesian experimental design (BED), but also plays a key part in facilitating downstream decision-making. Most recent BED methods use an amortized policy network to rapidly design experiments. However, the information gathered through these methods is suboptimal for down-the-line decision-making, as the experiments are not inherently designed with downstream objectives in mind. In this paper, we present an amortized decision-aware BED framework that prioritizes maximizing downstream decision utility. We introduce a novel architecture, the Transformer Neural Decision Process (TNDP), capable of instantly proposing the next experimental design, whilst inferring the downstream decision, thus effectively amortizing both tasks within a unified workflow. We demonstrate the performance of our method across several tasks, showing that it can deliver informative designs and facilitate accurate decision-making.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a decision-aware amortized Bayesian experimental design framework with a novel Transformer neural decision process architecture to optimize experimental designs for better decision-making.'}, 'pdf': {'value': '/pdf/67b2e48fbef5361774799536072d5907137d322c.pdf'}, 'supplementary_material': {'value': '/attachment/ae909fba2c1e3b7eb4782a71b9b094cfc7c180b9.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024amortized,\\ntitle={Amortized Bayesian Experimental Design for Decision-Making},\\nauthor={Daolang Huang and Yujia Guo and Luigi Acerbi and Samuel Kaski},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zBG7WogAvm}\\n}'}, 'paperhash': {'value': 'huang|amortized_bayesian_experimental_design_for_decisionmaking'}},forum = 'zBG7WogAvm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4055/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4055/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4055/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4055/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'zAuerb1KGx',number = 20754,cdate = 1715798196273,pdate = 1727288241008,odate = 1730874003136,mdate = 1730874003147,tcdate = 1715798196273,tmdate = 1730874003147,ddate = None,content = {'title': {'value': 'Multi-Label Learning with Stronger Consistency Guarantees'}, 'authors': {'value': ['Anqi Mao', 'Mehryar Mohri', 'Yutao Zhong']}, 'authorids': {'value': ['~Anqi_Mao1', '~Mehryar_Mohri2', '~Yutao_Zhong1']}, 'keywords': {'value': ['multi-label learning', 'consistency', 'surrogate loss', 'hamming loss', 'learning theory']}, 'abstract': {'value': 'We present a detailed study of surrogate losses and algorithms for multi-label learning, supported by $H$-consistency bounds. We first show that, for the simplest form of multi-label loss (the popular Hamming loss), the well-known consistent binary relevance surrogate suffers from a sub-optimal dependency on the number of labels in terms of $H$-consistency bounds, when using smooth losses such as logistic losses. Furthermore, this loss function fails to account for label correlations. To address these drawbacks, we introduce a novel surrogate loss, *multi-label logistic loss*,  that accounts for label correlations and benefits from label-independent $H$-consistency bounds. We then broaden our analysis to cover a more extensive family of multi-label losses, including all common ones and a new extension defined based on linear-fractional functions with respect to the confusion matrix. We also extend our multi-label logistic losses to more comprehensive multi-label comp-sum losses, adapting comp-sum losses from standard classification to the multi-label learning. We prove that this family of surrogate losses benefits from $H$-consistency bounds, and thus Bayes-consistency, across any general multi-label loss. Our work thus proposes a unified surrogate loss framework benefiting from strong consistency guarantees for any multi-label loss, significantly expanding upon previous work which only established Bayes-consistency and for specific loss functions. Additionally, we adapt constrained losses from standard classification to multi-label constrained losses in a similar way, which also benefit from $H$-consistency bounds and thus Bayes-consistency for any multi-label loss. We further describe efficient gradient computation algorithms for minimizing the multi-label logistic loss.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/238907b99661ce02cecd832f90a1e415af69d730.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmao2024multilabel,\\ntitle={Multi-Label Learning with Stronger Consistency Guarantees},\\nauthor={Anqi Mao and Mehryar Mohri and Yutao Zhong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zAuerb1KGx}\\n}'}, 'paperhash': {'value': 'mao|multilabel_learning_with_stronger_consistency_guarantees'}},forum = 'zAuerb1KGx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20754/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20754/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20754/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20754/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'zApFYcLg6K',number = 14763,cdate = 1715753440811,pdate = 1727288078720,odate = 1730873968301,mdate = 1730873968453,tcdate = 1715753440811,tmdate = 1730873968453,ddate = None,content = {'title': {'value': 'On Differentially Private U Statistics'}, 'authors': {'value': ['Kamalika Chaudhuri', 'Po-Ling Loh', 'Shourya Pandey', 'Purnamrita Sarkar']}, 'authorids': {'value': ['~Kamalika_Chaudhuri1', '~Po-Ling_Loh2', '~Shourya_Pandey1', '~Purnamrita_Sarkar1']}, 'keywords': {'value': ['Differential Privacy', 'Statistics', 'Mean Estimation']}, 'TLDR': {'value': 'We devise efficient algorithms for differentially private U-statistics in the Central DP model, achieving nearly optimal error rates in various settings. Previously, this was studied in the local DP model and with U-statistics of degree 2.'}, 'abstract': {'value': 'We consider the problem of privately estimating a parameter $\\\\mathbb{E}[h(X_1,\\\\dots,X_k)]$, where $X_1$, $X_2$, $\\\\dots$, $X_k$ are i.i.d. data from some distribution and $h$ is a permutation-invariant function. Without privacy constraints, the standard estimators for this task are U-statistics, which commonly arise in a wide range of problems, including nonparametric signed rank tests, symmetry testing, uniformity testing, and subgraph counts in random networks, and are the unique minimum variance unbiased estimators under mild conditions. Despite the recent outpouring of interest in private mean estimation, privatizing U-statistics has received little attention. While existing private mean estimation algorithms can be applied in a black-box manner to obtain confidence intervals, we show that they can lead to suboptimal private error, e.g., constant-factor inflation in the leading term, or even $\\\\Theta(1/n)$ rather than $O(1/n^2)$ in degenerate settings. To remedy this, we propose a new thresholding-based approach that reweights different subsets of the data using _local Hájek projections_. This leads to nearly optimal private error for non-degenerate U-statistics and a strong indication of near-optimality for degenerate U-statistics.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a34864164d8a57ea128abf709b53e25e642b7a1b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchaudhuri2024on,\\ntitle={On Differentially Private U Statistics},\\nauthor={Kamalika Chaudhuri and Po-Ling Loh and Shourya Pandey and Purnamrita Sarkar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zApFYcLg6K}\\n}'}, 'paperhash': {'value': 'chaudhuri|on_differentially_private_u_statistics'}},forum = 'zApFYcLg6K',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14763/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14763/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14763/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14763/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z86knmjoUq',number = 10243,cdate = 1715692119160,pdate = 1727287933668,odate = 1730873927180,mdate = 1730873927211,tcdate = 1715692119160,tmdate = 1730873927211,ddate = None,content = {'title': {'value': 'PURE: Prompt Evolution with Graph ODE for Out-of-distribution Fluid Dynamics Modeling'}, 'authors': {'value': ['Hao Wu', 'Changhu Wang', 'Fan Xu', 'Jinbao Xue', 'Chong Chen', 'Xian-Sheng Hua', 'Xiao Luo']}, 'authorids': {'value': ['~Hao_Wu39', '~Changhu_Wang4', '~Fan_Xu5', '~Jinbao_Xue2', '~Chong_Chen2', '~Xian-Sheng_Hua1', '~Xiao_Luo3']}, 'keywords': {'value': ['AI for Science']}, 'abstract': {'value': 'This work studies the problem of out-of-distribution fluid dynamics modeling. Previous works usually design effective neural operators to learn from mesh-based data structures. However, in real-world applications, they would suffer from distribution shifts from the variance of system parameters and \\ntemporal evolution of the dynamical system. In this paper, we propose a novel approach named \\\\underline{P}rompt Evol\\\\underline{u}tion with G\\\\underline{r}aph OD\\\\underline{E} (\\\\method{}) for out-of-distribution fluid dynamics modeling. The core of our \\\\method{} is to learn time-evolving prompts using a graph ODE to adapt spatio-temporal forecasting models to different scenarios. In particular, our \\\\method{} first learns from historical observations and system parameters in the frequency domain to explore multi-view context information, which could effectively initialize prompt embeddings. More importantly, we incorporate the interpolation of observation sequences into a graph ODE, which can capture the temporal evolution of prompt embeddings for model adaptation. These time-evolving prompt embeddings are then incorporated into basic forecasting models to overcome temporal distribution shifts. We also minimize the mutual information between prompt embeddings and observation embeddings to enhance the robustness of our model to different distributions. Extensive experiments on various benchmark datasets validate the superiority of the proposed \\\\method{} in comparison to various baselines.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c8b66405bc52ba1031d1591eec96d618612ff575.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024pure,\\ntitle={{PURE}: Prompt Evolution with Graph {ODE} for Out-of-distribution Fluid Dynamics Modeling},\\nauthor={Hao Wu and Changhu Wang and Fan Xu and Jinbao Xue and Chong Chen and Xian-Sheng Hua and Xiao Luo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z86knmjoUq}\\n}'}, 'paperhash': {'value': 'wu|pure_prompt_evolution_with_graph_ode_for_outofdistribution_fluid_dynamics_modeling'}},forum = 'z86knmjoUq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10243/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10243/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10243/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10243/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z7h7zMgyPJ',number = 8851,cdate = 1715670531998,pdate = 1727287892745,odate = 1730873915723,mdate = 1730873915735,tcdate = 1715670531998,tmdate = 1730873915735,ddate = None,content = {'title': {'value': 'The Many Faces of Optimal Weak-to-Strong Learning'}, 'authors': {'value': ['Mikael Møller Høgsgaard', 'Kasper Green Larsen', 'Markus Engelund Mathiasen']}, 'authorids': {'value': ['~Mikael_Møller_Høgsgaard1', '~Kasper_Green_Larsen1', '~Markus_Engelund_Mathiasen1']}, 'keywords': {'value': ['Learning Theory', 'Weak to Strong Learning', 'Boosting', 'Large Margin Classifiers', 'Generalization Bounds', 'Sample Complexity']}, 'abstract': {'value': 'Boosting is an extremely successful idea, allowing one to combine multiple low accuracy classifiers into a much more accurate voting classifier. In this work, we present a new and surprisingly simple Boosting algorithm that obtains a provably optimal sample complexity. Sample optimal Boosting algorithms have only recently been developed, and our new algorithm has the fastest runtime among all such algorithms and is the simplest to describe: Partition your training data into 5 disjoint pieces of equal size, run AdaBoost on each, and combine the resulting classifiers via a majority vote. In addition to this theoretical contribution, we also perform the first empirical comparison of the proposed sample optimal Boosting algorithms. Our pilot empirical study suggests that our new algorithm might outperform previous algorithms on large data sets.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/9a9811aaf9480e6aee3ebb3c8c9c52b771282c85.zip'}, 'TLDR': {'value': 'We propose a new, simpler, faster and optimal boosting algorithm in terms of sample complexity'}, 'pdf': {'value': '/pdf/2671bac5a8424c491302de1f86dcfa5df321520d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nh{\\\\o}gsgaard2024the,\\ntitle={The Many Faces of Optimal Weak-to-Strong Learning},\\nauthor={Mikael M{\\\\o}ller H{\\\\o}gsgaard and Kasper Green Larsen and Markus Engelund Mathiasen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z7h7zMgyPJ}\\n}'}, 'paperhash': {'value': 'høgsgaard|the_many_faces_of_optimal_weaktostrong_learning'}},forum = 'z7h7zMgyPJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8851/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8851/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8851/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8851/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z6reLFqv6w',number = 20568,cdate = 1715797143336,pdate = 1727288236723,odate = 1730874002092,mdate = 1730874002111,tcdate = 1715797143336,tmdate = 1730874002111,ddate = None,content = {'title': {'value': 'Learning diverse causally emergent representations from time series data'}, 'authors': {'value': ['David McSharry', 'Christos Kaplanis', 'Fernando E Rosas', 'Pedro A. M. Mediano']}, 'authorids': {'value': ['~David_McSharry1', '~Christos_Kaplanis2', '~Fernando_E_Rosas1', '~Pedro_A._M._Mediano1']}, 'keywords': {'value': ['emergence', 'representation learning']}, 'TLDR': {'value': 'learning emergent features using representation learning'}, 'abstract': {'value': 'Cognitive processes usually take place at a macroscopic scale in systems characterised by emergent properties, which make the whole ‘more than the sum of its parts.’ While recent proposals have provided quantitative, information-theoretic metrics to detect emergence in time series data, it is often highly non-trivial to identify the relevant macroscopic variables a priori. In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find emergent variables. The proposed method successfully detects emergent variables and recovers the ground-truth emergence values in a synthetic dataset. Furthermore, we show the method can be extended to learn multiple independent features, extracting a diverse set of emergent quantities. We finally show that a modified method scales to real experimental data from primate brain activity, paving the ground for future analyses uncovering the emergent structure of cognitive representations in biological and artificial intelligence systems.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8fbf255282581472847dd90c5114aca7f4d35e2d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmcsharry2024learning,\\ntitle={Learning diverse causally emergent representations from time series data},\\nauthor={David McSharry and Christos Kaplanis and Fernando E Rosas and Pedro A. M. Mediano},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z6reLFqv6w}\\n}'}, 'paperhash': {'value': 'mcsharry|learning_diverse_causally_emergent_representations_from_time_series_data'}},forum = 'z6reLFqv6w',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20568/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20568/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20568/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20568/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z6KNvOe9zQ',number = 5210,cdate = 1715514804744,pdate = 1727287776957,odate = 1730873882854,mdate = 1734710211884,tcdate = 1715514804744,tmdate = 1734710211884,ddate = None,content = {'title': {'value': 'Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning'}, 'authors': {'value': ['Chenyu Yang', 'Xizhou Zhu', 'Jinguo Zhu', 'Weijie Su', 'Junjie Wang', 'Xuan Dong', 'Wenhai Wang', 'Bin Li', 'Jie Zhou', 'Yu Qiao', 'Jifeng Dai']}, 'authorids': {'value': ['~Chenyu_Yang1', '~Xizhou_Zhu1', '~Jinguo_Zhu1', '~Weijie_Su2', '~Junjie_Wang8', '~Xuan_Dong5', '~Wenhai_Wang2', '~Bin_Li8', '~Jie_Zhou3', '~Yu_Qiao1', '~Jifeng_Dai1']}, 'keywords': {'value': ['vision pre-training', 'compression learning', 'interleaved image-text data']}, 'abstract': {'value': 'Recently, vision model pre-training has evolved from relying on manually annotated datasets to leveraging large-scale, web-crawled image-text data. Despite these advances, there is no pre-training method that effectively exploits the interleaved image-text data, which is very prevalent on the Internet. Inspired by the recent success of compression learning in natural language processing, we propose a novel vision model pre-training method called Latent Compression Learning (LCL) for interleaved image-text data. This method performs latent compression learning by maximizing the mutual information between the inputs and outputs of a causal attention model. The training objective can be decomposed into two basic tasks: 1) contrastive learning between visual representation and preceding context, and 2) generating subsequent text based on visual representation. Our experiments demonstrate that our method not only matches the performance of CLIP on paired pre-training datasets (e.g., LAION), but can also leverage interleaved pre-training data (e.g., MMC4) to learn robust visual representations from scratch, showcasing the potential of vision model pre-training with interleaved image-text data.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d285c73049fe1865644d41380d358b90dcf98a21.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024vision,\\ntitle={Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning},\\nauthor={Chenyu Yang and Xizhou Zhu and Jinguo Zhu and Weijie Su and Junjie Wang and Xuan Dong and Wenhai Wang and Bin Li and Jie Zhou and Yu Qiao and Jifeng Dai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z6KNvOe9zQ}\\n}'}, 'paperhash': {'value': 'yang|vision_model_pretraining_on_interleaved_imagetext_data_via_latent_compression_learning'}},forum = 'z6KNvOe9zQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5210/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5210/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5210/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5210/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z4eVwH484M',number = 1294,cdate = 1714546163529,pdate = 1727287656792,odate = 1730873847181,mdate = 1730873847202,tcdate = 1714546163529,tmdate = 1730873847202,ddate = None,content = {'title': {'value': 'Unveiling the Hidden: Online Vectorized HD Map Construction with Clip-Level Token Interaction and Propagation'}, 'authors': {'value': ['Nayeon Kim', 'Hongje Seong', 'Daehyun Ji', 'Sujin Jang']}, 'authorids': {'value': ['~Nayeon_Kim5', '~Hongje_Seong1', '~Daehyun_Ji1', '~Sujin_Jang2']}, 'keywords': {'value': ['vectorized HD map', 'clip-level pipeline', 'clip-level token', 'interaction', 'propagation']}, 'TLDR': {'value': 'This paper introduces a novel clip-level pipeline to explicitly unveils the invisible map elements.'}, 'abstract': {'value': 'Predicting and constructing road geometric information (e.g., lane lines, road markers) is a crucial task for safe autonomous driving, while such static map elements can be repeatedly occluded by various dynamic objects on the road. Recent studies have shown significantly improved vectorized high-definition (HD) map construction performance, but there has been insufficient investigation of temporal information across adjacent input frames (i.e., clips), which may lead to inconsistent and suboptimal prediction results. To tackle this, we introduce a novel paradigm of clip-level vectorized HD map construction, MapUnveiler, which explicitly unveils the occluded map elements within a clip input by relating dense image representations with efficient clip tokens. Additionally, MapUnveiler associates inter-clip information through clip token propagation, effectively utilizing long- term temporal map information. MapUnveiler runs efficiently with the proposed clip-level pipeline by avoiding redundant computation with temporal stride while building a global map relationship. Our extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on both the nuScenes and Argoverse2 benchmark datasets. We also showcase that MapUnveiler significantly outperforms state-of-the-art approaches in a challenging setting, achieving +10.7% mAP improvement in heavily occluded driving road scenes. The project page can be found at https://mapunveiler.github.io.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/7fdf9cf2a030d124f39ce6c0a6575eea1927c98c.zip'}, 'pdf': {'value': '/pdf/3a7293ee08a835f73148a1a4ebcffbb2a8b5e0a6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkim2024unveiling,\\ntitle={Unveiling the Hidden: Online Vectorized {HD} Map Construction with Clip-Level Token Interaction and Propagation},\\nauthor={Nayeon Kim and Hongje Seong and Daehyun Ji and Sujin Jang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z4eVwH484M}\\n}'}, 'paperhash': {'value': 'kim|unveiling_the_hidden_online_vectorized_hd_map_construction_with_cliplevel_token_interaction_and_propagation'}},forum = 'z4eVwH484M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1294/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1294/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1294/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1294/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'z4duW3KzlD',number = 14364,cdate = 1715749130155,pdate = 1727288067336,odate = 1730873965217,mdate = 1730873965231,tcdate = 1715749130155,tmdate = 1730873965231,ddate = None,content = {'title': {'value': 'Gated Inference Network: Inference and Learning State-Space Models'}, 'authors': {'value': ['Hamidreza Hashempoor', 'Wan Choi']}, 'authorids': {'value': ['~Hamidreza_Hashempoor1', '~Wan_Choi1']}, 'keywords': {'value': ['Time Series and Recurrent Networks']}, 'TLDR': {'value': 'Our algorithm efficiently models dynamical systems by observing high-dimensional noise-affected data, outperforming state-of-the-art counterparts in state estimation and image imputation tasks.'}, 'abstract': {'value': 'This paper advances temporal reasoning within dynamically changing high-dimensional noisy observations, focusing on a latent space that characterizes the nonlinear dynamics of objects in their environment. We introduce the *Gated Inference Network* (GIN), an efficient approximate Bayesian inference algorithm for state space models (SSMs) with nonlinear state transitions and emissions. GIN disentangles two latent representations: one representing the object derived from a nonlinear mapping model, and another representing the latent state describing its dynamics. This disentanglement enables direct state estimation and missing data imputation as the world evolves. To infer the latent state, we utilize a deep extended Kalman filter (EKF) approach that integrates a novel compact RNN structure to compute both the Kalman Gain (KG) and smoothing gain (SG), completing the data flow. This design results in a computational cost per step that is linearly faster than EKF but introduces issues such as the exploding gradient problem. To mitigate the exploding gradients caused by the compact RNN structure in our model, we propose a specialized learning method that ensures stable training and inference. The model is then trained end-to-end on videos depicting a diverse range of simulated and real-world physical systems, and outperforms its ounterparts —RNNs, autoregressive models, and variational approaches— in state estimation and missing data imputation tasks.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b60e4a43045e2eeab67a3948852e749fe455f871.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhashempoor2024gated,\\ntitle={Gated Inference Network: Inference and Learning State-Space Models},\\nauthor={Hamidreza Hashempoor and Wan Choi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z4duW3KzlD}\\n}'}, 'paperhash': {'value': 'hashempoor|gated_inference_network_inference_and_learning_statespace_models'}},forum = 'z4duW3KzlD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14364/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14364/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14364/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14364/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'z4FaPUslma',number = 13678,cdate = 1715742013568,pdate = 1727288046501,odate = 1730873959596,mdate = 1730873959615,tcdate = 1715742013568,tmdate = 1730873959615,ddate = None,content = {'title': {'value': 'Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame'}, 'authors': {'value': ['Evan Markou', 'Thalaiyasingam Ajanthan', 'Stephen Gould']}, 'authorids': {'value': ['~Evan_Markou1', '~Thalaiyasingam_Ajanthan1', '~Stephen_Gould1']}, 'keywords': {'value': ['neural collapse', 'equiangular tight frames', 'Riemannian optimisation', 'deep learning']}, 'abstract': {'value': 'Neural Collapse (NC) is a recently observed phenomenon in neural networks that characterises the solution space of the final classifier layer when trained until zero training loss. Specifically, NC suggests that the final classifier layer converges to a Simplex Equiangular Tight Frame (ETF), which maximally separates the weights corresponding to each class. By duality, the penultimate layer feature means also converge to the same simplex ETF. Since this simple symmetric structure is optimal, our idea is to utilise this property to improve convergence speed. Specifically, we introduce the notion of \\\\textit{nearest simplex ETF geometry} for the penultimate layer features at any given training iteration, by formulating it as a Riemannian optimisation. Then, at each iteration, the classifier weights are implicitly set to the nearest simplex ETF by solving this inner-optimisation, which is encapsulated within a declarative node to allow backpropagation. Our experiments on synthetic and real-world architectures on classification tasks demonstrate that our approach accelerates convergence and enhances training stability.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ac02c11fa162633bf19fadb27beddf13e3c58e97.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmarkou2024guiding,\\ntitle={Guiding Neural Collapse: Optimising Towards the Nearest Simplex Equiangular Tight Frame},\\nauthor={Evan Markou and Thalaiyasingam Ajanthan and Stephen Gould},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z4FaPUslma}\\n}'}, 'paperhash': {'value': 'markou|guiding_neural_collapse_optimising_towards_the_nearest_simplex_equiangular_tight_frame'}},forum = 'z4FaPUslma',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13678/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13678/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13678/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13678/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z2739hYuR3',number = 14977,cdate = 1715755839916,pdate = 1727288084361,odate = 1730873969662,mdate = 1736994725672,tcdate = 1715755839916,tmdate = 1736994725672,ddate = None,content = {'title': {'value': 'Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation'}, 'authors': {'value': ['Long-Fei Li', 'Yu-Jie Zhang', 'Peng Zhao', 'Zhi-Hua Zhou']}, 'authorids': {'value': ['~Long-Fei_Li1', '~Yu-Jie_Zhang1', '~Peng_Zhao1', '~Zhi-Hua_Zhou2']}, 'keywords': {'value': ['MNL function approximation', 'regret analysis', 'reinforcement learning', 'Markov decision process']}, 'abstract': {'value': 'We study a new class of MDPs that employs multinomial logit (MNL) function approximation to ensure valid probability distributions over the state space. Despite its significant benefits, incorporating the non-linear function raises substantial challenges in both *statistical* and *computational* efficiency. The best-known result of Hwang and Oh [2023] has achieved an $\\\\widetilde{\\\\mathcal{O}}(\\\\kappa^{-1}dH^2\\\\sqrt{K})$ regret upper bound, where $\\\\kappa$ is a problem-dependent quantity, $d$ is the feature dimension, $H$ is the episode length, and $K$ is the number of episodes. However, we observe that $\\\\kappa^{-1}$ exhibits polynomial dependence on the number of reachable states, which can be as large as the state space size in the worst case and thus undermines the motivation for function approximation. Additionally, their method requires storing all historical data and the time complexity scales linearly with the episode count, which is computationally expensive. In this work, we propose a statistically efficient algorithm that achieves a regret of $\\\\widetilde{\\\\mathcal{O}}(dH^2\\\\sqrt{K} + \\\\kappa^{-1}d^2H^2)$, eliminating the dependence on $\\\\kappa^{-1}$ in the dominant term for the first time. We then address the computational challenges by introducing an enhanced algorithm that achieves the same regret guarantee but with only constant cost. Finally, we establish the first lower bound for this problem, justifying the optimality of our results in $d$ and $K$.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c78403d8fed222d4a519ebbc82ee3a1bdfa7dc83.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024provably,\\ntitle={Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation},\\nauthor={Long-Fei Li and Yu-Jie Zhang and Peng Zhao and Zhi-Hua Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z2739hYuR3}\\n}'}, 'paperhash': {'value': 'li|provably_efficient_reinforcement_learning_with_multinomial_logit_function_approximation'}},forum = 'z2739hYuR3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14977/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14977/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14977/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14977/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z1GwaNoGnr',number = 637,cdate = 1714015179669,pdate = 1727287642337,odate = 1730873842497,mdate = 1735007788334,tcdate = 1714015179669,tmdate = 1735007788334,ddate = None,content = {'title': {'value': 'XMask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation'}, 'authors': {'value': ['Ziyi Wang', 'Yanbo Wang', 'Xumin Yu', 'Jie Zhou', 'Jiwen Lu']}, 'authorids': {'value': ['~Ziyi_Wang3', '~Yanbo_Wang6', '~Xumin_Yu2', '~Jie_Zhou3', '~Jiwen_Lu1']}, 'keywords': {'value': ['3D open vocabulary', 'semantic segmentation', 'cross-modal learning']}, 'abstract': {'value': 'Existing methodologies in open vocabulary 3D semantic segmentation primarily concentrate on establishing a unified feature space encompassing 3D, 2D, and textual modalities. Nevertheless, traditional techniques such as global feature alignment or vision-language model distillation tend to impose only approximate correspondence, struggling notably with delineating fine-grained segmentation boundaries. To address this gap, we propose a more meticulous mask-level alignment between 3D features and the 2D-text embedding space through a cross-modal mask reasoning framework, XMask3D. In our approach, we developed a mask generator based on the denoising UNet from a pre-trained diffusion model, leveraging its capability for precise textual control over dense pixel representations and enhancing the open-world adaptability of the generated masks. We further integrate 3D global features as implicit conditions into the pre-trained 2D denoising UNet, enabling the generation of segmentation masks with additional 3D geometry awareness. Subsequently, the generated 2D masks are employed to align mask-level 3D representations with the vision-language feature space, thereby augmenting the open vocabulary capability of 3D geometry embeddings. Finally, we fuse complementary 2D and 3D mask features, resulting in competitive performance across multiple benchmarks for 3D open vocabulary semantic segmentation. Code is available at https://github.com/wangzy22/XMask3D.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a8f19e2b718d05d788970bd70a67a13e857d6c3f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024xmaskd,\\ntitle={{XM}ask3D: Cross-modal Mask Reasoning for Open Vocabulary 3D Semantic Segmentation},\\nauthor={Ziyi Wang and Yanbo Wang and Xumin Yu and Jie Zhou and Jiwen Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z1GwaNoGnr}\\n}'}, 'paperhash': {'value': 'wang|xmask3d_crossmodal_mask_reasoning_for_open_vocabulary_3d_semantic_segmentation'}},forum = 'z1GwaNoGnr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission637/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission637/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission637/-/Revision', 'NeurIPS.cc/2024/Conference/Submission637/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'z0I2SbjN0R',number = 12192,cdate = 1715717966306,pdate = 1727287998000,odate = 1730873946529,mdate = 1730873946540,tcdate = 1715717966306,tmdate = 1730873946540,ddate = None,content = {'title': {'value': 'DiffusionPDE: Generative PDE-Solving under Partial Observation'}, 'authors': {'value': ['Jiahe Huang', 'Guandao Yang', 'Zichen Wang', 'Jeong Joon Park']}, 'authorids': {'value': ['~Jiahe_Huang1', '~Guandao_Yang1', '~Zichen_Wang10', '~Jeong_Joon_Park2']}, 'keywords': {'value': ['Guided Diffusion Model', 'Partial Differential Equation', 'Sparse Observation', 'Inverse Problem']}, 'abstract': {'value': 'We introduce a general framework for solving partial differential equations (PDEs) using generative diffusion models. In particular, we focus on the scenarios where we do not have the full knowledge of the scene necessary to apply classical solvers. Most existing forward or inverse PDE approaches perform poorly when the observations on the data or the underlying coefficients are incomplete, which is a common assumption for real-world measurements. In this work, we propose DiffusionPDE that can simultaneously fill in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces. We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'DiffusionPDE solves forward and inverse PDEs from partial observations using diffusion models.'}, 'pdf': {'value': '/pdf/d12cbf722d1e7501e11593285562cb5fb783d08a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024diffusionpde,\\ntitle={Diffusion{PDE}: Generative {PDE}-Solving under Partial Observation},\\nauthor={Jiahe Huang and Guandao Yang and Zichen Wang and Jeong Joon Park},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=z0I2SbjN0R}\\n}'}, 'paperhash': {'value': 'huang|diffusionpde_generative_pdesolving_under_partial_observation'}},forum = 'z0I2SbjN0R',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12192/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12192/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12192/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12192/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yzviAnpvU6',number = 5631,cdate = 1715559054228,pdate = 1727287790240,odate = 1730873886685,mdate = 1730873886705,tcdate = 1715559054228,tmdate = 1730873886705,ddate = None,content = {'title': {'value': 'ReLIZO: Sample Reusable Linear Interpolation-based Zeroth-order Optimization'}, 'authors': {'value': ['Xiaoxing Wang', 'Xiaohan Qin', 'Xiaokang Yang', 'Junchi Yan']}, 'authorids': {'value': ['~Xiaoxing_Wang1', '~Xiaohan_Qin1', '~Xiaokang_Yang1', '~Junchi_Yan2']}, 'keywords': {'value': ['Zero-order Optimization; Linear Interpolation; Reusing Strategy;']}, 'abstract': {'value': 'Gradient estimation is critical in zeroth-order optimization methods, which aims to obtain the descent direction by sampling update directions and querying function evaluations. Extensive research has been conducted including smoothing and linear interpolation. The former methods smooth the objective function, causing a biased gradient estimation, while the latter often enjoys more accurate estimates, at the cost of large amounts of samples and queries at each iteration to update variables. This paper resorts to the linear interpolation strategy and proposes to reduce the complexity of gradient estimation by reusing queries in the prior iterations while maintaining the sample size unchanged. Specifically, we model the gradient estimation as a quadratically constrained linear program problem and manage to derive the analytical solution. It innovatively decouples the required sample size from the variable dimension without extra conditions required, making it able to leverage the queries in the prior iterations. Moreover, part of the intermediate variables that contribute to the gradient estimation can be directly indexed, significantly reducing the computation complexity. Experiments on both simulation functions and real scenarios (black-box adversarial attacks neural architecture search, and parameter-efficient fine-tuning for large language models), show its efficacy and efficiency. Our code is available at https://github.com/Thinklab-SJTU/ReLIZO.git.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fc7ea3437f516b19fb6feff0c372deeda8df7019.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024relizo,\\ntitle={Re{LIZO}: Sample Reusable Linear Interpolation-based Zeroth-order Optimization},\\nauthor={Xiaoxing Wang and Xiaohan Qin and Xiaokang Yang and Junchi Yan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yzviAnpvU6}\\n}'}, 'paperhash': {'value': 'wang|relizo_sample_reusable_linear_interpolationbased_zerothorder_optimization'}},forum = 'yzviAnpvU6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5631/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5631/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5631/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5631/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'yySpldUsU2',number = 7753,cdate = 1715636612052,pdate = 1727287857391,odate = 1730873905990,mdate = 1730873906004,tcdate = 1715636612052,tmdate = 1730873906004,ddate = None,content = {'title': {'value': 'Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization'}, 'authors': {'value': ['Dang Nguyen', 'Paymon Haddad', 'Eric Gan', 'Baharan Mirzasoleiman']}, 'authorids': {'value': ['~Dang_Nguyen2', '~Paymon_Haddad1', '~Eric_Gan1', '~Baharan_Mirzasoleiman1']}, 'keywords': {'value': ['In-distribution generalization', 'Simplicity bias', 'Data modification', 'Sharpness-aware minimization']}, 'TLDR': {'value': 'We propose a sharpness-aware motivated data modification to improve in-distribution generalization performance.'}, 'abstract': {'value': 'Can we modify the training data distribution to encourage the underlying optimization method toward finding solutions with superior generalization performance on in-distribution data? In this work, we approach this question for the first time by comparing the inductive bias of gradient descent (GD) with that of sharpness-aware minimization (SAM). By studying a two-layer CNN, we rigorously prove that SAM learns different features more uniformly, particularly in early epochs. That is, SAM is less susceptible to simplicity bias compared to GD. We also show that examples constraining features that are learned early are separable from the rest based on the model’s output. Based on this observation, we propose a method that (i) clusters examples based on the network output early in training, (ii) identifies a cluster of examples with similar network output, and (iii) upsamples the rest of examples only once to alleviate the simplicity bias. We show empirically that USEFUL effectively improves the generalization performance on the original data distribution when training with various gradient methods, including (S)GD and SAM. Notably, we demonstrate that our method can be combined with SAM variants and existing data augmentation strategies to achieve, to the best of our knowledge, state-of-the-art performance for training ResNet18 on CIFAR10, STL10, CINIC10, Tiny-ImageNet; ResNet34 on CIFAR100; and VGG19 and DenseNet121 on CIFAR10.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b07ede96cc83e38f5a579bbba4e95073247adb82.pdf'}, 'supplementary_material': {'value': '/attachment/f7b953c5405e1474bf7885c0feb466397f08fe12.zip'}, '_bibtex': {'value': '@inproceedings{\\nnguyen2024changing,\\ntitle={Changing the Training Data Distribution to Reduce Simplicity Bias Improves In-distribution Generalization},\\nauthor={Dang Nguyen and Paymon Haddad and Eric Gan and Baharan Mirzasoleiman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yySpldUsU2}\\n}'}, 'paperhash': {'value': 'nguyen|changing_the_training_data_distribution_to_reduce_simplicity_bias_improves_indistribution_generalization'}},forum = 'yySpldUsU2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7753/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7753/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7753/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7753/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yxjWAJzUyV',number = 11838,cdate = 1715711726058,pdate = 1727287985432,odate = 1730873943113,mdate = 1730873943132,tcdate = 1715711726058,tmdate = 1730873943132,ddate = None,content = {'title': {'value': 'REBEL: Reinforcement Learning via Regressing Relative Rewards'}, 'authors': {'value': ['Zhaolin Gao', 'Jonathan Daniel Chang', 'Wenhao Zhan', 'Owen Oertell', 'Gokul Swamy', 'Kianté Brantley', 'Thorsten Joachims', 'J. Andrew Bagnell', 'Jason D. Lee', 'Wen Sun']}, 'authorids': {'value': ['~Zhaolin_Gao1', '~Jonathan_Daniel_Chang1', '~Wenhao_Zhan1', '~Owen_Oertell1', '~Gokul_Swamy1', '~Kianté_Brantley2', '~Thorsten_Joachims1', '~J._Andrew_Bagnell1', '~Jason_D._Lee1', '~Wen_Sun1']}, 'keywords': {'value': ['Reinforcement Learning', 'Reinforcement Learning from Human Feedback']}, 'abstract': {'value': 'While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications, including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping), and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a *minimalist* RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the *relative reward* between two completions to a prompt in terms of the policy, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and be extended to handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally efficient than PPO. When fine-tuning Llama-3-8B-Instruct, REBEL achieves strong performance in AlpacaEval 2.0, MT-Bench, and Open LLM Leaderboard. Implementation of REBEL can be found at <https://github.com/ZhaolinGao/REBEL>, and models trained by REBEL can be found at <https://huggingface.co/Cornell-AGI>.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/593bf12fbcf5e521841f522017b13aceee20e6e5.pdf'}, 'TLDR': {'value': 'We present REBEL, a new reinforcement learning algorithm that simplifies policy optimization to regressing relative rewards, offering strong theoretical guarantees and empirical performances.'}, '_bibtex': {'value': \"@inproceedings{\\ngao2024rebel,\\ntitle={{REBEL}: Reinforcement Learning via Regressing Relative Rewards},\\nauthor={Zhaolin Gao and Jonathan Daniel Chang and Wenhao Zhan and Owen Oertell and Gokul Swamy and Kiant{\\\\'e} Brantley and Thorsten Joachims and J. Andrew Bagnell and Jason D. Lee and Wen Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yxjWAJzUyV}\\n}\"}, 'paperhash': {'value': 'gao|rebel_reinforcement_learning_via_regressing_relative_rewards'}},forum = 'yxjWAJzUyV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11838/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11838/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11838/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11838/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yxOrSmS5wR',number = 2417,cdate = 1715067490986,pdate = 1727287689854,odate = 1730873857877,mdate = 1730873857894,tcdate = 1715067490986,tmdate = 1730873857894,ddate = None,content = {'title': {'value': 'AV-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting'}, 'authors': {'value': ['Mingfei Chen', 'Eli Shlizerman']}, 'authorids': {'value': ['~Mingfei_Chen2', '~Eli_Shlizerman1']}, 'keywords': {'value': ['audio-visual', 'audio scenes reconstruction', 'spatial audio', 'point-based scene rendering']}, 'abstract': {'value': 'We propose a novel approach for rendering high-quality spatial audio for 3D scenes that is in synchrony with the visual stream but does not rely or explicitly conditioned on the visual rendering. We demonstrate that such an approach enables the experience of immersive virtual tourism - performing a real-time dynamic navigation within the scene, experiencing both audio and visual content. Current audio-visual rendering approaches typically rely on visual cues, such as images, and thus visual artifacts could cause inconsistency in the audio quality. Furthermore, when such approaches are incorporated with visual rendering, audio generation at each viewpoint occurs after the rendering of the image of the viewpoint and thus could lead to audio lag that affects the integration of audio and visual streams. Our proposed approach, AV-Cloud, overcomes these challenges by learning the representation of the audio-visual scene based on a set of sparse AV anchor points, that constitute the Audio-Visual Cloud, and are derived from the camera calibration. The Audio-Visual Cloud serves as an audio-visual representation from which the generation of spatial audio for arbitrary listener location can be generated. In particular, we propose a novel module Audio-Visual Cloud Splatting which decodes AV anchor points into a spatial audio transfer function for the arbitrary viewpoint of the target listener. This function, applied through the Spatial Audio Render Head module, transforms monaural input into viewpoint-specific spatial audio. As a result, AV-Cloud efficiently renders the spatial audio aligned with any visual viewpoint and eliminates the need for pre-rendered images. We show that AV-Cloud surpasses current state-of-the-art accuracy on audio reconstruction, perceptive quality, and acoustic effects on two real-world datasets. AV-Cloud also outperforms previous methods when tested on scenes \"in the wild\".'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ddf8493e6466b57d0cba1ab7e5d9b9b4ab8c6d6d.pdf'}, 'supplementary_material': {'value': '/attachment/5f0a6a5f67f28a0bc37758f85bcb0e82c5f5fa80.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024avcloud,\\ntitle={{AV}-Cloud: Spatial Audio Rendering Through Audio-Visual Cloud Splatting},\\nauthor={Mingfei Chen and Eli Shlizerman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yxOrSmS5wR}\\n}'}, 'TLDR': {'value': 'We propose a novel approach, AV-Cloud, for rendering high-quality spatial audio in 3D scenes that is in synchrony with the visual stream but does not rely or explicitly conditioned on the visual rendering.'}, 'paperhash': {'value': 'chen|avcloud_spatial_audio_rendering_through_audiovisual_cloud_splatting'}},forum = 'yxOrSmS5wR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2417/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2417/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2417/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2417/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ywEQkCmImh',number = 5315,cdate = 1715523151968,pdate = 1727287780171,odate = 1730873883777,mdate = 1730873883801,tcdate = 1715523151968,tmdate = 1730873883801,ddate = None,content = {'title': {'value': 'Towards Multi-Domain Learning for Generalizable Video Anomaly Detection'}, 'authors': {'value': ['MyeongAh Cho', 'Taeoh Kim', 'Minho Shim', 'Dongyoon Wee', 'Sangyoun Lee']}, 'authorids': {'value': ['~MyeongAh_Cho1', '~Taeoh_Kim2', '~Minho_Shim1', '~Dongyoon_Wee1', '~Sangyoun_Lee1']}, 'keywords': {'value': ['Video anomaly detection', 'Multi-domain learning', 'Domain generalization']}, 'abstract': {'value': 'Most of the existing Video Anomaly Detection (VAD) studies have been conducted within single-domain learning, where training and evaluation are performed on a single dataset. However, the criteria for abnormal events differ across VAD datasets, making it problematic to apply a single-domain model to other domains. In this paper, we propose a new task called Multi-Domain learning forVAD (MDVAD) to explore various real-world abnormal events using multiple datasets for a general model. MDVAD involves training on datasets from multiple domains simultaneously, and we experimentally observe that Abnormal Conflicts between domains hinder learning and generalization. The task aims to address two key objectives: (i) better distinguishing between general normal and abnormal events across multiple domains, and (ii) being aware of ambiguous abnormal conflicts. This paper is the first to tackle abnormal conflict issue and introduces a new benchmark, baselines, and evaluation protocols for MDVAD. As baselines, we propose a framework with Null(Angular)-Multiple Instance Learning and an Abnormal Conflict classifier. Through experiments on a MDVAD benchmark composed of six VAD datasets and using four different evaluation protocols, we reveal abnormal conflicts and demonstrate that the proposed baseline effectively handles these conflicts, showing robustness and adaptability across multiple domains.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e8a9752b43978f5dd7a7f88fc83f109cdef34692.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncho2024towards,\\ntitle={Towards Multi-Domain Learning for Generalizable Video Anomaly Detection},\\nauthor={MyeongAh Cho and Taeoh Kim and Minho Shim and Dongyoon Wee and Sangyoun Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ywEQkCmImh}\\n}'}, 'paperhash': {'value': 'cho|towards_multidomain_learning_for_generalizable_video_anomaly_detection'}},forum = 'ywEQkCmImh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5315/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5315/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5315/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5315/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yvUHnBkCzd',number = 12625,cdate = 1715725325030,pdate = 1727288012470,odate = 1730873950519,mdate = 1730873950535,tcdate = 1715725325030,tmdate = 1730873950535,ddate = None,content = {'title': {'value': 'Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning'}, 'authors': {'value': ['Pouya M. Ghari', 'Yanning Shen']}, 'authorids': {'value': ['~Pouya_M._Ghari1', '~Yanning_Shen1']}, 'keywords': {'value': ['Federated Learning', 'Personalized Models', 'Real-time Predictions']}, 'abstract': {'value': 'Federated learning is renowned for its efficacy in distributed model training, ensuring that users, called clients, retain data privacy by not disclosing their data to the central server that orchestrates collaborations. Most previous work on federated learning assumes that clients possess static batches of training data. However, clients may also need to make real-time predictions on streaming data in non-stationary environments. In such dynamic environments, employing pre-trained models may be inefficient, as they struggle to adapt to the constantly evolving data streams. To address this challenge, clients can fine-tune models online, leveraging their observed data to enhance performance. Despite the potential benefits of client participation in federated online model fine-tuning, existing analyses have not conclusively demonstrated its superiority over local model fine-tuning. To bridge this gap, the present paper develops a novel personalized federated learning algorithm, wherein each client constructs a personalized model by combining a locally fine-tuned model with multiple federated models learned by the server over time. Theoretical analysis and experiments on real datasets corroborate the effectiveness of this approach for real-time predictions and federated model fine-tuning.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1f67e6c96793fc968860e4ecdc67eeb800a1dc2f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nghari2024personalized,\\ntitle={Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning},\\nauthor={Pouya M. Ghari and Yanning Shen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yvUHnBkCzd}\\n}'}, 'paperhash': {'value': 'ghari|personalized_federated_learning_with_mixture_of_models_for_adaptive_prediction_and_model_finetuning'}},forum = 'yvUHnBkCzd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12625/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12625/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12625/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12625/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yppcLFeZgy',number = 13909,cdate = 1715744084919,pdate = 1727288054362,odate = 1730873961945,mdate = 1730873961982,tcdate = 1715744084919,tmdate = 1730873961982,ddate = None,content = {'title': {'value': 'MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering'}, 'authors': {'value': ['YIZHEN LUO', 'Zikun Nie', 'Massimo Hong', 'Suyuan Zhao', 'Hao Zhou', 'Zaiqing Nie']}, 'authorids': {'value': ['~YIZHEN_LUO1', '~Zikun_Nie1', '~Massimo_Hong1', '~Suyuan_Zhao1', '~Hao_Zhou5', '~Zaiqing_Nie2']}, 'keywords': {'value': ['protein language modeling', 'mutation explanation', 'directed evolution']}, 'TLDR': {'value': 'A unified framework harvesting protein language models for mutation explanation and engineering'}, 'abstract': {'value': 'Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein *delta* network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6ba89a23eb0008a9e5fa6007a9fcb9c765216d9f.pdf'}, 'supplementary_material': {'value': '/attachment/8bae49c0ebb45cf620ca63017ab769d8afe4b0eb.zip'}, '_bibtex': {'value': '@inproceedings{\\nluo2024mutaplm,\\ntitle={Muta{PLM}: Protein Language Modeling for Mutation Explanation and Engineering},\\nauthor={YIZHEN LUO and Zikun Nie and Massimo Hong and Suyuan Zhao and Hao Zhou and Zaiqing Nie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yppcLFeZgy}\\n}'}, 'paperhash': {'value': 'luo|mutaplm_protein_language_modeling_for_mutation_explanation_and_engineering'}},forum = 'yppcLFeZgy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13909/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13909/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13909/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13909/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ypggxVWIv2',number = 5522,cdate = 1715545732427,pdate = 1727287786585,odate = 1730873885523,mdate = 1730873885541,tcdate = 1715545732427,tmdate = 1730873885541,ddate = None,content = {'title': {'value': 'GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations'}, 'authors': {'value': ['Jinhao Duan', 'Renming Zhang', 'James Diffenderfer', 'Bhavya Kailkhura', 'Lichao Sun', 'Elias Stengel-Eskin', 'Mohit Bansal', 'Tianlong Chen', 'Kaidi Xu']}, 'authorids': {'value': ['~Jinhao_Duan1', '~Renming_Zhang1', '~James_Diffenderfer1', '~Bhavya_Kailkhura1', '~Lichao_Sun1', '~Elias_Stengel-Eskin1', '~Mohit_Bansal2', '~Tianlong_Chen1', '~Kaidi_Xu1']}, 'keywords': {'value': ['Large Language Models', 'Game Theory', 'Strategic Reasoning', 'Benchmark']}, 'abstract': {'value': \"As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.\"}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1616ae3f3c1970951b0401486556c3a49f3df00c.pdf'}, 'supplementary_material': {'value': '/attachment/6e3a2c2e604838262bed558a247fee75307ea2a9.zip'}, '_bibtex': {'value': '@inproceedings{\\nduan2024gtbench,\\ntitle={{GTB}ench: Uncovering the Strategic Reasoning Capabilities of {LLM}s via Game-Theoretic Evaluations},\\nauthor={Jinhao Duan and Renming Zhang and James Diffenderfer and Bhavya Kailkhura and Lichao Sun and Elias Stengel-Eskin and Mohit Bansal and Tianlong Chen and Kaidi Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ypggxVWIv2}\\n}'}, 'paperhash': {'value': 'duan|gtbench_uncovering_the_strategic_reasoning_capabilities_of_llms_via_gametheoretic_evaluations'}},forum = 'ypggxVWIv2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5522/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5522/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5522/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5522/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ypaqE8UwsC',number = 8054,cdate = 1715651401151,pdate = 1727287866582,odate = 1730873908849,mdate = 1730873908869,tcdate = 1715651401151,tmdate = 1730873908869,ddate = None,content = {'title': {'value': 'Federated Ensemble-Directed Offline Reinforcement Learning'}, 'authors': {'value': ['Desik Rengarajan', 'Nitin Ragothaman', 'Dileep Kalathil', 'Srinivas Shakkottai']}, 'authorids': {'value': ['~Desik_Rengarajan1', '~Nitin_Ragothaman1', '~Dileep_Kalathil1', '~Srinivas_Shakkottai1']}, 'keywords': {'value': ['Deep Reinforcement Learning', 'Offline Reinforcement Learning', 'Federated Learning']}, 'TLDR': {'value': 'A novel federated offline reinforcement learning algorithm'}, 'abstract': {'value': 'We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Na\\\\\"{i}vely combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies.  In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning  Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach.  We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real-world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on a mobile robot. We provide our code and a video of our experiments at \\\\url{https://github.com/DesikRengarajan/FEDORA}.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ebb3c4a49c535b4cabc2bd5d7686f30b108d2a55.pdf'}, 'supplementary_material': {'value': '/attachment/b658741d56ebceb05dfff999f7938a326e2c17d8.zip'}, '_bibtex': {'value': '@inproceedings{\\nrengarajan2024federated,\\ntitle={Federated Ensemble-Directed Offline Reinforcement Learning},\\nauthor={Desik Rengarajan and Nitin Ragothaman and Dileep Kalathil and Srinivas Shakkottai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ypaqE8UwsC}\\n}'}, 'paperhash': {'value': 'rengarajan|federated_ensembledirected_offline_reinforcement_learning'}},forum = 'ypaqE8UwsC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8054/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8054/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8054/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8054/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ypPzyflbYs',number = 10529,cdate = 1715694996707,pdate = 1727287943744,odate = 1730873930109,mdate = 1730873930120,tcdate = 1715694996707,tmdate = 1730873930120,ddate = None,content = {'title': {'value': 'Neural Concept Binder'}, 'authors': {'value': ['Wolfgang Stammer', 'Antonia Wüst', 'David Steinmann', 'Kristian Kersting']}, 'authorids': {'value': ['~Wolfgang_Stammer1', '~Antonia_Wüst1', '~David_Steinmann1', '~Kristian_Kersting1']}, 'keywords': {'value': ['Concept Discovery', 'Interpretable Artificial Intelligence', 'Interactive Machine Learning', 'Disentanglement']}, 'abstract': {'value': 'The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct. Achieving this in an unsupervised manner requires human users to understand the model\\'s learned concepts and, if necessary, revise incorrect ones. To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as \"concept-slot encodings\". NCB employs two types of binding: \"soft binding\", which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent \"hard binding\", achieved through hierarchical clustering and retrieval-based inference. This enables obtaining expressive, discrete representations from unlabeled images. Moreover, the structured nature of NCB\\'s concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks. We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/83a61e046b4272eb1e838707fd28087549cbe396.pdf'}, '_bibtex': {'value': '@inproceedings{\\nstammer2024neural,\\ntitle={Neural Concept Binder},\\nauthor={Wolfgang Stammer and Antonia W{\\\\\"u}st and David Steinmann and Kristian Kersting},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ypPzyflbYs}\\n}'}, 'paperhash': {'value': 'stammer|neural_concept_binder'}},forum = 'ypPzyflbYs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10529/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10529/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10529/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10529/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ypFgcT147Z',number = 7362,cdate = 1715619364103,pdate = 1727287844569,odate = 1730873902391,mdate = 1730873902411,tcdate = 1715619364103,tmdate = 1730873902411,ddate = None,content = {'title': {'value': 'Decoupling Semantic Similarity from Spatial Alignment for Neural Networks.'}, 'authors': {'value': ['Tassilo Wald', 'Constantin Ulrich', 'Priyank Jaini', 'Gregor Koehler', 'David Zimmerer', 'Stefan Denner', 'Fabian Isensee', 'Michael Baumgartner', 'Klaus Maier-Hein']}, 'authorids': {'value': ['~Tassilo_Wald1', '~Constantin_Ulrich1', '~Priyank_Jaini1', '~Gregor_Koehler1', '~David_Zimmerer1', '~Stefan_Denner1', '~Fabian_Isensee1', '~Michael_Baumgartner2', '~Klaus_Maier-Hein1']}, 'keywords': {'value': ['Representational Similarity', 'Representational Similarity Analysis', 'Computer Vision']}, 'TLDR': {'value': 'We make representational similarity matrices permutation invariance and show resulting improvements in retrieval.'}, 'abstract': {'value': 'What representation do deep neural networks learn? How similar are images to each other for neural networks? Despite the overwhelming success of deep learning methods key questions about their internal workings still remain largely unanswered, due to their internal high dimensionality and complexity. To address this, one approach is to measure the similarity of activation responses to various inputs.\\nRepresentational Similarity Matrices (RSMs) distill this similarity into scalar values for each input pair.\\nThese matrices encapsulate the entire similarity structure of a system, indicating which input lead to similar responses.\\nWhile the similarity between images is ambiguous, we argue that the spatial location of semantic objects does neither influence human perception nor deep learning classifiers. Thus this should be reflected in the definition of similarity between image responses for computer vision systems. Revisiting the established similarity calculations for RSMs we expose their sensitivity to spatial alignment. In this paper we propose to solve this through _semantic RSMs_, which are invariant to spatial permutation. We measure semantic similarity between input responses by formulating it as a set-matching problem. Further, we quantify the superiority of  _semantic_ RSMs over _spatio-semantic_ RSMs through image retrieval and by comparing the similarity between representations to the similarity between predicted class probabilities.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d22a2517d54b412df61755b57dfc902e0053fba1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwald2024decoupling,\\ntitle={Decoupling Semantic Similarity from Spatial Alignment for Neural Networks.},\\nauthor={Tassilo Wald and Constantin Ulrich and Priyank Jaini and Gregor Koehler and David Zimmerer and Stefan Denner and Fabian Isensee and Michael Baumgartner and Klaus Maier-Hein},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ypFgcT147Z}\\n}'}, 'paperhash': {'value': 'wald|decoupling_semantic_similarity_from_spatial_alignment_for_neural_networks'}},forum = 'ypFgcT147Z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7362/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7362/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7362/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7362/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ypEamFKu2O',number = 16207,cdate = 1715767849640,pdate = 1727288119890,odate = 1730873977798,mdate = 1737001410875,tcdate = 1715767849640,tmdate = 1737001410875,ddate = None,content = {'title': {'value': \"PGN: The RNN's New Successor is Effective for Long-Range Time Series Forecasting\"}, 'authors': {'value': ['Yuxin Jia', 'Youfang Lin', 'Jing Yu', 'Shuo Wang', 'Tianhao Liu', 'Huaiyu Wan']}, 'authorids': {'value': ['~Yuxin_Jia1', '~Youfang_Lin1', '~Jing_Yu8', '~Shuo_Wang17', '~Tianhao_Liu2', '~Huaiyu_Wan1']}, 'keywords': {'value': ['information propagation paths', \"the RNN's new successor\", 'long-range time series forecasting', 'comprehensive semantic information']}, 'TLDR': {'value': 'We propose a novel paradigm, PGN, as the new successor to RNN, and propose a novel temporal modeling framework called TPGN, based on PGN.'}, 'abstract': {'value': \"Due to the recurrent structure of RNN, the long information propagation path poses limitations in capturing long-term dependencies, gradient explosion/vanishing issues, and inefficient sequential execution. Based on this, we propose a novel paradigm called Parallel Gated Network (PGN) as the new successor to RNN. PGN directly captures information from previous time steps through the designed Historical Information Extraction (HIE) layer and leverages gated mechanisms to select and fuse it with the current time step information. This reduces the information propagation path to $\\\\mathcal{O}(1)$, effectively addressing the limitations of RNN. To enhance PGN's performance in long-range time series forecasting tasks, we propose a novel temporal modeling framework called Temporal PGN (TPGN). TPGN incorporates two branches to comprehensively capture the semantic information of time series. One branch utilizes PGN to capture long-term periodic patterns while preserving their local characteristics. The other branch employs patches to capture short-term information and aggregate the global representation of the series. TPGN achieves a theoretical complexity of $\\\\mathcal{O}(\\\\sqrt{L})$, ensuring efficiency in its operations. Experimental results on five benchmark datasets demonstrate the state-of-the-art (SOTA) performance and high efficiency of TPGN, further confirming the effectiveness of PGN as the new successor to RNN in long-range time series forecasting. The code is available in this repository: https://github.com/Water2sea/TPGN.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fedcae5d40b0b77e815930762cf718eaa6edbdff.pdf'}, '_bibtex': {'value': \"@inproceedings{\\njia2024pgn,\\ntitle={{PGN}: The {RNN}'s New Successor is Effective for Long-Range Time Series Forecasting},\\nauthor={Yuxin Jia and Youfang Lin and Jing Yu and Shuo Wang and Tianhao Liu and Huaiyu Wan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ypEamFKu2O}\\n}\"}, 'paperhash': {'value': 'jia|pgn_the_rnns_new_successor_is_effective_for_longrange_time_series_forecasting'}},forum = 'ypEamFKu2O',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16207/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16207/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16207/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16207/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ynJr0RW6FR',number = 2638,cdate = 1715136016707,pdate = 1727287696278,odate = 1730873859689,mdate = 1730873859700,tcdate = 1715136016707,tmdate = 1730873859700,ddate = None,content = {'title': {'value': 'ReGS: Reference-based Controllable Scene Stylization with Gaussian Splatting'}, 'authors': {'value': ['Yiqun Mei', 'Jiacong Xu', 'Vishal M. Patel']}, 'authorids': {'value': ['~Yiqun_Mei1', '~Jiacong_Xu1', '~Vishal_M._Patel1']}, 'keywords': {'value': ['Gaussian Splatting', 'Appearance Editing']}, 'abstract': {'value': 'Referenced-based scene stylization that edits the appearance based on a content-aligned reference image is an emerging research area. Starting with a pretrained neural radiance field (NeRF), existing methods typically learn a novel appearance that matches the given style. Despite their effectiveness, they inherently suffer from time-consuming volume rendering, and thus are impractical for many real-time applications. In this work, we propose ReGS, which adapts 3D Gaussian Splatting (3DGS) for reference-based stylization to enable real-time stylized view synthesis. Editing the appearance of a pretrained 3DGS is challenging as it uses discrete Gaussians as 3D representation, which tightly bind appearance with geometry. Simply optimizing the appearance as prior methods do is often insufficient for modeling continuous textures in the given reference image. To address this challenge, we propose a novel texture-guided control mechanism that adaptively adjusts local responsible Gaussians to a new geometric arrangement, serving for desired texture details. The proposed process is guided by texture clues for effective appearance editing, and regularized by scene depth for preserving original geometric structure. With these novel designs, we show ReGs can produce state-of-the-art stylization results that respect the reference texture while embracing real-time rendering speed for free-view navigation.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present a method to enable precise appearance editing for 3D Gaussian Splatting.'}, 'pdf': {'value': '/pdf/ff2c2a64aeb6fea451908d363d55da1992fca363.pdf'}, 'supplementary_material': {'value': '/attachment/03d46a6680a888891cb3451bb08578c96fbcc074.zip'}, '_bibtex': {'value': '@inproceedings{\\nmei2024regs,\\ntitle={Re{GS}: Reference-based Controllable Scene Stylization with Gaussian Splatting},\\nauthor={Yiqun Mei and Jiacong Xu and Vishal M. Patel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ynJr0RW6FR}\\n}'}, 'paperhash': {'value': 'mei|regs_referencebased_controllable_scene_stylization_with_gaussian_splatting'}},forum = 'ynJr0RW6FR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2638/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2638/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2638/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2638/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yltJAlwtW9',number = 10327,cdate = 1715692912527,pdate = 1727287936298,odate = 1730873928008,mdate = 1730873928021,tcdate = 1715692912527,tmdate = 1730873928021,ddate = None,content = {'title': {'value': 'Information-theoretic Generalization Analysis for Expected Calibration Error'}, 'authors': {'value': ['Futoshi Futami', 'Masahiro Fujisawa']}, 'authorids': {'value': ['~Futoshi_Futami1', '~Masahiro_Fujisawa1']}, 'keywords': {'value': ['information thery', 'information-theoretic generalization error analysis', 'generalization error', 'expected calibration error', 'calibration error', 'binning']}, 'abstract': {'value': 'While the expected calibration error (ECE), which employs binning, is widely adopted to evaluate the calibration performance of machine learning models, theoretical understanding of its estimation bias is limited. In this paper, we present the first comprehensive analysis of the estimation bias in the two common binning strategies, uniform mass and uniform width binning.\\nOur analysis establishes upper bounds on the bias, achieving an improved convergence rate. Moreover, our bounds reveal, for the first time, the optimal number of bins to minimize the estimation bias. We further extend our bias analysis to generalization error analysis based on the information-theoretic approach, deriving upper bounds that enable the numerical evaluation of how small the ECE is for unknown data. Experiments using deep learning models show that our bounds are nonvacuous thanks to this information-theoretic generalization analysis approach.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/b2543ac5e3637d8a5c66cd924fec0a9177dcec00.zip'}, 'pdf': {'value': '/pdf/c4a25ce5ac23050bc6d164b9b2269d2890c8bde3.pdf'}, 'TLDR': {'value': 'This paper offers a comprehensive analysis of the expected calibration error using the information-theoretic generalization analysis.'}, '_bibtex': {'value': '@inproceedings{\\nfutami2024informationtheoretic,\\ntitle={Information-theoretic Generalization Analysis for Expected Calibration Error},\\nauthor={Futoshi Futami and Masahiro Fujisawa},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yltJAlwtW9}\\n}'}, 'paperhash': {'value': 'futami|informationtheoretic_generalization_analysis_for_expected_calibration_error'}},forum = 'yltJAlwtW9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10327/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10327/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10327/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10327/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ylceJ2xIw5',number = 7055,cdate = 1715611697085,pdate = 1727287833783,odate = 1730873898872,mdate = 1730873898882,tcdate = 1715611697085,tmdate = 1730873898882,ddate = None,content = {'title': {'value': 'Fair Wasserstein Coresets'}, 'authors': {'value': ['Zikai Xiong', 'Niccolo Dalmasso', 'Shubham Sharma', 'Freddy Lecue', 'Daniele Magazzeni', 'Vamsi K. Potluru', 'Tucker Balch', 'Manuela Veloso']}, 'authorids': {'value': ['~Zikai_Xiong1', '~Niccolo_Dalmasso1', '~Shubham_Sharma4', '~Freddy_Lecue1', '~Daniele_Magazzeni1', '~Vamsi_K._Potluru1', '~Tucker_Balch2', '~Manuela_Veloso1']}, 'keywords': {'value': ['Algorithmic Fairness', 'Nonconvex Optimization', 'Coresets']}, 'abstract': {'value': \"Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. While current approaches focus on creating fair synthetic representative samples by optimizing local properties relative to the original samples, their impact on downstream learning processes has yet to be explored.  In this work, we present fair Wasserstein coresets ($\\\\texttt{FWC}$), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. $\\\\texttt{FWC}$ uses an efficient majority minimization algorithm to minimize the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of $\\\\texttt{FWC}$ is equivalent to Lloyd's algorithm for k-medians and k-means clustering. Experiments conducted on both synthetic and real datasets show that $\\\\texttt{FWC}$:  (i) achieves a competitive fairness-performance tradeoff in downstream models compared to existing approaches, (ii) improves downstream fairness when added to the existing training data and (iii) can be used to reduce biases in predictions from large language models (GPT-3.5 and GPT-4).\"}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/93985e308e0356a2b95c8e021f79d007aeda2429.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxiong2024fair,\\ntitle={Fair Wasserstein Coresets},\\nauthor={Zikai Xiong and Niccolo Dalmasso and Shubham Sharma and Freddy Lecue and Daniele Magazzeni and Vamsi K. Potluru and Tucker Balch and Manuela Veloso},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ylceJ2xIw5}\\n}'}, 'TLDR': {'value': 'We present Fair Wasserstein Coreset (FWC), a novel coreset approach to generate fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks.'}, 'paperhash': {'value': 'xiong|fair_wasserstein_coresets'}},forum = 'ylceJ2xIw5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7055/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7055/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7055/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7055/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'yktQNqtepd',number = 5173,cdate = 1715509784315,pdate = 1727287775978,odate = 1730873882610,mdate = 1734836793589,tcdate = 1715509784315,tmdate = 1734836793589,ddate = None,content = {'title': {'value': 'Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection'}, 'authors': {'value': ['Chaoda Zheng', 'Feng Wang', 'Naiyan Wang', 'Shuguang Cui', 'Zhen Li']}, 'authorids': {'value': ['~Chaoda_Zheng1', '~Feng_Wang1', '~Naiyan_Wang1', '~Shuguang_Cui1', '~Zhen_Li6']}, 'keywords': {'value': ['Object Centric', 'Occupancy', 'LiDAR', 'Detection', 'Long Sequence']}, 'abstract': {'value': \"While 3D object bounding box (bbox) representation has been widely used in autonomous driving perception, it lacks the ability to capture the precise details of an object's intrinsic geometry. Recently, occupancy has emerged as a promising alternative for 3D scene perception. However, constructing a high-resolution occupancy map remains infeasible for large scenes due to computational constraints. Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes. This representation not only provides intricate details for detected objects but also enables higher voxel resolution in practical applications. We advance the development of object-centric occupancy perception from both data and algorithm perspectives. On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline. From the algorithmic standpoint, we introduce a novel object-centric occupancy completion network equipped with an implicit shape decoder that manages dynamic-size occupancy generation. This network accurately predicts the complete object-centric occupancy volume for inaccurate object proposals by leveraging temporal information from long sequences. Our method demonstrates robust performance in completing object shapes under noisy detection and tracking conditions. Additionally, we show that our occupancy features significantly enhance the detection results of state-of-the-art 3D object detectors, especially for incomplete or distant objects in the Waymo Open Dataset.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a new object-centric occupancy concept to enhance 3D perception in data and algorithmic perspectives.'}, 'pdf': {'value': '/pdf/7caaf2ac1f758304a70b57129814d809e45dc1b5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024towards,\\ntitle={Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection},\\nauthor={Chaoda Zheng and Feng Wang and Naiyan Wang and Shuguang Cui and Zhen Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yktQNqtepd}\\n}'}, 'paperhash': {'value': 'zheng|towards_flexible_3d_perception_objectcentric_occupancy_completion_augments_3d_object_detection'}},forum = 'yktQNqtepd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5173/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5173/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5173/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5173/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ykQnxko1cJ',number = 11025,cdate = 1715700218246,pdate = 1727287958330,odate = 1730873934396,mdate = 1730873934414,tcdate = 1715700218246,tmdate = 1730873934414,ddate = None,content = {'title': {'value': 'CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition'}, 'authors': {'value': ['Zhonglin Sun', 'Siyang Song', 'Ioannis Patras', 'Georgios Tzimiropoulos']}, 'authorids': {'value': ['~Zhonglin_Sun1', '~Siyang_Song1', '~Ioannis_Patras2', '~Georgios_Tzimiropoulos1']}, 'keywords': {'value': ['synthetic face recognition', 'diffusion models', 'center-based semi-hard']}, 'abstract': {'value': 'Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely **Ce**nter-based Se**mi**-hard Synthetic Face\\nGeneration (**CemiFace**) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. The code will be available at:https://github.com/szlbiubiubiu/CemiFace'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/30f78a219d61e45796c28fce873caf8b9bd87ab7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024cemiface,\\ntitle={CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition},\\nauthor={Zhonglin Sun and Siyang Song and Ioannis Patras and Georgios Tzimiropoulos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ykQnxko1cJ}\\n}'}, 'supplementary_material': {'value': '/attachment/db17dfcbdd7ec13ce977fe3001b4a83cfea3d44d.zip'}, 'TLDR': {'value': 'diffusion model to generate semi-hard samples for synthetic face recognition'}, 'paperhash': {'value': 'sun|cemiface_centerbased_semihard_synthetic_face_generation_for_face_recognition'}},forum = 'ykQnxko1cJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11025/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11025/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11025/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11025/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ykACV1IhjD',number = 9560,cdate = 1715681747367,pdate = 1727287914690,odate = 1730873921796,mdate = 1736942750713,tcdate = 1715681747367,tmdate = 1736942750713,ddate = None,content = {'title': {'value': 'Controlling Continuous Relaxation for Combinatorial Optimization'}, 'authors': {'value': ['Yuma Ichikawa']}, 'authorids': {'value': ['~Yuma_Ichikawa1']}, 'keywords': {'value': ['Combinatorial Optimization', 'Unsupervised Learning for Combinatorial Optimization', 'Learning for Combinatorial Optimization', 'Graph Neual Networks']}, 'TLDR': {'value': 'Overcoming a local optima issue and rounding issue for unsupervised learning based combinatorial optimization solvers'}, 'abstract': {'value': 'Unsupervised learning (UL)-based solvers for combinatorial optimization (CO) train a neural network that generates a soft solution by directly optimizing the CO objective using a continuous relaxation strategy. These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems. However, UL-based solvers face two practical issues: (I) an optimization issue, where UL-based solvers are easily trapped at local optima, and (II) a rounding issue, where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results. This study proposes a Continuous Relaxation Annealing (CRA) strategy, an effective rounding-free learning method for UL-based solvers. CRA introduces a penalty term that dynamically shifts from prioritizing continuous solutions, effectively smoothing the non-convexity of the objective function, to enforcing discreteness, eliminating artificial rounding. Experimental results demonstrate that CRA significantly enhances the performance of UL-based solvers, outperforming existing UL-based solvers and greedy algorithms in complex CO problems. Additionally, CRA effectively eliminates artificial rounding and accelerates the learning process.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/885375ae024fb0ad2338fe00a1eb658617f6ce3c.pdf'}, 'supplementary_material': {'value': '/attachment/ecb0ebc609709640d4a1077a6a77cec8af865aad.zip'}, '_bibtex': {'value': '@inproceedings{\\nichikawa2024controlling,\\ntitle={Controlling Continuous Relaxation for Combinatorial Optimization},\\nauthor={Yuma Ichikawa},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ykACV1IhjD}\\n}'}, 'paperhash': {'value': 'ichikawa|controlling_continuous_relaxation_for_combinatorial_optimization'}},forum = 'ykACV1IhjD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9560/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9560/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9560/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9560/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yiXZZC5qDI',number = 14176,cdate = 1715747029162,pdate = 1727288062393,odate = 1730873964040,mdate = 1730873964056,tcdate = 1715747029162,tmdate = 1730873964056,ddate = None,content = {'title': {'value': 'From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models'}, 'authors': {'value': ['Zhuoshi Pan', 'Yuguang Yao', 'Gaowen Liu', 'Bingquan Shen', 'H. Vicky Zhao', 'Ramana Rao Kompella', 'Sijia Liu']}, 'authorids': {'value': ['~Zhuoshi_Pan2', '~Yuguang_Yao1', '~Gaowen_Liu4', '~Bingquan_Shen1', '~H._Vicky_Zhao1', '~Ramana_Rao_Kompella1', '~Sijia_Liu1']}, 'keywords': {'value': ['Diffusion model', 'data poisoning', 'data replication', 'diffusion classifier']}, 'TLDR': {'value': 'We reveal the bilateral data poisoning effects in diffusion models when the training data is poisoned like BadNets.'}, 'abstract': {'value': \"While state-of-the-art diffusion models (DMs) excel in image generation, concerns regarding their security persist. Earlier research highlighted DMs' vulnerability to data poisoning attacks, but these studies placed stricter requirements than conventional methods like 'BadNets' in image classification. This is because the art necessitates modifications to the diffusion training and sampling procedures. Unlike the prior work, we investigate whether BadNets-like data poisoning methods can directly degrade the generation by DMs. In other words, if only the training dataset is contaminated (without manipulating the diffusion process), how will this affect the performance of learned DMs? In this setting, we uncover bilateral data poisoning effects that not only serve an adversarial purpose (compromising the functionality of DMs) but also offer a defensive advantage (which can be leveraged for defense in classification tasks against poisoning attacks). We show that a BadNets-like data poisoning attack remains effective in DMs for producing incorrect images (misaligned with the intended text conditions). Meanwhile, poisoned DMs exhibit an increased ratio of triggers, a phenomenon we refer to as 'trigger amplification', among the generated images. This insight can be then used to enhance the detection of poisoned training data. In addition, even under a low poisoning ratio, studying the poisoning effects of DMs is also valuable for designing robust image classifiers against such attacks. Last but not least, we establish a meaningful linkage between data poisoning and the phenomenon of data replications by exploring DMs' inherent data memorization tendencies. Code is available at https://github.com/OPTML-Group/BiBadDiff.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/85b57c5d8bead49ab7cc6ae03009e613abc6dd86.pdf'}, 'supplementary_material': {'value': '/attachment/3a0e2a13593ea6ef621c3e26114d5847789f70bf.zip'}, '_bibtex': {'value': '@inproceedings{\\npan2024from,\\ntitle={From Trojan Horses to Castle Walls: Unveiling Bilateral Data Poisoning Effects in Diffusion Models},\\nauthor={Zhuoshi Pan and Yuguang Yao and Gaowen Liu and Bingquan Shen and H. Vicky Zhao and Ramana Rao Kompella and Sijia Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yiXZZC5qDI}\\n}'}, 'paperhash': {'value': 'pan|from_trojan_horses_to_castle_walls_unveiling_bilateral_data_poisoning_effects_in_diffusion_models'}},forum = 'yiXZZC5qDI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14176/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14176/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14176/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14176/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yhd2kHHNtB',number = 9810,cdate = 1715686097303,pdate = 1727287921392,odate = 1730873923685,mdate = 1730873923706,tcdate = 1715686097303,tmdate = 1730873923706,ddate = None,content = {'title': {'value': 'Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments'}, 'authors': {'value': ['Wen-Bo Du', 'Tian Qin', 'Tian-Zuo Wang', 'Zhi-Hua Zhou']}, 'authorids': {'value': ['~Wen-Bo_Du1', '~Tian_Qin1', '~Tian-Zuo_Wang1', '~Zhi-Hua_Zhou2']}, 'keywords': {'value': ['decision-making', 'structural rehearsal model', 'non-stationary environment', 'alteration cost']}, 'TLDR': {'value': 'We present the AUF-MICNS algorithm for suggesting appropriate decision actions to avoid undesired outcomes with minimal action cost in non-stationary environments'}, 'abstract': {'value': 'Machine learning (ML) has achieved remarkable success in prediction tasks. In many real-world scenarios, rather than solely predicting an outcome using an ML model, the crucial concern is how to make decisions to prevent the occurrence of undesired outcomes, known as the *avoiding undesired future (AUF)* problem. To this end, a new framework called *rehearsal learning* has been proposed recently, which works effectively in stationary environments by leveraging the influence relations among variables. In real tasks, however, the environments are usually non-stationary, where the influence relations may be *dynamic*, leading to the failure of AUF by the existing method. In this paper, we introduce a novel sequential methodology that effectively updates the estimates of dynamic influence relations, which are crucial for rehearsal learning to prevent undesired outcomes in non-stationary environments. Meanwhile, we take the cost of decision actions into account and provide the formulation of AUF problem with minimal action cost under non-stationarity. We prove that in linear Gaussian cases, the problem can be transformed into the well-studied convex quadratically constrained quadratic program (QCQP). In this way, we establish the first polynomial-time rehearsal-based approach for addressing the AUF problem. Theoretical and experimental results validate the effectiveness and efficiency of our method under certain circumstances.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/55d4e7f5ce2356d9c39fa5ab1bfe2753b2323f87.pdf'}, 'supplementary_material': {'value': '/attachment/9b7caf1ecd079f3602cd8bc81b96064aca72974e.zip'}, '_bibtex': {'value': '@inproceedings{\\ndu2024avoiding,\\ntitle={Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments},\\nauthor={Wen-Bo Du and Tian Qin and Tian-Zuo Wang and Zhi-Hua Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yhd2kHHNtB}\\n}'}, 'paperhash': {'value': 'du|avoiding_undesired_future_with_minimal_cost_in_nonstationary_environments'}},forum = 'yhd2kHHNtB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9810/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9810/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9810/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9810/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ygDl8q02gA',number = 12269,cdate = 1715719476204,pdate = 1727288000748,odate = 1730873947577,mdate = 1730873947595,tcdate = 1715719476204,tmdate = 1730873947595,ddate = None,content = {'title': {'value': 'Optimal Algorithms for Learning Partitions with Faulty Oracles'}, 'authors': {'value': ['Adela Frances DePavia', 'Olga Medrano Martín del Campo', 'Erasmo Tani']}, 'authorids': {'value': ['~Adela_Frances_DePavia1', '~Olga_Medrano_Martín_del_Campo1', '~Erasmo_Tani1']}, 'keywords': {'value': ['clustering; error tolerant; partitions; query complexity; oracle advice; graph learning; active learning']}, 'abstract': {'value': \"We consider a clustering problem where a learner seeks to partition a finite set by querying a faulty oracle. This models applications where learners crowdsource information from non-expert human workers or conduct noisy experiments to determine group structure. The learner aims to exactly recover a partition by submitting queries of the form ``are $u$ and $v$ in the same group?'' for any pair of elements $u$ and $v$ in the set. Moreover, because the learner only has access to faulty sources of information, they require an error-tolerant algorithm for this task: i.e. they must fully recover the correct partition, even if up to $\\\\ell$ answers are incorrect, for some error-tolerance parameter $\\\\ell$. We study the question: for any given error-tolerance $\\\\ell$, what is the minimum number of queries needed to learn a finite set partition of $n$ elements into $k$ groups? We design algorithms for this task and prove that they achieve optimal query complexity. To analyze our algorithms, we first highlight a  connection between this task and correlation clustering. We then use this connection to build a Rényi-Ulam style analytical framework for this problem, which yields matching lower bounds. Our analysis also reveals an inherent asymmetry between the query complexity necessary to be robust against false negative errors as opposed to false positive errors.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We design algorithms for the problem of learning partitions from faulty same-cluster oracle queries and prove that they achieve optimal query complexity.'}, 'pdf': {'value': '/pdf/2abac03a655a803c28aaeea7d0cefd63e537be44.pdf'}, '_bibtex': {'value': \"@inproceedings{\\ndepavia2024optimal,\\ntitle={Optimal Algorithms for Learning Partitions with Faulty Oracles},\\nauthor={Adela Frances DePavia and Olga Medrano Mart{\\\\'\\\\i}n del Campo and Erasmo Tani},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ygDl8q02gA}\\n}\"}, 'paperhash': {'value': 'depavia|optimal_algorithms_for_learning_partitions_with_faulty_oracles'}},forum = 'ygDl8q02gA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12269/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12269/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12269/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12269/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yfQwyxiSJ7',number = 3778,cdate = 1715325116555,pdate = 1727287730549,odate = 1730873869467,mdate = 1730873869488,tcdate = 1715325116555,tmdate = 1730873869488,ddate = None,content = {'title': {'value': 'Color-Oriented Redundancy Reduction in Dataset Distillation'}, 'authors': {'value': ['Bowen Yuan', 'Zijian Wang', 'Mahsa Baktashmotlagh', 'Yadan Luo', 'Zi Huang']}, 'authorids': {'value': ['~Bowen_Yuan3', '~Zijian_Wang2', '~Mahsa_Baktashmotlagh1', '~Yadan_Luo1', '~Zi_Huang1']}, 'keywords': {'value': ['Computer Vision', 'Data Distillation', 'Parameterization']}, 'TLDR': {'value': 'A novel parameterization method for data distillation by exploring the redundancy in color space.'}, 'abstract': {'value': 'Dataset Distillation (DD) is designed to generate condensed representations of extensive image datasets, enhancing training efficiency. Despite recent advances, there remains considerable potential for improvement, particularly in addressing the notable redundancy within the color space of distilled images. In this paper, we propose a two-fold optimization strategy to minimize color redundancy at the individual image and overall dataset levels, respectively. At the image level, we employ a palette network, a specialized neural network, to dynamically allocate colors from a reduced color space to each pixel. The palette network identifies essential areas in synthetic images for model training, and consequently assigns more unique colors to them. At the dataset level, we develop a color-guided initialization strategy to minimize redundancy among images. Representative images with the least replicated color patterns are selected based on the information gain. A comprehensive performance study involving various datasets and evaluation scenarios is conducted, demonstrating the superior performance of our proposed color-aware DD compared to existing DD methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/20b534cf5fff43e4e9a8229eb66f4841e6dba9df.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyuan2024colororiented,\\ntitle={Color-Oriented Redundancy Reduction in Dataset Distillation},\\nauthor={Bowen Yuan and Zijian Wang and Mahsa Baktashmotlagh and Yadan Luo and Zi Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yfQwyxiSJ7}\\n}'}, 'paperhash': {'value': 'yuan|colororiented_redundancy_reduction_in_dataset_distillation'}},forum = 'yfQwyxiSJ7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3778/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3778/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3778/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3778/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'yeFx5NQmr7',number = 5311,cdate = 1715523036052,pdate = 1727287779959,odate = 1730873883720,mdate = 1730873883740,tcdate = 1715523036052,tmdate = 1730873883740,ddate = None,content = {'title': {'value': 'Learning 3D Garment Animation from Trajectories of A Piece of Cloth'}, 'authors': {'value': ['Yidi Shao', 'Chen Change Loy', 'Bo Dai']}, 'authorids': {'value': ['~Yidi_Shao1', '~Chen_Change_Loy2', '~Bo_Dai2']}, 'keywords': {'value': ['Garment', 'Cloth', 'Simulation', 'Constitutive Model', '3D']}, 'abstract': {'value': 'Garment animation is ubiquitous in various applications, such as virtual reality, gaming, and film producing. Recently, learning-based approaches obtain compelling performance in animating diverse garments under versatile scenarios. Nevertheless, to mimic the deformations of the observed garments, data-driven methods require large scale of garment data, which are both resource-wise expensive and time-consuming. In addition, forcing models to match the dynamics of observed garment animation may hinder the potentials to generalize to unseen cases. In this paper, instead of using garment-wise supervised-learning we adopt a disentangled scheme to learn how to animate observed garments: 1). learning constitutive behaviors from the observed cloth; 2). dynamically animate various garments constrained by the learned constitutive laws. Specifically, we propose Energy Unit network (EUNet) to model the constitutive relations in the format of energy. Without the priors from analytical physics models and differentiable simulation engines, EUNet is able to directly capture the constitutive behaviors from the observed piece of cloth and uniformly describes the change of energy caused by deformations, such as stretching and bending. We further apply the pre-trained EUNet to animate various garments based on energy optimizations. The disentangled scheme alleviates the need of garment data and enables us to utilize the dynamics of a piece of cloth for animating garments. Experiments show that while EUNet effectively delivers the energy gradients due to the deformations, models constrained by EUNet achieve more stable and physically plausible performance comparing with those trained in garment-wise supervised manner.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d57b0731216ccd13a02117aa1f63730ec58dae56.pdf'}, 'supplementary_material': {'value': '/attachment/1c7cbcf8bbd3cf3a0847668690be98d26aa50b61.zip'}, 'TLDR': {'value': 'To learn to animate the garments, we adopt the disentangled scheme where we model the constitutive laws of a piece of cloth from the observations and animate garments using energy optimization constrained by our model.'}, '_bibtex': {'value': '@inproceedings{\\nshao2024learning,\\ntitle={Learning 3D Garment Animation from Trajectories of A Piece of Cloth},\\nauthor={Yidi Shao and Chen Change Loy and Bo Dai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yeFx5NQmr7}\\n}'}, 'paperhash': {'value': 'shao|learning_3d_garment_animation_from_trajectories_of_a_piece_of_cloth'}},forum = 'yeFx5NQmr7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5311/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5311/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5311/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5311/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ybiUVIxJth',number = 11663,cdate = 1715708317861,pdate = 1727287978920,odate = 1730873940947,mdate = 1730873940965,tcdate = 1715708317861,tmdate = 1730873940965,ddate = None,content = {'title': {'value': 'Policy Aggregation'}, 'authors': {'value': ['Parand A. Alamdari', 'Soroush Ebadian', 'Ariel D. Procaccia']}, 'authorids': {'value': ['~Parand_A._Alamdari1', '~Soroush_Ebadian1', '~Ariel_D._Procaccia1']}, 'keywords': {'value': ['Markov decision process', 'reinforcement learning', 'AI alignment']}, 'abstract': {'value': 'We consider the challenge of AI value alignment with multiple individuals that have different reward functions and optimal policies in an underlying Markov decision process. We formalize this problem as one of *policy aggregation*, where the goal is to identify a desirable collective policy. We argue that an approach informed by social choice theory is especially suitable. Our key insight is that social choice methods can be reinterpreted by identifying ordinal preferences with volumes of subsets of the *state-action occupancy polytope*. Building on this insight, we demonstrate that a variety of methods — including approval voting, Borda count, the proportional veto core, and quantile fairness — can be practically applied to policy aggregation.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fbe1222a75cee72847eef7783a09fb1b0b56c748.pdf'}, '_bibtex': {'value': '@inproceedings{\\nalamdari2024policy,\\ntitle={Policy Aggregation},\\nauthor={Parand A. Alamdari and Soroush Ebadian and Ariel D. Procaccia},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ybiUVIxJth}\\n}'}, 'paperhash': {'value': 'alamdari|policy_aggregation'}},forum = 'ybiUVIxJth',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11663/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11663/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11663/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11663/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ybMrn4tdn0',number = 6940,cdate = 1715608351159,pdate = 1727287829730,odate = 1730873897807,mdate = 1730873897828,tcdate = 1715608351159,tmdate = 1730873897828,ddate = None,content = {'title': {'value': 'Auditing Local Explanations is Hard'}, 'authors': {'value': ['Robi Bhattacharjee', 'Ulrike von Luxburg']}, 'authorids': {'value': ['~Robi_Bhattacharjee1', '~Ulrike_von_Luxburg1']}, 'keywords': {'value': ['explanation', 'auditing', 'trust', 'regulation']}, 'TLDR': {'value': 'We show that verifying local explanations can require exorbitant amounts of data.'}, 'abstract': {'value': \"In sensitive contexts, providers of machine learning algorithms are increasingly required to give explanations for their algorithms' decisions. However, explanation receivers might not trust the provider, who potentially could output misleading or manipulated explanations. In this work, we investigate an auditing framework in which a third-party auditor or a collective of users attempts to sanity-check explanations: they can query model decisions and the corresponding local explanations, pool all the information received, and then check for basic consistency properties. We prove upper and lower bounds on the amount of queries that are needed for an auditor to succeed within this framework. Our results show that successful auditing requires a potentially exorbitant number of queries -- particularly in high dimensional cases. Our analysis also reveals that a key property is the ``locality'' of the provided explanations --- a quantity that so far has not been paid much attention to in the explainability literature. Looking forward, our results suggest that for complex high-dimensional settings, merely providing a pointwise prediction and explanation could be insufficient, as there is no way for the users to verify that the provided explanations are not completely made-up.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/da6c6a3020c220d933287ab79ebff6b0838a4941.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbhattacharjee2024auditing,\\ntitle={Auditing Local Explanations is Hard},\\nauthor={Robi Bhattacharjee and Ulrike von Luxburg},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ybMrn4tdn0}\\n}'}, 'paperhash': {'value': 'bhattacharjee|auditing_local_explanations_is_hard'}},forum = 'ybMrn4tdn0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6940/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6940/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6940/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6940/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ybLXvqJyQA',number = 10012,cdate = 1715689124067,pdate = 1727287927538,odate = 1730873925380,mdate = 1730873925394,tcdate = 1715689124067,tmdate = 1730873925394,ddate = None,content = {'title': {'value': 'Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms'}, 'authors': {'value': ['Marc Wanner', 'Laura Lewis', 'Chiranjib Bhattacharyya', 'Devdatt Dubhashi', 'Alexandru Gheorghiu']}, 'authorids': {'value': ['~Marc_Wanner1', '~Laura_Lewis2', '~Chiranjib_Bhattacharyya1', '~Devdatt_Dubhashi1', '~Alexandru_Gheorghiu1']}, 'keywords': {'value': ['Deep Learning', 'Learning Theory', 'Quantum Many-Body Problems']}, 'abstract': {'value': 'A fundamental problem in quantum many-body physics is that of finding ground states of local\\nHamiltonians. A number of recent works gave provably efficient machine learning (ML) algorithms\\nfor learning ground states. Specifically, [Huang et al. Science 2022], introduced an approach for learning\\nproperties of the ground state of an $n$-qubit gapped local Hamiltonian $H$ from only $n^{\\\\mathcal{O}(1)}$ data\\npoints sampled from Hamiltonians in the same phase of matter. This was subsequently improved\\nby [Lewis et al. Nature Communications 2024], to $\\\\mathcal{O}(\\\\log 𝑛)$ samples when the geometry of the $n$-qubit system is known.\\nIn this work, we introduce two approaches that achieve a constant sample complexity, independent\\nof system size $n$, for learning ground state properties. Our first algorithm consists of a simple\\nmodification of the ML model used by Lewis et al. and applies to a property of interest known beforehand. Our second algorithm, which applies even if a description of\\nthe property is not known, is a deep neural network model. While empirical results showing the\\nperformance of neural networks have been demonstrated, to our knowledge, this is the first rigorous\\nsample complexity bound on a neural network model for predicting ground state properties. We also perform numerical experiments that confirm the improved scaling of our approach compared to earlier results.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/83a1900fe49a28fcac07aec59f2f276432aaaff2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwanner2024predicting,\\ntitle={Predicting Ground State Properties: Constant Sample Complexity and Deep Learning Algorithms},\\nauthor={Marc Wanner and Laura Lewis and Chiranjib Bhattacharyya and Devdatt Dubhashi and Alexandru Gheorghiu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ybLXvqJyQA}\\n}'}, 'paperhash': {'value': 'wanner|predicting_ground_state_properties_constant_sample_complexity_and_deep_learning_algorithms'}},forum = 'ybLXvqJyQA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10012/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10012/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10012/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10012/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ybHPzL7eYT',number = 3523,cdate = 1715297245379,pdate = 1727287723546,odate = 1730873867324,mdate = 1730873867342,tcdate = 1715297245379,tmdate = 1730873867342,ddate = None,content = {'title': {'value': 'Large Spatial Model: End-to-end Unposed Images to Semantic 3D'}, 'authors': {'value': ['Zhiwen Fan', 'Jian Zhang', 'Wenyan Cong', 'Peihao Wang', 'Renjie Li', 'Kairun Wen', 'Shijie Zhou', 'Achuta Kadambi', 'Zhangyang Wang', 'Danfei Xu', 'Boris Ivanovic', 'Marco Pavone', 'Yue Wang']}, 'authorids': {'value': ['~Zhiwen_Fan2', '~Jian_Zhang54', '~Wenyan_Cong1', '~Peihao_Wang1', '~Renjie_Li3', '~Kairun_Wen1', '~Shijie_Zhou1', '~Achuta_Kadambi2', '~Zhangyang_Wang1', '~Danfei_Xu1', '~Boris_Ivanovic1', '~Marco_Pavone1', '~Yue_Wang2']}, 'keywords': {'value': ['3D Reconstruction', '3D Scene Understanding', 'Gaussian Splatting']}, 'abstract': {'value': 'Reconstructing and understanding 3D structures from a limited number of images is a classical problem in computer vision. Traditional approaches typically decompose this task into multiple subtasks, involving several stages of complex mappings between different data representations. For example, dense reconstruction using Structure-from-Motion (SfM) requires transforming images into key points, optimizing camera parameters, and estimating structures. Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks. This multi-stage paradigm leads to significant processing times and engineering complexity.\\n\\nIn this work, we introduce the Large Spatial Model (LSM), which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views. Built on a general Transformer-based framework, LSM predicts global geometry via pixel-aligned point maps. To improve spatial attribute regression, we adopt local context aggregation with multi-scale fusion, enhancing the accuracy of fine local details. To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning. Comprehensive experiments on various tasks demonstrate that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose a method that utilizes two unposed and uncalibrated images as input, and reconstructs the explicit radiance field, encompassing geometry, appearance, and semantics in real-time.'}, 'pdf': {'value': '/pdf/ee1ddc8c08fa974be5694c1cffee72f12da261ad.pdf'}, 'supplementary_material': {'value': '/attachment/a56101d73213494352b81a74c2e7ef24fe402a86.zip'}, '_bibtex': {'value': '@inproceedings{\\nfan2024large,\\ntitle={Large Spatial Model: End-to-end Unposed Images to Semantic 3D},\\nauthor={Zhiwen Fan and Jian Zhang and Wenyan Cong and Peihao Wang and Renjie Li and Kairun Wen and Shijie Zhou and Achuta Kadambi and Zhangyang Wang and Danfei Xu and Boris Ivanovic and Marco Pavone and Yue Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ybHPzL7eYT}\\n}'}, 'paperhash': {'value': 'fan|large_spatial_model_endtoend_unposed_images_to_semantic_3d'}},forum = 'ybHPzL7eYT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3523/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3523/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3523/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission3523/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yXpfrLMIr2',number = 390,cdate = 1713883432928,pdate = 1727287636656,odate = 1730873840607,mdate = 1730873840626,tcdate = 1713883432928,tmdate = 1730873840626,ddate = None,content = {'title': {'value': 'Binarized Diffusion Model for Image Super-Resolution'}, 'authors': {'value': ['Zheng Chen', 'Haotong Qin', 'Yong Guo', 'Xiongfei Su', 'Xin Yuan', 'Linghe Kong', 'Yulun Zhang']}, 'authorids': {'value': ['~Zheng_Chen11', '~Haotong_Qin1', '~Yong_Guo1', '~Xiongfei_Su1', '~Xin_Yuan4', '~Linghe_Kong1', '~Yulun_Zhang1']}, 'keywords': {'value': ['diffusion model', 'binarization', 'image super-resolution']}, 'abstract': {'value': 'Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment. Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs. Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation. In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization. We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection. Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA). The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation alability of the binarized module. Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A binarized diffusion model, BI-DiffSR, for image SR.'}, 'pdf': {'value': '/pdf/bdcbf03ed2b041a6f0730d46d010316bdcad8da7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024binarized,\\ntitle={Binarized Diffusion Model for Image Super-Resolution},\\nauthor={Zheng Chen and Haotong Qin and Yong Guo and Xiongfei Su and Xin Yuan and Linghe Kong and Yulun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yXpfrLMIr2}\\n}'}, 'supplementary_material': {'value': '/attachment/73026ac5b5480ff7b51165e63e432098f9fd9a63.zip'}, 'paperhash': {'value': 'chen|binarized_diffusion_model_for_image_superresolution'}},forum = 'yXpfrLMIr2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission390/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission390/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission390/-/Revision', 'NeurIPS.cc/2024/Conference/Submission390/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yXW2dCTQdi',number = 11170,cdate = 1715701808478,pdate = 1727287962832,odate = 1730873935511,mdate = 1730873935527,tcdate = 1715701808478,tmdate = 1730873935527,ddate = None,content = {'title': {'value': 'Controlled maximal variability along with reliable performance in recurrent neural networks'}, 'authors': {'value': ['Chiara Mastrogiuseppe', 'Rubén Moreno-Bote']}, 'authorids': {'value': ['~Chiara_Mastrogiuseppe1', '~Rubén_Moreno-Bote1']}, 'keywords': {'value': ['Reinforcement Learning', 'Computational Neuroscience', 'Neural Variability', 'Recurrent Neural Network', 'Maximum Occupancy Principle', 'Maximum Entropy Reinforcement Learning']}, 'TLDR': {'value': 'Maximizing cumulative future action entropy allows recurrent neural networks to perform tasks while maximizing variability.'}, 'abstract': {'value': \"Natural behaviors, even stereotyped ones, exhibit variability. Despite its role in exploring and learning, the function and neural basis of this variability is still not well understood. Given the coupling between neural activity and behavior, we ask what type of neural variability does not compromise behavioral performance. While previous studies typically curtail variability to allow for high task performance in neural networks, our approach takes the reversed perspective. We investigate how to generate maximal neural variability while at the same time having high network performance. \\nTo do so, we extend to neural activity the maximum occupancy principle (MOP) developed for behavior, and refer to this new neural principle as NeuroMOP. NeuroMOP posits that the goal of the nervous system is to maximize future action-state entropy, a reward-free, intrinsic motivation that entails creating all possible activity patterns while avoiding terminal or dangerous ones.\\nWe show that this goal can be achieved through a neural network controller that injects currents (actions) into a recurrent neural network of fixed random weights to maximize future cumulative action-state entropy. \\nHigh activity variability can be induced while adhering to an energy constraint or while avoiding terminal states defined by specific neurons' activities, also in a context-dependent manner. The network solves these tasks by flexibly switching between stochastic and deterministic modes as needed and projecting noise onto a null space. Based on future maximum entropy production, NeuroMOP contributes to a novel theory of neural variability that reconciles stochastic and deterministic behaviors within a single framework.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bd4ce492257f560caead52dd6d5cf618cf2665fb.pdf'}, 'supplementary_material': {'value': '/attachment/8f8c00e81f8a6216c0d6ac14a152a196d05ae8ee.zip'}, '_bibtex': {'value': \"@inproceedings{\\nmastrogiuseppe2024controlled,\\ntitle={Controlled maximal variability along with reliable performance in recurrent neural networks},\\nauthor={Chiara Mastrogiuseppe and Rub{\\\\'e}n Moreno-Bote},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yXW2dCTQdi}\\n}\"}, 'paperhash': {'value': 'mastrogiuseppe|controlled_maximal_variability_along_with_reliable_performance_in_recurrent_neural_networks'}},forum = 'yXW2dCTQdi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11170/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11170/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11170/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11170/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'yWq89o19wf',number = 12156,cdate = 1715717334970,pdate = 1727287996555,odate = 1730873945913,mdate = 1730873945931,tcdate = 1715717334970,tmdate = 1730873945931,ddate = None,content = {'title': {'value': 'User-Creator Feature Polarization in Recommender Systems with Dual Influence'}, 'authors': {'value': ['Tao Lin', 'Kun Jin', 'Andrew Estornell', 'Xiaoying Zhang', 'Yiling Chen', 'Yang Liu']}, 'authorids': {'value': ['~Tao_Lin2', '~Kun_Jin1', '~Andrew_Estornell1', '~Xiaoying_Zhang3', '~Yiling_Chen1', '~Yang_Liu3']}, 'keywords': {'value': ['recommender systems', 'performativity', 'preference dynamics', 'diversity', 'polarization']}, 'TLDR': {'value': 'We show that recommender systems with dual influences on users and creators are guaranteed to polarize, and discuss how to prevent it.'}, 'abstract': {'value': \"Recommender systems serve the dual purpose of presenting relevant content to users and helping content creators reach their target audience. The dual nature of these systems naturally influences both users and creators: users' preferences are affected by the items they are recommended, while creators may be incentivized to alter their content to attract more users. We define a model, called user-creator feature dynamics, to capture the dual influence of recommender systems. We prove that a recommender system with dual influence is guaranteed to polarize, causing diversity loss in the system. We then investigate, both theoretically and empirically, approaches for mitigating polarization and promoting diversity in recommender systems. Unexpectedly, we find that common diversity-promoting approaches do not work in the presence of dual influence, while relevancy-optimizing methods like top-$k$ truncation can prevent polarization and improve diversity of the system.\"}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/85ebe96bf9fedbc2fcdde28d66e0c8df3b4c3061.pdf'}, 'supplementary_material': {'value': '/attachment/aec25873925b9e309d00954f94bd9e571d640026.zip'}, '_bibtex': {'value': '@inproceedings{\\nlin2024usercreator,\\ntitle={User-Creator Feature Polarization in Recommender Systems with Dual Influence},\\nauthor={Tao Lin and Kun Jin and Andrew Estornell and Xiaoying Zhang and Yiling Chen and Yang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yWq89o19wf}\\n}'}, 'paperhash': {'value': 'lin|usercreator_feature_polarization_in_recommender_systems_with_dual_influence'}},forum = 'yWq89o19wf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12156/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12156/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12156/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12156/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yWSxjlFsmX',number = 8092,cdate = 1715652748746,pdate = 1727287868021,odate = 1730873909275,mdate = 1730873909292,tcdate = 1715652748746,tmdate = 1730873909292,ddate = None,content = {'title': {'value': 'Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?'}, 'authors': {'value': ['Yang Dai', 'Oubo Ma', 'Longfei Zhang', 'Xingxing Liang', 'Shengchao Hu', 'Mengzhu Wang', 'Shouling Ji', 'Jincai Huang', 'Li Shen']}, 'authorids': {'value': ['~Yang_Dai2', '~Oubo_Ma1', '~Longfei_Zhang3', '~Xingxing_Liang1', '~Shengchao_Hu1', '~Mengzhu_Wang1', '~Shouling_Ji1', '~Jincai_Huang1', '~Li_Shen1']}, 'keywords': {'value': ['Offline RL; Trajectory Optimization; Mamba']}, 'abstract': {'value': \"Transformer-based trajectory optimization methods have demonstrated exceptional performance in offline Reinforcement Learning (offline RL). Yet, it poses challenges due to substantial parameter size and limited scalability, which is particularly critical in sequential decision-making scenarios where resources are constrained such as in robots and drones with limited computational power. Mamba, a promising new linear-time sequence model, offers performance on par with transformers while delivering substantially fewer parameters on long sequences. As it remains unclear whether Mamba is compatible with trajectory optimization, this work aims to conduct comprehensive experiments to explore the potential of Decision Mamba (dubbed DeMa) in offline RL from the aspect of data structures and essential components with the following insights: (1) Long sequences impose a significant computational burden without contributing to performance improvements since DeMa's focus on sequences diminishes approximately exponentially. Consequently, we introduce a Transformer-like DeMa as opposed to an RNN-like DeMa. (2) For the components of DeMa, we identify the hidden attention mechanism as a critical factor in its success, which can also work well with other residual structures and does not require position embedding. Extensive evaluations demonstrate that our specially designed DeMa is compatible with trajectory optimization and surpasses previous methods, outperforming Decision Transformer (DT) with higher performance while using 30\\\\% fewer parameters in Atari, and exceeding DT with only a quarter of the parameters in MuJoCo.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e8f05bc8b78365623dc8f45e047f65b46390a923.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndai2024is,\\ntitle={Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?},\\nauthor={Yang Dai and Oubo Ma and Longfei Zhang and Xingxing Liang and Shengchao Hu and Mengzhu Wang and Shouling Ji and Jincai Huang and Li Shen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yWSxjlFsmX}\\n}'}, 'TLDR': {'value': 'An investigation of Mamba in offline RL.'}, 'paperhash': {'value': 'dai|is_mamba_compatible_with_trajectory_optimization_in_offline_reinforcement_learning'}},forum = 'yWSxjlFsmX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8092/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8092/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8092/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8092/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'yW3tlSwusb',number = 21092,cdate = 1715800012188,pdate = 1727288248390,odate = 1730874004624,mdate = 1730874004642,tcdate = 1715800012188,tmdate = 1730874004642,ddate = None,content = {'title': {'value': 'Accelerating ERM for data-driven algorithm design using output-sensitive techniques'}, 'authors': {'value': ['Maria Florina Balcan', 'Christopher Seiler', 'Dravyansh Sharma']}, 'authorids': {'value': ['~Maria_Florina_Balcan1', '~Christopher_Seiler1', '~Dravyansh_Sharma1']}, 'keywords': {'value': ['Learning Theory', 'Data-driven Algorithm Design']}, 'abstract': {'value': 'Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters. An important open problem is the design of computationally efficient data-driven algorithms for combinatorial algorithm families with multiple parameters. As one fixes the problem instance and varies the parameters, the “dual” loss function typically has a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp transition boundaries. Motivated by prior empirical work, we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances. The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces. Our approach involves two novel ingredients – an output-sensitive algorithm for enumerating polytopes induced by a set of hyperplanes using tools from computational geometry, and an execution graph which compactly represents all the states the algorithm could attain for all possible parameter values. We illustrate our techniques by giving algorithms for pricing problems, linkage-based clustering and dynamic-programming based sequence alignment.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e63c1b5cb2a01f0376fee17da54317b5c6b743cc.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbalcan2024accelerating,\\ntitle={Accelerating {ERM} for data-driven algorithm design using output-sensitive techniques},\\nauthor={Maria Florina Balcan and Christopher Seiler and Dravyansh Sharma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yW3tlSwusb}\\n}'}, 'paperhash': {'value': 'balcan|accelerating_erm_for_datadriven_algorithm_design_using_outputsensitive_techniques'}},forum = 'yW3tlSwusb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21092/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21092/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21092/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21092/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yVzWlFhpRW',number = 10416,cdate = 1715693884347,pdate = 1727287939830,odate = 1730873928754,mdate = 1730873928766,tcdate = 1715693884347,tmdate = 1730873928766,ddate = None,content = {'title': {'value': 'Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking'}, 'authors': {'value': ['Roland Stolz', 'Hanna Krasowski', 'Jakob Thumm', 'Michael Eichelbeck', 'Philipp Gassert', 'Matthias Althoff']}, 'authorids': {'value': ['~Roland_Stolz1', '~Hanna_Krasowski1', '~Jakob_Thumm1', '~Michael_Eichelbeck1', '~Philipp_Gassert1', '~Matthias_Althoff1']}, 'keywords': {'value': ['Reinforcement Learning', 'Policy Gradient', 'Action Masking', 'Robotics', 'Continuous Actions']}, 'abstract': {'value': 'Continuous action spaces in reinforcement learning (RL) are commonly defined as multidimensional intervals. While intervals usually reflect the action boundaries for tasks well, they can be challenging for learning because the typically large global action space leads to frequent exploration of irrelevant actions. Yet, little task knowledge can be sufficient to identify significantly smaller state-specific sets of relevant actions. Focusing learning on these relevant actions can significantly improve training efficiency and effectiveness. In this paper, we propose to focus learning on the set of relevant actions and introduce three continuous action masking methods for exactly mapping the action space to the state-dependent set of relevant actions. Thus, our methods ensure that only relevant actions are executed, enhancing the predictability of the RL agent and enabling its use in safety-critical applications. We further derive the implications of the proposed methods on the policy gradient. Using proximal policy optimization ( PPO), we evaluate our methods on four control tasks, where the relevant action set is computed based on the system dynamics and a relevant state set. Our experiments show that the three action masking methods achieve higher final rewards and converge faster than the baseline without action masking.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This work introduces three action masking methods for continuous action spaces to focus the exploration of reinforcement learning on state-specific relevant actions, which enhances learning efficiency and effectiveness.'}, 'pdf': {'value': '/pdf/8a1eecafd6f9a1b8919878b8b034860552119122.pdf'}, 'supplementary_material': {'value': '/attachment/f1d4803a68bdb2523684be6a035fa100b8f5e177.zip'}, '_bibtex': {'value': '@inproceedings{\\nstolz2024excluding,\\ntitle={Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking},\\nauthor={Roland Stolz and Hanna Krasowski and Jakob Thumm and Michael Eichelbeck and Philipp Gassert and Matthias Althoff},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yVzWlFhpRW}\\n}'}, 'paperhash': {'value': 'stolz|excluding_the_irrelevant_focusing_reinforcement_learning_through_continuous_action_masking'}},forum = 'yVzWlFhpRW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10416/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10416/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10416/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10416/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yVu5dnPlqA',number = 5479,cdate = 1715537702544,pdate = 1727287784967,odate = 1730873884961,mdate = 1731981285673,tcdate = 1715537702544,tmdate = 1731981285673,ddate = None,content = {'title': {'value': 'MAmmoTH2: Scaling Instructions from the Web'}, 'authors': {'value': ['Xiang Yue', 'Tianyu Zheng', 'Ge Zhang', 'Wenhu Chen']}, 'authorids': {'value': ['~Xiang_Yue1', '~Tianyu_Zheng1', '~Ge_Zhang5', '~Wenhu_Chen3']}, 'keywords': {'value': ['large language models', 'instruction tuning', 'reasoning']}, 'abstract': {'value': 'Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B’s (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bd04b97c020c0de7784c77b26776ae56292d2d38.pdf'}, 'TLDR': {'value': 'We introduce a scalable approach to harvest 10M high-quality instruction data from web corpus for fine-tuning language models, significantly boosting their reasoning performance without costly human annotation or GPT-4 distillation.'}, '_bibtex': {'value': '@inproceedings{\\nyue2024mammoth,\\ntitle={{MA}mmo{TH}2: Scaling Instructions from the Web},\\nauthor={Xiang Yue and Tianyu Zheng and Ge Zhang and Wenhu Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yVu5dnPlqA}\\n}'}, 'paperhash': {'value': 'yue|mammoth2_scaling_instructions_from_the_web'}},forum = 'yVu5dnPlqA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5479/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5479/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5479/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5479/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'yUqUBGioBG',number = 18272,cdate = 1715784614235,pdate = 1727288176088,odate = 1730873988836,mdate = 1730873988847,tcdate = 1715784614235,tmdate = 1730873988847,ddate = None,content = {'title': {'value': 'Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations'}, 'authors': {'value': ['Yuli Slavutsky', 'Yuval Benjamini']}, 'authorids': {'value': ['~Yuli_Slavutsky1', '~Yuval_Benjamini1']}, 'keywords': {'value': ['Zero-Shot Learning', 'Distribution Shift', 'Out of Distribution Generalization', 'Robust Representation Learning']}, 'TLDR': {'value': 'This work tackles the challenge of learning data representations robust to class distribution shifts in zero-shot learning, by constructing synthetic data environments and harnessing out-of-distribution generalization techniques.'}, 'abstract': {'value': 'Zero-shot learning methods typically assume that the new, unseen classes encountered during deployment come from the same distribution as the the classes in the training set. However, real-world scenarios often involve class distribution shifts (e.g., in age or gender for person identification), posing challenges for zero-shot classifiers that rely on learned representations from training classes. In this work, we propose and analyze a model that assumes that the attribute responsible for the shift is unknown in advance. We show that in this setting, standard training may lead to non-robust representations. To mitigate this, we develop an algorithm for learning robust representations in which (a) synthetic data environments are constructed via hierarchical sampling, and (b) environment balancing penalization, inspired by out-of-distribution problems, is applied. We show that our algorithm improves generalization to diverse class distributions in both simulations and experiments on real-world datasets.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/56a6242c839cd0715aaa64931c08b31501e061f9.pdf'}, 'supplementary_material': {'value': '/attachment/98526b4d3d8fc8cf29cbd36b03e4a727d45f19a9.zip'}, '_bibtex': {'value': '@inproceedings{\\nslavutsky2024class,\\ntitle={Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations},\\nauthor={Yuli Slavutsky and Yuval Benjamini},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yUqUBGioBG}\\n}'}, 'paperhash': {'value': 'slavutsky|class_distribution_shifts_in_zeroshot_learning_learning_robust_representations'}},forum = 'yUqUBGioBG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18272/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18272/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18272/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18272/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yUckuDjAE0',number = 15656,cdate = 1715762398170,pdate = 1727288103406,odate = 1730873974030,mdate = 1730873974042,tcdate = 1715762398170,tmdate = 1730873974042,ddate = None,content = {'title': {'value': 'Learning Bregman Divergences with Application to Robustness'}, 'authors': {'value': ['Mohamed-Hicham LEGHETTAS', 'Markus Püschel']}, 'authorids': {'value': ['~Mohamed-Hicham_LEGHETTAS1', '~Markus_Püschel1']}, 'keywords': {'value': ['Bregman divergence', 'similarity and distance learning', 'mirror descent', 'corruption robustness.']}, 'TLDR': {'value': 'Just as the KL divergence is derived from the Shannon entropy, we generate Bregman divergences from learned base functions and apply them to obtain similarity measures for real-world image corruptions, which we then use for robustness training.'}, 'abstract': {'value': 'We propose a novel and general method to learn Bregman divergences from raw high-dimensional data that measure similarity between images in pixel space. As a prototypical application, we learn divergences that consider real-world corruptions of images (e.g., blur) as close to the original and noisy perturbations as far, even if in $L^p$-distance the opposite holds. We also show that the learned Bregman divergence excels on datasets of human perceptual similarity judgment, suggesting its utility in a range of applications. We then define adversarial attacks by replacing the projected gradient descent (PGD) with the mirror descent associated with the learned Bregman divergence, and use them to improve the state-of-the-art in robustness through adversarial training for common image corruptions. In particular, for the contrast corruption that was found problematic in prior work we achieve an accuracy that exceeds the $L^p$- and the LPIPS-based adversarially trained neural networks by a margin of 27.16\\\\% on the CIFAR-10-C corruption data set.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5f88f2e26fc915345a56b203801d1cc3dab0b5c5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nleghettas2024learning,\\ntitle={Learning Bregman Divergences with Application to Robustness},\\nauthor={Mohamed-Hicham LEGHETTAS and Markus P{\\\\\"u}schel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yUckuDjAE0}\\n}'}, 'paperhash': {'value': 'leghettas|learning_bregman_divergences_with_application_to_robustness'}},forum = 'yUckuDjAE0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15656/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15656/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15656/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15656/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yURca4wi2L',number = 11294,cdate = 1715703279479,pdate = 1727287966914,odate = 1730873937329,mdate = 1730873937352,tcdate = 1715703279479,tmdate = 1730873937352,ddate = None,content = {'title': {'value': 'Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations'}, 'authors': {'value': ['Haoming Cai', 'Jingxi Chen', 'Brandon Y. Feng', 'Weiyun Jiang', 'Mingyang Xie', 'Kevin Zhang', 'Cornelia Fermuller', 'Yiannis Aloimonos', 'Ashok Veeraraghavan', 'Christopher Metzler']}, 'authorids': {'value': ['~Haoming_Cai2', '~Jingxi_Chen1', '~Brandon_Y._Feng1', '~Weiyun_Jiang1', '~Mingyang_Xie1', '~Kevin_Zhang3', '~Cornelia_Fermuller3', '~Yiannis_Aloimonos1', '~Ashok_Veeraraghavan1', '~Christopher_Metzler1']}, 'keywords': {'value': ['Atmospheric Turbulence Mitigation']}, 'abstract': {'value': \"Atmospheric turbulence, caused by random fluctuations in the atmosphere's refractive index, introduces complex spatio-temporal distortions in imagery captured at long range. Video Atmospheric Turbulence Mitigation (ATM) aims to restore videos affected by these distortions. However, existing video ATM methods, both supervised and self-supervised, struggle to maintain temporally consistent mitigation across frames, leading to visually incoherent results. This limitation arises from the stochastic nature of atmospheric turbulence, which varies across space and time. Inspired by the observation that atmospheric turbulence induces high-frequency temporal variations, we propose ConVRT, a novel framework for consistent video restoration through turbulence. ConVRT introduces a neural video representation that explicitly decouples spatial and temporal information into a spatial content field and a temporal deformation field, enabling targeted regularization of the network's temporal representation capability. By leveraging the low-pass filtering properties of the regularized temporal representations, ConVRT effectively mitigates turbulence-induced temporal frequency variations and promotes temporal consistency. Furthermore, our training framework seamlessly integrates supervised pre-training on synthetic turbulence data with self-supervised learning on real-world videos, significantly improving the temporally consistent mitigation of ATM methods on diverse real-world data. More information can be found on our project page: https://convrt-2024.github.io/\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/7349d5429098cce355e5c0e065007015968bd4c3.zip'}, 'pdf': {'value': '/pdf/b24f5f37299924fd9dcef2c90341e7676d541ddb.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncai2024temporally,\\ntitle={Temporally Consistent Atmospheric Turbulence Mitigation with Neural Representations},\\nauthor={Haoming Cai and Jingxi Chen and Brandon Y. Feng and Weiyun Jiang and Mingyang Xie and Kevin Zhang and Cornelia Fermuller and Yiannis Aloimonos and Ashok Veeraraghavan and Christopher Metzler},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yURca4wi2L}\\n}'}, 'TLDR': {'value': 'Improving temporal consistency in video atmospheric turbulence mitigation by separating out and regularizing temporal-wise neural representations'}, 'paperhash': {'value': 'cai|temporally_consistent_atmospheric_turbulence_mitigation_with_neural_representations'}},forum = 'yURca4wi2L',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11294/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11294/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11294/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11294/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yTTomSJsSW',number = 13703,cdate = 1715742275817,pdate = 1727288047441,odate = 1730873959772,mdate = 1730873959783,tcdate = 1715742275817,tmdate = 1730873959783,ddate = None,content = {'title': {'value': 'Aligning Large Language Models with Representation Editing: A Control Perspective'}, 'authors': {'value': ['Lingkai Kong', 'Haorui Wang', 'Wenhao Mu', 'Yuanqi Du', 'Yuchen Zhuang', 'Yifei Zhou', 'Yue Song', 'Rongzhi Zhang', 'Kai Wang', 'Chao Zhang']}, 'authorids': {'value': ['~Lingkai_Kong1', '~Haorui_Wang1', '~Wenhao_Mu1', '~Yuanqi_Du1', '~Yuchen_Zhuang1', '~Yifei_Zhou1', '~Yue_Song1', '~Rongzhi_Zhang2', '~Kai_Wang5', '~Chao_Zhang15']}, 'keywords': {'value': ['Large language model', 'Alignment', 'Representation editing']}, 'TLDR': {'value': 'We propose a new method to align large language model from a stochastic optimal control perspective.'}, 'abstract': {'value': \"Aligning large language models (LLMs) with human objectives is crucial for real-world applications. However, fine-tuning LLMs for alignment often suffers from unstable training and requires substantial computing resources. Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities. To address these challenges, we propose aligning LLMs through representation editing. The core of our method is to view a pre-trained autoregressive LLM as a discrete-time stochastic dynamical system. To achieve alignment for specific objectives, we introduce external control signals into the state space of this language dynamical system. We train a value function directly on the hidden states according to the Bellman equation, enabling gradient-based optimization to obtain the optimal control signals at test time. Our experiments demonstrate that our method outperforms existing test-time alignment techniques while requiring significantly fewer resources compared to fine-tuning methods. Our code is available at [https://github.com/Lingkai-Kong/RE-Control](https://github.com/Lingkai-Kong/RE-Control).\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5b01199621eef2e71cc22c61871a279fc51beeba.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkong2024aligning,\\ntitle={Aligning Large Language Models with Representation Editing: A Control Perspective},\\nauthor={Lingkai Kong and Haorui Wang and Wenhao Mu and Yuanqi Du and Yuchen Zhuang and Yifei Zhou and Yue Song and Rongzhi Zhang and Kai Wang and Chao Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yTTomSJsSW}\\n}'}, 'paperhash': {'value': 'kong|aligning_large_language_models_with_representation_editing_a_control_perspective'}},forum = 'yTTomSJsSW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13703/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13703/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13703/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13703/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yS9xU6ANiA',number = 15431,cdate = 1715760178402,pdate = 1727288097915,odate = 1730873972966,mdate = 1730873972987,tcdate = 1715760178402,tmdate = 1730873972987,ddate = None,content = {'title': {'value': 'Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation'}, 'authors': {'value': ['Yikang Chen', 'Dehui du', 'Lili Tian']}, 'authorids': {'value': ['~Yikang_Chen1', '~Dehui_du1', '~Lili_Tian1']}, 'keywords': {'value': ['causality', 'causal inference', 'counterfactual estimation', 'importance sampling', 'normalizing flows']}, 'abstract': {'value': 'We propose an importance sampling method for tractable and efficient estimation of counterfactual expressions in general settings, named Exogenous Matching. By minimizing a common upper bound of counterfactual estimators, we transform the variance minimization problem into a conditional distribution learning problem, enabling its integration with existing conditional distribution modeling approaches. We validate the theoretical results through experiments under various types and settings of Structural Causal Models (SCMs) and demonstrate the outperformance on counterfactual estimation tasks compared to other existing importance sampling methods. We also explore the impact of injecting structural prior knowledge (counterfactual Markov boundaries) on the results. Finally, we apply this method to identifiable proxy SCMs and demonstrate the unbiasedness of the estimates, empirically illustrating the applicability of the method to practical scenarios.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d87540bc855077295fdd9a86acc73b4ca59a2097.pdf'}, 'supplementary_material': {'value': '/attachment/60a7dee54957af26b133d37ea7e39c27923e0de9.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024exogenous,\\ntitle={Exogenous Matching: Learning Good Proposals for Tractable Counterfactual Estimation},\\nauthor={Yikang Chen and Dehui du and Lili Tian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yS9xU6ANiA}\\n}'}, 'paperhash': {'value': 'chen|exogenous_matching_learning_good_proposals_for_tractable_counterfactual_estimation'}},forum = 'yS9xU6ANiA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15431/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15431/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15431/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15431/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yRuJqoWoCs',number = 8111,cdate = 1715653338332,pdate = 1727287868883,odate = 1730873909569,mdate = 1730873909583,tcdate = 1715653338332,tmdate = 1730873909583,ddate = None,content = {'title': {'value': '$SE(3)$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation'}, 'authors': {'value': ['Yinshuang Xu', 'Dian Chen', 'Katherine Liu', 'Sergey Zakharov', 'Rares Andrei Ambrus', 'Kostas Daniilidis', 'Vitor Campagnolo Guizilini']}, 'authorids': {'value': ['~Yinshuang_Xu1', '~Dian_Chen5', '~Katherine_Liu1', '~Sergey_Zakharov1', '~Rares_Andrei_Ambrus1', '~Kostas_Daniilidis1', '~Vitor_Campagnolo_Guizilini2']}, 'keywords': {'value': ['$SE(3)$ Equivariance', 'Stereo Depth Estimation']}, 'TLDR': {'value': 'We propose an $SE(3)$ equivariant model with spherical harmonics ray embeddings and demonstrate its effectiveness in the task of generalized stereo depth estimation.'}, 'abstract': {'value': 'Incorporating inductive bias by embedding geometric entities (such as rays) as input has proven successful in multi-view learning. However, the methods adopting this technique typically lack equivariance, which is crucial for effective 3D learning. Equivariance serves as a valuable inductive prior, aiding in the generation of robust multi-view features for 3D scene understanding. In this paper, we explore the application of equivariant multi-view learning to depth estimation, not only recognizing its significance for computer vision and robotics but also addressing the limitations of previous research. Most prior studies have either overlooked equivariance in this setting or achieved only approximate equivariance through data augmentation, which often leads to inconsistencies across different reference frames. To address this issue, we propose to embed $SE(3)$ equivariance into the Perceiver IO architecture. We employ Spherical Harmonics for positional encoding to ensure 3D rotation equivariance, and develop a specialized equivariant encoder and decoder within the Perceiver IO architecture. To validate our model, we applied it to the task of stereo depth estimation, achieving state of the art results on real-world datasets without explicit geometric constraints or extensive data augmentation.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e8039c44b88c7c3803572ada21d4746ef0778d7d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxu2024se,\\ntitle={\\\\${SE}(3)\\\\$ Equivariant Ray Embeddings for Implicit Multi-View Depth Estimation},\\nauthor={Yinshuang Xu and Dian Chen and Katherine Liu and Sergey Zakharov and Rares Andrei Ambrus and Kostas Daniilidis and Vitor Campagnolo Guizilini},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yRuJqoWoCs}\\n}'}, 'paperhash': {'value': 'xu|se3_equivariant_ray_embeddings_for_implicit_multiview_depth_estimation'}},forum = 'yRuJqoWoCs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8111/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8111/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8111/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8111/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'yRhrVaDOWE',number = 11023,cdate = 1715700195791,pdate = 1727287958270,odate = 1730873934301,mdate = 1730873934318,tcdate = 1715700195791,tmdate = 1730873934318,ddate = None,content = {'title': {'value': 'Diffusion-based Curriculum Reinforcement Learning'}, 'authors': {'value': ['Erdi Sayar', 'Giovanni Iacca', 'Ozgur S. Oguz', 'Alois Knoll']}, 'authorids': {'value': ['~Erdi_Sayar2', '~Giovanni_Iacca1', '~Ozgur_S._Oguz1', '~Alois_Knoll1']}, 'keywords': {'value': ['curriculum reinforcement learning', 'reinforcement learning', 'diffusion models']}, 'TLDR': {'value': 'A novel diffusion based curriculum reinforcement learning'}, 'abstract': {'value': 'Curriculum Reinforcement Learning (CRL) is an approach to facilitate the learning process of agents by structuring tasks in a sequence of increasing complexity. Despite its potential, many existing CRL methods struggle to efficiently guide agents toward desired outcomes, particularly in the absence of domain knowledge. This paper introduces DiCuRL (Diffusion Curriculum Reinforcement Learning), a novel method that leverages conditional diffusion models to generate curriculum goals. To estimate how close an agent is to achieving its goal, our method uniquely incorporates a $Q$-function and a trainable reward function based on Adversarial Intrinsic Motivation within the diffusion model. Furthermore, it promotes exploration through the inherent noising and denoising mechanism present in the diffusion models and is environment-agnostic. This combination allows for the generation of challenging yet achievable goals, enabling agents to learn effectively without relying on domain knowledge. We demonstrate the effectiveness of DiCuRL in three different maze environments and two robotic manipulation tasks simulated in MuJoCo, where it outperforms or matches nine state-of-the-art CRL algorithms from the literature.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c6d5c73ad71d17c7a0d816c227738b96c959bc7e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsayar2024diffusionbased,\\ntitle={Diffusion-based Curriculum Reinforcement Learning},\\nauthor={Erdi Sayar and Giovanni Iacca and Ozgur S. Oguz and Alois Knoll},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yRhrVaDOWE}\\n}'}, 'paperhash': {'value': 'sayar|diffusionbased_curriculum_reinforcement_learning'}},forum = 'yRhrVaDOWE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11023/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11023/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11023/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11023/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'yRRCH1OsGW',number = 20022,cdate = 1715793933176,pdate = 1727288223450,odate = 1730873998853,mdate = 1730873998865,tcdate = 1715793933176,tmdate = 1730873998865,ddate = None,content = {'title': {'value': 'Generative Modeling of Molecular Dynamics Trajectories'}, 'authors': {'value': ['Bowen Jing', 'Hannes Stark', 'Tommi Jaakkola', 'Bonnie Berger']}, 'authorids': {'value': ['~Bowen_Jing1', '~Hannes_Stark1', '~Tommi_S._Jaakkola1', '~Bonnie_Berger1']}, 'keywords': {'value': ['molecular dynamics', 'molecular simulation', 'transition paths', 'Boltzmann distribution', 'proteins']}, 'abstract': {'value': 'Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show preliminary results on scaling to protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at https://github.com/bjing2016/mdgen.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/da6d5f4f8604d64cfdf93e0c217c30eb2526a5cd.pdf'}, '_bibtex': {'value': '@inproceedings{\\njing2024generative,\\ntitle={Generative Modeling of Molecular Dynamics Trajectories},\\nauthor={Bowen Jing and Hannes Stark and Tommi Jaakkola and Bonnie Berger},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yRRCH1OsGW}\\n}'}, 'paperhash': {'value': 'jing|generative_modeling_of_molecular_dynamics_trajectories'}},forum = 'yRRCH1OsGW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20022/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20022/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20022/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20022/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yQL5tutdaH',number = 17207,cdate = 1715778113899,pdate = 1727288147210,odate = 1730873983483,mdate = 1730873983501,tcdate = 1715778113899,tmdate = 1730873983501,ddate = None,content = {'title': {'value': 'Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models'}, 'authors': {'value': ['Hongliang Wei', 'Xingtao Wang', 'Xianqi Zhang', 'Xiaopeng Fan', 'Debin Zhao']}, 'authorids': {'value': ['~Hongliang_Wei1', '~Xingtao_Wang1', '~Xianqi_Zhang1', '~Xiaopeng_Fan1', '~Debin_Zhao1']}, 'keywords': {'value': ['Large Vision-Language Models', 'Multimodal large language models', 'Multimodal', 'Object hallucination', 'Evaluation', 'Image caption']}, 'TLDR': {'value': 'The paper reveals  the correlation between the object hallucination degree and the length of image descriptions, and proposes a stable and comprehensive framework for evaluating object hallucination in large vision-language models..'}, 'abstract': {'value': 'Given different instructions, large vision-language models (LVLMs) exhibit different degrees of object hallucinations, posing a significant challenge to the evaluation of object hallucinations. Overcoming this challenge, existing object hallucination evaluation methods average the results obtained from a set of instructions. However, these methods fail to provide consistent evaluation across instruction sets that generate image descriptions of significantly different lengths. In this paper, we present the first systematic investigation of the effect of instructions on object hallucinations in LVLMs, with a specific focus on the role played by image description lengths. A valuable finding is that instructions indirectly affect hallucinations through the length of image descriptions. The longer the image description, the higher the object hallucination degree. Accordingly, we fit an informative length-hallucination curve, upon which a fine-grained evaluation framework named LeHaCE is introduced for evaluating object hallucinations at any given image description length. LeHaCE evaluates the object hallucination degree at a uniform image description length to mitigate the effect of description lengths, promoting stability and fairness. Moreover, LeHaCE incorporates the curve slope as an innovative hallucination evaluation metric, reflecting the extent to which the object hallucination degree is affected by the image description length, achieving a more comprehensive evaluation. Experimental results demonstrate that LeHaCE provides a more stable, fair, and comprehensive evaluation of object hallucinations in LVLMs compared to existing methods.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e83f0d24d3251d65852146a10845a58594271455.pdf'}, 'supplementary_material': {'value': '/attachment/cf926d128382eb1e41d016e813136a7829db8a55.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwei2024toward,\\ntitle={Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models},\\nauthor={Hongliang Wei and Xingtao Wang and Xianqi Zhang and Xiaopeng Fan and Debin Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yQL5tutdaH}\\n}'}, 'paperhash': {'value': 'wei|toward_a_stable_fair_and_comprehensive_evaluation_of_object_hallucination_in_large_visionlanguage_models'}},forum = 'yQL5tutdaH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17207/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17207/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17207/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17207/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yPPNi7vc7n',number = 8241,cdate = 1715656150012,pdate = 1727287873485,odate = 1730873910444,mdate = 1736646391906,tcdate = 1715656150012,tmdate = 1736646391906,ddate = None,content = {'title': {'value': \"Local Curvature Smoothing with Stein's Identity for Efficient Score Matching\"}, 'authors': {'value': ['GENKI OSADA', 'Makoto Shing', 'Takashi Nishide']}, 'authorids': {'value': ['~GENKI_OSADA1', '~Makoto_Shing1', '~Takashi_Nishide1']}, 'keywords': {'value': ['Score matching', 'score-based generative models', 'diffusion models']}, 'abstract': {'value': \"The training of score-based diffusion models (SDMs) is based on score matching. The challenge of score matching is that it includes a\\xa0computationally expensive Jacobian trace. While several methods have been proposed to avoid this computation, each has drawbacks, such as instability during training and approximating the learning as learning a denoising vector field rather than a true score.\\nWe propose a novel score matching variant, local curvature smoothing with Stein's identity (LCSS). The LCSS bypasses the Jacobian trace by applying Stein's identity, enabling regularization effectiveness and efficient computation. We show that LCSS surpasses existing methods in sample generation performance and matches the performance of denoising score matching, widely adopted by most SDMs, in evaluations such as FID, Inception score, and bits per dimension. Furthermore, we show that LCSS enables realistic image generation even at a high resolution of $1024 \\\\times 1024$.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/952d80f8d8e4120ffa7bc5db0426136ea9a8fdb5.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nosada2024local,\\ntitle={Local Curvature Smoothing with Stein's Identity for Efficient Score Matching},\\nauthor={GENKI OSADA and Makoto Shing and Takashi Nishide},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yPPNi7vc7n}\\n}\"}, 'paperhash': {'value': 'osada|local_curvature_smoothing_with_steins_identity_for_efficient_score_matching'}},forum = 'yPPNi7vc7n',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8241/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8241/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8241/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8241/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yOe6ajdslI',number = 8293,cdate = 1715657194111,pdate = 1727287875528,odate = 1730873911083,mdate = 1730873911102,tcdate = 1715657194111,tmdate = 1730873911102,ddate = None,content = {'title': {'value': 'AUC Maximization under Positive Distribution Shift'}, 'authors': {'value': ['Atsutoshi Kumagai', 'Tomoharu Iwata', 'Hiroshi Takahashi', 'Taishi Nishiyama', 'Yasuhiro Fujiwara']}, 'authorids': {'value': ['~Atsutoshi_Kumagai2', '~Tomoharu_Iwata1', '~Hiroshi_Takahashi1', '~Taishi_Nishiyama1', '~Yasuhiro_Fujiwara1']}, 'keywords': {'value': ['AUC maximization', 'distribution shift', 'domain adaptation', 'PU learning', 'imbalanced data']}, 'TLDR': {'value': 'In this paper, we propose a method for maximizing the AUC under the positive distribution shift by using labeled positive and unlabeled data in the training distribution and unlabeled data in the test distribution.'}, 'abstract': {'value': 'Maximizing the area under the receiver operating characteristic curve (AUC) is a popular approach to imbalanced binary classification problems. Existing AUC maximization methods usually assume that training and test distributions are identical. However, this assumption is often violated in practice due to {\\\\it a positive distribution shift}, where the negative-conditional density does not change but the positive-conditional density can vary. This shift often occurs in imbalanced classification since positive data are often more diverse and time-varying than negative data. To deal with this shift, we theoretically show that the AUC on the test distribution can be expressed by using the positive and marginal training densities and the marginal test density. Based on this result, we can maximize the AUC on the test distribution by using positive and unlabeled data in the training distribution and unlabeled data in the test distribution. The proposed method requires only positive labels in the training distribution as supervision. Moreover, the derived AUC has a simple form and thus is easy to implement. The effectiveness of the proposed method is shown with four real-world datasets.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6babeb80637c127be43e9a61c520ffe601db0123.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkumagai2024auc,\\ntitle={{AUC} Maximization under Positive Distribution Shift},\\nauthor={Atsutoshi Kumagai and Tomoharu Iwata and Hiroshi Takahashi and Taishi Nishiyama and Yasuhiro Fujiwara},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yOe6ajdslI}\\n}'}, 'paperhash': {'value': 'kumagai|auc_maximization_under_positive_distribution_shift'}},forum = 'yOe6ajdslI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8293/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8293/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8293/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8293/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yO5DVyCHZR',number = 9867,cdate = 1715687046959,pdate = 1727287923332,odate = 1730873924318,mdate = 1737011105707,tcdate = 1715687046959,tmdate = 1737011105707,ddate = None,content = {'title': {'value': 'A Simple and Optimal Approach for Universal Online Learning with Gradient Variations'}, 'authors': {'value': ['Yu-Hu Yan', 'Peng Zhao', 'Zhi-Hua Zhou']}, 'authorids': {'value': ['~Yu-Hu_Yan1', '~Peng_Zhao1', '~Zhi-Hua_Zhou2']}, 'keywords': {'value': ['universal online learning', 'gradient-variation regret']}, 'TLDR': {'value': 'This work proposes a simple universal approach with the optimal gradient-variation regret guarantees.'}, 'abstract': {'value': 'We investigate the problem of universal online learning with gradient-variation regret. Universal online learning aims to achieve regret guarantees without prior knowledge of the curvature of the online functions. Moreover, we study the problem-dependent gradient-variation regret as it plays a crucial role in bridging stochastic and adversarial optimization as well as game theory. In this work, we design a universal approach with the *optimal* gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by [Yan et al. [2023]](https://openreview.net/forum?id=AA1xrgAP5z). Our approach is *simple* since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only $1$ gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret. Concretely, previous works on gradient variations require controlling the algorithmic stability, which is challenging and leads to sub-optimal regret and less efficient algorithm design. Our analysis overcomes this issue by using a Bregman divergence negative term from linearization and a useful smoothness property.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a10687149c5522371473b053ba79e8a1fa64c75b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyan2024a,\\ntitle={A Simple and Optimal Approach for Universal Online Learning with Gradient Variations},\\nauthor={Yu-Hu Yan and Peng Zhao and Zhi-Hua Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yO5DVyCHZR}\\n}'}, 'paperhash': {'value': 'yan|a_simple_and_optimal_approach_for_universal_online_learning_with_gradient_variations'}},forum = 'yO5DVyCHZR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9867/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9867/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9867/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9867/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yMS7ansbr6',number = 6542,cdate = 1715597910885,pdate = 1727287818066,odate = 1730873894272,mdate = 1730873894290,tcdate = 1715597910885,tmdate = 1730873894290,ddate = None,content = {'title': {'value': 'Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes'}, 'authors': {'value': ['Weifeng Liu', 'Tianyi She', 'Jiawei Liu', 'Boheng Li', 'Dongyu Yao', 'Ziyou Liang', 'Run Wang']}, 'authorids': {'value': ['~Weifeng_Liu3', '~Tianyi_She2', '~Jiawei_Liu17', '~Boheng_Li1', '~Dongyu_Yao1', '~Ziyou_Liang1', '~Run_Wang1']}, 'keywords': {'value': ['DeepFake Detection', 'LipSync Detection', 'AI security']}, 'abstract': {'value': 'In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, but these methods also pose potential and severe security threats to humanity. DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud. However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods. Our preliminary experiments have shown that the effectiveness of the existing methods often drastically decrease or even fail when tackling lip-syncing videos.\\nIn this paper, for the first time, we propose a novel approach dedicated to lip-forgery identification that exploits the inconsistency between lip movements and audio signals. We also mimic human natural cognition by capturing subtle biological links between lips and head regions to boost accuracy. To better illustrate the effectiveness and advances of our proposed method, we create a high-quality LipSync dataset, AVLips, by employing the state-of-the-art lip generators. We hope this high-quality and diverse dataset could be well served the further research on this challenging and interesting field. Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines. Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations. Our method achieves an accuracy of up to 90.2% in real-world scenarios (e.g., WeChat video call) and shows its powerful capabilities in real scenario deployment.\\nTo facilitate the progress of this research community, we release all resources at https://github.com/AaronComo/LipFD.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e4d265e599081369073e1436266147e9fe673842.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nliu2024lips,\\ntitle={Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes},\\nauthor={Weifeng Liu and Tianyi She and Jiawei Liu and Boheng Li and Dongyu Yao and Ziyou Liang and Run Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yMS7ansbr6}\\n}'}, 'paperhash': {'value': 'liu|lips_are_lying_spotting_the_temporal_inconsistency_between_audio_and_visual_in_lipsyncing_deepfakes'}},forum = 'yMS7ansbr6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6542/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6542/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6542/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission6542/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yKvHJJE9le',number = 20789,cdate = 1715798344638,pdate = 1727288241997,odate = 1730874003386,mdate = 1734549600993,tcdate = 1715798344638,tmdate = 1734549600993,ddate = None,content = {'title': {'value': 'Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel'}, 'authors': {'value': ['Jialin Li', 'Marta Zagorowska', 'Giulia De Pasquale', 'Alisa Rupenyan', 'John Lygeros']}, 'authorids': {'value': ['~Jialin_Li6', '~Marta_Zagorowska1', '~Giulia_De_Pasquale1', '~Alisa_Rupenyan2', '~John_Lygeros1']}, 'keywords': {'value': ['Safe learning', 'Bayesian optimization', 'Time-varying optimization']}, 'abstract': {'value': 'Ensuring safety is a key aspect in sequential decision making problems, such as robotics or process control. The complexity of the underlying systems often makes finding the optimal decision challenging, especially when the safety-critical system is time-varying. Overcoming the problem of optimizing an unknown time-varying reward subject to unknown time-varying safety constraints, we propose TVSAFEOPT, a new algorithm built on Bayesian optimization with a spatio-temporal kernel. The algorithm is capable of safely tracking a time-varying safe region without the need for explicit change detection. Optimality guarantees are also provided for the algorithm when the optimization problem becomes stationary. We show that TVSAFEOPT compares favorably against SAFEOPT on synthetic data, both regarding safety and optimality. Evaluation on a realistic case study with gas compressors confirms that TVSAFEOPT ensures safety when solving time-varying optimization problems with unknown reward and safety functions.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8ccc16f75fc2b8ad413d7dc3ee80d673efeeab6c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024safe,\\ntitle={Safe Time-Varying Optimization based on Gaussian Processes with Spatio-Temporal Kernel},\\nauthor={Jialin Li and Marta Zagorowska and Giulia De Pasquale and Alisa Rupenyan and John Lygeros},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yKvHJJE9le}\\n}'}, 'paperhash': {'value': 'li|safe_timevarying_optimization_based_on_gaussian_processes_with_spatiotemporal_kernel'}},forum = 'yKvHJJE9le',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20789/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20789/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20789/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20789/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yDo1ynArjj',number = 5601,cdate = 1715554358762,pdate = 1727287789236,odate = 1730873886394,mdate = 1730873886413,tcdate = 1715554358762,tmdate = 1730873886413,ddate = None,content = {'title': {'value': 'Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion'}, 'authors': {'value': ['Boyuan Chen', 'Diego Martí Monsó', 'Yilun Du', 'Max Simchowitz', 'Russ Tedrake', 'Vincent Sitzmann']}, 'authorids': {'value': ['~Boyuan_Chen2', '~Diego_Martí_Monsó1', '~Yilun_Du1', '~Max_Simchowitz1', '~Russ_Tedrake1', '~Vincent_Sitzmann1']}, 'keywords': {'value': ['diffusion', 'sequence modeling', 'decision making', 'planning']}, 'abstract': {'value': \"This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture,  and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing/\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A novel way of training next-token prediction models to diffuse whole sequences at once enables long-horizon guidance, stable infinite rollout of continuous signals, and new planning & policy techniques.'}, 'pdf': {'value': '/pdf/5a6e9d157a4d33dc36773c5c32370c3c7941d6c2.pdf'}, 'supplementary_material': {'value': '/attachment/4b1a61d84de34141d52b2030bc58797dd96b5905.zip'}, '_bibtex': {'value': \"@inproceedings{\\nchen2024diffusion,\\ntitle={Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion},\\nauthor={Boyuan Chen and Diego Mart{\\\\'\\\\i} Mons{\\\\'o} and Yilun Du and Max Simchowitz and Russ Tedrake and Vincent Sitzmann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yDo1ynArjj}\\n}\"}, 'paperhash': {'value': 'chen|diffusion_forcing_nexttoken_prediction_meets_fullsequence_diffusion'}},forum = 'yDo1ynArjj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5601/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5601/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5601/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5601/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yDjojeIWO9',number = 253,cdate = 1713856307495,pdate = 1727287633797,odate = 1730873839468,mdate = 1730873839486,tcdate = 1713856307495,tmdate = 1730873839486,ddate = None,content = {'title': {'value': 'Transferable Adversarial Attacks on SAM and Its Downstream Models'}, 'authors': {'value': ['Song Xia', 'Wenhan Yang', 'Yi Yu', 'Xun Lin', 'Henghui Ding', 'LINGYU DUAN', 'Xudong Jiang']}, 'authorids': {'value': ['~Song_Xia1', '~Wenhan_Yang6', '~Yi_Yu5', '~Xun_Lin1', '~Henghui_Ding2', '~LINGYU_DUAN1', '~Xudong_Jiang1']}, 'keywords': {'value': ['Segment anything', 'Security of large fundation model', 'Transfer-based adversarial attack', 'Fine-tuning']}, 'abstract': {'value': 'The utilization of large foundational models has a dilemma: while fine-tuning downstream tasks from them holds promise for making use of the well-generalized knowledge in practical applications, their open accessibility also poses threats of adverse usage.\\nThis paper, for the first time, explores the feasibility of adversarial attacking various downstream models fine-tuned from the segment anything model (SAM), by solely utilizing the information from the open-sourced SAM. \\nIn contrast to prevailing transfer-based adversarial attacks, we demonstrate the existence of adversarial dangers even without accessing the downstream task and dataset to train a similar surrogate model.\\nTo enhance the effectiveness of the adversarial attack towards models fine-tuned on unknown datasets, we propose a universal meta-initialization (UMI) algorithm to extract the intrinsic vulnerability inherent in the foundation model, which is then utilized as the prior knowledge to guide the generation of adversarial perturbations.\\nMoreover, by formulating the gradient difference in the attacking process between the open-sourced SAM and its fine-tuned downstream models, we theoretically demonstrate that a deviation occurs in the adversarial update direction by directly maximizing the distance of encoded feature embeddings in the open-sourced SAM.\\nConsequently, we propose a gradient robust loss that simulates the associated uncertainty with gradient-based noise augmentation to enhance the robustness of generated adversarial examples (AEs) towards this deviation, thus improving the transferability.\\nExtensive experiments demonstrate the effectiveness of the proposed universal meta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs and their downstream models.\\nCode is available at https://github.com/xiasong0501/GRAT.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/57573eaf34a55e1f4cc6ab0db0b428f8ee35133a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxia2024transferable,\\ntitle={Transferable Adversarial Attacks on {SAM} and Its Downstream Models},\\nauthor={Song Xia and Wenhan Yang and Yi Yu and Xun Lin and Henghui Ding and LINGYU DUAN and Xudong Jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yDjojeIWO9}\\n}'}, 'paperhash': {'value': 'xia|transferable_adversarial_attacks_on_sam_and_its_downstream_models'}},forum = 'yDjojeIWO9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission253/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission253/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission253/-/Revision', 'NeurIPS.cc/2024/Conference/Submission253/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yCh1z6Dcto',number = 21556,cdate = 1715802479954,pdate = 1727288257855,odate = 1730874006171,mdate = 1730874006183,tcdate = 1715802479954,tmdate = 1730874006183,ddate = None,content = {'title': {'value': 'Stepping Forward on the Last Mile'}, 'authors': {'value': ['Chen Feng', 'Shaojie Zhuo', 'Xiaopeng Zhang', 'Ramchalam Kinattinkara Ramakrishnan', 'Zhaocong Yuan', 'Andrew Zou Li']}, 'authorids': {'value': ['~Chen_Feng9', '~Shaojie_Zhuo3', '~Xiaopeng_Zhang1', '~Ramchalam_Kinattinkara_Ramakrishnan1', '~Zhaocong_Yuan1', '~Andrew_Zou_Li1']}, 'keywords': {'value': ['On-device model adaptation', 'Fixed-point forward gradient learning', 'Low memory', 'Edge devices']}, 'TLDR': {'value': 'On-device model adaptation with fixed-point forward gradient learning'}, 'abstract': {'value': 'Continuously adapting pre-trained models to local data on resource constrained edge devices is the \\\\emph{last mile} for model deployment. However, as models increase in size and depth, backpropagation requires a large amount of memory, which becomes prohibitive for edge devices. In addition, most existing low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are designed as fixed-point inference accelerators, without training capabilities. Forward gradients, solely based on directional derivatives computed from two forward calls, have been recently used for model training, with substantial savings in computation and memory. However, the performance of quantized training with fixed-point forward gradients remains unclear. In this paper, we investigate the feasibility of on-device training using fixed-point forward gradients, by conducting comprehensive experiments across a variety of deep learning benchmark tasks in both vision and audio domains. We propose a series of algorithm enhancements that further reduce the memory footprint, and the accuracy gap compared to backpropagation. An empirical study on how training with forward gradients navigates in the loss landscape is further explored. Our results demonstrate that on the last mile of model customization on edge devices, training with fixed-point forward gradients is a feasible and practical approach.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6c685c934218042e4f6892abccb9b4a82a86be97.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfeng2024stepping,\\ntitle={Stepping Forward on the Last Mile},\\nauthor={Chen Feng and Shaojie Zhuo and Xiaopeng Zhang and Ramchalam Kinattinkara Ramakrishnan and Zhaocong Yuan and Andrew Zou Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yCh1z6Dcto}\\n}'}, 'paperhash': {'value': 'feng|stepping_forward_on_the_last_mile'}},forum = 'yCh1z6Dcto',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21556/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21556/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21556/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21556/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yBrxziByeG',number = 4309,cdate = 1715400110881,pdate = 1727287747214,odate = 1730873874972,mdate = 1730873874990,tcdate = 1715400110881,tmdate = 1730873874990,ddate = None,content = {'title': {'value': 'Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model'}, 'authors': {'value': ['Hao Zhang', 'Lei Cao', 'Jiayi Ma']}, 'authorids': {'value': ['~Hao_Zhang26', '~Lei_Cao10', '~Jiayi_Ma2']}, 'keywords': {'value': ['Image fusion', 'multi-modal fusion', 'text', 'diffusion']}, 'TLDR': {'value': 'Interactive Multi-modal Image Fusion'}, 'abstract': {'value': 'Existing multi-modal image fusion methods fail to address the compound degradations presented in source images, resulting in fusion images plagued by noise, color bias, improper exposure, etc. Additionally, these methods often overlook the specificity of foreground objects, weakening the salience of the objects of interest within the fused images. To address these challenges, this study proposes a novel interactive multi-modal image fusion framework based on the text-modulated diffusion model, called Text-DiFuse. First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion. This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion. Second, by embedding the combination of the text and zero-shot location model into the diffusion fusion process, a text-controlled fusion re-modulation strategy is developed. This enables user-customized text control to improve fusion performance and highlight foreground objects in the fused images. Extensive experiments on diverse public datasets show that our Text-DiFuse achieves state-of-the-art fusion performance across various scenarios with complex degradation. Moreover, the semantic segmentation experiment validates the significant enhancement in semantic performance achieved by our text-controlled fusion re-modulation strategy. The code is publicly available at https://github.com/Leiii-Cao/Text-DiFuse.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1870be1308452cfd34778a0947c89002562387ed.pdf'}, 'supplementary_material': {'value': '/attachment/48520d51bcdb809bda8090cf60d05790fbc0702a.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024textdifuse,\\ntitle={Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model},\\nauthor={Hao Zhang and Lei Cao and Jiayi Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yBrxziByeG}\\n}'}, 'paperhash': {'value': 'zhang|textdifuse_an_interactive_multimodal_image_fusion_framework_based_on_textmodulated_diffusion_model'}},forum = 'yBrxziByeG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4309/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4309/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4309/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4309/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yBHbeSpwYS',number = 4998,cdate = 1715494021160,pdate = 1727287767455,odate = 1730873880966,mdate = 1730873880984,tcdate = 1715494021160,tmdate = 1730873880984,ddate = None,content = {'title': {'value': 'In Pursuit of Causal Label Correlations for Multi-label Image Recognition'}, 'authors': {'value': ['Zhao-Min Chen', 'Xin Jin', 'YisuGe', 'Sixian Chan']}, 'authorids': {'value': ['~Zhao-Min_Chen3', '~Xin_Jin15', '~YisuGe1', '~Sixian_Chan1']}, 'keywords': {'value': ['Multi-label', 'Label correlation', 'Causal intervention']}, 'TLDR': {'value': 'Utilizing casual intervention theory to capture casual label correlations.'}, 'abstract': {'value': 'Multi-label image recognition aims to predict all objects present in an input image. A common belief is that modeling the correlations between objects is beneficial for multi-label recognition. However, this belief has been recently challenged as label correlations may mislead the classifier in testing, due to the possible contextual bias in training. Accordingly, a few of recent works not only discarded label correlation modeling, but also advocated to remove contextual information for multi-label image recognition. This work explicitly explores label correlations for multi-label image recognition based on a principled causal intervention approach. With causal intervention, we pursue causal label correlations and suppress spurious label correlations, as the former tend to convey useful contextual cues while the later may mislead the classifier. Specifically, we decouple label-specific features with a Transformer decoder attached to the backbone network, and model the confounders which may give rise to spurious correlations by clustering spatial features of all training images. Based on label-specific features and confounders, we employ a cross-attention module to implement causal intervention, quantifying the causal correlations from all object categories to each predicted object category. Finally, we obtain image labels by combining the predictions from decoupled features and causal label correlations. Extensive experiments clearly validate the effectiveness of our approach for multi-label image recognition in both common and cross-dataset settings.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/03a5e626cf9b315df0a1676f88fc6226ff69ec95.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024in,\\ntitle={In Pursuit of Causal Label Correlations for Multi-label Image Recognition},\\nauthor={Zhao-Min Chen and Xin Jin and YisuGe and Sixian Chan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yBHbeSpwYS}\\n}'}, 'paperhash': {'value': 'chen|in_pursuit_of_causal_label_correlations_for_multilabel_image_recognition'}},forum = 'yBHbeSpwYS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4998/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4998/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4998/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4998/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yAa5l92TtQ',number = 11119,cdate = 1715701133690,pdate = 1727287961303,odate = 1730873935117,mdate = 1730873935135,tcdate = 1715701133690,tmdate = 1730873935135,ddate = None,content = {'title': {'value': 'Proving Theorems Recursively'}, 'authors': {'value': ['Haiming Wang', 'Huajian Xin', 'Zhengying Liu', 'Wenda Li', 'Yinya Huang', 'Jianqiao Lu', 'Zhicheng YANG', 'Jing Tang', 'Jian Yin', 'Zhenguo Li', 'Xiaodan Liang']}, 'authorids': {'value': ['~Haiming_Wang1', '~Huajian_Xin1', '~Zhengying_Liu2', '~Wenda_Li1', '~Yinya_Huang1', '~Jianqiao_Lu1', '~Zhicheng_YANG5', '~Jing_Tang5', '~Jian_Yin3', '~Zhenguo_Li1', '~Xiaodan_Liang2']}, 'keywords': {'value': ['Nerual Theorem Proving', 'Language Model']}, 'abstract': {'value': \"Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation. However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs. To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover. Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture. Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels. This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels. Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods. POETRY on miniF2F achieves an average proving success rate improvement of 5.1%. Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/d9713b9fee3d90ce5089e2db9efa2325fcc37b0f.zip'}, 'pdf': {'value': '/pdf/c496858b5797ffde1be425dcc94d5a7221a5dfb9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024proving,\\ntitle={Proving Theorems Recursively},\\nauthor={Haiming Wang and Huajian Xin and Zhengying Liu and Wenda Li and Yinya Huang and Jianqiao Lu and Zhicheng YANG and Jing Tang and Jian Yin and Zhenguo Li and Xiaodan Liang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yAa5l92TtQ}\\n}'}, 'paperhash': {'value': 'wang|proving_theorems_recursively'}},forum = 'yAa5l92TtQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11119/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11119/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11119/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11119/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'yAAQWBMGiT',number = 8043,cdate = 1715651096946,pdate = 1727287866312,odate = 1730873908777,mdate = 1734550039541,tcdate = 1715651096946,tmdate = 1734550039541,ddate = None,content = {'title': {'value': 'Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning'}, 'authors': {'value': ['Yijun Dong', 'Hoang Phan', 'Xiang Pan', 'Qi Lei']}, 'authorids': {'value': ['~Yijun_Dong1', '~Hoang_Phan1', '~Xiang_Pan3', '~Qi_Lei1']}, 'keywords': {'value': ['Data selection', 'Finetuning', 'Sketching', 'Johnson-Lindenstrauss transform']}, 'abstract': {'value': 'We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\\\\mathcal{S}$; (ii) then the variance is reduced over $\\\\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\\\\mathcal{S}$ preserves the fast-rate generalization $O(\\\\dim(\\\\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/342630203974cf3966cb02c9c856602a6fdba381.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024sketchy,\\ntitle={Sketchy Moment Matching: Toward Fast and Provable Data Selection for Finetuning},\\nauthor={Yijun Dong and Hoang Phan and Xiang Pan and Qi Lei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=yAAQWBMGiT}\\n}'}, 'TLDR': {'value': 'We present a theory on data selection for high-dimensional ridge regression that inspires a fast and effective data selection algorithm for finetuning.'}, 'paperhash': {'value': 'dong|sketchy_moment_matching_toward_fast_and_provable_data_selection_for_finetuning'}},forum = 'yAAQWBMGiT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8043/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8043/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8043/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8043/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y9zIRxshzj',number = 18485,cdate = 1715785808564,pdate = 1727288185268,odate = 1730873990086,mdate = 1730873990104,tcdate = 1715785808564,tmdate = 1730873990104,ddate = None,content = {'title': {'value': 'Causal Discovery from Event Sequences by Local Cause-Effect Attribution'}, 'authors': {'value': ['Joscha Cüppers', 'Sascha Xu', 'Ahmed Musa', 'Jilles Vreeken']}, 'authorids': {'value': ['~Joscha_Cüppers1', '~Sascha_Xu1', '~Ahmed_Musa1', '~Jilles_Vreeken2']}, 'keywords': {'value': ['causality', 'causal discovery', 'event sequences']}, 'TLDR': {'value': 'We introduce a causal discovery method for event sequences, that matches individual events to individual causing events.'}, 'abstract': {'value': 'Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.\\n\\nWe base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order. Extensive evaluation shows that Cascade outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/d51dac59a82ef68b26357bde7250dabe465005c3.zip'}, 'pdf': {'value': '/pdf/58aefbdcb2bfb0c32b39ede7f68c3577797ad7e1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nc{\\\\\"u}ppers2024causal,\\ntitle={Causal Discovery from Event Sequences by Local Cause-Effect Attribution},\\nauthor={Joscha C{\\\\\"u}ppers and Sascha Xu and Ahmed Musa and Jilles Vreeken},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y9zIRxshzj}\\n}'}, 'paperhash': {'value': 'cüppers|causal_discovery_from_event_sequences_by_local_causeeffect_attribution'}},forum = 'y9zIRxshzj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18485/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18485/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18485/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18485/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y9sHKrdnRt',number = 146,cdate = 1713840589141,pdate = 1727287631416,odate = 1730873838688,mdate = 1730873838707,tcdate = 1713840589141,tmdate = 1730873838707,ddate = None,content = {'title': {'value': 'MC-DiT: Contextual Enhancement via Clean-to-Clean Reconstruction for Masked Diffusion Models'}, 'authors': {'value': ['Guanghao Zheng', 'Yuchen Liu', 'Wenrui Dai', 'Chenglin Li', 'Junni Zou', 'Hongkai Xiong']}, 'authorids': {'value': ['~Guanghao_Zheng1', '~Yuchen_Liu4', '~Wenrui_Dai1', '~Chenglin_Li2', '~Junni_Zou1', '~Hongkai_Xiong1']}, 'keywords': {'value': ['Diffusion', 'Transformer', 'Masked Autoencoder']}, 'abstract': {'value': 'Diffusion Transformer (DiT) is emerging as a cutting-edge trend in the landscape of generative diffusion models for image generation. Recently, masked-reconstruction strategies have been considered to improve the efficiency and semantic consistency in training DiT but suffer from deficiency in contextual information extraction. In this paper, we provide a new insight to reveal that noisy-to-noisy masked-reconstruction harms sufficient utilization of contextual information. We further demonstrate the insight with theoretical analysis and empirical study on the mutual information between unmasked and masked patches. Guided by such insight, we propose a novel training paradigm named MC-DiT for fully learning contextual information via diffusion denoising at different noise variances with clean-to-clean mask-reconstruction. Moreover, to avoid model collapse, we design two complementary branches of DiT decoders for enhancing the use of noisy patches and mitigating excessive reliance on clean patches in reconstruction. Extensive experimental results on 256$\\\\times$256 and 512$\\\\times$512 image generation on the ImageNet dataset demonstrate that the proposed MC-DiT achieves state-of-the-art performance in unconditional and conditional image generation with enhanced convergence speed.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/44798d431529adc7582ec95a03e0b069dec11d02.pdf'}, 'supplementary_material': {'value': '/attachment/94b98a0c9c66377060e2bc722ad7402f67603111.zip'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024mcdit,\\ntitle={{MC}-DiT: Contextual Enhancement via Clean-to-Clean Reconstruction for Masked Diffusion Models},\\nauthor={Guanghao Zheng and Yuchen Liu and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y9sHKrdnRt}\\n}'}, 'paperhash': {'value': 'zheng|mcdit_contextual_enhancement_via_cleantoclean_reconstruction_for_masked_diffusion_models'}},forum = 'y9sHKrdnRt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission146/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission146/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission146/-/Revision', 'NeurIPS.cc/2024/Conference/Submission146/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y9huwsnGRJ',number = 8698,cdate = 1715667839733,pdate = 1727287888067,odate = 1730873914015,mdate = 1730873914033,tcdate = 1715667839733,tmdate = 1730873914033,ddate = None,content = {'title': {'value': 'Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving'}, 'authors': {'value': ['Jianbiao Mei', 'Yukai Ma', 'Xuemeng Yang', 'Licheng Wen', 'Xinyu Cai', 'Xin Li', 'Daocheng Fu', 'Bo Zhang', 'Pinlong Cai', 'Min Dou', 'Botian Shi', 'Liang He', 'Yong Liu', 'Yu Qiao']}, 'authorids': {'value': ['~Jianbiao_Mei1', '~Yukai_Ma1', '~Xuemeng_Yang1', '~Licheng_Wen1', '~Xinyu_Cai2', '~Xin_Li50', '~Daocheng_Fu1', '~Bo_Zhang17', '~Pinlong_Cai1', '~Min_Dou1', '~Botian_Shi1', '~Liang_He2', '~Yong_Liu11', '~Yu_Qiao1']}, 'keywords': {'value': ['Autonomous Driving', 'Dual-process System', 'Knowledge-Driven', 'Vision Language Model']}, 'abstract': {'value': 'Autonomous driving has advanced significantly due to sensors, machine learning, and artificial intelligence improvements. However, prevailing methods struggle with intricate scenarios and causal relationships, hindering adaptability and interpretability in varied environments. To address the above problems, we introduce LeapAD, a novel paradigm for autonomous driving inspired by the human cognitive process. Specifically, LeapAD emulates human attention by selecting critical objects relevant to driving decisions, simplifying environmental interpretation, and mitigating decision-making complexities. Additionally, LeapAD incorporates an innovative dual-process decision-making module, which consists of an Analytic Process (System-II) for thorough analysis and reasoning, along with a Heuristic Process (System-I) for swift and empirical processing. The Analytic Process leverages its logical reasoning to accumulate linguistic driving experience, which is then transferred to the Heuristic Process by supervised fine-tuning. Through reflection mechanisms and a growing memory bank, LeapAD continuously improves itself from past mistakes in a closed-loop environment. Closed-loop testing in CARLA shows that LeapAD outperforms all methods relying solely on camera input, requiring 1-2 orders of magnitude less labeled data. Experiments also demonstrate that as the memory bank expands, the Heuristic Process with only 1.8B parameters can inherit the knowledge from a GPT-4 powered Analytic Process and achieve continuous performance improvement. Project page: https://pjlab-adg.github.io/LeapAD'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'LeapAD, a new autonomous driving paradigm inspired by human cognition, improves adaptability and interpretability in complex scenarios through dual-process decision-making and continuous learning from past experiences.'}, 'pdf': {'value': '/pdf/b1babf61241a3da282fb13f4bf5bd64b4d8f7e45.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmei2024continuously,\\ntitle={Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving},\\nauthor={Jianbiao Mei and Yukai Ma and Xuemeng Yang and Licheng Wen and Xinyu Cai and Xin Li and Daocheng Fu and Bo Zhang and Pinlong Cai and Min Dou and Botian Shi and Liang He and Yong Liu and Yu Qiao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y9huwsnGRJ}\\n}'}, 'paperhash': {'value': 'mei|continuously_learning_adapting_and_improving_a_dualprocess_approach_to_autonomous_driving'}},forum = 'y9huwsnGRJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8698/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8698/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8698/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8698/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'y929esCZNJ',number = 6152,cdate = 1715585411666,pdate = 1727287806548,odate = 1730873890909,mdate = 1730873890928,tcdate = 1715585411666,tmdate = 1730873890928,ddate = None,content = {'title': {'value': 'MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts'}, 'authors': {'value': ['Rachel Teo', 'Tan Minh Nguyen']}, 'authorids': {'value': ['~Rachel_Teo1', '~Tan_Minh_Nguyen1']}, 'keywords': {'value': ['Sparse Mixture of Experts', 'optimization', 'gradient descent', 'momentum', 'adam']}, 'TLDR': {'value': 'We first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem and then integrate momentum into SMoE to develop a new family of SMoEs, named MomentumSMoE.'}, 'abstract': {'value': \"Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase in parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs, named MomentumSMoE. We theoretically prove and numerically validate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/72b117c375c15a0ef6ea9c489740b45ea2c3e8ed.pdf'}, 'supplementary_material': {'value': '/attachment/174f42807904accb758129ab1cd95efc3a54efcd.zip'}, '_bibtex': {'value': '@inproceedings{\\nteo2024momentumsmoe,\\ntitle={Momentum{SM}oE: Integrating Momentum into Sparse Mixture of Experts},\\nauthor={Rachel Teo and Tan Minh Nguyen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y929esCZNJ}\\n}'}, 'paperhash': {'value': 'teo|momentumsmoe_integrating_momentum_into_sparse_mixture_of_experts'}},forum = 'y929esCZNJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6152/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6152/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6152/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6152/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y8Rm4VNRPH',number = 11970,cdate = 1715714221245,pdate = 1727287990092,odate = 1730873944189,mdate = 1736918075039,tcdate = 1715714221245,tmdate = 1736918075039,ddate = None,content = {'title': {'value': 'Parallelizing Linear Transformers with the Delta Rule over Sequence Length'}, 'authors': {'value': ['Songlin Yang', 'Bailin Wang', 'Yu Zhang', 'Yikang Shen', 'Yoon Kim']}, 'authorids': {'value': ['~Songlin_Yang1', '~Bailin_Wang3', '~Yu_Zhang36', '~Yikang_Shen1', '~Yoon_Kim1']}, 'keywords': {'value': ['linear transformer', 'recurrent neural network', 'hardware-aware algorithm', 'state space model', 'sequence modeling', 'efficient training']}, 'TLDR': {'value': 'DeltaNet, i.e., Linear Transformer with delta rule, suffers from slow recurrent training. This work develops a novel algorithm that enables the parallelization of training over sequence length, allowing for large-scale experiments'}, 'abstract': {'value': 'Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/397e5724c60c7bb0691c6436dc8a56f5a0336f4f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024parallelizing,\\ntitle={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},\\nauthor={Songlin Yang and Bailin Wang and Yu Zhang and Yikang Shen and Yoon Kim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y8Rm4VNRPH}\\n}'}, 'paperhash': {'value': 'yang|parallelizing_linear_transformers_with_the_delta_rule_over_sequence_length'}},forum = 'y8Rm4VNRPH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11970/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11970/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11970/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11970/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y8P633E5HQ',number = 11651,cdate = 1715708215520,pdate = 1727287978439,odate = 1730873940820,mdate = 1734900606780,tcdate = 1715708215520,tmdate = 1734900606780,ddate = None,content = {'title': {'value': 'Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters'}, 'authors': {'value': ['Ya-Wei Eileen Lin', 'Ronen Talmon', 'Ron Levie']}, 'authorids': {'value': ['~Ya-Wei_Eileen_Lin1', '~Ronen_Talmon2', '~Ron_Levie1']}, 'keywords': {'value': ['graph machine learning', 'graph signal processing', 'equivariant machine learning', 'geometric deep learning', 'spectral method', 'nonlinear method']}, 'TLDR': {'value': 'We present a novel equivariant graph neural network based on nonlinear operations in the spectral domain.'}, 'abstract': {'value': 'Equivariant machine learning is an approach for designing deep learning models that respect the symmetries of the problem, with the aim of reducing model complexity and improving generalization. \\nIn this paper, we focus on an extension of shift equivariance, which is the basis of convolution networks on images, to general graphs. Unlike images, graphs do not have a natural notion of domain translation. \\nTherefore, we consider the graph functional shifts as the symmetry group: the unitary operators that commute with the graph shift operator. \\nNotably, such symmetries operate in the signal space rather than directly in the spatial space.\\nWe remark that each linear filter layer of a standard spectral graph neural network (GNN) commutes with graph functional shifts, but the activation function breaks this symmetry. Instead, we propose nonlinear spectral filters (NLSFs) that are fully equivariant to graph functional shifts and show that they have universal approximation properties. \\nThe proposed NLSFs are based on a new form of spectral domain that is transferable between graphs. \\nWe demonstrate the superior performance of NLSFs over existing spectral GNNs in node and graph classification benchmarks.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7cf055b02c2ad0760653ed4c078ae8d3ffd7a0eb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024equivariant,\\ntitle={Equivariant Machine Learning on Graphs with Nonlinear Spectral Filters},\\nauthor={Ya-Wei Eileen Lin and Ronen Talmon and Ron Levie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y8P633E5HQ}\\n}'}, 'paperhash': {'value': 'lin|equivariant_machine_learning_on_graphs_with_nonlinear_spectral_filters'}},forum = 'y8P633E5HQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11651/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11651/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11651/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11651/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y8HUXkwAOg',number = 15991,cdate = 1715765547019,pdate = 1727288113268,odate = 1730873976386,mdate = 1730873976437,tcdate = 1715765547019,tmdate = 1730873976437,ddate = None,content = {'title': {'value': 'ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions'}, 'authors': {'value': ['Etienne Vareille', 'Michele Linardi', 'Ioannis Tsamardinos', 'Vassilis Christophides']}, 'authorids': {'value': ['~Etienne_Vareille1', '~Michele_Linardi1', '~Ioannis_Tsamardinos1', '~Vassilis_Christophides1']}, 'keywords': {'value': ['Multivariate Time Series Causal Discovery', 'Forecasting', 'Explanations', 'Multiple Markov Boundaries']}, 'abstract': {'value': 'We consider the problem of selecting all the minimal-size subsets of multivariate time-series (TS) variables whose past leads to an optimal predictive model for the future (forecasting) of a given target variable (multiple feature selection problem for times-series). Identifying these subsets leads to gaining insights, domain intuition,and a better understanding of the data-generating mechanism; it is often the first step in causal modeling. While identifying a single solution to the feature selection problem suffices for forecasting purposes, identifying all such minimal-size, optimally predictive subsets is necessary for knowledge discovery and important to avoid misleading a practitioner. We develop the theory of multiple feature selection for time-series data, propose the ChronoEpilogi algorithm, and prove its soundness and completeness under two mild, broad, non-parametric distributional assumptions, namely Compositionality of the distribution and Interchangeability of time-series variables in solutions. Experiments on synthetic and real datasets demonstrate the scalability of ChronoEpilogi to hundreds of TS variables and its efficacy in identifying multiple solutions. In the real datasets, ChronoEpilogi is shown to reduce the number of TS variables by 96% (on average) by conserving or even improving forecasting performance. Furthermore, it is on par with GroupLasso performance, with the added benefit of providing multiple solutions.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f177a5a2fd2abbbc575b39ef2994a06b9da31513.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvareille2024chronoepilogi,\\ntitle={ChronoEpilogi: Scalable Time Series Selection with Multiple Solutions},\\nauthor={Etienne Vareille and Michele Linardi and Ioannis Tsamardinos and Vassilis Christophides},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y8HUXkwAOg}\\n}'}, 'paperhash': {'value': 'vareille|chronoepilogi_scalable_time_series_selection_with_multiple_solutions'}},forum = 'y8HUXkwAOg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15991/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15991/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15991/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15991/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y7oxY5pq4j',number = 1161,cdate = 1714447295865,pdate = 1727287653721,odate = 1730873845991,mdate = 1730873846010,tcdate = 1714447295865,tmdate = 1730873846010,ddate = None,content = {'title': {'value': 'RobIR: Robust Inverse Rendering for High-Illumination Scenes'}, 'authors': {'value': ['Ziyi Yang', 'Chenyanzhen', 'Xinyu Gao', 'YazhenYuan', 'Wu Yu', 'Xiaowei Zhou', 'Xiaogang Jin']}, 'authorids': {'value': ['~Ziyi_Yang4', '~Chenyanzhen1', '~Xinyu_Gao1', '~YazhenYuan1', '~Wu_Yu1', '~Xiaowei_Zhou3', '~Xiaogang_Jin1']}, 'keywords': {'value': ['Inverse Rendering', 'BRDF Estimation', 'Illumination Estimation', 'NeRF', 'Inverse Graphics']}, 'abstract': {'value': \"Implicit representation has opened up new possibilities for inverse rendering. However, existing implicit neural inverse rendering methods struggle to handle strongly illuminated scenes with significant shadows and slight reflections. The existence of shadows and reflections can lead to an inaccurate understanding of the scene, making precise factorization difficult. To this end, we present RobIR, an implicit inverse rendering approach that uses ACES tone mapping and regularized visibility estimation to reconstruct accurate BRDF of the object. By accurately modeling the indirect radiance field, normal, visibility, and direct light simultaneously, we are able to accurately decouple environment lighting and the object's PBR materials without imposing strict constraints on the scene. Even in high-illumination scenes with shadows and specular reflections, our method can recover high-quality albedo and roughness with no shadow interference. RobIR outperforms existing methods in both quantitative and qualitative evaluations.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/92d9baadccb6c0d8ed17f5cd5f1fc5980a06e590.pdf'}, 'TLDR': {'value': 'A NeuS-based inverse rendering method for shadow and specular highlight removed BRDF estimation under high illumination scenes.'}, 'supplementary_material': {'value': '/attachment/1a991de6eefb6a4b6003a333189ee406cdaef46a.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024robir,\\ntitle={Rob{IR}: Robust Inverse Rendering for High-Illumination Scenes},\\nauthor={Ziyi Yang and Chenyanzhen and Xinyu Gao and YazhenYuan and Wu Yu and Xiaowei Zhou and Xiaogang Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y7oxY5pq4j}\\n}'}, 'paperhash': {'value': 'yang|robir_robust_inverse_rendering_for_highillumination_scenes'}},forum = 'y7oxY5pq4j',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1161/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1161/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1161/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1161/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y6qhVtFG77',number = 13863,cdate = 1715743698774,pdate = 1727288052930,odate = 1730873961289,mdate = 1736966939129,tcdate = 1715743698774,tmdate = 1736966939129,ddate = None,content = {'title': {'value': 'NeuroBOLT: Resting-state EEG-to-fMRI Synthesis with Multi-dimensional Feature Mapping'}, 'authors': {'value': ['Yamin Li', 'Ange Lou', 'Ziyuan Xu', 'SHENGCHAO ZHANG', 'Shiyu Wang', 'Dario J. Englot', 'Soheil Kolouri', 'Daniel Moyer', 'Roza G Bayrak', 'Catie Chang']}, 'authorids': {'value': ['~Yamin_Li2', '~Ange_Lou2', '~Ziyuan_Xu3', '~SHENGCHAO_ZHANG1', '~Shiyu_Wang8', '~Dario_J._Englot1', '~Soheil_Kolouri1', '~Daniel_Moyer3', '~Roza_G_Bayrak2', '~Catie_Chang1']}, 'keywords': {'value': ['EEG-to-fMRI synthesis', 'EEG', 'fMRI']}, 'abstract': {'value': 'Functional magnetic resonance imaging (fMRI) is an indispensable tool in modern neuroscience, providing a non-invasive window into whole-brain dynamics at millimeter-scale spatial resolution. However, fMRI is constrained by issues such as high operation costs and immobility. With the rapid advancements in cross-modality synthesis and brain decoding, the use of deep neural networks has emerged as a promising solution for inferring whole-brain, high-resolution fMRI features directly from electroencephalography (EEG), a more widely accessible and portable neuroimaging modality. Nonetheless, the complex projection from neural activity to fMRI hemodynamic responses and the spatial ambiguity of EEG pose substantial challenges both in modeling and interpretability. Relatively few studies to date have developed approaches for EEG-fMRI translation, and although they have made significant strides, the inference of fMRI signals in a given study has been limited to a small set of brain areas and to a single condition (i.e., either resting-state or a specific task). The capability to predict fMRI signals in other brain areas, as well as to generalize across conditions, remain critical gaps in the field. To tackle these challenges, we introduce a novel and generalizable framework: NeuroBOLT, i.e., Neuro-to-BOLD Transformer, which leverages multi-dimensional representation learning from temporal, spatial, and spectral domains to translate raw EEG data to the corresponding fMRI activity signals across the brain. Our experiments demonstrate that NeuroBOLT effectively reconstructs unseen resting-state fMRI signals from primary sensory, high-level cognitive areas, and deep subcortical brain regions, achieving state-of-the-art accuracy with the potential to generalize across varying conditions and sites, which significantly advances the integration of these two modalities.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bdea46a673736c763526ec0b31ddac30ab558611.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nli2024neurobolt,\\ntitle={Neuro{BOLT}: Resting-state {EEG}-to-f{MRI} Synthesis with Multi-dimensional Feature Mapping},\\nauthor={Yamin Li and Ange Lou and Ziyuan Xu and SHENGCHAO ZHANG and Shiyu Wang and Dario J. Englot and Soheil Kolouri and Daniel Moyer and Roza G Bayrak and Catie Chang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y6qhVtFG77}\\n}'}, 'TLDR': {'value': 'We propose NeuroBOLT, a versatile deep-learning solution for projecting scalp EEG to BOLD fMRI signals.'}, 'paperhash': {'value': 'li|neurobolt_restingstate_eegtofmri_synthesis_with_multidimensional_feature_mapping'}},forum = 'y6qhVtFG77',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13863/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13863/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13863/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13863/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y6JotynERr',number = 15202,cdate = 1715758137322,pdate = 1727288090932,odate = 1730873971401,mdate = 1730873971419,tcdate = 1715758137322,tmdate = 1730873971419,ddate = None,content = {'title': {'value': 'Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration'}, 'authors': {'value': ['Mahdi Morafah', 'Vyacheslav Kungurtsev', 'Hojin Matthew Chang', 'Chen Chen', 'Bill Lin']}, 'authorids': {'value': ['~Mahdi_Morafah1', '~Vyacheslav_Kungurtsev1', '~Hojin_Matthew_Chang1', '~Chen_Chen18', '~Bill_Lin1']}, 'keywords': {'value': ['Federated Learning', 'Heterogeneous Device Prototypes', 'Knowledge Distillation', 'Task Arithmetic', 'Machine Learning']}, 'abstract': {'value': \"Federated Learning (FL) has emerged as a promising paradigm for collaborative machine learning, while preserving user data privacy. Despite its potential, standard FL algorithms lack support for diverse heterogeneous device prototypes, which vary significantly in model and dataset sizes---from small IoT devices to large workstations. This limitation is only partially addressed by existing knowledge distillation (KD) techniques, which often fail to transfer knowledge effectively across a broad spectrum of device prototypes with varied capabilities. This failure primarily stems from two issues: the dilution of informative logits from more capable devices by those from less capable ones, and the use of a single integrated logits as the distillation target across all devices, which neglects their individual learning capacities and and the unique contributions of each device. To address these challenges, we introduce TAKFL, a novel KD-based framework that treats the knowledge transfer from each device prototype's ensemble as a separate task, independently distilling each to preserve its unique contributions and avoid dilution. TAKFL also incorporates a KD-based self-regularization technique to mitigate the issues related to the noisy and unsupervised ensemble distillation process. To integrate the separately distilled knowledge, we introduce an adaptive task arithmetic knowledge integration process, allowing each student model to customize the knowledge integration for optimal performance. Additionally, we present theoretical results demonstrating the effectiveness of task arithmetic in transferring knowledge across heterogeneous device prototypes with varying capacities. Comprehensive evaluations of our method across both computer vision (CV) and natural language processing (NLP) tasks demonstrate that TAKFL achieves state-of-the-art results in a variety of datasets and settings, significantly outperforming existing KD-based methods. Our code is released at https://github.com/MMorafah/TAKFL and the project website is available at https://mmorafah.github.io/takflpage .\"}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/775a6d52478bc6934cf412e2704981a99343583b.pdf'}, 'supplementary_material': {'value': '/attachment/fe17063e1e942daac661fb289ffbb1be693c72ca.zip'}, '_bibtex': {'value': '@inproceedings{\\nmorafah2024towards,\\ntitle={Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration},\\nauthor={Mahdi Morafah and Vyacheslav Kungurtsev and Hojin Matthew Chang and Chen Chen and Bill Lin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y6JotynERr}\\n}'}, 'paperhash': {'value': 'morafah|towards_diverse_device_heterogeneous_federated_learning_via_task_arithmetic_knowledge_integration'}},forum = 'y6JotynERr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15202/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15202/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15202/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15202/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y2fAmldTIf',number = 16935,cdate = 1715775616209,pdate = 1727288141226,odate = 1730873982365,mdate = 1730873982383,tcdate = 1715775616209,tmdate = 1730873982383,ddate = None,content = {'title': {'value': 'HEPrune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning'}, 'authors': {'value': ['Yancheng Zhang', 'Mengxin Zheng', 'Yuzhang Shang', 'Xun Chen', 'Qian Lou']}, 'authorids': {'value': ['~Yancheng_Zhang1', '~Mengxin_Zheng1', '~Yuzhang_Shang1', '~Xun_Chen1', '~Qian_Lou1']}, 'keywords': {'value': ['Confidential Training', 'Privacy', 'Pruning', 'Cryptographic Computing']}, 'TLDR': {'value': 'Efficient confidential deep neural network training with encrypted data pruning'}, 'abstract': {'value': 'Non-interactive cryptographic computing, Fully Homomorphic Encryption (FHE), provides a promising solution for private neural network training on encrypted data. One challenge of FHE-based private training is its large computational overhead, especially the multiple rounds of forward and backward execution on each encrypted data sample. Considering the existence of largely redundant data samples, pruning them will significantly speed up the training, as proven in plain non-FHE training. \\nExecuting the data pruning of encrypted data on the server side is not trivial since the knowledge calculation of data pruning needs complex and expensive executions on encrypted data. There is a lack of FHE-based data pruning protocol for efficient, private training. In this paper, we propose, \\\\textit{HEPrune}, to construct a FHE data-pruning protocol and then design an FHE-friendly data-pruning algorithm under client-aided or non-client-aided settings, respectively. We also observed that data sample pruning may not always remove ciphertexts, leaving large empty slots and limiting the effects of data pruning. Thus, in HEPrune, we further propose ciphertext-wise pruning to reduce ciphertext computation numbers without hurting accuracy. Experimental results show that our work can achieve a $16\\\\times$ speedup with only a $0.6\\\\%$ accuracy drop over prior work. \\nThe code is publicly available at \\\\href{https://github.com/UCF-Lou-Lab-PET/Private-Data-Prune}.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0446457111dbee1b34d15e6bd303f6f1da78ef41.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024heprune,\\ntitle={{HEP}rune: Fast Private Training of Deep Neural Networks With Encrypted Data Pruning},\\nauthor={Yancheng Zhang and Mengxin Zheng and Yuzhang Shang and Xun Chen and Qian Lou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y2fAmldTIf}\\n}'}, 'paperhash': {'value': 'zhang|heprune_fast_private_training_of_deep_neural_networks_with_encrypted_data_pruning'}},forum = 'y2fAmldTIf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16935/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16935/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16935/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16935/-/Camera_Ready_Revision', 'NeurIPS.cc/2024/Conference/-/Withdrawn_Submission'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'y10avdRFNK',number = 17735,cdate = 1715781661947,pdate = 1727288160670,odate = 1730873986020,mdate = 1730873986035,tcdate = 1715781661947,tmdate = 1730873986035,ddate = None,content = {'title': {'value': 'Learning diffusion at lightspeed'}, 'authors': {'value': ['Antonio Terpin', 'Nicolas Lanzetti', 'Martín Gadea', 'Florian Dorfler']}, 'authorids': {'value': ['~Antonio_Terpin1', '~Nicolas_Lanzetti1', '~Martín_Gadea1', '~Florian_Dorfler1']}, 'keywords': {'value': ['optimal transport', 'diffusion processes', 'gradient flows']}, 'abstract': {'value': 'Diffusion regulates numerous natural processes and the dynamics of many successful generative models. Existing models to learn the diffusion terms from observational data rely on complex bilevel optimization problems and model only the drift of the system.\\nWe propose a new simple model, JKOnet*, which bypasses the complexity of existing architectures while presenting significantly enhanced representational capabilities: JKOnet* recovers the potential, interaction, and internal energy components of the underlying diffusion process. JKOnet* minimizes a simple quadratic loss and outperforms other baselines in terms of sample efficiency, computational complexity, and accuracy. Additionally, JKOnet* provides a closed-form optimal solution for linearly parametrized functionals, and, when applied to predict the evolution of cellular processes from real-world data, it achieves state-of-the-art accuracy at a fraction of the computational cost of all existing methods.\\nOur methodology is based on the interpretation of diffusion processes as energy-minimizing trajectories in the probability space via the so-called JKO scheme, which we study via its first-order optimality conditions.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/71e85a95e3f40ebd277c5df65f9dff3c748e2ddb.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nterpin2024learning,\\ntitle={Learning diffusion at lightspeed},\\nauthor={Antonio Terpin and Nicolas Lanzetti and Mart{\\\\'\\\\i}n Gadea and Florian Dorfler},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=y10avdRFNK}\\n}\"}, 'paperhash': {'value': 'terpin|learning_diffusion_at_lightspeed'}},forum = 'y10avdRFNK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17735/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17735/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17735/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission17735/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xzCuBjHQbS',number = 14548,cdate = 1715751055011,pdate = 1727288072649,odate = 1730873966741,mdate = 1730873966772,tcdate = 1715751055011,tmdate = 1730873966772,ddate = None,content = {'title': {'value': 'Random Function Descent'}, 'authors': {'value': ['Felix Benning', 'Leif Döring']}, 'authorids': {'value': ['~Felix_Benning1', '~Leif_Döring1']}, 'keywords': {'value': ['optimization', 'hyperparameter tuning', 'Gaussian processes', 'random functions', 'random fields', 'average case analysis', 'bayesian optimization']}, 'abstract': {'value': \"Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection. In this paper we demonstrate the viability and advantages of replacing the classical 'convex function' framework with a 'random function' framework. With complexity $\\\\mathcal{O}(n^3d^3)$, where $n$ is the number of steps and $d$ the number of dimensions, Bayesian optimization with gradients has not been viable in large dimension so far. By bridging the gap between Bayesian optimization (i.e. random function optimization theory) and classical optimization we establish viability. Specifically, we use a 'stochastic Taylor approximation' to rediscover gradient descent, which is scalable in high dimension due to $\\\\mathcal{O}(nd)$ complexity. This rediscovery yields a specific step size schedule we call Random Function Descent (RFD). The advantage of this random function framework is that RFD is scale invariant and that it provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Grounding step size heuristics in average case analysis.'}, 'pdf': {'value': '/pdf/1ec86b65423b53f88a8ae3ca99f4895a30b0617d.pdf'}, 'supplementary_material': {'value': '/attachment/83538dd90d97da60b513f719f9a2b4e64c892263.zip'}, '_bibtex': {'value': '@inproceedings{\\nbenning2024random,\\ntitle={Random Function Descent},\\nauthor={Felix Benning and Leif D{\\\\\"o}ring},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xzCuBjHQbS}\\n}'}, 'paperhash': {'value': 'benning|random_function_descent'}},forum = 'xzCuBjHQbS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14548/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14548/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14548/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14548/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'xymhWyiZOp',number = 7675,cdate = 1715632926236,pdate = 1727287855115,odate = 1730873905180,mdate = 1730873905197,tcdate = 1715632926236,tmdate = 1730873905197,ddate = None,content = {'title': {'value': 'On the Use of Anchoring for Training Vision Models'}, 'authors': {'value': ['Vivek Narayanaswamy', 'Kowshik Thopalli', 'Rushil Anirudh', 'Yamen Mubarka', 'Wesam A. Sakla', 'Jayaraman J. Thiagarajan']}, 'authorids': {'value': ['~Vivek_Narayanaswamy1', '~Kowshik_Thopalli1', '~Rushil_Anirudh1', '~Yamen_Mubarka1', '~Wesam_A._Sakla1', '~Jayaraman_J._Thiagarajan3']}, 'keywords': {'value': ['Anomaly Detection', 'OOD Generalization', 'ML Safety', 'Anchoring', 'Deep Neural Networks']}, 'abstract': {'value': 'Anchoring is a recent, architecture-agnostic principle for training deep neural networks that has been shown to significantly improve uncertainty estimation, calibration, and extrapolation capabilities. In this paper, we systematically explore anchoring as a general protocol for training vision models, providing fundamental insights into its training and inference processes and their implications for generalization and safety. Despite its promise, we identify a critical problem in anchored training that can lead to an increased risk of learning undesirable shortcuts, thereby limiting its generalization capabilities. To address this, we introduce a new anchored training protocol that employs a simple regularizer to mitigate this issue and significantly enhances generalization. We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol. The open-source code is available at https://software.llnl.gov/anchoring.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/31b62781b7295d311f43718b5f5a178ec72948c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnarayanaswamy2024on,\\ntitle={On the Use of Anchoring for Training Vision Models},\\nauthor={Vivek Narayanaswamy and Kowshik Thopalli and Rushil Anirudh and Yamen Mubarka and Wesam A. Sakla and Jayaraman J. Thiagarajan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xymhWyiZOp}\\n}'}, 'paperhash': {'value': 'narayanaswamy|on_the_use_of_anchoring_for_training_vision_models'}},forum = 'xymhWyiZOp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7675/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7675/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7675/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7675/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'xxY8d4rnSb',number = 10497,cdate = 1715694721563,pdate = 1727287942725,odate = 1730873929674,mdate = 1730873929689,tcdate = 1715694721563,tmdate = 1730873929689,ddate = None,content = {'title': {'value': 'ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation'}, 'authors': {'value': ['Cédric Rommel', 'Victor Letzelter', 'Nermin Samet', 'Renaud Marlet', 'Matthieu Cord', 'Patrick Perez', 'Eduardo Valle']}, 'authorids': {'value': ['~Cédric_Rommel1', '~Victor_Letzelter1', '~Nermin_Samet1', '~Renaud_Marlet1', '~Matthieu_Cord1', '~Patrick_Perez1', '~Eduardo_Valle1']}, 'keywords': {'value': ['human pose estimation', 'depth ambiguity', 'multiple choice learning']}, 'TLDR': {'value': 'We prove previous 2D-to-3D human pose lifting methods suffer from topology inconsistencies invisible to standard evaluation metrics, and propose both new metrics and a new method circumventing these issues via constrained multiple hypotheses.'}, 'abstract': {'value': 'We propose ManiPose, a manifold-constrained multi-hypothesis model for human-pose 2D-to-3D lifting. We provide theoretical and empirical evidence that, due to the depth ambiguity inherent to monocular 3D human pose estimation, traditional regression models suffer from pose-topology consistency issues, which standard evaluation metrics (MPJPE, P-MPJPE and PCK) fail to assess.  ManiPose addresses depth ambiguity by proposing multiple candidate 3D poses for each 2D input, each with its estimated plausibility. Unlike previous multi-hypothesis approaches, ManiPose forgoes generative models, greatly facilitating its training and usage. By constraining the outputs to lie on the human pose manifold, ManiPose guarantees the consistency of all hypothetical poses, in contrast to previous works. We showcase the performance of ManiPose on real-world datasets, where it outperforms state-of-the-art models in pose consistency by a large margin while being very competitive on the MPJPE metric.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/aa9e7681c86797dad7f2bb93ba5e0c36ea2de62a.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nrommel2024manipose,\\ntitle={ManiPose: Manifold-Constrained Multi-Hypothesis 3D Human Pose Estimation},\\nauthor={C{\\\\'e}dric Rommel and Victor Letzelter and Nermin Samet and Renaud Marlet and Matthieu Cord and Patrick Perez and Eduardo Valle},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xxY8d4rnSb}\\n}\"}, 'paperhash': {'value': 'rommel|manipose_manifoldconstrained_multihypothesis_3d_human_pose_estimation'}},forum = 'xxY8d4rnSb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10497/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10497/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10497/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10497/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xvYI7TCiU6',number = 10037,cdate = 1715689367123,pdate = 1727287928183,odate = 1730873925580,mdate = 1736859148334,tcdate = 1715689367123,tmdate = 1736859148334,ddate = None,content = {'title': {'value': 'Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration'}, 'authors': {'value': ['Haowen Dou', 'Lujuan Dang', 'Zhirong Luan', 'Badong Chen']}, 'authorids': {'value': ['~Haowen_Dou1', '~Lujuan_Dang1', '~Zhirong_Luan1', '~Badong_Chen1']}, 'keywords': {'value': ['multi-agent reinforcement learning', 'sequential updating', 'exploration', 'Cauchy-Schwarz divergence']}, 'TLDR': {'value': 'A method that maximizes conditional Cauchy-Shwarz policy divergence between agents and between episodes to enhance exploration and heterogeneity for MARL.'}, 'abstract': {'value': 'Despite the success of Multi-Agent Reinforcement Learning (MARL) algorithms in cooperative tasks, previous works, unfortunately, face challenges in heterogeneous scenarios since they simply disable parameter sharing for agent specialization. Sequential updating scheme was thus proposed, naturally diversifying agents by encouraging agents to learn from preceding ones. However, the exploration strategy in sequential scheme has not been investigated. Benefiting from updating one-by-one, agents have the access to the information from preceding agents. Thus, in this work, we propose to exploit the preceding information to enhance exploration and heterogeneity sequentially. We present Multi-Agent Divergence Policy Optimization (MADPO), equipped with mutual policy divergence maximization framework. We quantify the policy discrepancies between episodes to enhance exploration and between agents to heterogenize agents, termed intra-agent and inter-agent policy divergence. To address the issue that traditional divergence measurements lack stability and directionality, we propose to employ the conditional Cauchy-Schwarz divergence to provide entropy-guided exploration incentives. Extensive experiments show that the proposed method outperforms state-of-the-art sequential updating approaches in two challenging multi-agent tasks with various heterogeneous scenarios. Source code is available at \\\\url{https://github.com/hwdou6677/MADPO}.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b7229947dbdbbf04ca5c8c83d49e4cd55a3a0c39.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndou2024measuring,\\ntitle={Measuring Mutual Policy Divergence for Multi-Agent Sequential Exploration},\\nauthor={Haowen Dou and Lujuan Dang and Zhirong Luan and Badong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xvYI7TCiU6}\\n}'}, 'paperhash': {'value': 'dou|measuring_mutual_policy_divergence_for_multiagent_sequential_exploration'}},forum = 'xvYI7TCiU6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10037/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10037/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10037/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10037/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xvVeSZoVJO',number = 9775,cdate = 1715685548047,pdate = 1727287920565,odate = 1730873923475,mdate = 1736232090810,tcdate = 1715685548047,tmdate = 1736232090810,ddate = None,content = {'title': {'value': 'RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling'}, 'authors': {'value': ['Tianhang Wang', 'Fan Lu', 'Zehan Zheng', 'Zhijun Li', 'Guang Chen', 'changjun jiang']}, 'authorids': {'value': ['~Tianhang_Wang1', '~Fan_Lu3', '~Zehan_Zheng1', '~Zhijun_Li2', '~Guang_Chen4', '~changjun_jiang2']}, 'keywords': {'value': ['collaborative perception']}, 'TLDR': {'value': 'robust camera-insensitivity collaborative perception'}, 'abstract': {'value': \"Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vector for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity setting. Our code and datasets will be available soon.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/50300dd9e9a38a5720ea27edc28c35276e11d4c3.pdf'}, 'supplementary_material': {'value': '/attachment/e66fe30496ee76f50eb61921093237088446e61c.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwang2024rcdn,\\ntitle={{RCDN}: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling},\\nauthor={Tianhang Wang and Fan Lu and Zehan Zheng and Zhijun Li and Guang Chen and changjun jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xvVeSZoVJO}\\n}'}, 'paperhash': {'value': 'wang|rcdn_towards_robust_camerainsensitivity_collaborative_perception_via_dynamic_featurebased_3d_neural_modeling'}},forum = 'xvVeSZoVJO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9775/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9775/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9775/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9775/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xvTMc9Ovx3',number = 4901,cdate = 1715481876875,pdate = 1727287764273,odate = 1730873880086,mdate = 1735004734758,tcdate = 1715481876875,tmdate = 1735004734758,ddate = None,content = {'title': {'value': 'On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance'}, 'authors': {'value': ['Zhixiong Nan', 'Yilong Chen', 'Tianfei Zhou', 'Tao Xiang']}, 'authorids': {'value': ['~Zhixiong_Nan2', '~Yilong_Chen3', '~Tianfei_Zhou2', '~Tao_Xiang2']}, 'keywords': {'value': ['Object importance estimation', 'Autonomous vehicles']}, 'abstract': {'value': \"This paper addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver's perspective as the input. Although this problem is significant for safer and smarter driving systems, the exploration of this problem remains limited. On one hand, publicly-available large-scale datasets are scarce in the community. To address this dilemma, this paper contributes a new large-scale dataset named Traffic Object Importance (TOI). On the other hand, existing methods often only consider either bottom-up feature or single-fold guidance, leading to limitations in handling highly dynamic and diverse traffic scenarios. Different from existing methods, this paper proposes a model that integrates multi-fold top-down guidance with the bottom-up feature. Specifically, three kinds of top-down guidance factors (i.e., driver intention, semantic context, and traffic rule) are integrated into our model. These factors are important for object importance estimation, but none of the existing methods simultaneously consider them. To our knowledge, this paper proposes the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with bottom-up feature. Extensive experiments demonstrate that our model outperforms state-of-the-art methods by large margins, achieving 23.1% Average Precision (AP) improvement compared with the recently proposed model (i.e., Goal).\"}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/87bd5670b38ec3e2f101018aef40eb48c6c26a89.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnan2024onroad,\\ntitle={On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance},\\nauthor={Zhixiong Nan and Yilong Chen and Tianfei Zhou and Tao Xiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xvTMc9Ovx3}\\n}'}, 'paperhash': {'value': 'nan|onroad_object_importance_estimation_a_new_dataset_and_a_model_with_multifold_topdown_guidance'}},forum = 'xvTMc9Ovx3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4901/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4901/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4901/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4901/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xutrKezbPF',number = 11922,cdate = 1715713106654,pdate = 1727287988118,odate = 1730873943748,mdate = 1737132681203,tcdate = 1715713106654,tmdate = 1737132681203,ddate = None,content = {'title': {'value': 'CIFD: Controlled Information Flow to Enhance Knowledge Distillation'}, 'authors': {'value': ['Yashas Malur Saidutta', 'Rakshith Sharma Srinivasa', 'Jaejin Cho', 'Ching-Hua Lee', 'Chouchang Yang', 'Yilin Shen', 'Hongxia Jin']}, 'authorids': {'value': ['~Yashas_Malur_Saidutta1', '~Rakshith_Sharma_Srinivasa1', '~Jaejin_Cho2', '~Ching-Hua_Lee2', '~Chouchang_Yang2', '~Yilin_Shen1', '~Hongxia_Jin1']}, 'keywords': {'value': ['Knowledge Distillation', 'Information Bottleneck', 'Rate-Distortion', 'Teaching Assistant', 'CLIP']}, 'TLDR': {'value': 'This paper proposes a Rate-Distortion theory based module that mimics teaching assistants for knowledge distillation, while being computationally far cheaper to train than conventional TAs.'}, 'abstract': {'value': \"Knowledge Distillation is the mechanism by which the insights gained from a larger teacher model are transferred to a smaller student model. However, the transfer suffers when the teacher model is significantly larger than the student. To overcome this, prior works have proposed training intermediately sized models, Teacher Assistants (TAs) to help the transfer process. However, training TAs is expensive, as training these models is a knowledge transfer task in itself. Further, these TAs are larger than the student model and training them especially in large data settings can be computationally intensive. In this paper, we propose a novel framework called Controlled Information Flow for Knowledge Distillation (CIFD) consisting of two components. First, we propose a significantly smaller alternatives to TAs, the Rate-Distortion Module (RDM) which uses the teacher's penultimate layer embedding and a information rate-constrained bottleneck layer to replace the Teacher Assistant model. RDMs are smaller and easier to train than TAs, especially in large data regimes, since they operate on the teacher embeddings and do not need to relearn low level input feature extractors. Also, by varying the information rate across the bottleneck, RDMs can replace TAs of different sizes. Secondly, we propose the use of Information Bottleneck Module in the student model, which is crucial for regularization in the presence of a large number of RDMs. We show comprehensive state-of-the-art results of the proposed method over large datasets like Imagenet. Further, we show the significant improvement in distilling CLIP like models over a huge 12M image-text dataset. It outperforms CLIP specialized distillation methods across five zero-shot classification datasets and two zero-shot image-text retrieval datasets.\"}, 'pdf': {'value': '/pdf/dd4b28772c38804f39a2eef11f4f97a9f8bf0f5a.pdf'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nsaidutta2024cifd,\\ntitle={{CIFD}: Controlled Information Flow to Enhance Knowledge Distillation},\\nauthor={Yashas Malur Saidutta and Rakshith Sharma Srinivasa and Jaejin Cho and Ching-Hua Lee and Chouchang Yang and Yilin Shen and Hongxia Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xutrKezbPF}\\n}'}, 'paperhash': {'value': 'saidutta|cifd_controlled_information_flow_to_enhance_knowledge_distillation'}},forum = 'xutrKezbPF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11922/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11922/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11922/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11922/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xtK3gZjQDC',number = 16218,cdate = 1715767951906,pdate = 1727288120152,odate = 1730873977884,mdate = 1730873977902,tcdate = 1715767951906,tmdate = 1730873977902,ddate = None,content = {'title': {'value': 'Towards Human-AI Complementarity with Prediction Sets'}, 'authors': {'value': ['Giovanni De Toni', 'Nastaran Okati', 'Suhas Thejaswi', 'Eleni Straitouri', 'Manuel Gomez Rodriguez']}, 'authorids': {'value': ['~Giovanni_De_Toni1', '~Nastaran_Okati2', '~Suhas_Thejaswi1', '~Eleni_Straitouri1', '~Manuel_Gomez_Rodriguez1']}, 'keywords': {'value': ['conformal prediction', 'decision support systems', 'human-ai complementarity']}, 'abstract': {'value': 'Decision support systems based on prediction sets have proven to be effective at helping human experts solve classification tasks. Rather than providing single-label predictions, these systems provide sets of label predictions constructed using conformal prediction, namely prediction sets, and ask human experts to predict label values from these sets. In this paper, we first show that the prediction sets constructed using conformal prediction are, in general, suboptimal in terms of average accuracy. Then, we show that the problem of finding the optimal prediction sets under which the human experts achieve the highest average accuracy is NP-hard. More strongly, unless P = NP, we show that the problem is hard to approximate to any factor less than the size of the label set. However, we introduce a simple and efficient greedy algorithm that, for a large class of expert models and non-conformity scores, is guaranteed to find prediction sets that provably offer equal or greater performance than those constructed using conformal prediction. Further, using a simulation study with both synthetic and real expert predictions, we demonstrate that, in practice, our greedy algorithm finds near-optimal prediction sets offering greater performance than conformal prediction.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Prediction sets based on conformal prediction can be suboptimal in achieving human-ai complementarity. We show that finding optimal prediction sets is NP-hard and we give a greedy algorithm with provably improved performance than conformal prediction'}, 'pdf': {'value': '/pdf/2b252bb64aa670e885a730dbaf8f392c032ec70b.pdf'}, 'supplementary_material': {'value': '/attachment/8974d4d930794c932d9d0ed6fe006e2a5dc3f70b.zip'}, '_bibtex': {'value': '@inproceedings{\\ntoni2024towards,\\ntitle={Towards Human-{AI} Complementarity with Prediction Sets},\\nauthor={Giovanni De Toni and Nastaran Okati and Suhas Thejaswi and Eleni Straitouri and Manuel Gomez Rodriguez},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xtK3gZjQDC}\\n}'}, 'paperhash': {'value': 'toni|towards_humanai_complementarity_with_prediction_sets'}},forum = 'xtK3gZjQDC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16218/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16218/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16218/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16218/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xse8QMGnyM',number = 14753,cdate = 1715753324108,pdate = 1727288078647,odate = 1730873968234,mdate = 1730873968250,tcdate = 1715753324108,tmdate = 1730873968250,ddate = None,content = {'title': {'value': 'Toward Approaches to Scalability in 3D Human Pose Estimation'}, 'authors': {'value': ['Jun-Hee Kim', 'Seong-Whan Lee']}, 'authorids': {'value': ['~Jun-Hee_Kim1', '~Seong-Whan_Lee3']}, 'keywords': {'value': ['3D Human Pose Estimation', 'Data generation', 'Pose Decompression']}, 'abstract': {'value': \"In the field of 3D Human Pose Estimation (HPE), scalability and generalization across diverse real-world scenarios remain significant challenges. This paper addresses two key bottlenecks to scalability: limited data diversity caused by 'popularity bias' and increased 'one-to-many' depth ambiguity arising from greater pose diversity. We introduce the Biomechanical Pose Generator (BPG), which leverages biomechanical principles, specifically the normal range of motion, to autonomously generate a wide array of plausible 3D poses without relying on a source dataset, thus overcoming the restrictions of popularity bias. To address depth ambiguity, we propose the Binary Depth Coordinates (BDC), which simplifies depth estimation into a binary classification of joint positions (front or back). This method decomposes a 3D pose into three core elements—2D pose, bone length, and binary depth decision—substantially reducing depth ambiguity and enhancing model robustness and accuracy, particularly in complex poses. Our results demonstrate that these approaches increase the diversity and volume of pose data while consistently achieving performance gains, even amid the complexities introduced by increased pose diversity.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f77e33cf32c4c31dd7e4130f762aac8101938e30.pdf'}, 'supplementary_material': {'value': '/attachment/ffd6aa33f07118e67868116ac943421300451ddf.zip'}, '_bibtex': {'value': '@inproceedings{\\nkim2024toward,\\ntitle={Toward Approaches to Scalability in 3D Human Pose Estimation},\\nauthor={Jun-Hee Kim and Seong-Whan Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xse8QMGnyM}\\n}'}, 'paperhash': {'value': 'kim|toward_approaches_to_scalability_in_3d_human_pose_estimation'}},forum = 'xse8QMGnyM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14753/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14753/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14753/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14753/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xrbgXJomJp',number = 15891,cdate = 1715764654839,pdate = 1727288110579,odate = 1730873975550,mdate = 1730873975562,tcdate = 1715764654839,tmdate = 1730873975562,ddate = None,content = {'title': {'value': 'Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning'}, 'authors': {'value': ['The Viet Bui', 'Tien Anh Mai', 'Thanh Hong Nguyen']}, 'authorids': {'value': ['~The_Viet_Bui1', '~Tien_Anh_Mai1', '~Thanh_Hong_Nguyen1']}, 'keywords': {'value': ['Multi-agent Imitation Learning', 'Inverse Q Learning', 'Centralized Learning']}, 'abstract': {'value': 'This paper concerns imitation learning (IL) in cooperative multi-agent systems.\\nThe learning problem under consideration poses several challenges, characterized by high-dimensional state and action spaces and intricate inter-agent dependencies. In a single-agent setting, IL was shown to be done efficiently via an inverse soft-Q learning process. However, extending this framework to a multi-agent context introduces the need to simultaneously learn both local value functions to capture local observations and individual actions, and a joint value function for exploiting centralized learning.\\nIn this work, we introduce a new multi-agent IL algorithm designed to address these challenges. Our approach enables the\\ncentralized learning by leveraging mixing networks to aggregate  decentralized Q functions.\\nWe further establish conditions for the mixing networks under which the multi-agent IL objective function exhibits convexity within the Q function space.\\nWe present  extensive experiments conducted on some challenging multi-agent game environments, including an advanced version of the Star-Craft multi-agent challenge (SMACv2), which demonstrates the effectiveness of our algorithm.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a8143c20b40d30d3986378da10c5654405072f65.pdf'}, 'supplementary_material': {'value': '/attachment/34ad182a1b09569cf694f828ed41b57c8869734b.zip'}, '_bibtex': {'value': '@inproceedings{\\nbui2024inverse,\\ntitle={Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning},\\nauthor={The Viet Bui and Tien Anh Mai and Thanh Hong Nguyen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xrbgXJomJp}\\n}'}, 'TLDR': {'value': 'An Inverse Q-Learning Algorithm for Multi-Agent Imitation Learning'}, 'paperhash': {'value': 'bui|inverse_factorized_soft_qlearning_for_cooperative_multiagent_imitation_learning'}},forum = 'xrbgXJomJp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15891/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15891/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15891/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15891/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xqrlhsbcwN',number = 8079,cdate = 1715652097798,pdate = 1727287867625,odate = 1730873909125,mdate = 1730873909142,tcdate = 1715652097798,tmdate = 1730873909142,ddate = None,content = {'title': {'value': 'Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient'}, 'authors': {'value': ['ShaoQi Wang', 'Chunjie Yang', 'Siwei Lou']}, 'authorids': {'value': ['~ShaoQi_Wang2', '~Chunjie_Yang1', '~Siwei_Lou1']}, 'keywords': {'value': ['Neural networks', \"network's structure design\", 'minimum variance estimation', 'online learning', 'training stability', 'natural gradient', 'soft sensor']}, 'TLDR': {'value': 'A novel network structure that efficiently approximates  minimum variance estimation and computes natural gradient achieving stable convergence and superior performance in industrial setting.'}, 'abstract': {'value': \"Neural networks (NN) are extensively studied in cutting-edge soft sensor models due to their  feature extraction and function approximation capabilities. Current research into network-based methods primarily focuses on models' offline accuracy. Notably, in industrial soft sensor context, online optimizing stability and interpretability are prioritized, followed by accuracy. This requires a clearer understanding of network's training process. To bridge this gap, we propose a novel NN named the Approximated Orthogonal Projection Unit (AOPU) which has solid mathematical basis and presents superior training stability. AOPU truncates the gradient backpropagation at dual parameters, optimizes the trackable parameters updates, and enhances the robustness of training. We further prove that AOPU attains minimum variance estimation in NN, wherein the truncated gradient approximates the natural gradient. Empirical results on two chemical process datasets clearly show that AOPU outperforms other models in achieving stable convergence, marking a significant advancement in soft sensor field.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b37fda1b4909f85ae3e295ff8e8ca5de81f5920d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024approximated,\\ntitle={Approximated Orthogonal Projection Unit: Stabilizing Regression Network Training Using Natural Gradient},\\nauthor={ShaoQi Wang and Chunjie Yang and Siwei Lou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xqrlhsbcwN}\\n}'}, 'paperhash': {'value': 'wang|approximated_orthogonal_projection_unit_stabilizing_regression_network_training_using_natural_gradient'}},forum = 'xqrlhsbcwN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8079/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8079/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8079/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8079/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'xqc8yyhScL',number = 11777,cdate = 1715710379550,pdate = 1727287982980,odate = 1730873942484,mdate = 1736817564584,tcdate = 1715710379550,tmdate = 1736817564584,ddate = None,content = {'title': {'value': 'Is Programming by Example Solved by LLMs?'}, 'authors': {'value': ['Wen-Ding Li', 'Kevin Ellis']}, 'authorids': {'value': ['~Wen-Ding_Li1', '~Kevin_Ellis1']}, 'keywords': {'value': ['programming by example', 'program synthesis', 'LLM', 'code generation']}, 'abstract': {'value': 'Programming-by-Examples (PBE) aims to generate an algorithm from input-output examples.\\nSuch systems are practically and theoretically important:\\nfrom an end-user perspective, they are deployed to millions of people, and from an AI perspective, PBE corresponds to a very general form of few-shot inductive inference.\\nGiven the success of Large Language Models (LLMs) in code-generation tasks, we investigate here the extent to which LLMs can be said to have \"solved\" PBE.\\nWe experiment on classic domains such as lists and strings, and an uncommon graphics programming domain not well represented in typical pretraining data.\\nWe find that pretrained models are not effective at PBE, but that they can be fine-tuned for much higher performance, provided the test problems are in-distribution.\\nWe analyze empirically what causes these models to succeed and fail, and take steps toward understanding how to achieve better out-of-distribution generalization.\\nCollectively these results suggest that LLMs make strong progress toward solving the typical suite of PBE tasks, potentially increasing the flexibility and applicability of PBE systems, while also identifying ways in which LLMs still fall short.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We explore methods for doing PBE with LLMs'}, 'pdf': {'value': '/pdf/c6f0a06631003e14e1689d1d352541b2fc07a831.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024is,\\ntitle={Is Programming by Example solved by {LLM}s?},\\nauthor={Wen-Ding Li and Kevin Ellis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xqc8yyhScL}\\n}'}, 'paperhash': {'value': 'li|is_programming_by_example_solved_by_llms'}},forum = 'xqc8yyhScL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11777/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11777/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11777/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11777/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xpRUi8amtC',number = 3653,cdate = 1715312457292,pdate = 1727287727069,odate = 1730873868368,mdate = 1734843451423,tcdate = 1715312457292,tmdate = 1734843451423,ddate = None,content = {'title': {'value': 'Scene Graph Generation with Role-Playing Large Language Models'}, 'authors': {'value': ['Guikun Chen', 'Jin Li', 'Wenguan Wang']}, 'authorids': {'value': ['~Guikun_Chen1', '~Jin_Li23', '~Wenguan_Wang4']}, 'keywords': {'value': ['Scene Graph Generation', 'Large Language Model']}, 'abstract': {'value': 'Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline – computing similarity between the query image and the text embeddings for each category (i.e., text classifiers). In this work, we argue that the text classifiers adopted by existing OVSGG methods, i.e., category-/part-level prompts, are scene-agnostic as they remain unchanged across contexts. Using such fixed text classifiers not only struggles to model visual relations with high variance, but also falls short in adapting to distinct contexts. To plug these intrinsic shortcomings, we devise SDSGG, a scene-specific description based OVSGG framework where the weights of text classifiers are adaptively adjusted according to the visual content. In particular, to generate comprehensive and diverse descriptions oriented to the scene, an LLM is asked to play different roles (e.g., biologist and engineer) to analyze and discuss the descriptive features of a given scene from different views. Unlike previous efforts simply treating the generated descriptions as mutually equivalent text classifiers, SDSGG is equipped with an advanced renormalization mechanism to adjust the influence of each text classifier based on its relevance to the presented scene (this is what the term “specific” means). Furthermore, to capture the complicated interplay between subjects and objects, we propose a new lightweight module called mutual visual adapter. It refines CLIP’s ability to recognize relations by learning an interaction-aware semantic space. Extensive experiments on prevalent benchmarks show that SDSGG significantly outperforms top-leading methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a6ceccff861a11ddd07c003913a667e72407795a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024scene,\\ntitle={Scene Graph Generation with Role-Playing Large Language Models},\\nauthor={Guikun Chen and Jin Li and Wenguan Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xpRUi8amtC}\\n}'}, 'paperhash': {'value': 'chen|scene_graph_generation_with_roleplaying_large_language_models'}},forum = 'xpRUi8amtC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3653/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3653/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3653/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3653/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xojbzSYIVS',number = 17900,cdate = 1715782512496,pdate = 1727288164490,odate = 1730873986682,mdate = 1735802568900,tcdate = 1715782512496,tmdate = 1735802568900,ddate = None,content = {'title': {'value': 'LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential Recommendation'}, 'authors': {'value': ['Qidong Liu', 'Xian Wu', 'Yejing Wang', 'Zijian Zhang', 'Feng Tian', 'Yefeng Zheng', 'Xiangyu Zhao']}, 'authorids': {'value': ['~Qidong_Liu2', '~Xian_Wu1', '~Yejing_Wang1', '~Zijian_Zhang5', '~Feng_Tian4', '~Yefeng_Zheng3', '~Xiangyu_Zhao1']}, 'keywords': {'value': ['Large Language Models', 'Recommender Systems', 'Sequential Recommendation', 'Long-tail']}, 'abstract': {'value': \"Sequential recommender systems (SRS) aim to predict users' subsequent choices based on their historical interactions and have found applications in diverse fields such as e-commerce and social media. However, in real-world systems, most users interact with only a handful of items, while the majority of items are seldom consumed. These two issues, known as the long-tail user and long-tail item challenges, often pose difficulties for existing SRS. These challenges can adversely affect user experience and seller benefits, making them crucial to address. Though a few works have addressed the challenges, they still struggle with the seesaw or noisy issues due to the intrinsic scarcity of interactions. The advancements in large language models (LLMs) present a promising solution to these problems from a semantic perspective. As one of the pioneers in this field, we propose the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR). This framework utilizes semantic embeddings derived from LLMs to enhance SRS without adding extra inference load. To address the long-tail item challenge, we design a dual-view modeling framework that combines semantics from LLMs and collaborative signals from conventional SRS. For the long-tail user challenge, we propose a retrieval augmented self-distillation method to enhance user preference representation using more informative interactions from similar users. To verify the effectiveness and versatility of our proposed enhancement framework, we conduct extensive experiments on three real-world datasets using three popular SRS models. The results consistently show that our method surpasses existing baselines. The implementation code is available in Supplementary Material.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/154f17c1f444becfea5d4859af7ffcf05d69ce31.pdf'}, 'supplementary_material': {'value': '/attachment/ad6992ebf420615a23e54af75c634dd6af73f8c0.zip'}, '_bibtex': {'value': '@inproceedings{\\nliu2024llmesr,\\ntitle={{LLM}-{ESR}: Large Language Models Enhancement for Long-tailed Sequential Recommendation},\\nauthor={Qidong Liu and Xian Wu and Yejing Wang and Zijian Zhang and Feng Tian and Yefeng Zheng and Xiangyu Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xojbzSYIVS}\\n}'}, 'paperhash': {'value': 'liu|llmesr_large_language_models_enhancement_for_longtailed_sequential_recommendation'}},forum = 'xojbzSYIVS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17900/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17900/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17900/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17900/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xoc4QOvbDs',number = 781,cdate = 1714084278372,pdate = 1727287644669,odate = 1730873843227,mdate = 1730873843250,tcdate = 1714084278372,tmdate = 1730873843250,ddate = None,content = {'title': {'value': 'Evaluate then Cooperate: Shapley-based View Cooperation Enhancement for Multi-view Clustering'}, 'authors': {'value': ['Fangdi Wang', 'Jiaqi Jin', 'Jingtao Hu', 'Suyuan Liu', 'Xihong Yang', 'Siwei Wang', 'Xinwang Liu', 'En Zhu']}, 'authorids': {'value': ['~Fangdi_Wang1', '~Jiaqi_Jin1', '~Jingtao_Hu1', '~Suyuan_Liu1', '~Xihong_Yang1', '~Siwei_Wang4', '~Xinwang_Liu1', '~En_Zhu1']}, 'keywords': {'value': ['Multi-view Clustering', 'Clustering Network']}, 'abstract': {'value': 'The fundamental goal of deep multi-view clustering is to achieve preferable task performance through inter-view cooperation. Although numerous DMVC approaches have been proposed, the collaboration role of individual views have not been well investigated in existing literature. Moreover, how to further enhance view cooperation for better fusion still needs to be explored. In this paper, we firstly consider DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, we introduce the Shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. Specially,  we employ the optimal transport distance between fused cluster distributions and single view component as the utility function for computing shapley values. Afterwards, we apply shapley values to assess the contribution of each view and utilize these contributions to promote view cooperation. Comprehensive experimental results well support the effectiveness of our framework adopting to existing DMVC frameworks, demonstrating the importance and necessity of enhancing the cooperation among views.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper evaluates the contributions of views in the task of unsupervised multi-view clustering and enhances model performance through a cooperation enhancing approach.'}, 'pdf': {'value': '/pdf/40a1d357eea0e19182fb452e305504eaa3502b19.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024evaluate,\\ntitle={Evaluate then Cooperate: Shapley-based View Cooperation Enhancement for Multi-view Clustering},\\nauthor={Fangdi Wang and Jiaqi Jin and Jingtao Hu and Suyuan Liu and Xihong Yang and Siwei Wang and Xinwang Liu and En Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xoc4QOvbDs}\\n}'}, 'paperhash': {'value': 'wang|evaluate_then_cooperate_shapleybased_view_cooperation_enhancement_for_multiview_clustering'}},forum = 'xoc4QOvbDs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission781/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission781/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission781/-/Revision', 'NeurIPS.cc/2024/Conference/Submission781/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xoCFd1WKpf',number = 4062,cdate = 1715353585901,pdate = 1727287738967,odate = 1730873871956,mdate = 1730873871979,tcdate = 1715353585901,tmdate = 1730873871979,ddate = None,content = {'title': {'value': 'Unified Lexical Representation for Interpretable Visual-Language Alignment'}, 'authors': {'value': ['Yifan Li', 'Yikai Wang', 'Yanwei Fu', 'Dongyu Ru', 'Zheng Zhang', 'Tong He']}, 'authorids': {'value': ['~Yifan_Li4', '~Yikai_Wang1', '~Yanwei_Fu2', '~Dongyu_Ru1', '~Zheng_Zhang1', '~Tong_He5']}, 'keywords': {'value': ['Multi-modal', 'Alignment', 'Retrieval', 'Sparse Retrieval', 'Lexical Representation']}, 'abstract': {'value': \"Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's groundbreaking work. \\nAlthough CLIP performs well, the typical direct latent feature alignment lacks clarity in its representation and similarity scores. \\nOn the other hand, lexical representation, a vector whose element represents the similarity between the sample and a word from the vocabulary, is a natural sparse representation and interpretable, providing exact matches for individual words.\\nHowever, lexical representations are difficult to learn due to no ground-truth supervision and false-discovery issues, and thus requires complex design to train effectively.\\nIn this paper, we introduce LexVLA, a more interpretable VLA framework by learning a unified lexical representation for both modalities without complex design. \\nWe use DINOv2 as our visual model for its local-inclined features and Llama 2, a generative language model, to leverage its in-context lexical prediction ability.\\nTo avoid the false discovery, we propose an overuse penalty to refrain the lexical representation from falsely frequently activating meaningless words.\\nWe demonstrate that these two pre-trained uni-modal models can be well-aligned by fine-tuning on the modest multi-modal dataset and avoid intricate training configurations. \\nOn cross-modal retrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset, outperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those trained from scratch on even bigger datasets (e.g., 1.1B data, including CC-12M).\\nWe conduct extensive experiments to analyze LexVLA. \\nCodes are available at https://github.com/Clementine24/LexVLA.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3206cbc8f56e0bf6c85cccc342384845c2232940.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024unified,\\ntitle={Unified Lexical Representation for Interpretable Visual-Language Alignment},\\nauthor={Yifan Li and Yikai Wang and Yanwei Fu and Dongyu Ru and Zheng Zhang and Tong He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xoCFd1WKpf}\\n}'}, 'TLDR': {'value': 'We introduce LexVLA, a interpretable Visual-Language Alignment framework by learning a unified lexical representation for both modalities without complex design.'}, 'paperhash': {'value': 'li|unified_lexical_representation_for_interpretable_visuallanguage_alignment'}},forum = 'xoCFd1WKpf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4062/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4062/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4062/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4062/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xnmm1jThkv',number = 14874,cdate = 1715754784098,pdate = 1727288081506,odate = 1730873968748,mdate = 1736789153625,tcdate = 1715754784098,tmdate = 1736789153625,ddate = None,content = {'title': {'value': 'Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models'}, 'authors': {'value': ['Sujai Hiremath', 'Jacqueline R. M. A. Maasch', 'Mengxiao Gao', 'Promit Ghosal', 'Kyra Gan']}, 'authorids': {'value': ['~Sujai_Hiremath1', '~Jacqueline_R._M._A._Maasch1', '~Mengxiao_Gao1', '~Promit_Ghosal2', '~Kyra_Gan1']}, 'keywords': {'value': ['global causal discovery', 'additive noise model', 'local structure']}, 'TLDR': {'value': 'We present a novel hybrid method for causal discovery that leverages local causal relationships in SEMs to construct a compact hierarchical topological sort, followed by a novel nonparametric constraint-based method for efficient edge discovery.'}, 'abstract': {'value': 'Learning the unique directed acyclic graph corresponding to an unknown causal model is a challenging task. Methods based on functional causal models can identify a unique graph, but either suffer from the curse of dimensionality or impose strong parametric assumptions. To address these challenges, we propose a novel hybrid approach for global causal discovery in observational data that leverages local causal substructures. We first present a topological sorting algorithm that leverages ancestral relationships in linear structural causal models to establish a compact top-down hierarchical ordering, encoding more causal information than linear orderings produced by existing methods. We demonstrate that this approach generalizes to nonlinear settings with arbitrary noise. We then introduce a nonparametric constraint-based algorithm that prunes spurious edges by searching for local conditioning sets, achieving greater accuracy than current methods. We provide theoretical guarantees for correctness and worst-case polynomial time complexities, with empirical validation on synthetic data.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0bca5f701021ef49d152b7de7f75842cb0a63c54.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhiremath2024hybrid,\\ntitle={Hybrid Top-Down Global Causal Discovery with Local Search for Linear and Nonlinear Additive Noise Models},\\nauthor={Sujai Hiremath and Jacqueline R. M. A. Maasch and Mengxiao Gao and Promit Ghosal and Kyra Gan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xnmm1jThkv}\\n}'}, 'paperhash': {'value': 'hiremath|hybrid_topdown_global_causal_discovery_with_local_search_for_linear_and_nonlinear_additive_noise_models'}},forum = 'xnmm1jThkv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14874/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14874/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14874/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14874/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'xjyU6zmZD7',number = 5748,cdate = 1715568114366,pdate = 1727287793786,odate = 1730873887908,mdate = 1730873887930,tcdate = 1715568114366,tmdate = 1730873887930,ddate = None,content = {'title': {'value': 'Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks'}, 'authors': {'value': ['Yufei Guo', 'Yuanpei Chen', 'Zecheng Hao', 'Weihang Peng', 'Zhou Jie', 'Yuhan Zhang', 'Xiaode Liu', 'Zhe Ma']}, 'authorids': {'value': ['~Yufei_Guo1', '~Yuanpei_Chen1', '~Zecheng_Hao1', '~Weihang_Peng2', '~Zhou_Jie3', '~Yuhan_Zhang1', '~Xiaode_Liu1', '~Zhe_Ma2']}, 'keywords': {'value': ['Spiking Neural Network', 'Training SNNs', 'Surrogate gradient']}, 'abstract': {'value': \"The Spiking Neural Network (SNN) is a biologically inspired neural network infrastructure that has recently garnered significant attention. It utilizes binary spike activations to transmit information, thereby replacing multiplications with additions and resulting in high energy efficiency. However, training an SNN directly poses a challenge due to the undefined gradient of the firing spike process. Although prior works have employed various surrogate gradient training methods that use an alternative function to replace the firing process during back-propagation, these approaches ignore an intrinsic problem: gradient vanishing. To address this issue, we propose a shortcut back-propagation method in the paper, which advocates for transmitting the gradient directly from the loss to the shallow layers. This enables us to present the gradient to the shallow layers directly, thereby significantly mitigating the gradient vanishing problem. Additionally, this method does not introduce any burden during the inference phase.\\nTo strike a balance between final accuracy and ease of training, we also propose an evolutionary training framework and implement it by inducing a balance coefficient that dynamically changes with the training epoch, which further improves the network's performance. Extensive experiments conducted over static and dynamic datasets using several popular network structures reveal that our method consistently outperforms state-of-the-art methods.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6dfff0aec6f93d33ffa638873f008d9ca6857190.pdf'}, 'supplementary_material': {'value': '/attachment/219213773c28852413f852e0755c33b1f50db66d.zip'}, '_bibtex': {'value': '@inproceedings{\\nguo2024take,\\ntitle={Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks},\\nauthor={Yufei Guo and Yuanpei Chen and Zecheng Hao and Weihang Peng and Zhou Jie and Yuhan Zhang and Xiaode Liu and Zhe Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xjyU6zmZD7}\\n}'}, 'paperhash': {'value': 'guo|take_a_shortcut_back_mitigating_the_gradient_vanishing_for_training_spiking_neural_networks'}},forum = 'xjyU6zmZD7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5748/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5748/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5748/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xjXYgdFM5M',number = 9584,cdate = 1715682090619,pdate = 1727287915307,odate = 1730873922086,mdate = 1730873922101,tcdate = 1715682090619,tmdate = 1730873922101,ddate = None,content = {'title': {'value': 'Reasons and Solutions for the Decline in Model Performance after Editing'}, 'authors': {'value': ['Xiusheng Huang', 'Jiaxiang Liu', 'Yequan Wang', 'Kang Liu']}, 'authorids': {'value': ['~Xiusheng_Huang1', '~Jiaxiang_Liu5', '~Yequan_Wang1', '~Kang_Liu1']}, 'keywords': {'value': ['Knowledge editing', 'performance evaluation']}, 'abstract': {'value': 'Knowledge editing technology has received widespread attention for low-cost updates of incorrect or outdated knowledge in large-scale language models. However, recent research has found that edited models often exhibit varying degrees of performance degradation. The reasons behind this phenomenon and potential solutions have not yet been provided. In order to investigate the reasons for the performance decline of the edited model and optimize the editing method, this work explores the underlying reasons from both data and model perspectives. Specifically, 1) from a data perspective, to clarify the impact of data on the performance of editing models, this paper first constructs a **M**ulti-**Q**uestion **D**ataset (**MQD**) to evaluate the impact of different types of editing data on model performance. The performance of the editing model is mainly affected by the diversity of editing targets and sequence length, as determined through experiments. 2) From a model perspective, this article explores the factors that affect the performance of editing models. The results indicate a strong correlation between the L1-norm of the editing model layer and the editing accuracy, and clarify that this is an important factor leading to the bottleneck of editing performance. Finally, in order to improve the performance of the editing model, this paper further proposes a **D**ump **for** **S**equence (**D4S**) method, which successfully overcomes the previous editing bottleneck by reducing the L1-norm of the editing layer, allowing users to perform multiple effective edits and minimizing model damage. Our code is available at https://github.com/nlpkeg/D4S.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/29125d34caf7e2e65a3da6e297ad342a2583e2d3.pdf'}, 'supplementary_material': {'value': '/attachment/d86773b0e28a00af1ddb1a50ba96917b67189e8f.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024reasons,\\ntitle={Reasons and Solutions for the Decline in Model Performance after Editing},\\nauthor={Xiusheng Huang and Jiaxiang Liu and Yequan Wang and Kang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xjXYgdFM5M}\\n}'}, 'paperhash': {'value': 'huang|reasons_and_solutions_for_the_decline_in_model_performance_after_editing'}},forum = 'xjXYgdFM5M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9584/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9584/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9584/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9584/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xgiurUq0ss',number = 7038,cdate = 1715611297020,pdate = 1727287833290,odate = 1730873898763,mdate = 1736900579890,tcdate = 1715611297020,tmdate = 1736900579890,ddate = None,content = {'title': {'value': 'DDK: Distilling Domain Knowledge for Efficient Large Language Models'}, 'authors': {'value': ['Jiaheng Liu', 'Chenchen Zhang', 'Jinyang Guo', 'Yuanxing Zhang', 'Haoran Que', 'Ken Deng', 'ZhiqiBai', 'Jie Liu', 'Ge Zhang', 'JiakaiWang', 'Yanan Wu', 'Congnan Liu', 'Jiamang Wang', 'Lin Qu', 'Wenbo Su', 'Bo Zheng']}, 'authorids': {'value': ['~Jiaheng_Liu1', '~Chenchen_Zhang3', '~Jinyang_Guo1', '~Yuanxing_Zhang3', '~Haoran_Que1', '~Ken_Deng1', '~ZhiqiBai1', '~Jie_Liu13', '~Ge_Zhang5', '~JiakaiWang1', '~Yanan_Wu2', '~Congnan_Liu2', '~Jiamang_Wang1', '~Lin_Qu2', '~Wenbo_Su2', '~Bo_Zheng5']}, 'keywords': {'value': ['Knowledge Distillation', 'Large Language Models', 'Model Acceralation']}, 'abstract': {'value': 'Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands.  Knowledge Distillation (KD) has emerged as an effective strategy to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a high-performing LLM (i.e., the teacher model). Prevailing techniques in LLM distillation typically use a black-box model API to generate high-quality pretrained and aligned datasets, or utilize white-box distillation by altering the loss function to better transfer knowledge from the teacher LLM. However, these methods ignore the knowledge differences between the student and teacher LLMs across domains. This results in excessive focus on domains with minimal performance gaps and insufficient attention to domains with large gaps, reducing overall performance. In this paper, we introduce a new LLM distillation framework called DDK, which dynamically adjusts the composition of the distillation dataset in a smooth manner according to the domain performance differences between the teacher and student models, making the distillation process more stable and effective. Extensive evaluations show that DDK significantly improves the performance of student models, outperforming both continuously pretrained baselines and existing knowledge distillation methods by a large margin.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9b8b61e97034e23e7b524afd2363f19fce21200a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024ddk,\\ntitle={{DDK}: Distilling Domain Knowledge for Efficient Large Language Models},\\nauthor={Jiaheng Liu and Chenchen Zhang and Jinyang Guo and Yuanxing Zhang and Haoran Que and Ken Deng and ZhiqiBai and Jie Liu and Ge Zhang and JiakaiWang and Yanan Wu and Congnan Liu and Jiamang Wang and Lin Qu and Wenbo Su and Bo Zheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xgiurUq0ss}\\n}'}, 'paperhash': {'value': 'liu|ddk_distilling_domain_knowledge_for_efficient_large_language_models'}},forum = 'xgiurUq0ss',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7038/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7038/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7038/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7038/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xgP5ynlZWf',number = 10437,cdate = 1715694086903,pdate = 1727287940826,odate = 1730873929153,mdate = 1730873929173,tcdate = 1715694086903,tmdate = 1730873929173,ddate = None,content = {'title': {'value': 'RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models'}, 'authors': {'value': ['Haoyu Chen', 'Wenbo Li', 'Jinjin Gu', 'Jingjing Ren', 'Sixiang Chen', 'Tian Ye', 'Renjing Pei', 'Kaiwen Zhou', 'Fenglong Song', 'Lei Zhu']}, 'authorids': {'value': ['~Haoyu_Chen2', '~Wenbo_Li6', '~Jinjin_Gu1', '~Jingjing_Ren1', '~Sixiang_Chen2', '~Tian_Ye3', '~Renjing_Pei1', '~Kaiwen_Zhou2', '~Fenglong_Song1', '~Lei_Zhu1']}, 'keywords': {'value': ['Image Restoration', 'Low level vision', 'Agent', 'Multimodal Large Language Model']}, 'abstract': {'value': 'Natural images captured by mobile devices often suffer from multiple types of degradation, such as noise, blur, and low light. Traditional image restoration methods require manual selection of specific tasks, algorithms, and execution sequences, which is time-consuming and may yield suboptimal results. All-in-one models, though capable of handling multiple tasks, typically support only a limited range and often produce overly smooth, low-fidelity outcomes due to their broad data distribution fitting. To address these challenges, we first define a new pipeline for restoring images with multiple degradations, and then introduce RestoreAgent, an intelligent image restoration system leveraging multimodal large language models. RestoreAgent autonomously assesses the type and extent of degradation in input images and performs restoration through (1) determining the appropriate restoration tasks, (2) optimizing the task sequence, (3) selecting the most suitable models, and (4) executing the restoration. Experimental results demonstrate the superior performance of RestoreAgent in handling complex degradation, surpassing human experts. Furthermore, the system’s modular design facilitates the fast integration of new tasks and models.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a7abb52bb68d236e32bce92953c8abf4bfa5f495.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024restoreagent,\\ntitle={RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large Language Models},\\nauthor={Haoyu Chen and Wenbo Li and Jinjin Gu and Jingjing Ren and Sixiang Chen and Tian Ye and Renjing Pei and Kaiwen Zhou and Fenglong Song and Lei Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xgP5ynlZWf}\\n}'}, 'paperhash': {'value': 'chen|restoreagent_autonomous_image_restoration_agent_via_multimodal_large_language_models'}},forum = 'xgP5ynlZWf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10437/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10437/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10437/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10437/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xeviQPXTMU',number = 7162,cdate = 1715613957693,pdate = 1727287837574,odate = 1730873899993,mdate = 1730873900011,tcdate = 1715613957693,tmdate = 1730873900011,ddate = None,content = {'title': {'value': 'FedGMark: Certifiably Robust Watermarking for Federated Graph Learning'}, 'authors': {'value': ['Yuxin Yang', 'Qiang Li', 'Yuan Hong', 'Binghui Wang']}, 'authorids': {'value': ['~Yuxin_Yang3', '~Qiang_Li12', '~Yuan_Hong1', '~Binghui_Wang2']}, 'keywords': {'value': ['Watermark', 'Federated Graph Learning']}, 'abstract': {'value': 'Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/75848fdd795ff86e8eff2d9277a1b8057ad9f7d9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024fedgmark,\\ntitle={Fed{GM}ark: Certifiably Robust Watermarking for Federated Graph Learning},\\nauthor={Yuxin Yang and Qiang Li and Yuan Hong and Binghui Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xeviQPXTMU}\\n}'}, 'paperhash': {'value': 'yang|fedgmark_certifiably_robust_watermarking_for_federated_graph_learning'}},forum = 'xeviQPXTMU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7162/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7162/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7162/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7162/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'xeXRhTUmcf',number = 6359,cdate = 1715590853824,pdate = 1727287812449,odate = 1730873892973,mdate = 1734692040823,tcdate = 1715590853824,tmdate = 1734692040823,ddate = None,content = {'title': {'value': 'Combining Statistical Depth and Fermat Distance for Uncertainty Quantification'}, 'authors': {'value': ['Hai-Vy Nguyen', 'Fabrice Gamboa', 'Reda Chhaibi', 'Sixin Zhang', 'Serge Gratton', 'Thierry Giaccone']}, 'authorids': {'value': ['~Hai-Vy_Nguyen1', '~Fabrice_Gamboa1', '~Reda_Chhaibi2', '~Sixin_Zhang2', '~Serge_Gratton2', '~Thierry_Giaccone1']}, 'keywords': {'value': ['Out-of-distribution', 'In-distribution', 'Uncertainty quantification', 'Fermat distance', 'Lens depth', 'Novelty detection', 'Feature spaces', 'Deep learning']}, 'abstract': {'value': 'We measure the out-of-domain uncertainty in the prediction of Neural Networks using a statistical notion called \"Lens Depth\\'\\' (LD) combined with Fermat Distance, which is able to capture precisely the \"depth\\'\\' of a point with respect to a distribution in feature space, without any distributional assumption. Our method also has no trainable parameter. The method is applied directly in the feature space at test time and does not intervene in training process. As such, it does not impact the performance of the original model. The proposed method gives excellent qualitative results on toy datasets and can give competitive or better uncertainty estimation on standard deep learning datasets compared to strong baseline methods.'}, 'pdf': {'value': '/pdf/c4068669caf9d9c9527416b6771bb88358891592.pdf'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nnguyen2024combining,\\ntitle={Combining Statistical Depth and Fermat Distance for Uncertainty Quantification},\\nauthor={Hai-Vy Nguyen and Fabrice Gamboa and Reda Chhaibi and Sixin Zhang and Serge Gratton and Thierry Giaccone},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xeXRhTUmcf}\\n}'}, 'paperhash': {'value': 'nguyen|combining_statistical_depth_and_fermat_distance_for_uncertainty_quantification'}},forum = 'xeXRhTUmcf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6359/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6359/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6359/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6359/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'xcqSOfHt4g',number = 12869,cdate = 1715730077498,pdate = 1727288020468,odate = 1730873952939,mdate = 1737017041202,tcdate = 1715730077498,tmdate = 1737017041202,ddate = None,content = {'title': {'value': 'Simplified and Generalized Masked Diffusion for Discrete Data'}, 'authors': {'value': ['Jiaxin Shi', 'Kehang Han', 'Zhe Wang', 'Arnaud Doucet', 'Michalis Titsias']}, 'authorids': {'value': ['~Jiaxin_Shi1', '~Kehang_Han1', '~Zhe_Wang13', '~Arnaud_Doucet2', '~Michalis_Titsias1']}, 'keywords': {'value': ['diffusion', 'discrete', 'masked diffusion', 'absorbing diffusion', 'diffusion model']}, 'TLDR': {'value': 'A simplified and generalized framework for training masked discrete diffusion models'}, 'abstract': {'value': 'Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64x64) bits per dimension that are better than autoregressive models of similar sizes.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/af16c0e21b31a4aa92236ff91bd4af0bfda1a2c9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshi2024simplified,\\ntitle={Simplified and Generalized Masked Diffusion for Discrete Data},\\nauthor={Jiaxin Shi and Kehang Han and Zhe Wang and Arnaud Doucet and Michalis Titsias},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xcqSOfHt4g}\\n}'}, 'paperhash': {'value': 'shi|simplified_and_generalized_masked_diffusion_for_discrete_data'}},forum = 'xcqSOfHt4g',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12869/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12869/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12869/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12869/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xcF2VbyZts',number = 4126,cdate = 1715364082884,pdate = 1727287741278,odate = 1730873872615,mdate = 1730873872633,tcdate = 1715364082884,tmdate = 1730873872633,ddate = None,content = {'title': {'value': 'SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization'}, 'authors': {'value': ['Wanhua Li', 'Zibin Meng', 'Jiawei Zhou', 'Donglai Wei', 'Chuang Gan', 'Hanspeter Pfister']}, 'authorids': {'value': ['~Wanhua_Li1', '~Zibin_Meng2', '~Jiawei_Zhou1', '~Donglai_Wei1', '~Chuang_Gan1', '~Hanspeter_Pfister1']}, 'keywords': {'value': ['Social Relation Reasoning', 'Large Language Models', 'Foundation Models', 'Prompt Optimization']}, 'abstract': {'value': 'Social relation reasoning aims to identify relation categories such as friends, spouses, and colleagues from images. While current methods adopt the paradigm of training a dedicated network end-to-end using labeled image data, they are limited in terms of generalizability and interpretability. To address these issues, we first present a simple yet well-crafted framework named SocialGPT, which combines the perception capability of Vision Foundation Models (VFMs) and the reasoning capability of Large Language Models (LLMs) within a modular framework, providing a strong baseline for social relation recognition. Specifically, we instruct VFMs to translate image content into a textual social story, and then utilize LLMs for text-based reasoning. SocialGPT introduces systematic design principles to adapt VFMs and LLMs separately and bridge their gaps. Without additional model training, it achieves competitive zero-shot results on two databases while offering interpretable answers, as LLMs can generate language-based explanations for the decisions. The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired. As we essentially convert a visual classification task into a generative task of LLMs, automatic prompt optimization encounters a unique long prompt optimization issue. To address this issue, we further propose the Greedy Segment Prompt Optimization (GSPO), which performs a greedy search by utilizing gradient information at the segment level. Experimental results show that GSPO significantly improves performance, and our method also generalizes to different image styles. The code is available at https://github.com/Mengzibin/SocialGPT.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present SocialGPT, a modular framework with greedy segment prompt optimization for social relation reasoning, which attains competitive results while also providing interpretable explanations.'}, 'pdf': {'value': '/pdf/70105568d7ff06cd079c119532147dd737450a1c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024socialgpt,\\ntitle={Social{GPT}: Prompting {LLM}s for Social Relation Reasoning via Greedy Segment Optimization},\\nauthor={Wanhua Li and Zibin Meng and Jiawei Zhou and Donglai Wei and Chuang Gan and Hanspeter Pfister},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xcF2VbyZts}\\n}'}, 'paperhash': {'value': 'li|socialgpt_prompting_llms_for_social_relation_reasoning_via_greedy_segment_optimization'}},forum = 'xcF2VbyZts',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4126/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4126/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4126/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4126/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xbuaSTqAEz',number = 11140,cdate = 1715701394841,pdate = 1727287961758,odate = 1730873935378,mdate = 1730873935396,tcdate = 1715701394841,tmdate = 1730873935396,ddate = None,content = {'title': {'value': 'Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning'}, 'authors': {'value': ['Jiawei Yao', 'Qi Qian', 'Juhua Hu']}, 'authorids': {'value': ['~Jiawei_Yao3', '~Qi_Qian1', '~Juhua_Hu1']}, 'keywords': {'value': ['Multiple Clustering', 'Multi-modal Model', 'Large Language Model']}, 'abstract': {'value': 'Multiple clustering aims to discover various latent structures of data from different aspects. Deep multiple clustering methods have achieved remarkable performance by exploiting complex patterns and relationships in data. However, existing works struggle to flexibly adapt to diverse user-specific needs in data grouping, which may require manual understanding of each clustering. To address these limitations, we introduce Multi-Sub, a novel end-to-end multiple clustering approach that incorporates a multi-modal subspace proxy learning framework in this work. Utilizing the synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual prompts expressing user preferences with their corresponding visual representations. This is achieved by automatically generating proxy words from large language models that act as subspace bases, thus allowing for the customized representation of data in terms specific to the user’s interests. Our method consistently outperforms existing baselines across a broad set of datasets in visual multiple clustering tasks. Our code is available at https://github.com/Alexander-Yao/Multi-Sub.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e80cabcafca9dea1dfdf22930f209be7f322a75a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyao2024customized,\\ntitle={Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning},\\nauthor={Jiawei Yao and Qi Qian and Juhua Hu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xbuaSTqAEz}\\n}'}, 'paperhash': {'value': 'yao|customized_multiple_clustering_via_multimodal_subspace_proxy_learning'}},forum = 'xbuaSTqAEz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11140/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11140/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11140/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11140/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xavWvnJTST',number = 19940,cdate = 1715793566879,pdate = 1727288221665,odate = 1730873998461,mdate = 1736954545308,tcdate = 1715793566879,tmdate = 1736954545308,ddate = None,content = {'title': {'value': 'Feedback control guides credit assignment in recurrent neural networks'}, 'authors': {'value': ['Klara Kaleb', 'Barbara Feulner', 'Juan A. Gallego', 'Claudia Clopath']}, 'authorids': {'value': ['~Klara_Kaleb1', '~Barbara_Feulner1', '~Juan_A._Gallego1', '~Claudia_Clopath1']}, 'keywords': {'value': ['biologically-plausible learning', 'RNNs', 'motor control', 'feedback control']}, 'abstract': {'value': \"How do brain circuits learn to generate behaviour?\\n  While significant strides have been made in understanding learning in artificial neural networks, applying this knowledge to biological networks remains challenging.\\n  For instance, while backpropagation is known to perform accurate credit assignment of error in artificial neural networks, how a similarly powerful process can be realized within the constraints of biological circuits remains largely unclear.\\n  One of the major challenges is that the brain's extensive recurrent connectivity requires the propagation of error through both space and time, a problem that is notoriously difficult to solve in vanilla recurrent neural networks.\\n  Moreover, the extensive feedback connections in the brain are known to influence forward network activity, but the interaction between feedback-driven activity changes and local, synaptic plasticity-based learning is not fully understood.\\n  Building on our previous work modelling motor learning, this work investigates the mechanistic properties of pre-trained networks with feedback control on a standard motor task.\\n  We show that feedback control of the ongoing recurrent network dynamics approximates the optimal first-order gradient with respect to the network activities, allowing for rapid, ongoing movement correction.\\n  Moreover, we show that trial-by-trial adaptation to a persistent perturbation using a local, biologically plausible learning rule that integrates recent activity and error feedback is both more accurate and more efficient with feedback control during learning, due to the decoupling of the recurrent network dynamics and the injection of an adaptive, second-order gradient into the network dynamics.\\n  Thus, our results suggest that feedback control may guide credit assignment in biological recurrent neural networks, enabling both rapid and efficient learning in the brain.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9dab0a630262d4f5546036f7479bf26afc15556b.pdf'}, 'supplementary_material': {'value': '/attachment/921085fdcdf365f9677bbb04b2b50b7c97fd751c.zip'}, '_bibtex': {'value': '@inproceedings{\\nkaleb2024feedback,\\ntitle={Feedback control guides credit assignment in recurrent neural networks},\\nauthor={Klara Kaleb and Barbara Feulner and Juan A. Gallego and Claudia Clopath},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xavWvnJTST}\\n}'}, 'TLDR': {'value': 'Feedback control may enable biological recurrent neural networks to achieve accurate and efficient credit assignment, facilitating real-time learning and adaptation in behavior generation.'}, 'paperhash': {'value': 'kaleb|feedback_control_guides_credit_assignment_in_recurrent_neural_networks'}},forum = 'xavWvnJTST',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19940/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19940/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19940/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19940/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'xaqPAkJnAS',number = 10421,cdate = 1715693919201,pdate = 1727287940085,odate = 1730873928890,mdate = 1730873928911,tcdate = 1715693919201,tmdate = 1730873928911,ddate = None,content = {'title': {'value': 'Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning'}, 'authors': {'value': ['Zhixiang Shen', 'Shuo Wang', 'zhao kang']}, 'authorids': {'value': ['~Zhixiang_Shen1', '~Shuo_Wang39', '~zhao_kang1']}, 'keywords': {'value': ['Multiplex Graph Learning', 'Graph Structure Learning', 'Multi-view Graph Clustering', 'Self-supervised Graph Learning']}, 'abstract': {'value': \"Unsupervised Multiplex Graph Learning (UMGL) aims to learn node representations on various edge types without manual labeling. However, existing research overlooks a key factor: the reliability of the graph structure. Real-world data often exhibit a complex nature and contain abundant task-irrelevant noise, severely compromising UMGL's performance. Moreover, existing methods primarily rely on contrastive learning to maximize mutual information across different graphs, limiting them to multiplex graph redundant scenarios and failing to capture view-unique task-relevant information. In this paper, we focus on a more realistic and challenging task: to unsupervisedly learn a fused graph from multiple graphs that preserve sufficient task-relevant information while removing task-irrelevant noise. Specifically, our proposed Information-aware Unsupervised Multiplex Graph Fusion framework (InfoMGF) uses graph structure refinement to eliminate irrelevant noise and simultaneously maximizes view-shared and view-unique task-relevant information, thereby tackling the frontier of non-redundant multiplex graph. Theoretical analyses further guarantee the effectiveness of InfoMGF. Comprehensive experiments against various baselines on different downstream tasks demonstrate its superior performance and robustness. Surprisingly, our unsupervised method even beats the sophisticated supervised approaches. The source code and datasets are available at https://github.com/zxlearningdeep/InfoMGF.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e5b9f6af4bcc1edd63aea9284ca2c3aba26fc5b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshen2024beyond,\\ntitle={Beyond Redundancy: Information-aware Unsupervised Multiplex Graph Structure Learning},\\nauthor={Zhixiang Shen and Shuo Wang and zhao kang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xaqPAkJnAS}\\n}'}, 'paperhash': {'value': 'shen|beyond_redundancy_informationaware_unsupervised_multiplex_graph_structure_learning'}},forum = 'xaqPAkJnAS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10421/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10421/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10421/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10421/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xabStWAUtr',number = 9984,cdate = 1715688761814,pdate = 1727287926504,odate = 1730873925092,mdate = 1737022825689,tcdate = 1715688761814,tmdate = 1737022825689,ddate = None,content = {'title': {'value': 'Co-occurrence is not Factual Association in Language Models'}, 'authors': {'value': ['Xiao Zhang', 'Miao Li', 'Ji Wu']}, 'authorids': {'value': ['~Xiao_Zhang9', '~Miao_Li10', '~Ji_Wu3']}, 'keywords': {'value': ['language model', 'knowledge learning', 'reasoning']}, 'abstract': {'value': 'Pretrained language models can encode a large amount of knowledge and utilize it for various reasoning tasks, yet they can still struggle to learn novel factual knowledge effectively from finetuning on limited textual demonstrations. In this work, we show that the reason for this deficiency is that language models are biased to learn word co-occurrence statistics instead of true factual associations. We identify the differences between two forms of knowledge representation in language models: knowledge in the form of co-occurrence statistics is encoded in the middle layers of the transformer model and does not generalize well to reasoning scenarios beyond simple question answering, while true factual associations are encoded in the lower layers and can be freely utilized in various reasoning tasks. Based on these observations, we propose two strategies to improve the learning of factual associations in language models. We show that training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics, significantly improving the generalization of newly learned knowledge. We also propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations when training on plain narrative text. On both synthetic and real-world corpora, the two proposed strategies improve the generalization of the knowledge learned during finetuning to reasoning scenarios such as indirect and multi-hop question answering.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Language models do not learn factual knowledge well from finetuning due to fitting word co-occurrence. Training with implicit association and removing word co-occurrence help model effectively learn knowledge that generalizes well to reasoning tasks.'}, 'pdf': {'value': '/pdf/a05ca368d45ceff595e9950cf21de3cd1baf43fe.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024cooccurrence,\\ntitle={Co-occurrence is not Factual Association in Language Models},\\nauthor={Xiao Zhang and Miao Li and Ji Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xabStWAUtr}\\n}'}, 'paperhash': {'value': 'zhang|cooccurrence_is_not_factual_association_in_language_models'}},forum = 'xabStWAUtr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9984/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9984/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9984/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9984/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xZxXNhndXU',number = 443,cdate = 1713908848780,pdate = 1727287637743,odate = 1730873840947,mdate = 1730873840965,tcdate = 1713908848780,tmdate = 1730873840965,ddate = None,content = {'title': {'value': 'Dynamic 3D Gaussian Fields for Urban Areas'}, 'authors': {'value': ['Tobias Fischer', 'Jonas Kulhanek', 'Samuel Rota Bulò', 'Lorenzo Porzi', 'Marc Pollefeys', 'Peter Kontschieder']}, 'authorids': {'value': ['~Tobias_Fischer3', '~Jonas_Kulhanek1', '~Samuel_Rota_Bulò3', '~Lorenzo_Porzi1', '~Marc_Pollefeys2', '~Peter_Kontschieder1']}, 'keywords': {'value': ['Neural Rendering', 'Gaussian Splatting', 'Dynamic Urban Areas']}, 'abstract': {'value': 'We present an efficient neural 3D scene representation for novel-view synthesis (NVS) in large-scale, dynamic urban areas. Existing works are not well suited for applications like mixed-reality or closed-loop simulation due to their limited visual quality and non-interactive rendering speeds. Recently, rasterization-based approaches have achieved high-quality NVS at impressive speeds. However, these methods are limited to small-scale, homogeneous data, i.e. they cannot handle severe appearance and geometry variations due to weather, season, and lighting and do not scale to larger, dynamic areas with thousands of images. We propose 4DGF, a neural scene representation that scales to large-scale dynamic urban areas, handles heterogeneous input data, and substantially improves rendering speeds. We use 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. We integrate scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. This decomposed approach enables flexible scene composition suitable for real-world applications. In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than 200x in rendering speed.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/65abb3d26796730cf34d63a16451810604040c39.pdf'}, 'supplementary_material': {'value': '/attachment/6d99e721c19040d980b665c775bf0eb2301cf0af.zip'}, '_bibtex': {'value': '@inproceedings{\\nfischer2024dynamic,\\ntitle={Dynamic 3D Gaussian Fields for Urban Areas},\\nauthor={Tobias Fischer and Jonas Kulhanek and Samuel Rota Bul{\\\\`o} and Lorenzo Porzi and Marc Pollefeys and Peter Kontschieder},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xZxXNhndXU}\\n}'}, 'TLDR': {'value': 'Given a set of heterogeneous input sequences of a common geographic area, we optimize a single dynamic scene representation that permits rendering of arbitrary viewpoints and scene configurations at interactive speeds.'}, 'paperhash': {'value': 'fischer|dynamic_3d_gaussian_fields_for_urban_areas'}},forum = 'xZxXNhndXU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission443/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission443/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission443/-/Revision', 'NeurIPS.cc/2024/Conference/Submission443/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xZKXGvLB0c',number = 18529,cdate = 1715785966880,pdate = 1727288186654,odate = 1730873990599,mdate = 1736863623889,tcdate = 1715785966880,tmdate = 1736863623889,ddate = None,content = {'title': {'value': 'Causal vs. Anticausal merging of predictors'}, 'authors': {'value': ['Sergio Hernan Garrido Mejia', 'Patrick Blöbaum', 'Bernhard Schölkopf', 'Dominik Janzing']}, 'authorids': {'value': ['~Sergio_Hernan_Garrido_Mejia1', '~Patrick_Blöbaum1', '~Bernhard_Schölkopf1', '~Dominik_Janzing3']}, 'keywords': {'value': ['Causality', 'Merging of predictors', 'Causal vs Anticausal', 'Maximum Entropy']}, 'TLDR': {'value': 'We study the asymmetries produced in the merging of predictors whenever we have causal information.'}, 'abstract': {'value': 'We study the differences arising from merging predictors in the causal and anticausal directions using the same data.\\nIn particular we study the asymmetries that arise in a simple model where we merge the predictors using one binary variable as target and two continuous variables as predictors.\\nWe use Causal Maximum Entropy (CMAXENT) as inductive bias to merge the predictors, however, we expect similar differences to hold also when we use other merging methods that take into account asymmetries between cause and effect.\\nWe show that if we observe all bivariate distributions, the CMAXENT solution reduces to a logistic regression in the causal direction and Linear Discriminant Analysis (LDA) in the anticausal direction.\\nFurthermore, we study how the decision boundaries of these two solutions differ whenever we observe only some of the bivariate distributions implications for Out-Of-Variable (OOV) generalisation.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/329aa661f69e5ddf452da79c89b300d87c6805fa.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmejia2024causal,\\ntitle={Causal vs. Anticausal merging of predictors},\\nauthor={Sergio Hernan Garrido Mejia and Patrick Bl{\\\\\"o}baum and Bernhard Sch{\\\\\"o}lkopf and Dominik Janzing},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xZKXGvLB0c}\\n}'}, 'paperhash': {'value': 'mejia|causal_vs_anticausal_merging_of_predictors'}},forum = 'xZKXGvLB0c',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18529/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18529/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18529/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18529/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'xXRnUU7xTL',number = 7042,cdate = 1715611443468,pdate = 1727287833352,odate = 1730873898845,mdate = 1730873898859,tcdate = 1715611443468,tmdate = 1730873898859,ddate = None,content = {'title': {'value': 'SelfCodeAlign: Self-Alignment for Code Generation'}, 'authors': {'value': ['Yuxiang Wei', 'Federico Cassano', 'Jiawei Liu', 'Yifeng Ding', 'Naman Jain', 'Zachary Mueller', 'Harm de Vries', 'Leandro Von Werra', 'Arjun Guha', 'LINGMING ZHANG']}, 'authorids': {'value': ['~Yuxiang_Wei2', '~Federico_Cassano1', '~Jiawei_Liu11', '~Yifeng_Ding2', '~Naman_Jain2', '~Zachary_Mueller1', '~Harm_de_Vries1', '~Leandro_Von_Werra1', '~Arjun_Guha3', '~LINGMING_ZHANG2']}, 'keywords': {'value': ['Large language models', 'Code generation', 'Instruction tuning', 'Self-Alignment']}, 'abstract': {'value': 'Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. For programming tasks, most models are finetuned with costly human-annotated instruction-response pairs or those generated by large, proprietary LLMs, which may not be permitted. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component’s effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. Overall, SelfCodeAlign shows for the first time that a strong instruction-tuned code LLM can result from self-alignment rather than distillation.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f4cd100d3f9f85fe8c929ea517dc4cbd24143e72.pdf'}, 'supplementary_material': {'value': '/attachment/bcec3bbed475d368a97007019676fbf3d48cb111.zip'}, '_bibtex': {'value': '@inproceedings{\\nwei2024selfcodealign,\\ntitle={SelfCodeAlign: Self-Alignment for Code Generation},\\nauthor={Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro Von Werra and Arjun Guha and LINGMING ZHANG},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xXRnUU7xTL}\\n}'}, 'TLDR': {'value': 'Self-alignment for code LLMs without human annotations'}, 'paperhash': {'value': 'wei|selfcodealign_selfalignment_for_code_generation'}},forum = 'xXRnUU7xTL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7042/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7042/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7042/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7042/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xW6ga9i4eA',number = 3650,cdate = 1715312343947,pdate = 1727287726964,odate = 1730873868265,mdate = 1730873868277,tcdate = 1715312343947,tmdate = 1730873868277,ddate = None,content = {'title': {'value': 'pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning'}, 'authors': {'value': ['Jiaqi Wang', 'Qi Li', 'Lingjuan Lyu', 'Fenglong Ma']}, 'authorids': {'value': ['~Jiaqi_Wang4', '~Qi_Li14', '~Lingjuan_Lyu1', '~Fenglong_Ma1']}, 'keywords': {'value': ['Heterogenous federated learning']}, 'abstract': {'value': 'Federated learning, a pioneering paradigm, enables collaborative model training without exposing users’ data to central servers. Most existing federated learning systems necessitate uniform model structures across all clients, restricting their practicality. Several methods have emerged to aggregate diverse client models; however, they either lack the ability of personalization, raise privacy and security concerns, need prior knowledge, or ignore the capability and functionality of personalized models. In this paper, we present an innovative approach, named pFedClub, which addresses these challenges. pFedClub introduces personalized federated learning through the substitution of controllable neural network blocks/layers. Initially, pFedClub dissects heterogeneous client models into blocks and organizes them into functional groups on the server. Utilizing the designed CMSR (Controllable Model Searching and Reproduction) algorithm, pFedClub generates a range of personalized candidate models for each client. A model matching technique is then applied to select the optimal personalized model, serving as a teacher model to guide each client’s training process. We conducted extensive experiments across three datasets, examining both IID and non-IID settings. The results demonstrate that pFedClub outperforms baseline approaches, achieving state-of-the-art performance. Moreover, our model insight analysis reveals that pFedClub generates personalized models of reasonable size in a controllable manner, significantly reducing computational costs.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e4ee792dd28b3bc552b8f290198f312b9e344159.pdf'}, 'supplementary_material': {'value': '/attachment/8e5cf8e938968efb4d4463104e8f5c5c124fa92c.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024pfedclub,\\ntitle={pFedClub: Controllable Heterogeneous Model Aggregation for Personalized Federated Learning},\\nauthor={Jiaqi Wang and Qi Li and Lingjuan Lyu and Fenglong Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xW6ga9i4eA}\\n}'}, 'paperhash': {'value': 'wang|pfedclub_controllable_heterogeneous_model_aggregation_for_personalized_federated_learning'}},forum = 'xW6ga9i4eA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3650/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3650/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3650/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3650/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xUoNgR1Byy',number = 6035,cdate = 1715581445413,pdate = 1727287803143,odate = 1730873890087,mdate = 1730873890106,tcdate = 1715581445413,tmdate = 1730873890106,ddate = None,content = {'title': {'value': 'Interpreting Learned Feedback Patterns in Large Language Models'}, 'authors': {'value': ['Luke Marks', 'Amir Abdullah', 'Clement Neo', 'Rauno Arike', 'David Krueger', 'Philip Torr', 'Fazl Barez']}, 'authorids': {'value': ['~Luke_Marks2', '~Amir_Abdullah1', '~Clement_Neo1', '~Rauno_Arike1', '~David_Krueger1', '~Philip_Torr1', '~Fazl_Barez1']}, 'keywords': {'value': ['Interpretability', 'Reward Models', 'Safety']}, 'abstract': {'value': \"Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term **Learned Feedback Pattern** (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the **safety** and **alignment** of LLMs.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/df55089e1689943ee66585be002d20df9b191eae.pdf'}, 'supplementary_material': {'value': '/attachment/572c88aa15b170dd8077543010aef15a77d25918.zip'}, '_bibtex': {'value': '@inproceedings{\\nmarks2024interpreting,\\ntitle={Interpreting Learned Feedback Patterns in Large Language Models},\\nauthor={Luke Marks and Amir Abdullah and Clement Neo and Rauno Arike and David Krueger and Philip Torr and Fazl Barez},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xUoNgR1Byy}\\n}'}, 'paperhash': {'value': 'marks|interpreting_learned_feedback_patterns_in_large_language_models'}},forum = 'xUoNgR1Byy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6035/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6035/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6035/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6035/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xUjBZR6b1T',number = 257,cdate = 1713856786618,pdate = 1727287633844,odate = 1730873839527,mdate = 1735007049917,tcdate = 1713856786618,tmdate = 1735007049917,ddate = None,content = {'title': {'value': 'ReVideo: Remake a Video with Motion and Content Control'}, 'authors': {'value': ['Chong Mou', 'Mingdeng Cao', 'Xintao Wang', 'Zhaoyang Zhang', 'Ying Shan', 'Jian Zhang']}, 'authorids': {'value': ['~Chong_Mou1', '~Mingdeng_Cao1', '~Xintao_Wang1', '~Zhaoyang_Zhang1', '~Ying_Shan2', '~Jian_Zhang22']}, 'keywords': {'value': ['Diffusion model', 'Video editing']}, 'abstract': {'value': 'Despite significant advancements in video generation and editing using diffusion models, achieving accurate and localized video editing remains a substantial challenge. Additionally, most existing video editing methods primarily focus on altering visual content, with limited research dedicated to motion editing. In this paper, we present a novel attempt to Remake a Video (ReVideo) which stands out from existing methods by allowing precise video editing in specific areas through the specification of both content and motion. Content editing is facilitated by modifying the first frame, while the trajectory-based motion control offers an intuitive user interaction experience. ReVideo addresses a new task involving the coupling and training imbalance between content and motion control. To tackle this, we develop a three-stage training strategy that progressively decouples these two aspects from coarse to fine. Furthermore, we propose a spatiotemporal adaptive fusion module to integrate content and motion control across various sampling steps and spatial locations. Extensive experiments demonstrate that our ReVideo has promising performance on several accurate video editing applications, i.e., (1) locally changing video content while keeping the motion constant, (2) keeping content unchanged and customizing new motion trajectories, (3) modifying both content and motion trajectories. Our method can also seamlessly extend these applications to multi-area editing without specific training, demonstrating its flexibility and robustness.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bb0cf0788a982c6b491da99b791d82fa60d2e219.pdf'}, 'supplementary_material': {'value': '/attachment/b5aa2cd28380b8f65bb6360652f7e186ce530b04.zip'}, '_bibtex': {'value': '@inproceedings{\\nmou2024revideo,\\ntitle={ReVideo: Remake a Video with Motion and Content Control},\\nauthor={Chong Mou and Mingdeng Cao and Xintao Wang and Zhaoyang Zhang and Ying Shan and Jian Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xUjBZR6b1T}\\n}'}, 'paperhash': {'value': 'mou|revideo_remake_a_video_with_motion_and_content_control'}},forum = 'xUjBZR6b1T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission257/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission257/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission257/-/Revision', 'NeurIPS.cc/2024/Conference/Submission257/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xSziO6gQgG',number = 3502,cdate = 1715292609603,pdate = 1727287722909,odate = 1730873867055,mdate = 1730873867066,tcdate = 1715292609603,tmdate = 1730873867066,ddate = None,content = {'title': {'value': 'Implicit Optimization Bias of Next-token Prediction in Linear Models'}, 'authors': {'value': ['Christos Thrampoulidis']}, 'authorids': {'value': ['~Christos_Thrampoulidis1']}, 'keywords': {'value': ['entropy', 'gradient descent', 'SVM', 'next-token prediction', 'linear models', 'separability', 'language models', 'word embeddings']}, 'TLDR': {'value': 'By framing next-token prediction training as sparse soft-label classification, we characterize the implicit optimization biases of gradient descent in language models trained to achieve their entropy lower bound.'}, 'abstract': {'value': \"We initiate an investigation into the optimization properties of next-token prediction (NTP), the dominant training paradigm for modern language models. Specifically, we study the structural properties of the solutions selected by gradient-based optimizers among the many possible minimizers of the NTP objective. By framing NTP as cross-entropy minimization  across \\\\emph{distinct} contexts, each tied with a \\\\emph{sparse}  conditional probability distribution across a finite vocabulary of tokens, we introduce ``NTP-separability conditions'' that enable reaching the data-entropy lower bound. With this setup, and focusing on linear models with fixed context embeddings, we characterize the optimization bias of gradient descent (GD): Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds. In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP. These findings extend previous research on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into the optimization and generalization properties of NTP, irrespective of the specific architecture used to generate the context embeddings.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/174e95ac9c3ffa8d220ddbe8561c2d8a3a48c25e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nthrampoulidis2024implicit,\\ntitle={Implicit Optimization Bias of Next-token Prediction in Linear Models},\\nauthor={Christos Thrampoulidis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xSziO6gQgG}\\n}'}, 'paperhash': {'value': 'thrampoulidis|implicit_optimization_bias_of_nexttoken_prediction_in_linear_models'}},forum = 'xSziO6gQgG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3502/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3502/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3502/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3502/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xSU27DgWEr',number = 13840,cdate = 1715743429993,pdate = 1727288051962,odate = 1730873960858,mdate = 1730873960875,tcdate = 1715743429993,tmdate = 1730873960875,ddate = None,content = {'title': {'value': 'On $f$-Divergence Principled Domain Adaptation: An Improved Framework'}, 'authors': {'value': ['Ziqiao Wang', 'Yongyi Mao']}, 'authorids': {'value': ['~Ziqiao_Wang1', '~Yongyi_Mao2']}, 'keywords': {'value': ['learning theory', 'unsupervised domain adaptation', 'f-divergence', 'generalization']}, 'TLDR': {'value': 'We give an improved theoretical framework for f-divergence-based domain learning'}, 'abstract': {'value': 'Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed in Acuna et al. (2021) by refining their $f$-divergence-based discrepancy and additionally introducing a new measure, $f$-domain discrepancy ($f$-DD). By removing the absolute value function and incorporating a scaling parameter, $f$-DD obtains novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Using a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of $f$-DD-based learning algorithms over previous works in popular UDA benchmarks.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e6f6280a04e2892629381753602bc9e403e994ea.pdf'}, 'supplementary_material': {'value': '/attachment/1caa0ef8404f4dadd4c615f26786c25c8acb2597.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024on,\\ntitle={On \\\\$f\\\\$-Divergence Principled Domain Adaptation: An Improved Framework},\\nauthor={Ziqiao Wang and Yongyi Mao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xSU27DgWEr}\\n}'}, 'paperhash': {'value': 'wang|on_fdivergence_principled_domain_adaptation_an_improved_framework'}},forum = 'xSU27DgWEr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13840/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13840/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13840/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13840/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xRdpCOdghl',number = 1800,cdate = 1714813336598,pdate = 1727287672090,odate = 1730873852325,mdate = 1735023388965,tcdate = 1714813336598,tmdate = 1735023388965,ddate = None,content = {'title': {'value': 'Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection'}, 'authors': {'value': ['Qian Shao', 'Jiangrui Kang', 'Qiyuan Chen', 'Zepeng Li', 'Hongxia Xu', 'Yiwen Cao', 'JIAJUAN LIANG', 'Jian Wu']}, 'authorids': {'value': ['~Qian_Shao2', '~Jiangrui_Kang1', '~Qiyuan_Chen1', '~Zepeng_Li2', '~Hongxia_Xu1', '~Yiwen_Cao2', '~JIAJUAN_LIANG1', '~Jian_Wu6']}, 'keywords': {'value': ['Semi-supervised learning', 'Sample selection', 'Low-budget learning']}, 'TLDR': {'value': 'Proposed a representative and diversified sample selection method to select data for annotation from the unlabeled data to improve the performance of semi-supervised learning approaches.'}, 'abstract': {'value': 'Semi-Supervised Learning (SSL) has become a preferred paradigm in many deep learning tasks, which reduces the need for human labor. Previous studies primarily focus on effectively utilising the labelled and unlabeled data to improve performance. However, we observe that how to select samples for labelling also significantly impacts performance, particularly under extremely low-budget settings. The sample selection task in SSL has been under-explored for a long time. To fill in this gap, we propose a Representative and Diverse Sample Selection approach (RDSS). By adopting a modified Frank-Wolfe algorithm to minimise a novel criterion $\\\\alpha$-Maximum Mean Discrepancy ($\\\\alpha$-MMD), RDSS samples a representative and diverse subset for annotation from the unlabeled data. We demonstrate that minimizing $\\\\alpha$-MMD enhances the generalization ability of low-budget learning. Experimental results show that RDSS consistently improves the performance of several popular SSL frameworks and outperforms the state-of-the-art sample selection approaches used in Active Learning (AL) and Semi-Supervised Active Learning (SSAL), even with constrained annotation budgets. Our code is available at [RDSS](https://github.com/YanhuiAILab/RDSS).'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9fd99d5fa35620629a56be581ef28d009815b175.pdf'}, 'supplementary_material': {'value': '/attachment/eac037c8809c8c76dfb5748e4b2c511278073120.zip'}, '_bibtex': {'value': '@inproceedings{\\nshao2024enhancing,\\ntitle={Enhancing Semi-Supervised Learning via Representative and Diverse Sample Selection},\\nauthor={Qian Shao and Jiangrui Kang and Qiyuan Chen and Zepeng Li and Hongxia Xu and Yiwen Cao and JIAJUAN LIANG and Jian Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xRdpCOdghl}\\n}'}, 'paperhash': {'value': 'shao|enhancing_semisupervised_learning_via_representative_and_diverse_sample_selection'}},forum = 'xRdpCOdghl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1800/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1800/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1800/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1800/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xRQxan3WkM',number = 11552,cdate = 1715706477464,pdate = 1727287974844,odate = 1730873939692,mdate = 1730873939712,tcdate = 1715706477464,tmdate = 1730873939712,ddate = None,content = {'title': {'value': 'The Implicit Bias of Adam on Separable Data'}, 'authors': {'value': ['Chenyang Zhang', 'Difan Zou', 'Yuan Cao']}, 'authorids': {'value': ['~Chenyang_Zhang6', '~Difan_Zou1', '~Yuan_Cao1']}, 'keywords': {'value': ['Adam', 'implicit bias']}, 'abstract': {'value': 'Adam has become one of the most favored optimizers in deep learning problems. Despite its success in practice, numerous mysteries persist regarding its theoretical understanding. In this paper, we study the implicit bias of Adam in linear logistic regression. Specifically, we show that when the training data are linearly separable, the iterates of Adam converge towards a linear classifier that achieves the maximum $\\\\ell_\\\\infty$-margin in direction. Notably, for a general class of diminishing learning rates, this convergence occurs within polynomial time. Our result shed light on the difference between Adam and (stochastic) gradient descent from a theoretical perspective.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d4a15e237e7fcad39521c47bdcdaeb9981dca258.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024the,\\ntitle={The Implicit Bias of Adam on Separable Data},\\nauthor={Chenyang Zhang and Difan Zou and Yuan Cao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xRQxan3WkM}\\n}'}, 'paperhash': {'value': 'zhang|the_implicit_bias_of_adam_on_separable_data'}},forum = 'xRQxan3WkM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11552/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11552/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11552/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11552/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xQWJBeK5rh',number = 10999,cdate = 1715699964698,pdate = 1727287957597,odate = 1730873934104,mdate = 1730873934116,tcdate = 1715699964698,tmdate = 1730873934116,ddate = None,content = {'title': {'value': 'Structural Inference of Dynamical Systems with Conjoined State Space Models'}, 'authors': {'value': ['Aoran Wang', 'Jun Pang']}, 'authorids': {'value': ['~Aoran_Wang1', '~Jun_Pang1']}, 'keywords': {'value': ['Structural Inference', 'AI4Science']}, 'abstract': {'value': \"This paper introduces SICSM, a novel structural inference framework that integrates Selective State Space Models (selective SSMs) with Generative Flow Networks (GFNs) to handle the challenges posed by dynamical systems with irregularly sampled trajectories and partial observations. \\nBy utilizing the robust temporal modeling capabilities of selective SSMs, our approach learns input-dependent transition functions that adapt to non-uniform time intervals, thereby enhancing the accuracy of structural inference. \\nBy aggregating dynamics across diverse temporal dependencies and channeling them into the GFN, the SICSM adeptly approximates the posterior distribution of the system's structure. \\nThis process not only enables precise inference of complex interactions within partially observed systems but also ensures the seamless integration of prior knowledge, enhancing the model’s accuracy and robustness.\\nExtensive evaluations on sixteen diverse datasets demonstrate that SICSM outperforms existing methods, particularly in scenarios characterized by irregular sampling and incomplete observations, which highlight its potential as a reliable tool for scientific discovery and system diagnostics in disciplines that demand precise modeling of complex interactions.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1b2f2766bee3909361bdfd7042fefaf2beb4f061.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024structural,\\ntitle={Structural Inference of Dynamical Systems with Conjoined State Space Models},\\nauthor={Aoran Wang and Jun Pang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xQWJBeK5rh}\\n}'}, 'paperhash': {'value': 'wang|structural_inference_of_dynamical_systems_with_conjoined_state_space_models'}},forum = 'xQWJBeK5rh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10999/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10999/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10999/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10999/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xOCAURlVM9',number = 991,cdate = 1714293032626,pdate = 1727287649310,odate = 1730873844619,mdate = 1730873844636,tcdate = 1714293032626,tmdate = 1730873844636,ddate = None,content = {'title': {'value': 'Assembly Fuzzy Representation on Hypergraph for Open-Set 3D Object Retrieval'}, 'authors': {'value': ['Yang Xu', 'Yifan Feng', 'Jun Zhang', 'Jun-Hai Yong', 'Yue Gao']}, 'authorids': {'value': ['~Yang_Xu10', '~Yifan_Feng1', '~Jun_Zhang17', '~Jun-Hai_Yong3', '~Yue_Gao4']}, 'keywords': {'value': ['Hypergraph', '3D Object Retrieval', 'Open-Set Learning', '3D Part Assembly', 'Fuzzy Representation']}, 'abstract': {'value': 'The lack of object-level labels presents a significant challenge for 3D object retrieval in the open-set environment. However, part-level shapes of objects often share commonalities across categories but remain underexploited in existing retrieval methods. In this paper, we introduce the Hypergraph-Based Assembly Fuzzy Representation (HARF) framework, which navigates the intricacies of open-set 3D object retrieval through a bottom-up lens of Part Assembly. To tackle the challenge of assembly isomorphism and unification, we propose the Hypergraph Isomorphism Convolution (HIConv) for smoothing and adopt the Isomorphic Assembly Embedding (IAE) module to generate assembly embeddings with geometric-semantic consistency. To address the challenge of open-set category generalization, our method employs high-order correlations and fuzzy representation to mitigate distribution skew through the Structure Fuzzy Reconstruction (SFR) module, by constructing a leveraged hypergraph based on local certainty and global uncertainty correlations. We construct three open-set retrieval datasets for 3D objects with part-level annotations: OP-SHNP, OP-INTRA, and OP-COSEG. Extensive experiments and ablation studies on these three benchmarks show our method outperforms current state-of-the-art methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8985b94f6c2cb16afe1fb713d3e65acc97d532b2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxu2024assembly,\\ntitle={Assembly Fuzzy Representation on Hypergraph for Open-Set 3D Object Retrieval},\\nauthor={Yang Xu and Yifan Feng and Jun Zhang and Jun-Hai Yong and Yue Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xOCAURlVM9}\\n}'}, 'paperhash': {'value': 'xu|assembly_fuzzy_representation_on_hypergraph_for_openset_3d_object_retrieval'}},forum = 'xOCAURlVM9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission991/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission991/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission991/-/Revision', 'NeurIPS.cc/2024/Conference/Submission991/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'xO9GHdmK76',number = 5718,cdate = 1715566800644,pdate = 1727287792639,odate = 1730873887506,mdate = 1730873887527,tcdate = 1715566800644,tmdate = 1730873887527,ddate = None,content = {'title': {'value': 'Infinite-Dimensional Feature Interaction'}, 'authors': {'value': ['Chenhui Xu', 'Fuxun Yu', 'Maoliang Li', 'Zihao Zheng', 'Zirui Xu', 'Jinjun Xiong', 'Xiang Chen']}, 'authorids': {'value': ['~Chenhui_Xu1', '~Fuxun_Yu1', '~Maoliang_Li1', '~Zihao_Zheng5', '~Zirui_Xu1', '~Jinjun_Xiong1', '~Xiang_Chen1']}, 'keywords': {'value': ['deep learning', 'kernel method', 'feature interaction']}, 'abstract': {'value': 'The past neural network design has largely focused on feature \\\\textit{representation space} dimension and its capacity scaling (e.g., width, depth), but overlooked the feature \\\\textit{interaction space} scaling. \\n Recent advancements have shown shifted focus towards element-wise multiplication to facilitate higher-dimensional feature interaction space for better information transformation. Despite this progress, multiplications predominantly capture low-order interactions, thus remaining confined to a finite-dimensional interaction space. To transcend this limitation, classic kernel methods emerge as a promising solution to engage features in an infinite-dimensional space. We introduce InfiNet, a model architecture that enables feature interaction within an infinite-dimensional space created by RBF kernel. Our experiments reveal that InfiNet achieves new state-of-the-art, owing to its capability to leverage infinite-dimensional interactions, significantly enhancing model performance.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b14995433876b9e28417e0ab94774923baeecd15.pdf'}, 'TLDR': {'value': 'We introduce InfiNet, a modern architecture using kernel to explore the infinite-dimensional feature interaction.'}, '_bibtex': {'value': '@inproceedings{\\nxu2024infinitedimensional,\\ntitle={Infinite-Dimensional Feature Interaction},\\nauthor={Chenhui Xu and Fuxun Yu and Maoliang Li and Zihao Zheng and Zirui Xu and Jinjun Xiong and Xiang Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xO9GHdmK76}\\n}'}, 'paperhash': {'value': 'xu|infinitedimensional_feature_interaction'}},forum = 'xO9GHdmK76',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5718/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5718/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5718/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5718/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xNncVKbwwS',number = 4050,cdate = 1715352683777,pdate = 1727287738528,odate = 1730873871713,mdate = 1734588075740,tcdate = 1715352683777,tmdate = 1734588075740,ddate = None,content = {'title': {'value': 'Universal Online Convex Optimization with $1$ Projection per Round'}, 'authors': {'value': ['Wenhao Yang', 'Yibo Wang', 'Peng Zhao', 'Lijun Zhang']}, 'authorids': {'value': ['~Wenhao_Yang3', '~Yibo_Wang2', '~Peng_Zhao1', '~Lijun_Zhang1']}, 'keywords': {'value': ['Online Convex Optimization', 'Universal Online Learning', 'Projection']}, 'abstract': {'value': 'To address the uncertainty in function types, recent progress in online convex optimization (OCO) has spurred the development of universal algorithms that simultaneously attain minimax rates for multiple types of convex functions. However, for a $T$-round online problem, state-of-the-art methods typically conduct $O(\\\\log T)$ projections onto the domain in each round, a process potentially time-consuming with complicated feasible sets. In this paper, inspired by the black-box reduction of Cutkosky and Orabona [2018], we employ a surrogate loss defined over simpler domains to develop universal OCO algorithms that only require $1$ projection. Embracing the framework of prediction with expert advice, we maintain a set of experts for each type of functions and aggregate their predictions via a meta-algorithm. The crux of our approach lies in a uniquely designed expert-loss for strongly convex functions, stemming from an innovative decomposition of the regret into the meta-regret and the expert-regret. Our analysis sheds new light on the surrogate loss, facilitating a rigorous examination of the discrepancy between the regret of the original loss and that of the surrogate loss, and carefully controlling meta-regret under the strong convexity condition. With only $1$ projection per round, we establish optimal regret bounds for general convex, exponentially concave, and strongly convex functions simultaneously. Furthermore, we enhance the expert-loss to exploit the smoothness property, and demonstrate that our algorithm can attain small-loss regret for multiple types of convex and smooth functions.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7dc76a57e2e7e68167e764bbd0b24f559f8773a9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024universal,\\ntitle={Universal Online Convex Optimization with \\\\$1\\\\$ Projection per Round},\\nauthor={Wenhao Yang and Yibo Wang and Peng Zhao and Lijun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xNncVKbwwS}\\n}'}, 'paperhash': {'value': 'yang|universal_online_convex_optimization_with_1_projection_per_round'}},forum = 'xNncVKbwwS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4050/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4050/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4050/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4050/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xNlQjS0dtO',number = 20202,cdate = 1715795063444,pdate = 1727288227689,odate = 1730873999810,mdate = 1737026970475,tcdate = 1715795063444,tmdate = 1737026970475,ddate = None,content = {'title': {'value': 'Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates'}, 'authors': {'value': ['Kaifeng Lyu', 'Haoyu Zhao', 'Xinran Gu', 'Dingli Yu', 'Anirudh Goyal', 'Sanjeev Arora']}, 'authorids': {'value': ['~Kaifeng_Lyu2', '~Haoyu_Zhao1', '~Xinran_Gu2', '~Dingli_Yu1', '~Anirudh_Goyal1', '~Sanjeev_Arora1']}, 'keywords': {'value': ['safety prompt', 'AI alignment']}, 'TLDR': {'value': 'To address the issue that fine-tuning an aligned model degrades safety, we introduce the “Pure Tune, Safe Test” (PTST) Principle: fine-tune models without a safety prompt, but include it at test time.'}, 'abstract': {'value': \"Public LLMs such as the Llama 2-Chat underwent alignment training and were considered safe. Recently Qi et al. (2024) reported that even benign fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the models. The current paper is about methods and best practices to mitigate such loss of alignment. We focus on the setting where a public model is fine-tuned before serving users for specific usage, where the model should improve on the downstream task while maintaining alignment. Through extensive experiments on several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt templates used during fine-tuning and inference play a crucial role in preserving safety alignment, and proposes the “Pure Tuning, Safe Testing” (PTST) strategy --- fine-tune models without a safety prompt, but include it at test time. This seemingly counterintuitive strategy incorporates an intended distribution shift to encourage alignment preservation. Fine-tuning experiments on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of unsafe behaviors.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/beb490ad102910ffee423d69bbff98080fde654b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlyu2024keeping,\\ntitle={Keeping {LLM}s Aligned After Fine-tuning: The Crucial Role of Prompt Templates},\\nauthor={Kaifeng Lyu and Haoyu Zhao and Xinran Gu and Dingli Yu and Anirudh Goyal and Sanjeev Arora},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xNlQjS0dtO}\\n}'}, 'paperhash': {'value': 'lyu|keeping_llms_aligned_after_finetuning_the_crucial_role_of_prompt_templates'}},forum = 'xNlQjS0dtO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20202/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20202/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20202/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20202/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xNZEjFe0mh',number = 11919,cdate = 1715713051347,pdate = 1727287988014,odate = 1730873943656,mdate = 1730873943675,tcdate = 1715713051347,tmdate = 1730873943675,ddate = None,content = {'title': {'value': 'Communication-Efficient Federated Group Distributionally Robust Optimization'}, 'authors': {'value': ['Zhishuai Guo', 'Tianbao Yang']}, 'authorids': {'value': ['~Zhishuai_Guo1', '~Tianbao_Yang1']}, 'keywords': {'value': ['Federated Learning', 'Group Distributional Robust Optimization', 'Communication Efficiency']}, 'abstract': {'value': 'Federated learning faces challenges due to the heterogeneity in data volumes and distributions at different clients, which can compromise model generalization ability to various distributions. \\nExisting approaches to address this issue based on group distributionally robust optimization (GDRO) often lead to high communication and sample complexity.\\nTo this end, this work introduces algorithms tailored for communication-efficient Federated Group Distributionally Robust Optimization (FGDRO). Our contributions are threefold: Firstly, we introduce the FGDRO-CVaR algorithm, which optimizes the average top-K losses while reducing communication complexity to $O(1/\\\\epsilon^4)$, where $\\\\epsilon$ denotes the desired precision level. Secondly, our FGDRO-KL algorithm is crafted to optimize KL regularized FGDRO, cutting communication complexity to $O(1/\\\\epsilon^3)$. Lastly, we propose FGDRO-KL-Adam to utilize Adam-type local updates in FGDRO-KL, which not only maintains a communication cost of $O(1/\\\\epsilon^3)$ but also shows potential to surpass SGD-type local steps in practical applications.\\nThe effectiveness of our algorithms has been demonstrated on a variety of real-world tasks, including natural language processing and computer vision.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a65e23614800d52b04091555fc2509133c2dc354.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguo2024communicationefficient,\\ntitle={Communication-Efficient Federated Group Distributionally Robust Optimization},\\nauthor={Zhishuai Guo and Tianbao Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xNZEjFe0mh}\\n}'}, 'paperhash': {'value': 'guo|communicationefficient_federated_group_distributionally_robust_optimization'}},forum = 'xNZEjFe0mh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11919/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11919/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11919/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11919/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xM5m7J6Lbl',number = 9175,cdate = 1715675860014,pdate = 1727287903036,odate = 1730873918404,mdate = 1730873918423,tcdate = 1715675860014,tmdate = 1730873918423,ddate = None,content = {'title': {'value': 'Can an AI Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies'}, 'authors': {'value': ['Frédéric Berdoz', 'Roger Wattenhofer']}, 'authorids': {'value': ['~Frédéric_Berdoz1', '~Roger_Wattenhofer1']}, 'keywords': {'value': ['Alignment', 'Planning', 'Social Choice', 'AI Safety']}, 'TLDR': {'value': 'This paper introduces probably approximately aligned (PAA) and safe policies in the context of social decision processes.'}, 'abstract': {'value': 'While autonomous agents often surpass humans in their ability to handle vast and complex data, their potential misalignment (i.e., lack of transparency regarding their true objective) has thus far hindered their use in critical applications such as social decision processes. More importantly, existing alignment methods provide no formal guarantees on the safety of such models. Drawing from utility and social choice theory, we provide a novel quantitative definition of alignment in the context of social decision-making. Building on this definition, we introduce probably approximately aligned (i.e., near-optimal) policies, and we derive a sufficient condition for their existence. Lastly, recognizing the practical difficulty of satisfying this condition, we introduce the relaxed concept of safe (i.e., nondestructive) policies, and we propose a simple yet robust method to safeguard the black-box policy of any autonomous agent, ensuring all its actions are verifiably safe for the society.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bf68ea6396e873fe823317adb57a897d04b26805.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nberdoz2024can,\\ntitle={Can an {AI} Agent Safely Run a Government? Existence of Probably Approximately Aligned Policies},\\nauthor={Fr{\\\\'e}d{\\\\'e}ric Berdoz and Roger Wattenhofer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xM5m7J6Lbl}\\n}\"}, 'paperhash': {'value': 'berdoz|can_an_ai_agent_safely_run_a_government_existence_of_probably_approximately_aligned_policies'}},forum = 'xM5m7J6Lbl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9175/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9175/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9175/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9175/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xL7Ve14AHA',number = 1619,cdate = 1714721106786,pdate = 1727287666454,odate = 1730873850470,mdate = 1735223915329,tcdate = 1714721106786,tmdate = 1735223915329,ddate = None,content = {'title': {'value': 'Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network'}, 'authors': {'value': ['Zih-Syuan Huang', 'Ching-pei Lee']}, 'authorids': {'value': ['~Zih-Syuan_Huang1', '~Ching-pei_Lee2']}, 'keywords': {'value': ['structured neural networks', 'variance reduction', 'manifold identification', 'proximal methods', 'adaptive methods', 'inexact subproblem solution']}, 'TLDR': {'value': 'A regularized adaptive momentum dual averaging algorithm for training structured neural networks with guarantees for finding the locally optimal structure, and exhibits outstanding performance in language, speech and image classification tasks.'}, 'abstract': {'value': 'We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAMDA is guaranteed to obtain the best structure possible among all methods converging to the same point, making it the first regularized adaptive method outputting models that possess outstanding predictive performance while being (locally) optimally structured. Extensive numerical experiments in large-scale modern computer vision, language modeling, and speech tasks show that the proposed RAMDA is efficient and consistently outperforms state of the art for training structured neural network. Implementation of our algorithm is available at https://www.github.com/ismoptgroup/RAMDA.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f308a40b9bbe024a4cee23bc7fdaab4fdbd357c3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024regularized,\\ntitle={Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network},\\nauthor={Zih-Syuan Huang and Ching-pei Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xL7Ve14AHA}\\n}'}, 'paperhash': {'value': 'huang|regularized_adaptive_momentum_dual_averaging_with_an_efficient_inexact_subproblem_solver_for_training_structured_neural_network'}},forum = 'xL7Ve14AHA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1619/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1619/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1619/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1619/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xImeJtdUiw',number = 10148,cdate = 1715690962635,pdate = 1727287931002,odate = 1730873926322,mdate = 1730873926334,tcdate = 1715690962635,tmdate = 1730873926334,ddate = None,content = {'title': {'value': 'Multi-modal Transfer Learning between Biological Foundation Models'}, 'authors': {'value': ['Juan Jose Garau-Luis', 'Patrick Philippe Bordes', 'Liam Gonzalez', 'Maša Roller', 'Bernardo P de Almeida', 'Christopher F. Blum', 'Lorenz Hexemer', 'Stefan Laurent', 'Maren Lang', 'Thomas PIERROT', 'Guillaume Richard']}, 'authorids': {'value': ['~Juan_Jose_Garau-Luis1', '~Patrick_Philippe_Bordes1', '~Liam_Gonzalez1', '~Maša_Roller1', '~Bernardo_P_de_Almeida1', '~Christopher_F._Blum1', '~Lorenz_Hexemer1', '~Stefan_Laurent1', '~Maren_Lang1', '~Thomas_PIERROT1', '~Guillaume_Richard1']}, 'keywords': {'value': ['Multi-Modal', 'Transformers', 'BioAI']}, 'abstract': {'value': 'Biological sequences encode fundamental instructions for the building blocks of life, in the form of DNA, RNA, and proteins. Modeling these sequences is key to understand disease mechanisms and is an active research area in computational biology. Recently, Large Language Models have shown great promise in solving certain biological tasks but current approaches are limited to a single sequence modality (DNA, RNA, or protein). Key problems in genomics intrinsically involve multiple modalities, but it remains unclear how to adapt general-purpose sequence models to those cases. In this work we propose a multi-modal model that connects DNA, RNA, and proteins by leveraging information from different pre-trained modality-specific encoders. We demonstrate its capabilities by applying it to the largely unsolved problem of predicting how multiple \\\\rna transcript isoforms originate from the same gene (i.e. same DNA sequence) and map to different transcription expression levels across various human tissues. We show that our model, dubbed IsoFormer, is able to accurately predict differential transcript expression, outperforming existing methods and leveraging the use of multiple modalities. Our framework also achieves efficient transfer knowledge from the encoders pre-training as well as in between modalities. We open-source our model, paving the way for new multi-modal gene expression approaches.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ef1635b8ab00b68d5872359001ea59e93a3a9846.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngarau-luis2024multimodal,\\ntitle={Multi-modal Transfer Learning between Biological Foundation Models},\\nauthor={Juan Jose Garau-Luis and Patrick Philippe Bordes and Liam Gonzalez and Ma{\\\\v{s}}a Roller and Bernardo P de Almeida and Christopher F. Blum and Lorenz Hexemer and Stefan Laurent and Maren Lang and Thomas PIERROT and Guillaume Richard},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xImeJtdUiw}\\n}'}, 'paperhash': {'value': 'garauluis|multimodal_transfer_learning_between_biological_foundation_models'}},forum = 'xImeJtdUiw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10148/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10148/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10148/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10148/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xDrKZOZEOc',number = 10512,cdate = 1715694883312,pdate = 1727287943174,odate = 1730873929860,mdate = 1735830705472,tcdate = 1715694883312,tmdate = 1735830705472,ddate = None,content = {'title': {'value': 'Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization'}, 'authors': {'value': ['Yang Li', 'Jinpei Guo', 'Runzhong Wang', 'Hongyuan Zha', 'Junchi Yan']}, 'authorids': {'value': ['~Yang_Li32', '~Jinpei_Guo1', '~Runzhong_Wang1', '~Hongyuan_Zha1', '~Junchi_Yan2']}, 'keywords': {'value': ['Neural Combinatorial Optimization', 'Generative Modeling']}, 'abstract': {'value': 'Diffusion models have recently advanced Combinatorial Optimization (CO) as a powerful backbone for neural solvers. However, their iterative sampling process requiring denoising across multiple noise levels incurs substantial overhead. We propose to learn direct mappings from different noise levels to the optimal solution for a given instance, facilitating high-quality generation with minimal shots. This is achieved through an optimization consistency training protocol, which, for a given instance, minimizes the difference among samples originating from varying generative trajectories and time steps relative to the optimal solution. The proposed model enables fast single-step solution generation while retaining the option of multi-step sampling to trade for sampling quality, which offers a more effective and efficient alternative backbone for neural solvers. In addition, within the training-to-testing (T2T) framework, to bridge the gap between training on historical instances and solving new instances, we introduce a novel consistency-based gradient search scheme during the test stage, enabling more effective exploration of the solution space learned during training. It is achieved by updating the latent solution probabilities under objective gradient guidance during the alternation of noise injection and denoising steps. We refer to this model as Fast T2T. Extensive experiments on two popular tasks, the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast T2T regarding both solution quality and efficiency, even outperforming LKH given limited time budgets. Notably, Fast T2T with merely one-step generation and one-step gradient search can mostly outperform the SOTA diffusion-based counterparts that require hundreds of steps, while achieving tens of times speedup.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fe5e7fe1fabed428781b4a7575b830ae83a8c609.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024fast,\\ntitle={Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization},\\nauthor={Yang Li and Jinpei Guo and Runzhong Wang and Hongyuan Zha and Junchi Yan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xDrKZOZEOc}\\n}'}, 'paperhash': {'value': 'li|fast_t2t_optimization_consistency_speeds_up_diffusionbased_trainingtotesting_solving_for_combinatorial_optimization'}},forum = 'xDrKZOZEOc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10512/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10512/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10512/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10512/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xCUXJqQySD',number = 15133,cdate = 1715757388928,pdate = 1727288088973,odate = 1730873970822,mdate = 1730873970834,tcdate = 1715757388928,tmdate = 1730873970834,ddate = None,content = {'title': {'value': 'Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning'}, 'authors': {'value': ['Keying Kuang', 'Frances Dean', 'Jack B. Jedlicki', 'David Ouyang', 'Anthony Philippakis', 'David Sontag', 'Ahmed Alaa']}, 'authorids': {'value': ['~Keying_Kuang1', '~Frances_Dean1', '~Jack_B._Jedlicki1', '~David_Ouyang1', '~Anthony_Philippakis1', '~David_Sontag1', '~Ahmed_Alaa1']}, 'keywords': {'value': ['Inverse Problems', 'Digital Twins', 'Personalized Medicine', 'Non-Invasive Imaging', 'Physics-Informed']}, 'abstract': {'value': \"A digital twin is a virtual replica of a real-world physical phenomena that uses mathematical modeling to characterize and simulate its defining features. By constructing digital twins for disease processes, we can perform in-silico simulations that mimic patients' health conditions and counterfactual outcomes under hypothetical interventions in a virtual setting. This eliminates the need for invasive procedures or uncertain treatment decisions. In this paper, we propose a method to identify digital twin model parameters using only noninvasive patient health data. We approach the digital twin modeling as a composite inverse problem, and observe that its structure resembles pretraining and finetuning in self-supervised learning (SSL). Leveraging this, we introduce a physics-informed SSL algorithm that initially pretrains a neural network on the pretext task of learning a differentiable simulator of a physiological process. Subsequently, the model is trained to reconstruct physiological measurements from noninvasive modalities while being constrained by the physical equations learned in pretraining. We apply our method to identify digital twins of cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate its utility in unsupervised disease detection and in-silico clinical trials.\"}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b6609bf4e5f0ce3248a6b5f958f0f2f6b75a2c04.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkuang2024medrealsim,\\ntitle={Med-Real2Sim: Non-Invasive Medical Digital Twins using Physics-Informed Self-Supervised Learning},\\nauthor={Keying Kuang and Frances Dean and Jack B. Jedlicki and David Ouyang and Anthony Philippakis and David Sontag and Ahmed Alaa},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xCUXJqQySD}\\n}'}, 'paperhash': {'value': 'kuang|medreal2sim_noninvasive_medical_digital_twins_using_physicsinformed_selfsupervised_learning'}},forum = 'xCUXJqQySD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15133/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15133/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15133/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15133/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'xCIbVuXwPM',number = 5762,cdate = 1715568557643,pdate = 1727287794308,odate = 1730873887944,mdate = 1736185010850,tcdate = 1715568557643,tmdate = 1736185010850,ddate = None,content = {'title': {'value': 'Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification'}, 'authors': {'value': ['Enrique Nueve', 'Dhamma Kimpara', 'Bo Waggoner', 'Jessica Finocchiaro']}, 'authorids': {'value': ['~Enrique_Nueve1', '~Dhamma_Kimpara2', '~Bo_Waggoner1', '~Jessica_Finocchiaro1']}, 'keywords': {'value': ['Loss Functions', 'Consistency', 'Property Elicitation']}, 'TLDR': {'value': 'This work explores theoretical guarantees for partial consistency when violating known prediction dimension bounds.'}, 'abstract': {'value': 'In multiclass classification over $n$ outcomes, we typically optimize some surrogate loss $L: \\\\mathbb{R}^d \\\\times\\\\mathcal{Y} \\\\to \\\\mathbb{R}$ assigning real-valued error to predictions in $\\\\mathbb{R}^d$. In this paradigm, outcomes must be embedded into the reals with dimension $d \\\\approx n$ in order to design a consistent surrogate loss. Consistent losses are well-motivated theoretically, yet for large $n$, such as in information retrieval and structured prediction tasks, their optimization may be computationally infeasible. In practice, outcomes are typically embedded into some $\\\\mathbb{R}^d$ for $d \\\\ll n$, with little known about their suitability for multiclass classification. We investigate two approaches for trading off consistency and dimensionality in multiclass classification while using a convex surrogate loss. We first formalize partial consistency when the optimized surrogate has dimension $d \\\\ll n$. \\nWe then check if partial consistency holds under a given embedding and low-noise assumption, providing insight into when to use a particular embedding into $\\\\mathbb{R}^d$. Finally, we present a new method to construct (fully) consistent losses with $d \\\\ll n$ out of multiple problem instances. Our practical approach leverages parallelism to sidestep lower bounds on $d$.'}, 'pdf': {'value': '/pdf/8719768b890f591c4ee35ef267ec626035024441.pdf'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/7b17f0f4a25b7d636031a7a95219ad031738cd25.zip'}, '_bibtex': {'value': '@inproceedings{\\nnueve2024trading,\\ntitle={Trading off Consistency and Dimensionality of Convex Surrogates for Multiclass Classification},\\nauthor={Enrique Nueve and Dhamma Kimpara and Bo Waggoner and Jessica Finocchiaro},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=xCIbVuXwPM}\\n}'}, 'paperhash': {'value': 'nueve|trading_off_consistency_and_dimensionality_of_convex_surrogates_for_multiclass_classification'}},forum = 'xCIbVuXwPM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5762/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5762/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5762/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'x9eFgahVBI',number = 20403,cdate = 1715796191637,pdate = 1727288233008,odate = 1730874001098,mdate = 1730874001117,tcdate = 1715796191637,tmdate = 1730874001117,ddate = None,content = {'title': {'value': 'From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When'}, 'authors': {'value': ['Kevin Christian Wibisono', 'Yixin Wang']}, 'authorids': {'value': ['~Kevin_Christian_Wibisono1', '~Yixin_Wang1']}, 'keywords': {'value': ['in-context learning', 'large language models', 'unstructured data', 'continuous bag of words', 'co-occurrence']}, 'TLDR': {'value': 'We demonstrate the significance of co-occurrence, positional information, noise, and data structures for in-context learning from training on unstructured data.'}, 'abstract': {'value': \"Large language models (LLMs) like transformers demonstrate impressive in-context learning (ICL) capabilities, allowing them to make\\npredictions for new tasks based on prompt exemplars without parameter updates. While existing ICL theories often assume structured training data resembling ICL tasks (e.g., x-y pairs for linear regression), LLMs are typically trained unsupervised on unstructured text, such as web content, which lacks clear parallels to tasks like word analogy. To address this gap, we examine what enables ICL in models trained on unstructured data, focusing on critical sequence model requirements and training data structure. We find that many ICL capabilities can\\nemerge simply from co-occurrence of semantically related word pairs in unstructured data; word analogy completion, for example, can provably arise purely through co-occurrence modeling, using classical language models like continuous bag of words (CBOW), without needing positional information or attention mechanisms. However, positional information becomes crucial for logic reasoning tasks requiring generalization to unseen tokens. Finally, we identify two cases where ICL fails: one in logic reasoning tasks that require generalizing to new, unseen patterns, and another in analogy completion where relevant word pairs appear only in fixed training positions. These findings suggest that LLMs' ICL abilities depend heavily on the structural elements within their training data.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fa1542b6c9fd42a5d63296cab95e6690e9d75499.pdf'}, 'supplementary_material': {'value': '/attachment/0faf9530828b94ca355d666ecc5e8ffe651a967f.zip'}, '_bibtex': {'value': '@inproceedings{\\nwibisono2024from,\\ntitle={From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When},\\nauthor={Kevin Christian Wibisono and Yixin Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x9eFgahVBI}\\n}'}, 'paperhash': {'value': 'wibisono|from_unstructured_data_to_incontext_learning_exploring_what_tasks_can_be_learned_and_when'}},forum = 'x9eFgahVBI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20403/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20403/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20403/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20403/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x7usmidzxj',number = 13580,cdate = 1715741004188,pdate = 1727288043770,odate = 1730873958951,mdate = 1737021907091,tcdate = 1715741004188,tmdate = 1737021907091,ddate = None,content = {'title': {'value': 'On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions'}, 'authors': {'value': ['Yusu Hong', 'Junhong Lin']}, 'authorids': {'value': ['~Yusu_Hong1', '~Junhong_Lin1']}, 'keywords': {'value': ['Adam', 'optimal rate', 'high probability convergence', 'relaxed assumptions']}, 'abstract': {'value': 'In this paper, we study Adam in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. We consider a general noise model which governs affine variance noise, bounded noise, and sub-Gaussian noise. We show that Adam with a specific hyper-parameter setup can find a stationary point with a $\\\\mathcal{O}(\\\\text{poly}(\\\\log T)/\\\\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to capture the smooth property of many practical objective functions more accurately.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9eb7ec4036b150c6641f5c68c16748db9695dbd6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhong2024on,\\ntitle={On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions},\\nauthor={Yusu Hong and Junhong Lin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x7usmidzxj}\\n}'}, 'paperhash': {'value': 'hong|on_convergence_of_adam_for_stochastic_optimization_under_relaxed_assumptions'}},forum = 'x7usmidzxj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13580/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13580/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13580/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13580/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x7pjdDod6Z',number = 2381,cdate = 1715060312325,pdate = 1727287688948,odate = 1730873857607,mdate = 1730873857622,tcdate = 1715060312325,tmdate = 1730873857622,ddate = None,content = {'title': {'value': 'MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model'}, 'authors': {'value': ['Minghua Liu', 'Chong Zeng', 'Xinyue Wei', 'Ruoxi Shi', 'Linghao Chen', 'Chao Xu', 'Mengqi Zhang', 'Zhaoning Wang', 'Xiaoshuai Zhang', 'Isabella Liu', 'Hongzhi Wu', 'Hao Su']}, 'authorids': {'value': ['~Minghua_Liu1', '~Chong_Zeng1', '~Xinyue_Wei1', '~Ruoxi_Shi1', '~Linghao_Chen2', '~Chao_Xu6', '~Mengqi_Zhang2', '~Zhaoning_Wang2', '~Xiaoshuai_Zhang1', '~Isabella_Liu1', '~Hongzhi_Wu1', '~Hao_Su1']}, 'keywords': {'value': ['sparse view 3D reconstruction', '3D generation', '3D AIGC', 'reconstruction model']}, 'TLDR': {'value': 'We introduce MeshFormer, a sparse-view reconstruction model that can deliver high-quality meshes and be trained efficiently.'}, 'abstract': {'value': \"Open-world 3D reconstruction models have recently garnered significant attention. However, without sufficient 3D inductive bias, existing methods typically entail expensive training costs and struggle to extract high-quality 3D meshes. In this work, we introduce MeshFormer, a sparse-view reconstruction model that explicitly leverages 3D native structure, input guidance, and training supervision. Specifically, instead of using a triplane representation, we store features in 3D sparse voxels and combine transformers with 3D convolutions to leverage an explicit 3D structure and projective bias. In addition to sparse-view RGB input, we require the network to take input and generate corresponding normal maps. The input normal maps can be predicted by 2D diffusion models, significantly aiding in the guidance and refinement of the geometry's learning. Moreover, by combining Signed Distance Function (SDF) supervision with surface rendering, we directly learn to generate high-quality meshes without the need for complex multi-stage training processes. By incorporating these explicit 3D biases, MeshFormer can be trained efficiently and deliver high-quality textured meshes with fine-grained geometric details. It can also be integrated with 2D diffusion models to enable fast single-image-to-3D and text-to-3D tasks. **Videos are available at https://meshformer3d.github.io/**\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0137993914b1c34b105ba8ce5545d99389e3b12a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024meshformer,\\ntitle={MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model},\\nauthor={Minghua Liu and Chong Zeng and Xinyue Wei and Ruoxi Shi and Linghao Chen and Chao Xu and Mengqi Zhang and Zhaoning Wang and Xiaoshuai Zhang and Isabella Liu and Hongzhi Wu and Hao Su},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x7pjdDod6Z}\\n}'}, 'paperhash': {'value': 'liu|meshformer_highquality_mesh_generation_with_3dguided_reconstruction_model'}},forum = 'x7pjdDod6Z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2381/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2381/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2381/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2381/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x7AD0343Jz',number = 21583,cdate = 1715802619071,pdate = 1727288258340,odate = 1730874006283,mdate = 1730874006296,tcdate = 1715802619071,tmdate = 1730874006296,ddate = None,content = {'title': {'value': 'Limits of Transformer Language Models on Learning to Compose Algorithms'}, 'authors': {'value': ['Jonathan Thomm', 'Giacomo Camposampiero', 'Aleksandar Terzic', 'Michael Hersche', 'Bernhard Schölkopf', 'Abbas Rahimi']}, 'authorids': {'value': ['~Jonathan_Thomm1', '~Giacomo_Camposampiero1', '~Aleksandar_Terzic1', '~Michael_Hersche1', '~Bernhard_Schölkopf1', '~Abbas_Rahimi1']}, 'keywords': {'value': ['Few-shot Compositional Learning', 'Compositionality', 'Sample Efficiency', 'Algorithmic Learning', 'Large Language Models', 'Transformers']}, 'TLDR': {'value': 'We analyze the capabilities of Transformer language models in learning compositional discrete tasks and observe that compositional learning is very sample inefficient.'}, 'abstract': {'value': 'We analyze the capabilities of Transformer language models in learning compositional discrete tasks. To this end, we evaluate training LLaMA models and prompting GPT-4 and Gemini on four tasks demanding to learn a composition of several discrete sub-tasks. In particular, we measure how well these models can reuse primitives observable in the sub-tasks to learn the composition task. Our results indicate that compositional learning in state-of-the-art Transformer language models is highly sample inefficient: LLaMA requires more data samples than relearning all sub-tasks from scratch to learn the compositional task; in-context prompting with few samples is unreliable and fails at executing the sub-tasks or correcting the errors in multi-round code generation. Further, by leveraging complexity theory, we support these findings with a theoretical analysis focused on the sample inefficiency of gradient descent in memorizing feedforward models. We open source our code at https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b5c608c4483d2ed10fc624c08dd00561cddf4f4c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nthomm2024limits,\\ntitle={Limits of Transformer Language Models on Learning to Compose Algorithms},\\nauthor={Jonathan Thomm and Giacomo Camposampiero and Aleksandar Terzic and Michael Hersche and Bernhard Sch{\\\\\"o}lkopf and Abbas Rahimi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x7AD0343Jz}\\n}'}, 'paperhash': {'value': 'thomm|limits_of_transformer_language_models_on_learning_to_compose_algorithms'}},forum = 'x7AD0343Jz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21583/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21583/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21583/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21583/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x69O84Df2G',number = 10341,cdate = 1715693078068,pdate = 1727287936658,odate = 1730873928220,mdate = 1736958765364,tcdate = 1715693078068,tmdate = 1736958765364,ddate = None,content = {'title': {'value': 'Multi-Reward Best Policy Identification'}, 'authors': {'value': ['Alessio Russo', 'Filippo Vannella']}, 'authorids': {'value': ['~Alessio_Russo1', '~Filippo_Vannella1']}, 'keywords': {'value': ['multiple rewards', 'best policy identification', 'pure exploration', 'active exploration', 'reinforcement learning', 'sequential decision making']}, 'abstract': {'value': 'Rewards are a critical aspect of formulating  Reinforcement Learning (RL) problems; often, one may be interested in testing multiple reward functions, or the problem may naturally involve multiple rewards. \\nIn this study, we investigate the _Multi-Reward Best Policy Identification_ (MR-BPI) problem, where the goal is to determine the best policy for all rewards in a given set $\\\\mathcal{R}$ with minimal sample complexity and a prescribed confidence level. We derive a fundamental instance-specific lower bound on the sample complexity required by any Probably Correct (PC) algorithm in this setting. This bound guides the design of an optimal exploration policy attaining minimal sample complexity. However, this lower bound involves solving a hard non-convex optimization problem. We address this challenge by devising a convex approximation, enabling the design of sample-efficient algorithms.  We propose MR-NaS, a PC algorithm with competitive performance on hard-exploration tabular environments. Extending this approach to Deep RL (DRL), we also introduce DBMR-BPI, an efficient algorithm for model-free exploration in multi-reward settings.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5486797112e04e32f08ece8809e3e0a9d0845d17.pdf'}, 'supplementary_material': {'value': '/attachment/ca99df705ace592e9b3439b6ae89fd84c2742d0d.zip'}, '_bibtex': {'value': '@inproceedings{\\nrusso2024multireward,\\ntitle={Multi-Reward Best Policy Identification},\\nauthor={Alessio Russo and Filippo Vannella},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x69O84Df2G}\\n}'}, 'paperhash': {'value': 'russo|multireward_best_policy_identification'}},forum = 'x69O84Df2G',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10341/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10341/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10341/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10341/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x4Kk4FxLs3',number = 13383,cdate = 1715739027616,pdate = 1727288037602,odate = 1730873957569,mdate = 1730873957589,tcdate = 1715739027616,tmdate = 1730873957589,ddate = None,content = {'title': {'value': 'Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation'}, 'authors': {'value': ['Lingxiao Zhao', 'Xueying Ding', 'Leman Akoglu']}, 'authorids': {'value': ['~Lingxiao_Zhao1', '~Xueying_Ding1', '~Leman_Akoglu3']}, 'keywords': {'value': ['diffusion', 'generative model', 'graph generation', 'graph generative model']}, 'TLDR': {'value': 'Combining autoregressive method with diffusion for modeling graph distribution with SOTA graph generation performance.'}, 'abstract': {'value': 'Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely un-ordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block’s probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN (Maronet al., 2019). Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c1a62f1c53db519f90d14aed3ca68d4ed80a9146.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024pard,\\ntitle={Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation},\\nauthor={Lingxiao Zhao and Xueying Ding and Leman Akoglu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x4Kk4FxLs3}\\n}'}, 'paperhash': {'value': 'zhao|pard_permutationinvariant_autoregressive_diffusion_for_graph_generation'}},forum = 'x4Kk4FxLs3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13383/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13383/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13383/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13383/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'x4HMnqs6IE',number = 5829,cdate = 1715571405503,pdate = 1727287796279,odate = 1730873888619,mdate = 1734578819469,tcdate = 1715571405503,tmdate = 1734578819469,ddate = None,content = {'title': {'value': '$\\\\text{ID}^3$: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition'}, 'authors': {'value': ['Jianqing Xu', 'Shen Li', 'Jiaying Wu', 'Miao Xiong', 'Ailin Deng', 'Jiazhen Ji', 'Yuge Huang', 'Guodong Mu', 'Wenjie Feng', 'Shouhong Ding', 'Bryan Hooi']}, 'authorids': {'value': ['~Jianqing_Xu1', '~Shen_Li2', '~Jiaying_Wu2', '~Miao_Xiong2', '~Ailin_Deng1', '~Jiazhen_Ji1', '~Yuge_Huang1', '~Guodong_Mu1', '~Wenjie_Feng1', '~Shouhong_Ding3', '~Bryan_Hooi1']}, 'keywords': {'value': ['synthetic face recognition']}, 'abstract': {'value': 'Synthetic face recognition (SFR) aims to generate synthetic face datasets that mimic the distribution of real face data, which allows for training face recognition models in a privacy-preserving manner. Despite the remarkable potential of diffusion models in image generation, current diffusion-based SFR models struggle with generalization to real-world faces. To address this limitation, we outline three key objectives for SFR: (1) promoting diversity across identities (inter-class diversity), (2) ensuring diversity within each identity by injecting various facial attributes (intra-class diversity), and (3) maintaining identity consistency within each identity group (intra-class identity preservation). Inspired by these goals, we introduce a diffusion-fueled SFR model termed $\\\\text{ID}^3$. $\\\\text{ID}^3$ employs an ID-preserving loss to generate diverse yet identity-consistent facial appearances. Theoretically, we show that minimizing this loss is equivalent to maximizing the lower bound of an adjusted conditional log-likelihood over ID-preserving data. This equivalence motivates an ID-preserving sampling algorithm, which operates over an adjusted gradient vector field, enabling the generation of fake face recognition datasets that approximate the distribution of real-world faces. Extensive experiments across five challenging benchmarks validate the advantages of $\\\\text{ID}^3$.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/773ad46986854b909ee32c632457744c279a0962.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxu2024textid,\\ntitle={\\\\${\\\\textbackslash}text\\\\{{ID}\\\\}{\\\\textasciicircum}3\\\\$: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition},\\nauthor={Jianqing Xu and Shen Li and Jiaying Wu and Miao Xiong and Ailin Deng and Jiazhen Ji and Yuge Huang and Guodong Mu and Wenjie Feng and Shouhong Ding and Bryan Hooi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x4HMnqs6IE}\\n}'}, 'paperhash': {'value': 'xu|\\\\textid^3_identitypreservingyetdiversified_diffusion_models_for_synthetic_face_recognition'}},forum = 'x4HMnqs6IE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5829/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5829/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5829/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5829/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x4EoTQW7ka',number = 9316,cdate = 1715678248046,pdate = 1727287906992,odate = 1730873919797,mdate = 1737115989655,tcdate = 1715678248046,tmdate = 1737115989655,ddate = None,content = {'title': {'value': 'DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation'}, 'authors': {'value': ['Sunghyeon Woo', 'Baeseong park', 'Byeongwook Kim', 'Minjung Jo', 'Se Jung Kwon', 'Dongsuk Jeon', 'Dongsoo Lee']}, 'authorids': {'value': ['~Sunghyeon_Woo1', '~Baeseong_park1', '~Byeongwook_Kim1', '~Minjung_Jo1', '~Se_Jung_Kwon1', '~Dongsuk_Jeon1', '~Dongsoo_Lee1']}, 'keywords': {'value': ['Training Acceleration', 'Memory Efficient Fine-Tuning', 'Large Language Models', 'Backpropagation Optimization.']}, 'TLDR': {'value': 'DropBP, randomly dropping backward propagation based on layer sensitivity,  significantly accelerates fine-tuning in Large Language Models (LLMs) with considerable memory reduction.'}, 'abstract': {'value': 'Large language models (LLMs) have achieved significant success across various domains. However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation. While parameter-efficient fine-tuning (PEFT) considerably reduces the training memory associated with parameters, it does not address the significant computational costs and activation memory. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs and activation memory while maintaining accuracy. DropBP randomly drops layers during backward propagation, which is essentially equivalent to training shallow submodules generated by undropped layers and residual connections. Additionally, DropBP calculates the sensitivity of each layer to assign an appropriate drop rate, thereby stabilizing the training process. DropBP is not only applicable to full fine-tuning but can also be orthogonally integrated with all types of PEFT by dropping layers during backward propagation. Specifically, DropBP can reduce training time by 44% with comparable accuracy to the baseline, accelerate convergence to the same perplexity by 1.5$\\\\times$, and enable training with a sequence length 6.2$\\\\times$ larger on a single NVIDIA-A100 GPU. Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100 GPU and 117% on an Intel Gaudi2 HPU. The code is available at [https://github.com/WooSunghyeon/dropbp](https://github.com/WooSunghyeon/dropbp).'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/796111fc2cfe878a800d38ba648d7c6104be3373.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwoo2024dropbp,\\ntitle={Drop{BP}: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation},\\nauthor={Sunghyeon Woo and Baeseong park and Byeongwook Kim and Minjung Jo and Se Jung Kwon and Dongsuk Jeon and Dongsoo Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x4EoTQW7ka}\\n}'}, 'paperhash': {'value': 'woo|dropbp_accelerating_finetuning_of_large_language_models_by_dropping_backward_propagation'}},forum = 'x4EoTQW7ka',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9316/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9316/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9316/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9316/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'x33oWJQyH0',number = 4556,cdate = 1715427432873,pdate = 1727287754088,odate = 1730873877077,mdate = 1730873877095,tcdate = 1715427432873,tmdate = 1730873877095,ddate = None,content = {'title': {'value': 'Unsupervised Object Detection with Theoretical Guarantees'}, 'authors': {'value': ['Marian Longa', 'Joao F. Henriques']}, 'authorids': {'value': ['~Marian_Longa1', '~Joao_F._Henriques1']}, 'keywords': {'value': ['unsupervised object detection', 'object detection', 'unsupervised learning', 'representation learning']}, 'TLDR': {'value': 'We propose the first unsupervised object detection method that we prove has theoretical guarantees of recovering the true object positions up to small shifts.'}, 'abstract': {'value': \"Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder and decoder receptive field sizes, the object sizes, and the widths of the Gaussians used in the rendering process. We perform detailed analysis of how the error depends on each of these variables and perform synthetic experiments validating our theoretical predictions up to a precision of individual pixels. We also perform experiments on CLEVR-based data and show that, unlike current SOTA object detection methods (SAM, CutLER), our method's prediction errors always lie within our theoretical bounds. We hope that this work helps open up an avenue of research into object detection methods with theoretical guarantees.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7878a3bbb19a093b6e8f4e67ce9a0e0e1dfa65b6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlonga2024unsupervised,\\ntitle={Unsupervised Object Detection with Theoretical Guarantees},\\nauthor={Marian Longa and Joao F. Henriques},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x33oWJQyH0}\\n}'}, 'paperhash': {'value': 'longa|unsupervised_object_detection_with_theoretical_guarantees'}},forum = 'x33oWJQyH0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4556/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4556/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4556/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4556/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x2zY4hZcmg',number = 11634,cdate = 1715707939197,pdate = 1727287977906,odate = 1730873940651,mdate = 1734654546103,tcdate = 1715707939197,tmdate = 1734654546103,ddate = None,content = {'title': {'value': 'Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning'}, 'authors': {'value': ['Arko Banerjee', 'Kia Rahmani', 'Joydeep Biswas', 'Isil Dillig']}, 'authorids': {'value': ['~Arko_Banerjee1', '~Kia_Rahmani1', '~Joydeep_Biswas1', '~Isil_Dillig1']}, 'keywords': {'value': ['Safe Reinforcement Learning', 'Model Predictive Shielding', 'Planning', 'MCTS']}, 'TLDR': {'value': 'The paper proposes a novel method for integrating a dynamic planner within a safe reinforcement learning framework, enabling progress towards the goal while recovering from unsafe situations.'}, 'abstract': {'value': 'Among approaches for provably safe reinforcement learning, Model Predictive Shielding (MPS) has proven effective at complex tasks in continuous, high-dimensional state spaces, by leveraging a *backup policy* to ensure safety when the learned policy attempts to take risky actions. However, while MPS can ensure safety both during and after training, it often hinders task progress due to the conservative and task-oblivious nature of backup policies.\\nThis paper introduces *Dynamic Model Predictive Shielding* (DMPS), which optimizes reinforcement learning objectives while maintaining provable safety. DMPS employs a local planner to dynamically select safe recovery actions that maximize both short-term progress as well as long-term rewards. Crucially,  the planner and the neural policy  play a synergistic role in DMPS. When planning recovery actions for ensuring safety,  the planner utilizes the neural policy to estimate long-term rewards, allowing it to *observe* beyond its short-term planning horizon. \\nConversely, the neural policy under training learns from the recovery plans proposed by the planner, converging to policies that are both *high-performing* and *safe* in practice.\\nThis approach guarantees safety during and after training, with bounded recovery regret that decreases exponentially with planning horizon depth. Experimental results demonstrate that DMPS converges to policies that rarely require shield interventions after training and achieve higher rewards compared to several state-of-the-art baselines.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c706ae06d8c7d1a093c6b0c855ff445166fb6629.pdf'}, 'supplementary_material': {'value': '/attachment/8fcfb7bfbfe2d328ca847d4c47888bf071c63929.zip'}, '_bibtex': {'value': '@inproceedings{\\nbanerjee2024dynamic,\\ntitle={Dynamic Model Predictive Shielding for Provably Safe Reinforcement Learning},\\nauthor={Arko Banerjee and Kia Rahmani and Joydeep Biswas and Isil Dillig},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x2zY4hZcmg}\\n}'}, 'paperhash': {'value': 'banerjee|dynamic_model_predictive_shielding_for_provably_safe_reinforcement_learning'}},forum = 'x2zY4hZcmg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11634/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11634/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11634/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11634/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'x2780VcMOI',number = 19904,cdate = 1715793364300,pdate = 1727288220607,odate = 1730873998288,mdate = 1736845021863,tcdate = 1715793364300,tmdate = 1736845021863,ddate = None,content = {'title': {'value': 'A Polar coordinate system represents syntax in large language models'}, 'authors': {'value': ['Pablo J. Diego Simon', \"Stéphane d'Ascoli\", 'Emmanuel Chemla', 'Yair Lakretz', 'Jean-Remi King']}, 'authorids': {'value': ['~Pablo_J._Diego_Simon1', \"~Stéphane_d'Ascoli1\", '~Emmanuel_Chemla1', '~Yair_Lakretz2', '~Jean-Remi_King1']}, 'keywords': {'value': ['Natural Language Processing', 'Large Language Models', 'Interpretability', 'Syntax', 'Linguistics', 'Cognitive Science']}, 'TLDR': {'value': 'We show that the presence and type of syntactic relations in a sentence can be inferred respectively from distances and orientations in the activation space of language models'}, 'abstract': {'value': \"Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a ''Structural Probe'' can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the \\\\emph{existence} but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a ''Polar Probe'' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4e0896e6752ad21f70d6149f2f3582b264bfeedc.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nsimon2024a,\\ntitle={A Polar coordinate system represents syntax in large language models},\\nauthor={Pablo J. Diego Simon and St{\\\\'e}phane d'Ascoli and Emmanuel Chemla and Yair Lakretz and Jean-Remi King},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=x2780VcMOI}\\n}\"}, 'paperhash': {'value': 'simon|a_polar_coordinate_system_represents_syntax_in_large_language_models'}},forum = 'x2780VcMOI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19904/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19904/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19904/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19904/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wzof7Y66xs',number = 9347,cdate = 1715678580583,pdate = 1727287907995,odate = 1730873920101,mdate = 1737292135933,tcdate = 1715678580583,tmdate = 1737292135933,ddate = None,content = {'title': {'value': 'Hierarchical Selective Classification'}, 'authors': {'value': ['Shani Goren', 'Ido Galil', 'Ran El-Yaniv']}, 'authorids': {'value': ['~Shani_Goren1', '~Ido_Galil1', '~Ran_El-Yaniv1']}, 'keywords': {'value': ['Hierarchical Selective Classification', 'Hierarchical Uncertainty', 'Selective Classification', 'Non-Bayesian Uncertainty Estimation', 'CLIP']}, 'TLDR': {'value': 'We extend selective classification to a hierarchical setting, showing outstanding results for various models.'}, 'abstract': {'value': 'Deploying deep neural networks for risk-sensitive tasks necessitates an uncertainty estimation mechanism. This paper introduces *hierarchical selective classification*, extending selective classification to a hierarchical setting. Our approach leverages the inherent structure of class relationships, enabling models to reduce the specificity of their predictions when faced with uncertainty. In this paper, we first formalize hierarchical risk and coverage, and introduce hierarchical risk-coverage curves. Next, we develop algorithms for hierarchical selective classification (which we refer to as \"inference rules\"), and propose an efficient algorithm that guarantees a target accuracy constraint with high probability. Lastly, we conduct extensive empirical studies on over a thousand ImageNet classifiers, revealing that training regimes such as CLIP, pretraining on ImageNet21k and knowledge distillation boost hierarchical selective performance.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c1f9ac8af6d8f4be4a306ca805ddb73cdadf977f.pdf'}, 'supplementary_material': {'value': '/attachment/fb32d0bcb881e6d92dbfbcac971ca6f95cab8087.zip'}, '_bibtex': {'value': '@inproceedings{\\ngoren2024hierarchical,\\ntitle={Hierarchical Selective Classification},\\nauthor={Shani Goren and Ido Galil and Ran El-Yaniv},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wzof7Y66xs}\\n}'}, 'paperhash': {'value': 'goren|hierarchical_selective_classification'}},forum = 'wzof7Y66xs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9347/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9347/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9347/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9347/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wz2KvvEk44',number = 8266,cdate = 1715656614575,pdate = 1727287874353,odate = 1730873910687,mdate = 1730873910708,tcdate = 1715656614575,tmdate = 1730873910708,ddate = None,content = {'title': {'value': 'Focus On What Matters: Separated Models For Visual-Based RL Generalization'}, 'authors': {'value': ['Di Zhang', 'Bowen Lv', 'Hai Zhang', 'Feifan Yang', 'Junqiao Zhao', 'Hang Yu', 'Chang Huang', 'Hongtu Zhou', 'Chen Ye', 'changjun jiang']}, 'authorids': {'value': ['~Di_Zhang5', '~Bowen_Lv1', '~Hai_Zhang2', '~Feifan_Yang2', '~Junqiao_Zhao1', '~Hang_Yu14', '~Chang_Huang5', '~Hongtu_Zhou1', '~Chen_Ye1', '~changjun_jiang2']}, 'keywords': {'value': ['Reinforcement Learning', 'Visual-based RL', 'Generalization']}, 'abstract': {'value': \"A primary challenge for visual-based Reinforcement Learning (RL) is to generalize effectively across unseen environments. Although previous studies have explored different auxiliary tasks to enhance generalization, few adopt image reconstruction due to concerns about exacerbating overfitting to task-irrelevant features during training. Perceiving the pre-eminence of image reconstruction in representation learning, we propose SMG (\\\\blue{S}eparated \\\\blue{M}odels for \\\\blue{G}eneralization), a novel approach that exploits image reconstruction for generalization. SMG introduces two model branches to extract task-relevant and task-irrelevant representations separately from visual observations via cooperatively reconstruction. Built upon this architecture, we further emphasize the importance of task-relevant features for generalization. Specifically, SMG incorporates two additional consistency losses to guide the agent's focus toward task-relevant areas across different scenarios, thereby achieving free from overfitting. Extensive experiments in DMC demonstrate the SOTA performance of SMG in generalization, particularly excelling in video-background settings. Evaluations on robotic manipulation tasks further confirm the robustness of SMG in real-world applications. Source code is available at \\\\url{https://anonymous.4open.science/r/SMG/}.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4038405ec2477f5290f6738f4c80053e969f1bfe.pdf'}, 'TLDR': {'value': 'We propose SMG, which utilizes a reconstruction-based auxiliary task to extract task-relevant representations from visual observations and further strengths the generalization ability of RL agents with the help of two consistency losses.'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024focus,\\ntitle={Focus On What Matters: Separated Models For Visual-Based {RL} Generalization},\\nauthor={Di Zhang and Bowen Lv and Hai Zhang and Feifan Yang and Junqiao Zhao and Hang Yu and Chang Huang and Hongtu Zhou and Chen Ye and changjun jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wz2KvvEk44}\\n}'}, 'paperhash': {'value': 'zhang|focus_on_what_matters_separated_models_for_visualbased_rl_generalization'}},forum = 'wz2KvvEk44',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8266/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8266/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8266/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8266/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wyYsCI3K7U',number = 6030,cdate = 1715581330845,pdate = 1727287803019,odate = 1730873890057,mdate = 1730873890074,tcdate = 1715581330845,tmdate = 1730873890074,ddate = None,content = {'title': {'value': 'LoRANN: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search'}, 'authors': {'value': ['Elias Jääsaari', 'Ville Hyvönen', 'Teemu Roos']}, 'authorids': {'value': ['~Elias_Jääsaari1', '~Ville_Hyvönen1', '~Teemu_Roos1']}, 'keywords': {'value': ['Approximate nearest neighbor search', 'vector search', 'k-nn search', 'multivariate regression', 'vector databases']}, 'abstract': {'value': 'Approximate nearest neighbor (ANN) search is a key component in many modern machine learning pipelines; recent use cases include retrieval-augmented generation (RAG) and vector databases. Clustering-based ANN algorithms, that use score computation methods based on product quantization (PQ), are often used in industrial-scale applications due to their scalability and suitability for distributed and disk-based implementations. However, they have slower query times than the leading graph-based ANN algorithms. In this work, we propose a new supervised score computation method based on the observation that inner product approximation is a multivariate (multi-output) regression problem that can be solved efficiently by reduced-rank regression. Our experiments show that on modern high-dimensional data sets, the proposed reduced-rank regression (RRR) method is superior to PQ in both query latency and memory usage. We also introduce LoRANN, a clustering-based ANN library that leverages the proposed score computation method. LoRANN is competitive with the leading graph-based algorithms and outperforms the state-of-the-art GPU ANN methods on high-dimensional data sets.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/818415f345eae75fa997bddb07da900aca844fb4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nj{\\\\\"a}{\\\\\"a}saari2024lorann,\\ntitle={Lo{RANN}: Low-Rank Matrix Factorization for Approximate Nearest Neighbor Search},\\nauthor={Elias J{\\\\\"a}{\\\\\"a}saari and Ville Hyv{\\\\\"o}nen and Teemu Roos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wyYsCI3K7U}\\n}'}, 'paperhash': {'value': 'jääsaari|lorann_lowrank_matrix_factorization_for_approximate_nearest_neighbor_search'}},forum = 'wyYsCI3K7U',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6030/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6030/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6030/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6030/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ww62xltEfB',number = 6258,cdate = 1715588508537,pdate = 1727287809360,odate = 1730873891459,mdate = 1730873891476,tcdate = 1715588508537,tmdate = 1730873891476,ddate = None,content = {'title': {'value': 'A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness'}, 'authors': {'value': ['Yuri Kinoshita', 'Taro Toyoizumi']}, 'authorids': {'value': ['~Yuri_Kinoshita1', '~Taro_Toyoizumi1']}, 'keywords': {'value': ['bi-Lipschitzness', 'theoretical guarantee', 'tight control', 'direct parameterization', 'inductive bias', 'convex neural network', 'Legendre-Fenchel transformation']}, 'abstract': {'value': 'While neural networks can enjoy an outstanding flexibility and exhibit unprecedented performance, the mechanism behind their behavior is still not well-understood. To tackle this fundamental challenge, researchers have tried to restrict and manipulate some of their properties in order to gain new insights and better control on them. Especially, throughout the past few years, the concept of *bi-Lipschitzness* has been proved as a beneficial inductive bias in many areas. However, due to its complexity, the design and control of bi-Lipschitz architectures are falling behind, and a model that is precisely designed for bi-Lipschitzness realizing a direct and simple control of the constants along with solid theoretical analysis is lacking. In this work, we investigate and propose a novel framework for bi-Lipschitzness that can achieve such a clear and tight control based on convex neural networks and the Legendre-Fenchel duality. Its desirable properties are illustrated with concrete experiments to illustrate its broad range of applications.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3b2ddda7e4b50b265094c5f987886a84be0b3958.pdf'}, 'supplementary_material': {'value': '/attachment/2b16413a9557031aa286c0ea85b4fae8ed2a19ae.zip'}, '_bibtex': {'value': '@inproceedings{\\nkinoshita2024a,\\ntitle={A provable control of sensitivity of neural networks through a direct parameterization of the overall bi-Lipschitzness},\\nauthor={Yuri Kinoshita and Taro Toyoizumi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ww62xltEfB}\\n}'}, 'paperhash': {'value': 'kinoshita|a_provable_control_of_sensitivity_of_neural_networks_through_a_direct_parameterization_of_the_overall_bilipschitzness'}},forum = 'ww62xltEfB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6258/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6258/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6258/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6258/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wvQHQgnpGN',number = 10812,cdate = 1715697922890,pdate = 1727287951936,odate = 1730873932355,mdate = 1734582900820,tcdate = 1715697922890,tmdate = 1734582900820,ddate = None,content = {'title': {'value': 'Solving Zero-Sum Markov Games with Continuous State via Spectral Dynamic Embedding'}, 'authors': {'value': ['Chenhao Zhou', 'Zebang Shen', 'Chao Zhang', 'Hanbin Zhao', 'Hui Qian']}, 'authorids': {'value': ['~Chenhao_Zhou2', '~Zebang_Shen1', '~Chao_Zhang19', '~Hanbin_Zhao1', '~Hui_Qian1']}, 'keywords': {'value': ['zero-sum Markov game', 'reinforcement learning', 'dynamic programming']}, 'abstract': {'value': 'In this paper, we propose a provably efficient natural policy gradient algorithm called Spectral Dynamic Embedding Policy Optimization (\\\\SDEPO) for two-player zero-sum stochastic Markov games with continuous state space and finite action space.\\n  In the policy evaluation procedure of our algorithm, a novel kernel embedding method is employed to construct a finite-dimensional linear approximations to the state-action value function.\\n  We explicitly analyze the approximation error in policy evaluation, and show that \\\\SDEPO\\\\  achieves an $\\\\tilde{O}(\\\\frac{1}{(1-\\\\gamma)^3\\\\epsilon})$ last-iterate convergence to the $\\\\epsilon-$optimal Nash equilibrium, which is independent of the cardinality of the state space.\\n  The complexity result matches the best-known results for global convergence of policy gradient algorithms for single agent setting.\\n  Moreover, we also propose a practical variant of \\\\SDEPO\\\\ to deal with continuous action space and empirical results demonstrate the practical superiority of the proposed method.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/12158a5941ae353d09451219c72139c623a117bb.pdf'}, 'supplementary_material': {'value': '/attachment/3d7a74447aa7babe4adf3163213b53f340c85edb.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024solving,\\ntitle={Solving Zero-Sum Markov Games with Continous State via Spectral Dynamic Embedding},\\nauthor={Chenhao Zhou and Zebang Shen and Chao Zhang and Hanbin Zhao and Hui Qian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wvQHQgnpGN}\\n}'}, 'paperhash': {'value': 'zhou|solving_zerosum_markov_games_with_continuous_state_via_spectral_dynamic_embedding'}},forum = 'wvQHQgnpGN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10812/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10812/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10812/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10812/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wsqDJHPUHN',number = 10005,cdate = 1715688969999,pdate = 1727287927329,odate = 1730873925329,mdate = 1737028599433,tcdate = 1715688969999,tmdate = 1737028599433,ddate = None,content = {'title': {'value': \"On the Ability of Developers' Training Data Preservation of Learnware\"}, 'authors': {'value': ['Hao-Yi Lei', 'Zhi-Hao Tan', 'Zhi-Hua Zhou']}, 'authorids': {'value': ['~Hao-Yi_Lei1', '~Zhi-Hao_Tan1', '~Zhi-Hua_Zhou2']}, 'keywords': {'value': ['Learnware', 'Model Specification', 'Reduced Kernel Mean Embedding', 'Data Preservation', 'Synthetic Data', 'Learnware Dock System']}, 'TLDR': {'value': 'We conducted a theoretical analysis of the data protection capabilities of the Reduced Kernel Mean Embeding (RKME) specification in learnware.'}, 'abstract': {'value': \"The learnware paradigm aims to enable users to leverage numerous existing well-trained models instead of building machine learning models from scratch. In this paradigm, developers worldwide can submit their well-trained models spontaneously into a learnware dock system, and the system helps developers generate specification for each model to form a learnware. As the key component, a specification should characterize the capabilities of the model, enabling it to be adequately identified and reused, while preserving the developer's original data. Recently, the RKME (Reduced Kernel Mean Embedding) specification was proposed and most commonly utilized. This paper provides a theoretical analysis of RKME specification about its preservation ability for developer's training data. By modeling it as a geometric problem on manifolds and utilizing tools from geometric analysis, we prove that the RKME specification is able to disclose none of the developer's original data and possesses robust defense against common inference attacks, while preserving sufficient information for effective learnware identification.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ca2805a8aaa53ff63c596616b50b597d8d958618.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nlei2024on,\\ntitle={On the Ability of Developers' Training Data Preservation of Learnware},\\nauthor={Hao-Yi Lei and Zhi-Hao Tan and Zhi-Hua Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wsqDJHPUHN}\\n}\"}, 'paperhash': {'value': 'lei|on_the_ability_of_developers_training_data_preservation_of_learnware'}},forum = 'wsqDJHPUHN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10005/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10005/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10005/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10005/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wsHMb4J2o9',number = 11944,cdate = 1715713583702,pdate = 1727287988973,odate = 1730873943877,mdate = 1734641496713,tcdate = 1715713583702,tmdate = 1734641496713,ddate = None,content = {'title': {'value': 'The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks'}, 'authors': {'value': ['Lénaïc Chizat', 'Praneeth Netrapalli']}, 'authorids': {'value': ['~Lénaïc_Chizat1', '~Praneeth_Netrapalli1']}, 'keywords': {'value': ['Feature Learning', 'Deep Neural Networks', 'One SGD step', 'Hyperparameter scaling', 'dynamical isometry']}, 'TLDR': {'value': 'We propose an elementary approach to quantifying feature learning in wide and deep neural networks and to derive hyperparameter scalings.'}, 'abstract': {'value': 'Deep learning succeeds by doing hierarchical feature learning, yet tuning hyper-parameters (HP) such as initialization scales, learning rates etc., only give indirect control over this behavior. In this paper, we introduce a key notion to predict and control feature learning: the angle $\\\\theta_\\\\ell$ between the feature updates and the backward pass (at layer index $\\\\ell$). We show that the magnitude of feature updates after one GD step, at any training time, can be expressed via a simple and general *feature speed formula* in terms of this angle $\\\\theta_\\\\ell$, the loss decay, and the magnitude of the backward pass. This angle $\\\\theta_\\\\ell$ is controlled by the conditioning of the layer-to-layer Jacobians and at random initialization, it is determined by the spectrum of a certain kernel, which coincides with the Neural Tangent Kernel when $\\\\ell=\\\\text{depth}$. Given $\\\\theta_\\\\ell$, the feature speed formula provides us with rules to adjust HPs (scales and learning rates) so as to satisfy certain dynamical properties, such as feature learning and loss decay. We investigate the implications of our approach for ReLU MLPs and ResNets in the large width-then-depth limit. Relying on prior work, we show that in ReLU MLPs with iid initialization, the angle degenerates with depth as $\\\\cos(\\\\theta_\\\\ell)=\\\\Theta(1/\\\\sqrt{\\\\ell})$. In contrast, ResNets with branch scale  $O(1/\\\\sqrt{\\\\text{depth}})$ maintain a non-degenerate angle $\\\\cos(\\\\theta_\\\\ell)=\\\\Theta(1)$. We use these insights to recover key properties of known HP scalings (such as $\\\\mu$P), and also introduce a new HP scaling for large depth ReLU MLPs with favorable theoretical properties.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b9078a3029a8ba5f5d9f97f0236d84340733175b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchizat2024the,\\ntitle={The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks},\\nauthor={L{\\\\\\'e}na{\\\\\"\\\\i}c Chizat and Praneeth Netrapalli},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wsHMb4J2o9}\\n}'}, 'paperhash': {'value': 'chizat|the_feature_speed_formula_a_flexible_approach_to_scale_hyperparameters_of_deep_neural_networks'}},forum = 'wsHMb4J2o9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11944/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11944/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11944/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11944/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wsGzvhnoaX',number = 8627,cdate = 1715666314899,pdate = 1727287885696,odate = 1730873913522,mdate = 1735304497462,tcdate = 1715666314899,tmdate = 1735304497462,ddate = None,content = {'title': {'value': 'Quantum Algorithms for Non-smooth Non-convex Optimization'}, 'authors': {'value': ['Chengchang Liu', 'Chaowen Guan', 'Jianhao He', 'John C.S. Lui']}, 'authorids': {'value': ['~Chengchang_Liu1', '~Chaowen_Guan1', '~Jianhao_He1', '~John_C.S._Lui2']}, 'keywords': {'value': ['quantum computing', 'non-convex non-smooth optimization']}, 'TLDR': {'value': 'We show the quantum speedups for finding the Goldstein stationary point by proposing gradient-free quantum algorithms.'}, 'abstract': {'value': 'This paper considers the problem for finding the  $(\\\\delta,\\\\epsilon)$-Goldstein stationary point of Lipschitz continuous objective, which is a rich function class to cover a great number of important applications. \\nWe construct a novel zeroth-order quantum estimator for the gradient of the smoothed surrogate. \\nBased on such estimator, we propose a novel quantum algorithm  that achieves a query complexity of $\\\\tilde{\\\\mathcal{O}}(d^{3/2}\\\\delta^{-1}\\\\epsilon^{-3})$ on the stochastic function value oracle, where $d$ is the dimension of the problem. \\nWe also enhance the query complexity to $\\\\tilde{\\\\mathcal{O}}(d^{3/2}\\\\delta^{-1}\\\\epsilon^{-7/3})$ by introducing a variance reduction variant. \\nOur findings demonstrate the clear advantages of utilizing quantum techniques for non-convex non-smooth optimization, as they outperform the optimal classical methods on the dependency of $\\\\epsilon$ by a factor of $\\\\epsilon^{-2/3}$.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/364623854979ad37fb6af986b474c827d812af53.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024quantum,\\ntitle={Quantum Algorithms for Non-smooth Non-convex Optimization},\\nauthor={Chengchang Liu and Chaowen Guan and Jianhao He and John C.S. Lui},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wsGzvhnoaX}\\n}'}, 'paperhash': {'value': 'liu|quantum_algorithms_for_nonsmooth_nonconvex_optimization'}},forum = 'wsGzvhnoaX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8627/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8627/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8627/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8627/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wqs2RMq4CW',number = 12992,cdate = 1715733042450,pdate = 1727288024861,odate = 1730873954136,mdate = 1730873954154,tcdate = 1715733042450,tmdate = 1730873954154,ddate = None,content = {'title': {'value': 'Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification'}, 'authors': {'value': ['Haolin Liu', 'Artin Tajdini', 'Andrew Wagenmaker', 'Chen-Yu Wei']}, 'authorids': {'value': ['~Haolin_Liu8', '~Artin_Tajdini1', '~Andrew_Wagenmaker1', '~Chen-Yu_Wei1']}, 'keywords': {'value': ['corruption-robust', 'linear bandits', 'misspecification', 'reinforcement learning']}, 'TLDR': {'value': 'We obtain minimax optimal bounds for corruption-robust linear bandits, and show that they can be used to obtain novel gap-dependent misspecification bounds in bandits and RL.'}, 'abstract': {'value': 'In linear bandits, how can a learner effectively learn when facing corrupted rewards? While significant work has explored this question, a holistic understanding across different adversarial models and corruption measures is lacking, as is a full characterization of the minimax regret bounds. In this work, we compare two types of corruptions commonly considered: strong corruption, where the corruption level depends on the learner’s chosen action, and weak corruption, where the corruption level does not depend on the learner’s chosen action. We provide a unified framework to analyze these corruptions. For stochastic linear bandits, we fully characterize the gap between the minimax regret under strong and weak corruptions. We also initiate the study of corrupted adversarial linear bandits, obtaining upper and lower bounds with matching dependencies on the corruption level. Next, we reveal a connection between corruption-robust learning and learning with gap-dependent misspecification—a setting first studied by Liu et al. (2023a), where the misspecification level of an action or policy is proportional to its suboptimality. We present a general reduction that enables any corruption-robust algorithm to handle gap-dependent misspecification. This allows us to recover the results of Liu et al. (2023a) in a black-box manner and significantly generalize them to settings like linear MDPs, yielding the first results for gap-dependent misspecification in reinforcement learning. However, this general reduction does not attain the optimal rate for gap-dependent misspecification. Motivated by this, we develop a specialized algorithm that achieves optimal bounds for gap-dependent misspecification in linear bandits, thus answering an open question posed by Liu et al. (2023a).'}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d72fd0fc0d62a924ff97b58e851197a74e7f045b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024corruptionrobust,\\ntitle={Corruption-Robust Linear Bandits: Minimax Optimality and Gap-Dependent Misspecification},\\nauthor={Haolin Liu and Artin Tajdini and Andrew Wagenmaker and Chen-Yu Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wqs2RMq4CW}\\n}'}, 'paperhash': {'value': 'liu|corruptionrobust_linear_bandits_minimax_optimality_and_gapdependent_misspecification'}},forum = 'wqs2RMq4CW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12992/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12992/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12992/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12992/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wqLC4G1GN3',number = 20602,cdate = 1715797351904,pdate = 1727288237520,odate = 1730874002350,mdate = 1734809375005,tcdate = 1715797351904,tmdate = 1734809375005,ddate = None,content = {'title': {'value': 'Solving Inverse Problems via Diffusion Optimal Control'}, 'authors': {'value': ['Henry Li', 'Marcus Aloysius Pereira']}, 'authorids': {'value': ['~Henry_Li2', '~Marcus_Aloysius_Pereira1']}, 'keywords': {'value': ['diffusion models', 'inverse problems', 'optimal control']}, 'abstract': {'value': 'Existing approaches to diffusion-based inverse problem solvers frame the signal recovery task as a probabilistic sampling episode, where the solution is drawn from the desired posterior distribution. This framework suffers from several critical drawbacks, including the intractability of the conditional likelihood function, strict dependence on the score network approximation, and poor $\\\\mathbf{x}_0$ prediction quality. We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode. We derive a diffusion-based optimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algorithm. This framework is fully general and able to handle any differentiable forward measurement operator, including super-resolution, inpainting, Gaussian deblurring, nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, we show that the idealized posterior sampling equation can be recovered as a special case of our algorithm. We then evaluate our method against a selection of neural inverse problem solvers, and establish a new baseline in image reconstruction with inverse problems.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/05d77a296e09e3facb8599ac339e22b4399d0782.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024solving,\\ntitle={Solving Inverse Problems via Diffusion Optimal Control},\\nauthor={Henry Li and Marcus Aloysius Pereira},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wqLC4G1GN3}\\n}'}, 'paperhash': {'value': 'li|solving_inverse_problems_via_diffusion_optimal_control'}},forum = 'wqLC4G1GN3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20602/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20602/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20602/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20602/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wpGJ2AX6SZ',number = 18035,cdate = 1715783350124,pdate = 1727288168012,odate = 1730873987515,mdate = 1730873987534,tcdate = 1715783350124,tmdate = 1730873987534,ddate = None,content = {'title': {'value': 'Human Expertise in Algorithmic Prediction'}, 'authors': {'value': ['Rohan Alur', 'Manish Raghavan', 'Devavrat Shah']}, 'authorids': {'value': ['~Rohan_Alur1', '~Manish_Raghavan1', '~Devavrat_Shah1']}, 'keywords': {'value': ['human/AI collaboration', 'human/AI complementarity', 'multicalibration', 'machine learning for healthcare', 'trustworthy machine learning']}, 'TLDR': {'value': 'We introduce a novel framework for incorporating human judgment into algorithmic predictions; our approach focuses on the use of human judgment to distinguish inputs which ‘look the same’ to any feasible predictive algorithm.'}, 'abstract': {'value': 'We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach leverages human judgment to distinguish inputs which are *algorithmically indistinguishable*, or \"look the same\" to predictive algorithms.  We argue that this framing clarifies the problem of human-AI collaboration in prediction tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm\\'s training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of \"side information\", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement.  We find empirically that although algorithms often outperform their human counterparts *on average*, human judgment can improve algorithmic predictions on *specific* instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4f5dc6075a84c5c600343c682e95020208b5f943.pdf'}, 'supplementary_material': {'value': '/attachment/98758ad76148097ae1a4913defbb572f573f0e90.zip'}, '_bibtex': {'value': '@inproceedings{\\nalur2024human,\\ntitle={Human Expertise in Algorithmic Prediction},\\nauthor={Rohan Alur and Manish Raghavan and Devavrat Shah},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wpGJ2AX6SZ}\\n}'}, 'paperhash': {'value': 'alur|human_expertise_in_algorithmic_prediction'}},forum = 'wpGJ2AX6SZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18035/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18035/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18035/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18035/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'woRFmNJiLp',number = 16512,cdate = 1715771409719,pdate = 1727288128281,odate = 1730873979626,mdate = 1730873979638,tcdate = 1715771409719,tmdate = 1730873979638,ddate = None,content = {'title': {'value': 'Alignment at Pre-training! Towards Native Alignment for Arabic LLMs'}, 'authors': {'value': ['Juhao Liang', 'Zhenyang Cai', 'Jianqing Zhu', 'Huang Huang', 'Kewei Zong', 'Bang An', 'Mosen Alharthi', 'Juncai He', 'Lian Zhang', 'Haizhou Li', 'Benyou Wang', 'Jinchao Xu']}, 'authorids': {'value': ['~Juhao_Liang1', '~Zhenyang_Cai1', '~Jianqing_Zhu2', '~Huang_Huang2', '~Kewei_Zong1', '~Bang_An3', '~Mosen_Alharthi1', '~Juncai_He1', '~Lian_Zhang2', '~Haizhou_Li3', '~Benyou_Wang2', '~Jinchao_Xu1']}, 'keywords': {'value': ['Large language model', 'Safety of LLMs', 'LLM Pre-training']}, 'TLDR': {'value': 'A novel and effective alignment method is proposed for LLMs in the pre-training stage to reduce the downstream adaptation cost.'}, 'abstract': {'value': \"The alignment of large language models (LLMs) is critical for developing effective and safe language models. Traditional approaches focus on aligning models during the instruction tuning or reinforcement learning stages, referred to in this paper as `\\\\textit{post alignment}'. We argue that alignment during the pre-training phase, which we term 'native alignment', warrants investigation. Native alignment aims to prevent unaligned content from the beginning, rather than relying on post-hoc processing. This approach leverages extensively aligned pre-training data to enhance the effectiveness and usability of pre-trained models. Our study specifically explores the application of native alignment in the context of Arabic LLMs. We conduct comprehensive experiments and ablation studies to evaluate the impact of native alignment on model performance and alignment stability. Additionally, we release open-source Arabic LLMs that demonstrate state-of-the-art performance on various benchmarks, providing significant benefits to the Arabic LLM community.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cbd79f21b25bc68a35292ca9eb5ce3ac4d6d318c.pdf'}, 'supplementary_material': {'value': '/attachment/55552c496370e9475895fff1050c7ee629dd9e90.zip'}, '_bibtex': {'value': '@inproceedings{\\nliang2024alignment,\\ntitle={Alignment at Pre-training! Towards Native Alignment for Arabic {LLM}s},\\nauthor={Juhao Liang and Zhenyang Cai and Jianqing Zhu and Huang Huang and Kewei Zong and Bang An and Mosen Alharthi and Juncai He and Lian Zhang and Haizhou Li and Benyou Wang and Jinchao Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=woRFmNJiLp}\\n}'}, 'paperhash': {'value': 'liang|alignment_at_pretraining_towards_native_alignment_for_arabic_llms'}},forum = 'woRFmNJiLp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16512/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16512/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16512/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16512/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'woENr7FJaI',number = 234,cdate = 1713854912413,pdate = 1727287633428,odate = 1730873839373,mdate = 1730873839393,tcdate = 1713854912413,tmdate = 1730873839393,ddate = None,content = {'title': {'value': 'Automated Multi-level Preference for MLLMs'}, 'authors': {'value': ['Mengxi Zhang', 'Wenhao Wu', 'Yu Lu', 'YuXin Song', 'KANG RONG', 'Huanjin Yao', 'Jianbo Zhao', 'Fanglong Liu', 'Haocheng Feng', 'Jingdong Wang', 'Yifan Sun']}, 'authorids': {'value': ['~Mengxi_Zhang1', '~Wenhao_Wu2', '~Yu_Lu11', '~YuXin_Song1', '~KANG_RONG1', '~Huanjin_Yao1', '~Jianbo_Zhao3', '~Fanglong_Liu1', '~Haocheng_Feng1', '~Jingdong_Wang1', '~Yifan_Sun2']}, 'keywords': {'value': ['Multimodal Large Language Models', 'Hallucinations', 'Reinforcement Learning from Human Feedback']}, 'abstract': {'value': \"Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (*i.e.*, superior, inferior), and find that adopting multi-level preferences (*e.g.*, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (**AMP**) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a5533caccb0d2513850f2e35a5cf67613481d4b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024automated,\\ntitle={Automated Multi-level Preference for {MLLM}s},\\nauthor={Mengxi Zhang and Wenhao Wu and Yu Lu and YuXin Song and KANG RONG and Huanjin Yao and Jianbo Zhao and Fanglong Liu and Haocheng Feng and Jingdong Wang and Yifan Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=woENr7FJaI}\\n}'}, 'paperhash': {'value': 'zhang|automated_multilevel_preference_for_mllms'}},forum = 'woENr7FJaI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission234/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission234/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission234/-/Revision', 'NeurIPS.cc/2024/Conference/Submission234/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wnPlJNiqfA',number = 8014,cdate = 1715650284404,pdate = 1727287865532,odate = 1730873908400,mdate = 1730873908411,tcdate = 1715650284404,tmdate = 1730873908411,ddate = None,content = {'title': {'value': 'KFNN: K-Free Nearest Neighbor For Crowdsourcing'}, 'authors': {'value': ['Wenjun Zhang', 'Liangxiao Jiang', 'Chaoqun Li']}, 'authorids': {'value': ['~Wenjun_Zhang4', '~Liangxiao_Jiang1', '~Chaoqun_Li1']}, 'keywords': {'value': ['Crowdsourcing learning', 'Label integration', 'K-free nearest neighbor']}, 'TLDR': {'value': 'We propose a novel label integration algorithm called K-free nearest neighbor (KFNN).'}, 'abstract': {'value': 'To reduce annotation costs, it is common in crowdsourcing to collect only a few noisy labels from different crowd workers for each instance. However, the limited noisy labels restrict the performance of label integration algorithms in inferring the unknown true label for the instance. Recent works have shown that leveraging neighbor instances can help alleviate this problem. Yet, these works all assume that each instance has the same neighborhood size, which defies common sense. To address this gap, we propose a novel label integration algorithm called K-free nearest neighbor (KFNN). In KFNN, the neighborhood size of each instance is automatically determined based on its attributes and noisy labels. Specifically, KFNN initially estimates a Mahalanobis distance distribution from the attribute space to model the relationship between each instance and all classes. This distance distribution is then utilized to enhance the multiple noisy label distribution of each instance. Subsequently, a Kalman filter is designed to mitigate the impact of noise incurred by neighbor instances. Finally, KFNN determines the optimal neighborhood size by the max-margin learning. Extensive experimental results demonstrate that KFNN significantly outperforms all the other state-of-the-art algorithms and exhibits greater robustness in various crowdsourcing scenarios.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0b3a999c175feae55c108033441b1455e2a2d2d8.pdf'}, 'supplementary_material': {'value': '/attachment/718cbdf91b6ba3c8c630d1539fac3a6180e128b1.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024kfnn,\\ntitle={{KFNN}: K-Free Nearest Neighbor For Crowdsourcing},\\nauthor={Wenjun Zhang and Liangxiao Jiang and Chaoqun Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wnPlJNiqfA}\\n}'}, 'paperhash': {'value': 'zhang|kfnn_kfree_nearest_neighbor_for_crowdsourcing'}},forum = 'wnPlJNiqfA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8014/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8014/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8014/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8014/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wm9JZq7RCe',number = 21294,cdate = 1715801132543,pdate = 1727288251988,odate = 1730874005331,mdate = 1731693185062,tcdate = 1715801132543,tmdate = 1731693185062,ddate = None,content = {'title': {'value': 'An Analysis of Tokenization: Transformers under Markov Data'}, 'authors': {'value': ['Nived Rajaraman', 'Jiantao Jiao', 'Kannan Ramchandran']}, 'authorids': {'value': ['~Nived_Rajaraman1', '~Jiantao_Jiao1', '~Kannan_Ramchandran1']}, 'keywords': {'value': ['Tokenization', 'LLMs', 'interpretability']}, 'TLDR': {'value': 'Transformers without tokenization are very slow to learn Markov sources; we present a theoretical model and empirical observations showing that the addition of tokenization enables them to learn such processes much more efficiently'}, 'abstract': {'value': 'While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al. 2022, Xue et al. 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\\\\text{th}}$-order Markov processes for $k > 1$, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically are incredibly slow or fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al. 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\\\\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d6c78ee455f5fe6feda13258c2c22ffcc162624c.pdf'}, 'supplementary_material': {'value': '/attachment/33138127e7d0632e5c20d0dc8606b9fbf98c1c41.zip'}, '_bibtex': {'value': '@inproceedings{\\nrajaraman2024an,\\ntitle={An Analysis of Tokenization: Transformers under Markov Data},\\nauthor={Nived Rajaraman and Jiantao Jiao and Kannan Ramchandran},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wm9JZq7RCe}\\n}'}, 'paperhash': {'value': 'rajaraman|an_analysis_of_tokenization_transformers_under_markov_data'}},forum = 'wm9JZq7RCe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21294/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21294/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21294/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21294/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wlqfOvlTQz',number = 2975,cdate = 1715196827011,pdate = 1727287706651,odate = 1730873862418,mdate = 1730873862439,tcdate = 1715196827011,tmdate = 1730873862439,ddate = None,content = {'title': {'value': 'Reinforcement Learning with Lookahead Information'}, 'authors': {'value': ['Nadav Merlis']}, 'authorids': {'value': ['~Nadav_Merlis1']}, 'keywords': {'value': ['Reinforcement Learning', 'Regret Minimization', 'Lookahead']}, 'TLDR': {'value': 'We study RL settings where either immediate rewards or transitions are observed before acting and show how to achieve tight regret compared to stronger baselines with similar information.'}, 'abstract': {'value': 'We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state _before deciding which action to take_. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information -- linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ad7a5666a27d4242faa064f772f46ff2791265c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmerlis2024reinforcement,\\ntitle={Reinforcement Learning with Lookahead Information},\\nauthor={Nadav Merlis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wlqfOvlTQz}\\n}'}, 'paperhash': {'value': 'merlis|reinforcement_learning_with_lookahead_information'}},forum = 'wlqfOvlTQz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2975/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2975/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2975/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2975/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wlcm21C4nk',number = 6466,cdate = 1715594559307,pdate = 1727287815371,odate = 1730873893613,mdate = 1730873893625,tcdate = 1715594559307,tmdate = 1730873893625,ddate = None,content = {'title': {'value': 'Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation'}, 'authors': {'value': ['Chengting Yu', 'Lei Liu', 'Gaoang Wang', 'Erping Li', 'Aili Wang']}, 'authorids': {'value': ['~Chengting_Yu1', '~Lei_Liu26', '~Gaoang_Wang2', '~Erping_Li1', '~Aili_Wang2']}, 'keywords': {'value': ['Spiking Neural Networks', 'Training Method', 'Training Efficiency Optimization']}, 'abstract': {'value': 'Recent insights have revealed that rate-coding is a primary form of information representation captured by surrogate-gradient-based Backpropagation Through Time (BPTT) in training deep Spiking Neural Networks (SNNs). Motivated by these findings, we propose rate-based backpropagation, a training strategy specifically designed to exploit rate-based representations to reduce the complexity of BPTT. Our method minimizes reliance on detailed temporal derivatives by focusing on averaged dynamics, streamlining the computational graph to reduce memory and computational demands of SNNs training. We substantiate the rationality of the gradient approximation between BPTT and the proposed method through both theoretical analysis and empirical observations. Comprehensive experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS validate that our method achieves comparable performance to BPTT counterparts, and surpasses state-of-the-art efficient training techniques. By leveraging the inherent benefits of rate-coding, this work sets the stage for more scalable and efficient SNNs training within resource-constrained environments.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4eb38a11be001248c145a0cd2381f9d6503b19c.pdf'}, 'supplementary_material': {'value': '/attachment/a4775862497eb748b75db7be1d6984317a1abd5b.zip'}, '_bibtex': {'value': '@inproceedings{\\nyu2024advancing,\\ntitle={Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation},\\nauthor={Chengting Yu and Lei Liu and Gaoang Wang and Erping Li and Aili Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wlcm21C4nk}\\n}'}, 'paperhash': {'value': 'yu|advancing_training_efficiency_of_deep_spiking_neural_networks_through_ratebased_backpropagation'}},forum = 'wlcm21C4nk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6466/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6466/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6466/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6466/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wlLjYl0Gi6',number = 5119,cdate = 1715504600865,pdate = 1727287774368,odate = 1730873882359,mdate = 1730873882384,tcdate = 1715504600865,tmdate = 1730873882384,ddate = None,content = {'title': {'value': 'Efficient LLM Scheduling by Learning to Rank'}, 'authors': {'value': ['Yichao Fu', 'Siqi Zhu', 'Runlong Su', 'Aurick Qiao', 'Ion Stoica', 'Hao Zhang']}, 'authorids': {'value': ['~Yichao_Fu1', '~Siqi_Zhu1', '~Runlong_Su1', '~Aurick_Qiao1', '~Ion_Stoica1', '~Hao_Zhang2']}, 'keywords': {'value': ['Large Language Models']}, 'abstract': {'value': 'In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori. Consequently, most LLM serving systems employ a simple First-come-first-serve (FCFS) scheduling strategy, leading to Head-Of-Line (HOL) blocking and reduced throughput and service quality. \\nIn this paper, we reexamine this assumption -- we show that, although predicting the exact generation length of each request is infeasible, it is possible to predict the relative ranks of output lengths in a batch of requests, using learning to rank. The ranking information offers valuable guidance for scheduling requests. Building on this insight, we develop a novel scheduler for LLM inference and serving that can approximate the shortest-job-first (SJF) schedule better than existing approaches. We integrate this scheduler with the state-of-the-art LLM serving system and show significant performance improvement in several important applications: 2.8x lower latency in chatbot serving and 6.5x higher throughput in synthetic data generation. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ef9ade264c14ae815c219f762df83610938eb101.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfu2024efficient,\\ntitle={Efficient {LLM} Scheduling by Learning to Rank},\\nauthor={Yichao Fu and Siqi Zhu and Runlong Su and Aurick Qiao and Ion Stoica and Hao Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wlLjYl0Gi6}\\n}'}, 'paperhash': {'value': 'fu|efficient_llm_scheduling_by_learning_to_rank'}},forum = 'wlLjYl0Gi6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5119/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5119/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5119/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5119/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wl44W8xpc7',number = 9069,cdate = 1715674135410,pdate = 1727287899987,odate = 1730873917148,mdate = 1737122650148,tcdate = 1715674135410,tmdate = 1737122650148,ddate = None,content = {'title': {'value': 'Learning Infinitesimal Generators of Continuous Symmetries from Data'}, 'authors': {'value': ['Gyeonghoon Ko', 'Hyunsu Kim', 'Juho Lee']}, 'authorids': {'value': ['~Gyeonghoon_Ko1', '~Hyunsu_Kim2', '~Juho_Lee2']}, 'keywords': {'value': ['Symmetry Discovery', 'Geometric Deep Learning', 'Lie Point Symmetry', 'Neural PDE Solver']}, 'abstract': {'value': 'Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages. Our codes are available at \\\\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e3d85d3bc33df6728f45d00227c30fe131b595d9.pdf'}, 'supplementary_material': {'value': '/attachment/aeb7374cb0032635f43721b9aba30963b4ee8076.zip'}, '_bibtex': {'value': '@inproceedings{\\nko2024learning,\\ntitle={Learning Infinitesimal Generators of Continuous Symmetries from Data},\\nauthor={Gyeonghoon Ko and Hyunsu Kim and Juho Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wl44W8xpc7}\\n}'}, 'paperhash': {'value': 'ko|learning_infinitesimal_generators_of_continuous_symmetries_from_data'}},forum = 'wl44W8xpc7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9069/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9069/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9069/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9069/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wkwGedn19x',number = 1564,cdate = 1714688511671,pdate = 1727287664776,odate = 1730873849926,mdate = 1730873849950,tcdate = 1714688511671,tmdate = 1730873849950,ddate = None,content = {'title': {'value': 'Scaling White-Box Transformers for Vision'}, 'authors': {'value': ['Jinrui Yang', 'Xianhang Li', 'Druv Pai', 'Yuyin Zhou', 'Yi Ma', 'Yaodong Yu', 'Cihang Xie']}, 'authorids': {'value': ['~Jinrui_Yang2', '~Xianhang_Li1', '~Druv_Pai1', '~Yuyin_Zhou1', '~Yi_Ma4', '~Yaodong_Yu4', '~Cihang_Xie3']}, 'keywords': {'value': ['white-box deep neural networks', 'representation learning', 'transformer', 'sparse coding', 'scaling']}, 'abstract': {'value': 'CRATE, a white-box transformer architecture designed to learn compressed and sparse representations, offers an intriguing alternative to standard vision transformers (ViTs) due to its inherent mathematical interpretability. Despite extensive investigations into the scaling behaviors of language and vision transformers, the scalability of CRATE remains an open question which this paper aims to address. \\nSpecifically, we propose CRATE-$\\\\alpha$, featuring strategic yet minimal modifications to the sparse coding block in the CRATE architecture design, and a light training recipe designed to improve the scalability of CRATE.\\nThrough extensive experiments, we demonstrate that CRATE-$\\\\alpha$ can effectively scale with larger model sizes and datasets. \\nFor example, our CRATE-$\\\\alpha$-B substantially outperforms the prior best CRATE-B model accuracy on ImageNet classification by 3.7%, achieving an accuracy of 83.2%. Meanwhile, when scaling further, our CRATE-$\\\\alpha$-L obtains an ImageNet classification accuracy of 85.1%. More notably, these model performance improvements are achieved while preserving, and potentially even enhancing the interpretability of learned CRATE models, as we demonstrate through showing that the learned token representations of increasingly larger trained CRATE-$\\\\alpha$ models yield increasingly higher-quality unsupervised object segmentation of images.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/640c6525356d48f59d0459992a3f5c3432d7955b.pdf'}, 'TLDR': {'value': 'We propose methods to scale white-box, transformer-like deep network architectures.'}, '_bibtex': {'value': '@inproceedings{\\nyang2024scaling,\\ntitle={Scaling White-Box Transformers for Vision},\\nauthor={Jinrui Yang and Xianhang Li and Druv Pai and Yuyin Zhou and Yi Ma and Yaodong Yu and Cihang Xie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wkwGedn19x}\\n}'}, 'supplementary_material': {'value': '/attachment/9607b591487ef91382d20dc2c8ff1691fb320a1f.zip'}, 'paperhash': {'value': 'yang|scaling_whitebox_transformers_for_vision'}},forum = 'wkwGedn19x',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1564/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1564/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1564/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1564/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wjbTHLUSzU',number = 14642,cdate = 1715752030911,pdate = 1727288075394,odate = 1730873967409,mdate = 1730873967429,tcdate = 1715752030911,tmdate = 1730873967429,ddate = None,content = {'title': {'value': 'TSDS: Data Selection for Task-Specific Model Finetuning'}, 'authors': {'value': ['Zifan Liu', 'Amin Karbasi', 'Theodoros Rekatsinas']}, 'authorids': {'value': ['~Zifan_Liu2', '~Amin_Karbasi3', '~Theodoros_Rekatsinas2']}, 'keywords': {'value': ['data selection', 'finetuning', 'foundation model', 'large language model', 'optimal transport']}, 'abstract': {'value': 'Finetuning foundation models for specific tasks is an emerging paradigm in modern machine learning. The efficacy of task-specific finetuning largely depends on the selection of appropriate training data. We present TSDS (Task-Specific Data Selection), a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task. To do so, we formulate data selection for task-specific finetuning as an optimization problem with a distribution alignment loss based on optimal transport to capture the discrepancy between the selected data and the target distribution. In addition, we add a regularizer to encourage the diversity of the selected data and incorporate kernel density estimation into the regularizer to reduce the negative effects of near-duplicates among the candidate data.\\nWe connect our optimization problem to nearest neighbor search and design efficient algorithms to compute the optimal solution based on approximate nearest neighbor search techniques.\\nWe evaluate our method on data selection for both continued pretraining and instruction tuning of language models.\\nWe show that instruction tuning using data selected by our method with a 1\\\\% selection ratio often outperforms using the full dataset and beats the baseline selection methods by 1.5 points in F1 score on average.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present a framework to select data for task-specific model finetuning, guided by a small but representative set of examples from the target task.'}, 'pdf': {'value': '/pdf/dd38be7e2aeaa0617ca80c18333fe34e51c4dcb6.pdf'}, 'supplementary_material': {'value': '/attachment/ff8e18ce48f0707d2b3422190a6e6fc6a9117f67.zip'}, '_bibtex': {'value': '@inproceedings{\\nliu2024tsds,\\ntitle={{TSDS}: Data Selection for Task-Specific Model Finetuning},\\nauthor={Zifan Liu and Amin Karbasi and Theodoros Rekatsinas},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wjbTHLUSzU}\\n}'}, 'paperhash': {'value': 'liu|tsds_data_selection_for_taskspecific_model_finetuning'}},forum = 'wjbTHLUSzU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14642/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14642/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14642/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14642/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wiMaws0FWB',number = 11582,cdate = 1715706942822,pdate = 1727287975599,odate = 1730873939938,mdate = 1730873939950,tcdate = 1715706942822,tmdate = 1730873939950,ddate = None,content = {'title': {'value': 'Implicit Bias of Mirror Flow on Separable Data'}, 'authors': {'value': ['Scott Pesme', 'Radu-Alexandru Dragomir', 'Nicolas Flammarion']}, 'authorids': {'value': ['~Scott_Pesme1', '~Radu-Alexandru_Dragomir1', '~Nicolas_Flammarion1']}, 'keywords': {'value': ['Implicit bias', 'Mirror descent', 'Classification']}, 'TLDR': {'value': 'We provide the implicit bias of mirror flow in the classification setting.'}, 'abstract': {'value': 'We examine the continuous-time counterpart of mirror descent, namely mirror flow, on classification problems which are linearly separable. Such problems are minimised ‘at infinity’ and have many possible solutions; we study which solution is preferred by the algorithm depending on the mirror potential. For exponential tailed losses and under mild assumptions on the potential, we show that the iterates converge in direction towards a $\\\\phi_\\\\infty$-maximum margin classifier. The function $\\\\phi_\\\\infty$ is the horizon function of the mirror potential and characterises its shape ‘at infinity’. When the potential is separable, a simple formula allows to compute this function. We analyse several examples of potentials and provide numerical experiments highlighting our results.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/eca2a88950a4c4612135cbb0fbad857b5b6af6af.pdf'}, '_bibtex': {'value': '@inproceedings{\\npesme2024implicit,\\ntitle={Implicit Bias of Mirror Flow on Separable Data},\\nauthor={Scott Pesme and Radu-Alexandru Dragomir and Nicolas Flammarion},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wiMaws0FWB}\\n}'}, 'paperhash': {'value': 'pesme|implicit_bias_of_mirror_flow_on_separable_data'}},forum = 'wiMaws0FWB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11582/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11582/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11582/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11582/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wiK6bwuxjE',number = 5772,cdate = 1715568883238,pdate = 1727287794719,odate = 1730873888121,mdate = 1730873888141,tcdate = 1715568883238,tmdate = 1730873888141,ddate = None,content = {'title': {'value': 'MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders'}, 'authors': {'value': ['Xueying Jiang', 'Sheng Jin', 'Xiaoqin Zhang', 'Ling Shao', 'Shijian Lu']}, 'authorids': {'value': ['~Xueying_Jiang1', '~Sheng_Jin3', '~Xiaoqin_Zhang4', '~Ling_Shao1', '~Shijian_Lu1']}, 'keywords': {'value': ['Monocular 3D Object Detection', 'Masked Autoencoders']}, 'TLDR': {'value': 'This paper presents MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue in monocular 3D object detection by masking and reconstructing objects in the feature space.'}, 'abstract': {'value': 'Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed feature-space occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/525c91ccdb8e051ae4ee3dc5b9a7bbac283b9be6.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiang2024monomae,\\ntitle={Mono{MAE}: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders},\\nauthor={Xueying Jiang and Sheng Jin and Xiaoqin Zhang and Ling Shao and Shijian Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wiK6bwuxjE}\\n}'}, 'paperhash': {'value': 'jiang|monomae_enhancing_monocular_3d_detection_through_depthaware_masked_autoencoders'}},forum = 'wiK6bwuxjE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5772/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5772/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5772/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5772/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'wiEHZSV15I',number = 4225,cdate = 1715388493921,pdate = 1727287744321,odate = 1730873873810,mdate = 1736172486004,tcdate = 1715388493921,tmdate = 1736172486004,ddate = None,content = {'title': {'value': 'Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting'}, 'authors': {'value': ['Jinliang Deng', 'Feiyang Ye', 'Du Yin', 'Xuan Song', 'Ivor Tsang', 'Hui Xiong']}, 'authorids': {'value': ['~Jinliang_Deng1', '~Feiyang_Ye4', '~Du_Yin1', '~Xuan_Song2', '~Ivor_Tsang1', '~Hui_Xiong1']}, 'keywords': {'value': ['Time Series Forecasting', 'Spatial-temporal Prediction', 'Decomposition']}, 'abstract': {'value': 'Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches. While longer sequences inherently offer richer information for enhanced predictive precision, prevailing studies often respond by escalating model complexity. These intricate models can inflate into millions of parameters, resulting in prohibitive parameter scales. Our study demonstrates, through both theoretical and empirical evidence, that decomposition is key to containing excessive model inflation while achieving uniformly superior and robust results across various datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of time series data, our proposed model outperforms existing benchmarks, using over 99\\\\% fewer parameters than the majority of competing methods. Through this work, we aim to unleash the power of a restricted set of parameters by capitalizing on domain characteristics—a timely reminder that in the realm of LTSF, bigger is not invariably better. The code is available at \\\\url{https://anonymous.4open.science/r/SSCNN-321D/}.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/948d95efe5e6afae5d29dfdcaa09d2ce2b04a7bc.pdf'}, 'supplementary_material': {'value': '/attachment/184d86b85600e98dcf3d662230d67da38288eca5.zip'}, '_bibtex': {'value': '@inproceedings{\\ndeng2024parsimony,\\ntitle={Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting},\\nauthor={Jinliang Deng and Feiyang Ye and Du Yin and Xuan Song and Ivor Tsang and Hui Xiong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wiEHZSV15I}\\n}'}, 'paperhash': {'value': 'deng|parsimony_or_capability_decomposition_delivers_both_in_longterm_time_series_forecasting'}},forum = 'wiEHZSV15I',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4225/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4225/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4225/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4225/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wgpmDyJgsg',number = 12647,cdate = 1715725726352,pdate = 1727288013222,odate = 1730873950701,mdate = 1730873950713,tcdate = 1715725726352,tmdate = 1730873950713,ddate = None,content = {'title': {'value': 'Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis'}, 'authors': {'value': ['Qitao Zhao', 'Shubham Tulsiani']}, 'authorids': {'value': ['~Qitao_Zhao1', '~Shubham_Tulsiani1']}, 'keywords': {'value': ['computer vision', '3D vision', '3D reconstruction', 'camera pose estimation', 'analysis-by-synthesis', 'generative model']}, 'abstract': {'value': \"Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9deb0cefa84b633dd45b98a2c28dfa4cb9a5847d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024sparseview,\\ntitle={Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis},\\nauthor={Qitao Zhao and Shubham Tulsiani},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wgpmDyJgsg}\\n}'}, 'paperhash': {'value': 'zhao|sparseview_pose_estimation_and_reconstruction_via_analysis_by_generative_synthesis'}},forum = 'wgpmDyJgsg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12647/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12647/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12647/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12647/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wfU2CdgmWt',number = 15247,cdate = 1715758473648,pdate = 1727288092266,odate = 1730873971503,mdate = 1730873971521,tcdate = 1715758473648,tmdate = 1730873971521,ddate = None,content = {'title': {'value': 'Stochastic Optimal Control Matching'}, 'authors': {'value': ['Carles Domingo-Enrich', 'Jiequn Han', 'Brandon Amos', 'Joan Bruna', 'Ricky T. Q. Chen']}, 'authorids': {'value': ['~Carles_Domingo-Enrich1', '~Jiequn_Han1', '~Brandon_Amos1', '~Joan_Bruna1', '~Ricky_T._Q._Chen1']}, 'keywords': {'value': ['Stochastic Optimal Control', 'Diffusion']}, 'TLDR': {'value': 'We propose the first least squares loss to solve Stochastic Optimal Control problems, and show that it outperforms existing losses experimentally.'}, 'abstract': {'value': 'Stochastic optimal control, which has the goal of driving the behavior of noisy systems, is broadly applicable in science, engineering and artificial intelligence. Our work introduces Stochastic Optimal Control Matching (SOCM), a novel Iterative Diffusion Optimization (IDO) technique for stochastic optimal control that stems from the same philosophy as the conditional score matching loss for diffusion models. That is, the control is learned via a least squares problem by trying to fit a matching vector field. The training loss, which is closely connected to the cross-entropy loss, is optimized with respect to both the control function and a family of reparameterization matrices which appear in the matching vector field. The optimization with respect to the reparameterization matrices aims at minimizing the variance of the matching vector field. Experimentally, our algorithm achieves lower error than all the existing IDO techniques for stochastic optimal control for three out of four control problems, in some cases by an order of magnitude. The key idea underlying SOCM is the path-wise reparameterization trick, a novel technique that may be of independent interest.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/05e54e1a0fe05f44b274a196c29c8e974d51908d.pdf'}, 'supplementary_material': {'value': '/attachment/fdf69fe9eca55d989d7ec58b91bd0b134ef726f9.zip'}, '_bibtex': {'value': '@inproceedings{\\ndomingo-enrich2024stochastic,\\ntitle={Stochastic Optimal Control Matching},\\nauthor={Carles Domingo-Enrich and Jiequn Han and Brandon Amos and Joan Bruna and Ricky T. Q. Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wfU2CdgmWt}\\n}'}, 'paperhash': {'value': 'domingoenrich|stochastic_optimal_control_matching'}},forum = 'wfU2CdgmWt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15247/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15247/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15247/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15247/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'weemASPtzg',number = 8013,cdate = 1715650274982,pdate = 1727287865473,odate = 1730873908364,mdate = 1735760737636,tcdate = 1715650274982,tmdate = 1735760737636,ddate = None,content = {'title': {'value': 'Linear Causal Representation Learning from Unknown Multi-node Interventions'}, 'authors': {'value': ['Burak Varıcı', 'Emre Acartürk', 'Karthikeyan Shanmugam', 'Ali Tajer']}, 'authorids': {'value': ['~Burak_Varıcı1', '~Emre_Acartürk1', '~Karthikeyan_Shanmugam1', '~Ali_Tajer1']}, 'keywords': {'value': ['Causal representation learning', 'interventions', 'score-based methods', 'identifiability']}, 'abstract': {'value': 'Despite the multifaceted recent advances in interventional causal representation learning (CRL), they primarily focus on the stylized assumption of single-node interventions. This assumption is not valid in a wide range of applications, and generally, the subset of nodes intervened in an interventional environment is *fully unknown*. This paper focuses on interventional CRL under unknown multi-node (UMN) interventional environments and establishes the first identifiability results for *general* latent causal models (parametric or nonparametric) under stochastic interventions (soft or hard) and linear transformation from the latent to observed space. Specifically, it is established that given sufficiently diverse interventional environments, (i) identifiability *up to ancestors* is possible using only *soft* interventions, and (ii) *perfect* identifiability is possible using *hard* interventions. Remarkably, these guarantees match the best-known results for more restrictive single-node interventions. Furthermore, CRL algorithms are also provided that achieve the identifiability guarantees. A central step in designing these algorithms is establishing the relationships between UMN interventional CRL and score functions associated with the statistical models of different interventional environments. Establishing these relationships also serves as constructive proof of the identifiability guarantees.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/97134ff9ae5f0c497e5980844484a73aa21a38ba.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvar{\\\\i}c{\\\\i}2024linear,\\ntitle={Linear Causal Representation Learning from Unknown Multi-node Interventions},\\nauthor={Burak Var{\\\\i}c{\\\\i} and Emre Acart{\\\\\"u}rk and Karthikeyan Shanmugam and Ali Tajer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=weemASPtzg}\\n}'}, 'TLDR': {'value': 'We prove identifiability results and design algorithms for linear causal representation learning from unknown multi-node stochastic interventions.'}, 'paperhash': {'value': 'varc|linear_causal_representation_learning_from_unknown_multinode_interventions'}},forum = 'weemASPtzg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8013/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wduRaBDRBS',number = 4154,cdate = 1715368393701,pdate = 1727287742235,odate = 1730873873055,mdate = 1730873873075,tcdate = 1715368393701,tmdate = 1730873873075,ddate = None,content = {'title': {'value': 'Video Token Merging for Long Video Understanding'}, 'authors': {'value': ['Seon-Ho Lee', 'Jue Wang', 'Zhikang Zhang', 'David Fan', 'Xinyu Li']}, 'authorids': {'value': ['~Seon-Ho_Lee1', '~Jue_Wang8', '~Zhikang_Zhang1', '~David_Fan2', '~Xinyu_Li4']}, 'keywords': {'value': ['Long video understanding', 'token merging']}, 'abstract': {'value': 'As the scale of data and models for video understanding rapidly expand, handling long-form video input in transformer-based models presents a practical challenge. Rather than resorting to input sampling or token dropping, which may result in information loss, token merging shows promising results when used in collaboration with transformers. However, the application of token merging for long-form video processing is not trivial. We begin with the premise that token merging should not rely solely on the similarity of video tokens; the saliency of tokens should also be considered. To address this, we explore various video token merging strategies for long-form video classification, starting with a simple extension of image token merging, moving to region-concentrated merging, and finally proposing a learnable video token merging (VTM) algorithm that dynamically merges tokens based on their saliency. Extensive experimental results show that we achieve better or comparable performances on the LVU, COIN, and Breakfast datasets. Moreover, our approach significantly reduces memory costs by 84% and boosts throughput by approximately 6.89 times compared to baseline algorithms.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/02ca725ea4f0f64f7c82a9d5389359bf6b97e1bf.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024video,\\ntitle={Video Token Merging for Long Video Understanding},\\nauthor={Seon-Ho Lee and Jue Wang and Zhikang Zhang and David Fan and Xinyu Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wduRaBDRBS}\\n}'}, 'paperhash': {'value': 'lee|video_token_merging_for_long_video_understanding'}},forum = 'wduRaBDRBS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4154/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4154/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4154/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4154/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wdGvRud1LS',number = 6942,cdate = 1715608421632,pdate = 1727287829791,odate = 1730873897857,mdate = 1734648000679,tcdate = 1715608421632,tmdate = 1734648000679,ddate = None,content = {'title': {'value': 'Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios'}, 'authors': {'value': ['Shihan Ma', 'Bo Hu', 'Tianyu Jia', 'Alexander Kenneth Clarke', 'Blanka Zicher', 'Arnault H. Caillet', 'Dario Farina', 'Jose C Principe']}, 'authorids': {'value': ['~Shihan_Ma2', '~Bo_Hu3', '~Tianyu_Jia2', '~Alexander_Kenneth_Clarke1', '~Blanka_Zicher1', '~Arnault_H._Caillet1', '~Dario_Farina1', '~Jose_C_Principe1']}, 'keywords': {'value': ['EEG-EMG fusion', 'statistical dependence', 'orthonormal decomposition', 'cortico-muscular connectivity']}, 'abstract': {'value': 'The cortico-spinal neural pathway is fundamental for motor control and movement execution, and in humans it is typically studied using concurrent electroencephalography (EEG) and electromyography (EMG) recordings. However, current approaches for capturing high-level and contextual connectivity between these recordings have important limitations. Here, we present a novel application of statistical dependence estimators based on orthonormal decomposition of density ratios to model the relationship between cortical and muscle oscillations. Our method extends from traditional scalar-valued measures by learning eigenvalues, eigenfunctions, and projection spaces of density ratios from realizations of the signal, addressing the interpretability, scalability, and local temporal dependence of cortico-muscular connectivity. We experimentally demonstrate that eigenfunctions learned from cortico-muscular connectivity can accurately classify movements and subjects. Moreover, they reveal channel and temporal dependencies that confirm the activation of specific EEG channels during movement.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3bdba64a512929e72c61ef5333a70f8c11950c8b.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nma2024learning,\\ntitle={Learning Cortico-Muscular Dependence through Orthonormal Decomposition of Density Ratios},\\nauthor={Shihan Ma and Bo Hu and Tianyu Jia and Alexander Kenneth Clarke and Blanka Zicher and Arnault H. Caillet and Dario Farina and Jose C Principe},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wdGvRud1LS}\\n}'}, 'paperhash': {'value': 'ma|learning_corticomuscular_dependence_through_orthonormal_decomposition_of_density_ratios'}},forum = 'wdGvRud1LS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6942/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6942/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6942/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission6942/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'wcxHbAY8B3',number = 3674,cdate = 1715314402909,pdate = 1727287727670,odate = 1730873868615,mdate = 1730873868627,tcdate = 1715314402909,tmdate = 1730873868627,ddate = None,content = {'title': {'value': 'GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting'}, 'authors': {'value': ['Xiufeng Huang', 'Ruiqi Li', 'Yiu-ming Cheung', 'Ka Chun Cheung', 'Simon See', 'Renjie Wan']}, 'authorids': {'value': ['~Xiufeng_Huang1', '~Ruiqi_Li7', '~Yiu-ming_Cheung1', '~Ka_Chun_Cheung1', '~Simon_See1', '~Renjie_Wan1']}, 'keywords': {'value': ['3D Gaussian splatting; Uncertainty estimation; Digital watermarking;']}, 'abstract': {'value': '3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D assets. To protect the copyright of these assets, digital watermarking techniques can be applied to embed ownership information discreetly within 3DGS mod- els. However, existing watermarking methods for meshes, point clouds, and implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS models use explicit 3D Gaussians with distinct structures and do not rely on neural networks. Naively embedding the watermark on a pre-trained 3DGS can cause obvious distortion in rendered images. In our work, we propose an uncertainty- based method that constrains the perturbation of model parameters to achieve invisible watermarking for 3DGS. At the message decoding stage, the copyright messages can be reliably extracted from both 3D Gaussians and 2D rendered im- ages even under various forms of 3D and 2D distortions. We conduct extensive experiments on the Blender, LLFF, and MipNeRF-360 datasets to validate the effectiveness of our proposed method, demonstrating state-of-the-art performance on both message decoding accuracy and view synthesis quality.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/430c524014071593c5a9851aef0295f14993f2e5.pdf'}, 'supplementary_material': {'value': '/attachment/3a668841f53ebde2b56d6c6b00618ca5bddb4a8d.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024gaussianmarker,\\ntitle={GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting},\\nauthor={Xiufeng Huang and Ruiqi Li and Yiu-ming Cheung and Ka Chun Cheung and Simon See and Renjie Wan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wcxHbAY8B3}\\n}'}, 'paperhash': {'value': 'huang|gaussianmarker_uncertaintyaware_copyright_protection_of_3d_gaussian_splatting'}},forum = 'wcxHbAY8B3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3674/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3674/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3674/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3674/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wcX04Wn34u',number = 4652,cdate = 1715436692507,pdate = 1727287756692,odate = 1730873877794,mdate = 1730873877812,tcdate = 1715436692507,tmdate = 1730873877812,ddate = None,content = {'title': {'value': 'LiT: Unifying LiDAR \"Languages\" with LiDAR Translator'}, 'authors': {'value': ['Yixing Lao', 'Tao Tang', 'Xiaoyang Wu', 'Peng Chen', 'Kaicheng Yu', 'Hengshuang Zhao']}, 'authorids': {'value': ['~Yixing_Lao1', '~Tao_Tang4', '~Xiaoyang_Wu1', '~Peng_Chen9', '~Kaicheng_Yu1', '~Hengshuang_Zhao2']}, 'keywords': {'value': ['LiDAR Translation', 'Domain Unification', '3D Detection']}, 'TLDR': {'value': 'LiT, the LiDAR Translator, directly translates point clouds across domains, unifying diverse LiDAR datasets to support scalable cross-domain and multi-domain learning for autonomous driving.'}, 'abstract': {'value': 'LiDAR data exhibits significant domain gaps due to variations in sensors, vehicles, and driving environments, creating “language barriers” that limit the effective use of data across domains and the scalability of LiDAR perception models. To address these challenges, we introduce the LiDAR Translator (LiT), a framework that directly translates LiDAR data across domains, enabling both cross-domain adaptation and multi-domain joint learning. LiT integrates three key components: a scene modeling module for precise foreground and background reconstruction, a LiDAR modeling module that models LiDAR rays statistically and simulates ray-drop, and a fast, hardware-accelerated ray casting engine. LiT enables state-of-the-art zero-shot and unified domain detection across diverse LiDAR datasets, marking a step toward data-driven domain unification for autonomous driving systems. Source code and demos are available at: https://yxlao.github.io/lit.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b9f11e717fae53a1228a5b9c208bb323f8080693.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nlao2024lit,\\ntitle={LiT: Unifying Li{DAR} ''Languages'' with Li{DAR} Translator},\\nauthor={Yixing Lao and Tao Tang and Xiaoyang Wu and Peng Chen and Kaicheng Yu and Hengshuang Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wcX04Wn34u}\\n}\"}, 'paperhash': {'value': 'lao|lit_unifying_lidar_languages_with_lidar_translator'}},forum = 'wcX04Wn34u',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4652/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4652/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4652/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4652/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wblxm5zdkE',number = 9274,cdate = 1715677664919,pdate = 1727287905847,odate = 1730873919395,mdate = 1735388274810,tcdate = 1715677664919,tmdate = 1735388274810,ddate = None,content = {'title': {'value': 'Real-Time Selection Under General Constraints via Predictive Inference'}, 'authors': {'value': ['Yuyang Huo', 'Lin Lu', 'Haojie Ren', 'Changliang Zou']}, 'authorids': {'value': ['~Yuyang_Huo1', '~Lin_Lu2', '~Haojie_Ren1', '~Changliang_Zou2']}, 'keywords': {'value': ['Online multiple testing; Predictive inference; False selection rate; Individual and interactive constraints; Local false discovery rate.']}, 'TLDR': {'value': 'We address online sample selection, introducing II-COS, a decision rule that efficiently identifies preferable samples meeting practical requirements by managing individual and interactive constraints.'}, 'abstract': {'value': 'Real-time decision-making gets more attention in the big data era. Here, we consider the problem of sample selection in the online setting, where one encounters a possibly infinite sequence of individuals collected over time with covariate information available. The goal is to select samples of interest that are characterized by their unobserved responses until the user-specified stopping time. We derive a new decision rule that enables us to find more preferable samples that meet practical requirements by simultaneously controlling two types of general constraints: individual and interactive constraints, which include the widely utilized False Selection Rate (FSR), cost limitations, and diversity of selected samples. The key elements of our approach involve quantifying the uncertainty of response predictions via predictive inference and addressing individual and interactive constraints in a sequential manner. Theoretical and numerical results demonstrate the effectiveness of the proposed method in controlling both individual and interactive constraints.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0de67725a73ec05f810a39fe0a220b3ca19c7aa0.pdf'}, 'supplementary_material': {'value': '/attachment/ce846f83458cfd12a3b676105f74908abaae3444.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuo2024realtime,\\ntitle={Real-Time Selection Under General Constraints via Predictive Inference},\\nauthor={Yuyang Huo and Lin Lu and Haojie Ren and Changliang Zou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wblxm5zdkE}\\n}'}, 'paperhash': {'value': 'huo|realtime_selection_under_general_constraints_via_predictive_inference'}},forum = 'wblxm5zdkE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9274/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9274/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9274/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9274/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wbE0QCBWji',number = 16554,cdate = 1715771857739,pdate = 1727288129636,odate = 1730873979786,mdate = 1736782397128,tcdate = 1715771857739,tmdate = 1736782397128,ddate = None,content = {'title': {'value': 'Constructing Semantics-Aware Adversarial Examples with a Probabilistic Perspective'}, 'authors': {'value': ['Andi Zhang', 'Mingtian Zhang', 'Damon Wischik']}, 'authorids': {'value': ['~Andi_Zhang2', '~Mingtian_Zhang1', '~Damon_Wischik1']}, 'keywords': {'value': ['Adversarial Examples', 'Probabilistic Generative Models', 'Diffusion Models', 'Energy-based Models']}, 'abstract': {'value': \"We propose a probabilistic perspective on adversarial examples, allowing us to embed subjective understanding of semantics as a distribution into the process of generating adversarial examples, in a principled manner. Despite significant pixel-level modifications compared to traditional adversarial attacks, our method preserves the overall semantics of the image, making the changes difficult for humans to detect. This extensive pixel-level modification enhances our method's ability to deceive classifiers designed to defend against adversarial attacks. Our empirical findings indicate that the proposed methods achieve higher success rates in circumventing adversarial defense mechanisms, while remaining difficult for human observers to detect.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ece2805ee40a3e53c2a7cddbb3f60d6c1d2e1619.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024constructing,\\ntitle={Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective},\\nauthor={Andi Zhang and Mingtian Zhang and Damon Wischik},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wbE0QCBWji}\\n}'}, 'paperhash': {'value': 'zhang|constructing_semanticsaware_adversarial_examples_with_a_probabilistic_perspective'}},forum = 'wbE0QCBWji',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16554/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16554/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16554/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16554/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'waQ5X4qc3W',number = 19831,cdate = 1715793001375,pdate = 1727288219146,odate = 1730873997956,mdate = 1730873997973,tcdate = 1715793001375,tmdate = 1730873997973,ddate = None,content = {'title': {'value': 'Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective'}, 'authors': {'value': ['Yongxin Zhu', 'Bocheng Li', 'Hang Zhang', 'Xin Li', 'Linli Xu', 'Lidong Bing']}, 'authorids': {'value': ['~Yongxin_Zhu1', '~Bocheng_Li1', '~Hang_Zhang6', '~Xin_Li40', '~Linli_Xu1', '~Lidong_Bing2']}, 'keywords': {'value': ['Latent space', 'Image sequential modeling', 'Image tokenizer']}, 'abstract': {'value': 'Latent-based image generative models, such as Latent Diffusion Models (LDMs) and Mask Image Models (MIMs), have achieved notable success in image generation tasks. These models typically leverage reconstructive autoencoders like VQGAN or VAE to encode pixels into a more compact latent space and learn the data distribution in the latent space instead of directly from pixels. However, this practice raises a pertinent question: Is it truly the optimal choice? In response, we begin with an intriguing observation: despite sharing the same latent space, autoregressive models significantly lag behind LDMs and MIMs in image generation. This finding contrasts sharply with the field of NLP, where the autoregressive model GPT has established a commanding presence. To address this discrepancy, we introduce a unified perspective on the relationship between latent space and generative models, emphasizing the stability of latent space in image generative modeling. Furthermore, we propose a simple but effective discrete image tokenizer to stabilize the latent space for image generative modeling by applying K-Means on the latent features of self-supervised learning models. Experimental results show that image autoregressive modeling with our tokenizer (DiGIT) benefits both image understanding and image generation with the next token prediction principle, which is inherently straightforward for GPT models but challenging for other generative models. Remarkably, for the first time, a GPT-style autoregressive model for images outperforms LDMs, which also exhibits substantial improvement akin to GPT when scaling up model size. Our findings underscore the potential of an optimized latent space and the integration of discrete tokenization in advancing the capabilities of image generative models. The code is available at \\\\url{https://github.com/DAMO-NLP-SG/DiGIT}.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fc5f5e9cc6e5aa9d1caa81081a8fd3e0b1c5f218.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024stabilize,\\ntitle={Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective},\\nauthor={Yongxin Zhu and Bocheng Li and Hang Zhang and Xin Li and Linli Xu and Lidong Bing},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=waQ5X4qc3W}\\n}'}, 'paperhash': {'value': 'zhu|stabilize_the_latent_space_for_image_autoregressive_modeling_a_unified_perspective'}},forum = 'waQ5X4qc3W',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19831/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19831/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19831/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19831/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wZigMVFURk',number = 1429,cdate = 1714630286356,pdate = 1727287660864,odate = 1730873848859,mdate = 1736827639041,tcdate = 1714630286356,tmdate = 1736827639041,ddate = None,content = {'title': {'value': 'RoPINN: Region Optimized Physics-Informed Neural Networks'}, 'authors': {'value': ['Haixu Wu', 'Huakun Luo', 'Yuezhou Ma', 'Jianmin Wang', 'Mingsheng Long']}, 'authorids': {'value': ['~Haixu_Wu1', '~Huakun_Luo1', '~Yuezhou_Ma1', '~Jianmin_Wang1', '~Mingsheng_Long5']}, 'keywords': {'value': ['Physics-informed Neural Networks', 'PINN Training', 'Deep Learning']}, 'abstract': {'value': 'Physics-informed neural networks (PINNs) have been widely applied to solve partial differential equations (PDEs) by enforcing outputs and gradients of deep models to satisfy target equations. Due to the limitation of numerical computation, PINNs are conventionally optimized on finite selected points. However, since PDEs are usually defined on continuous domains, solely optimizing models on scattered points may be insufficient to obtain an accurate solution for the whole domain. To mitigate this inherent deficiency of the default scatter-point optimization, this paper proposes and theoretically studies a new training paradigm as region optimization. Concretely, we propose to extend the optimization process of PINNs from isolated points to their continuous neighborhood regions, which can theoretically decrease the generalization error, especially for hidden high-order constraints of PDEs. A practical training algorithm, Region Optimized PINN (RoPINN), is seamlessly derived from this new paradigm, which is implemented by a straightforward but effective Monte Carlo sampling method. By calibrating the sampling process into trust regions, RoPINN finely balances optimization and generalization error. Experimentally, RoPINN consistently boosts the performance of diverse PINNs on a wide range of PDEs without extra backpropagation or gradient calculation. Code is available at this repository: https://github.com/thuml/RoPINN.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper proposes and theoretically studies a new training paradigm as region optimization. RoPINN is derived from the theory as a practical training algorithm, which can consistently benefit diverse PINN backbones on extensive PDEs.'}, 'pdf': {'value': '/pdf/88a370c2bd8305f59706ceb1247e978608e578ff.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024ropinn,\\ntitle={Ro{PINN}: Region Optimized Physics-Informed Neural Networks},\\nauthor={Haixu Wu and Huakun Luo and Yuezhou Ma and Jianmin Wang and Mingsheng Long},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wZigMVFURk}\\n}'}, 'paperhash': {'value': 'wu|ropinn_region_optimized_physicsinformed_neural_networks'}},forum = 'wZigMVFURk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1429/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1429/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1429/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1429/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wZgw4CrxwK',number = 20935,cdate = 1715799179751,pdate = 1727288245192,odate = 1730874003908,mdate = 1736899671328,tcdate = 1715799179751,tmdate = 1736899671328,ddate = None,content = {'title': {'value': 'Incentivizing Quality Text Generation via Statistical Contracts'}, 'authors': {'value': ['Eden Saig', 'Ohad Einav', 'Inbal Talgam-Cohen']}, 'authorids': {'value': ['~Eden_Saig1', '~Ohad_Einav1', '~Inbal_Talgam-Cohen2']}, 'keywords': {'value': ['Contract Theory', 'Contract Design', 'Moral Hazard', 'Natural Language Generation', 'LLM evaluation', 'Hypothesis Testing']}, 'TLDR': {'value': 'We design pay-for-performance contracts that incentivize the use of high-quality LLMs for text generation.'}, 'abstract': {'value': 'While the success of large language models (LLMs) increases demand for machine-generated text, current pay-per-token pricing schemes create a misalignment of incentives known in economics as moral hazard: Text-generating agents have strong incentive to cut costs by preferring a cheaper model over the cutting-edge one, and this can be done “behind the scenes” since the agent performs inference internally. In this work, we approach this issue from an economic perspective, by proposing a pay-for-performance, contract-based framework for incentivizing quality. We study a principal-agent game where the agent generates text using costly inference, and the contract determines the principal’s payment for the text according to an automated quality evaluation. Since standard contract theory is inapplicable when internal inference costs are unknown, we introduce cost-robust contracts. As our main theoretical contribution, we characterize optimal cost-robust contracts through a direct correspondence to optimal composite hypothesis tests from statistics, generalizing a result of Saig et al. (NeurIPS’23). We evaluate our framework empirically by deriving contracts for a range of objectives and LLM evaluation benchmarks, and find that cost-robust contracts sacrifice only a marginal increase in objective value compared to their cost-aware counterparts.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1fdf8e5ba7d29e7866528df491d68f17a90f0e68.pdf'}, 'supplementary_material': {'value': '/attachment/1d1c6a71a59af8f798d2cf33631637e3dd64117a.zip'}, '_bibtex': {'value': '@inproceedings{\\nsaig2024incentivizing,\\ntitle={Incentivizing Quality Text Generation via Statistical Contracts},\\nauthor={Eden Saig and Ohad Einav and Inbal Talgam-Cohen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wZgw4CrxwK}\\n}'}, 'paperhash': {'value': 'saig|incentivizing_quality_text_generation_via_statistical_contracts'}},forum = 'wZgw4CrxwK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20935/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20935/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20935/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20935/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wWyumwEYV8',number = 6065,cdate = 1715582265076,pdate = 1727287804084,odate = 1730873890349,mdate = 1730873890360,tcdate = 1715582265076,tmdate = 1730873890360,ddate = None,content = {'title': {'value': 'A Sober Look at the Robustness of CLIPs to Spurious Features'}, 'authors': {'value': ['Qizhou Wang', 'Yong Lin', 'Yongqiang Chen', 'Ludwig Schmidt', 'Bo Han', 'Tong Zhang']}, 'authorids': {'value': ['~Qizhou_Wang1', '~Yong_Lin2', '~Yongqiang_Chen1', '~Ludwig_Schmidt1', '~Bo_Han1', '~Tong_Zhang2']}, 'keywords': {'value': ['CLIP', 'Distribution Shift']}, 'abstract': {'value': 'Large vision language models, such as CLIP, demonstrate impressive robustness to spurious features than single-modal models trained on ImageNet. However, existing test datasets are typically curated based on ImageNet-trained models, which aim to capture the spurious features inherited in ImageNet. Benchmarking CLIP models based on the ImageNet-oriented spurious features may not be sufficient to reflect the extent to which CLIP models are robust to spurious correlations within CLIP training data, e.g., LAION. To this end, we craft a new challenging dataset named CounterAnimal designed to reveal the reliance of CLIP models on realistic spurious features. Specifically, we split animal photos into groups according to the backgrounds, and then identify a pair of groups for each class where a CLIP model shows high-performance drops across the two groups. Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models. We provide theoretical insights that the CLIP objective cannot offer additional robustness. Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data. We find that they still help mitigate the spurious features, providing a promising path for future developments.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/416b6767b5924fc0e5fe05a6729b748f4fdecdc6.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwang2024a,\\ntitle={A Sober Look at the Robustness of {CLIP}s to Spurious Features},\\nauthor={Qizhou Wang and Yong Lin and Yongqiang Chen and Ludwig Schmidt and Bo Han and Tong Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wWyumwEYV8}\\n}'}, 'paperhash': {'value': 'wang|a_sober_look_at_the_robustness_of_clips_to_spurious_features'}},forum = 'wWyumwEYV8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6065/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6065/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6065/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission6065/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wWiAR5mqXq',number = 9469,cdate = 1715680308841,pdate = 1727287912059,odate = 1730873920986,mdate = 1735923533521,tcdate = 1715680308841,tmdate = 1735923533521,ddate = None,content = {'title': {'value': 'Reflective Multi-Agent Collaboration based on Large Language Models'}, 'authors': {'value': ['Xiaohe Bo', 'Zeyu Zhang', 'Quanyu Dai', 'Xueyang Feng', 'Lei Wang', 'Rui Li', 'Xu Chen', 'Ji-Rong Wen']}, 'authorids': {'value': ['~Xiaohe_Bo1', '~Zeyu_Zhang6', '~Quanyu_Dai1', '~Xueyang_Feng1', '~Lei_Wang46', '~Rui_Li16', '~Xu_Chen13', '~Ji-Rong_Wen1']}, 'keywords': {'value': ['Large Language Models', 'Multi-Agent Systems', 'Reflection Mechanism']}, 'abstract': {'value': 'Benefiting from the powerful language expression and planning capabilities of Large Language Models (LLMs), LLM-based autonomous agents have achieved promising performance in various downstream tasks. Recently, based on the development of single-agent systems, researchers propose to construct LLM-based multi-agent systems to tackle more complicated tasks. In this paper, we propose a novel framework, named COPPER, to enhance the collaborative capabilities of LLM-based agents with the self-reflection mechanism. To improve the quality of reflections, we propose to fine-tune a shared reflector, which automatically tunes the prompts of actor models using our counterfactual PPO mechanism. On the one hand, we propose counterfactual rewards to assess the contribution of a single agent’s reflection within the system, alleviating the credit assignment problem. On the other hand, we propose to train a shared reflector, which enables the reflector to generate personalized reflections according to agent roles, while reducing the computational resource requirements and improving training stability. We conduct experiments on three datasets to evaluate the performance of our model in multi-hop question answering, mathematics, and chess scenarios. Experimental results show that COPPER possesses stronger reflection capabilities and exhibits excellent generalization performance across different actor models.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3b17b8aba5d866085a47c8258c92406af2fc2e10.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbo2024reflective,\\ntitle={Reflective Multi-Agent Collaboration based on Large Language Models},\\nauthor={Xiaohe Bo and Zeyu Zhang and Quanyu Dai and Xueyang Feng and Lei Wang and Rui Li and Xu Chen and Ji-Rong Wen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wWiAR5mqXq}\\n}'}, 'paperhash': {'value': 'bo|reflective_multiagent_collaboration_based_on_large_language_models'}},forum = 'wWiAR5mqXq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9469/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9469/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9469/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9469/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wWguwYhpAY',number = 5640,cdate = 1715559990275,pdate = 1727287790627,odate = 1730873886730,mdate = 1730873886748,tcdate = 1715559990275,tmdate = 1730873886748,ddate = None,content = {'title': {'value': 'Neural Experts: Mixture of Experts for Implicit Neural Representations'}, 'authors': {'value': ['Yizhak Ben-Shabat', 'Chamin P Hewa Koneputugodage', 'Sameera Ramasinghe', 'Stephen Gould']}, 'authorids': {'value': ['~Yizhak_Ben-Shabat1', '~Chamin_P_Hewa_Koneputugodage1', '~Sameera_Ramasinghe1', '~Stephen_Gould1']}, 'keywords': {'value': ['Implicit Neural Representation', 'Surface Reconstruction', 'Mixture of Experts']}, 'TLDR': {'value': 'We propose a mixture of experts architecture for reconstructing various signals (3D surfaces, images, audio) using a neural representation.'}, 'abstract': {'value': 'Implicit neural representations (INRs) have proven effective in various tasks including image, shape, audio, and video reconstruction. These INRs typically learn the implicit field from sampled input points. This is often done using a single network for the entire domain, imposing many global constraints on a single function. \\nIn this paper, we propose a mixture of experts (MoE) implicit neural representation approach that enables learning local piece-wise continuous functions that simultaneously learns to subdivide the domain and fit it locally. \\nWe show that incorporating a mixture of experts architecture into existing INR formulations provides a boost in speed, accuracy, and memory requirements. Additionally, we introduce novel conditioning and pretraining methods for the gating network that improves convergence to the desired solution. \\nWe evaluate the effectiveness of our approach on multiple reconstruction tasks, including surface reconstruction, image reconstruction, and audio signal reconstruction and show improved performance compared to non-MoE methods. Code is available at our project page https://sitzikbs.github.io/neural-experts-projectpage/ .'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/29a5178e806f04207f02516fcb74d8395ed9af42.pdf'}, 'supplementary_material': {'value': '/attachment/72038e325e14a2f62f6a366c62dc86c907ddec63.zip'}, '_bibtex': {'value': '@inproceedings{\\nben-shabat2024neural,\\ntitle={Neural Experts: Mixture of Experts for Implicit Neural Representations},\\nauthor={Yizhak Ben-Shabat and Chamin P Hewa Koneputugodage and Sameera Ramasinghe and Stephen Gould},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wWguwYhpAY}\\n}'}, 'paperhash': {'value': 'benshabat|neural_experts_mixture_of_experts_for_implicit_neural_representations'}},forum = 'wWguwYhpAY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5640/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5640/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5640/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5640/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wTIzpqX121',number = 9990,cdate = 1715688812141,pdate = 1727287926835,odate = 1730873925214,mdate = 1730873925232,tcdate = 1715688812141,tmdate = 1730873925232,ddate = None,content = {'title': {'value': 'Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks'}, 'authors': {'value': ['Joel Oskarsson', 'Tomas Landelius', 'Marc Peter Deisenroth', 'Fredrik Lindsten']}, 'authorids': {'value': ['~Joel_Oskarsson1', '~Tomas_Landelius1', '~Marc_Peter_Deisenroth1', '~Fredrik_Lindsten1']}, 'keywords': {'value': ['weather forecasting', 'graph neural network', 'probabilistic', 'ensemble forecasting', 'latent variable model', 'earth system modeling']}, 'abstract': {'value': 'In recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a probabilistic graph neural network model for weather forecasting, capturing uncertainty by generating ensemble forecasts.'}, 'pdf': {'value': '/pdf/08914a753f1c9eaf87d2102390cab1b1f7a9663e.pdf'}, '_bibtex': {'value': '@inproceedings{\\noskarsson2024probabilistic,\\ntitle={Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks},\\nauthor={Joel Oskarsson and Tomas Landelius and Marc Peter Deisenroth and Fredrik Lindsten},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wTIzpqX121}\\n}'}, 'paperhash': {'value': 'oskarsson|probabilistic_weather_forecasting_with_hierarchical_graph_neural_networks'}},forum = 'wTIzpqX121',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9990/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9990/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9990/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9990/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wT6GHk5ShC',number = 5383,cdate = 1715527356893,pdate = 1727287782259,odate = 1730873884304,mdate = 1730873884324,tcdate = 1715527356893,tmdate = 1730873884324,ddate = None,content = {'title': {'value': 'Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective'}, 'authors': {'value': ['Xinhao Yao', 'Xiaolin Hu', 'Shenzhi Yang', 'Yong Liu']}, 'authorids': {'value': ['~Xinhao_Yao1', '~Xiaolin_Hu6', '~Shenzhi_Yang2', '~Yong_Liu7']}, 'keywords': {'value': ['Large language models', 'In-Context Learning', 'SVD', 'Theoretical generalization bounds']}, 'abstract': {'value': 'Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities. With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates. In this paper, we show an exciting  phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements than in shallow layers. However, the underlying mechanism of those findings still remains an open question. To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories. This helps us reasonably explain the surprising experimental findings.  Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference. Experiments on benchmark datasets and open source LLMs display the method effectiveness.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We show SVD-based weight pruning boosts in-context learning in large language models, proved by theoretical analysis, and propose an effective, intuitive algorithm for downstream tasks.'}, 'pdf': {'value': '/pdf/2b62bd7805c4971355586b1fc5697d6266237e68.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyao2024enhancing,\\ntitle={Enhancing In-Context Learning Performance with just {SVD}-Based Weight Pruning: A Theoretical Perspective},\\nauthor={Xinhao Yao and Xiaolin Hu and Shenzhi Yang and Yong Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wT6GHk5ShC}\\n}'}, 'paperhash': {'value': 'yao|enhancing_incontext_learning_performance_with_just_svdbased_weight_pruning_a_theoretical_perspective'}},forum = 'wT6GHk5ShC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5383/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5383/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5383/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5383/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wT5AgMVkaJ',number = 5941,cdate = 1715577858466,pdate = 1727287800072,odate = 1730873889529,mdate = 1741893898173,tcdate = 1715577858466,tmdate = 1741893898173,ddate = None,content = {'title': {'value': 'Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms'}, 'authors': {'value': ['Miaosen Zhang', 'Yuxing Wei', 'Zhen Xing', 'Yifei Ma', 'Zuxuan Wu', 'Ji Li', 'Zheng Zhang', 'Qi Dai', 'Chong Luo', 'Xin Geng', 'Baining Guo']}, 'authorids': {'value': ['~Miaosen_Zhang1', '~Yuxing_Wei1', '~Zhen_Xing2', '~Yifei_Ma2', '~Zuxuan_Wu1', '~Ji_Li7', '~Zheng_Zhang4', '~Qi_Dai4', '~Chong_Luo1', '~Xin_Geng1', '~Baining_Guo1']}, 'keywords': {'value': ['Image retrieval', 'Alignment', 'Aesthetics', 'Vision-Language Models']}, 'abstract': {'value': \"Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.  Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Method for aligning visual retrieval model with human aesthetics'}, 'pdf': {'value': '/pdf/ea5f1333d5f234e8c6e2fe92907a8aba4c99a5cb.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzhang2024aligning,\\ntitle={Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms},\\nauthor={Miaosen Zhang and Yuxing Wei and Zhen Xing and Yifei Ma and Zuxuan Wu and Ji Li and Zheng Zhang and Qi Dai and Chong Luo and Xin Geng and Baining Guo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wT5AgMVkaJ}\\n}'}, 'paperhash': {'value': 'zhang|aligning_vision_models_with_human_aesthetics_in_retrieval_benchmarks_and_algorithms'}},forum = 'wT5AgMVkaJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5941/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5941/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5941/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission5941/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wT2TIfHKp8',number = 14628,cdate = 1715751909061,pdate = 1727288075024,odate = 1730873967283,mdate = 1736937064079,tcdate = 1715751909061,tmdate = 1736937064079,ddate = None,content = {'title': {'value': 'Taming the Long Tail in Human Mobility Prediction'}, 'authors': {'value': ['Xiaohang Xu', 'Renhe Jiang', 'Chuang Yang', 'zipei fan', 'Kaoru Sezaki']}, 'authorids': {'value': ['~Xiaohang_Xu1', '~Renhe_Jiang1', '~Chuang_Yang3', '~zipei_fan1', '~Kaoru_Sezaki1']}, 'keywords': {'value': ['Human Mobility', 'Next POI Prediction', 'Long-Tail Learning']}, 'abstract': {'value': \"With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the $\\\\underline{\\\\bf{Lo}}$ng-$\\\\underline{\\\\bf{T}}$ail Adjusted $\\\\underline{\\\\bf{Next}}$ POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works.\"}, 'primary_area': {'value': 'machine_learning_for_social_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A human mobility prediction model for long-tail distribution issue'}, 'pdf': {'value': '/pdf/948e70fa886488a4436315922b773598b84b073d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxu2024taming,\\ntitle={Taming the Long Tail in Human Mobility Prediction},\\nauthor={Xiaohang Xu and Renhe Jiang and Chuang Yang and zipei fan and Kaoru Sezaki},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wT2TIfHKp8}\\n}'}, 'paperhash': {'value': 'xu|taming_the_long_tail_in_human_mobility_prediction'}},forum = 'wT2TIfHKp8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14628/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14628/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14628/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14628/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wT2KhEb97a',number = 3818,cdate = 1715328280126,pdate = 1727287731831,odate = 1730873869825,mdate = 1734681949975,tcdate = 1715328280126,tmdate = 1734681949975,ddate = None,content = {'title': {'value': 'Iterative Methods via Locally Evolving Set Process'}, 'authors': {'value': ['Baojian Zhou', 'Yifan Sun', 'Reza Babanezhad Harikandeh', 'Xingzhi Guo', 'Deqing Yang', 'Yanghua Xiao']}, 'authorids': {'value': ['~Baojian_Zhou2', '~Yifan_Sun1', '~Reza_Babanezhad_Harikandeh1', '~Xingzhi_Guo1', '~Deqing_Yang1', '~Yanghua_Xiao1']}, 'keywords': {'value': ['local computation', 'Personalized PageRank', 'graph clustering']}, 'TLDR': {'value': 'This paper introduces a novel approach to efficiently solve sparse linear systems by localizing standard iterative solvers using a locally evolving set process.'}, 'abstract': {'value': 'Given the damping factor $\\\\alpha$ and precision tolerance $\\\\epsilon$, \\\\citet{andersen2006local} introduced Approximate Personalized PageRank (APPR), the \\\\textit{de facto local method} for approximating the PPR vector, with runtime bounded by $\\\\Theta(1/(\\\\alpha\\\\epsilon))$ independent of the graph size. Recently,  Fountoulakis \\\\& Yang asked whether faster local algorithms could be developed using $\\\\tilde{\\\\mathcal{O}}(1/(\\\\sqrt{\\\\alpha}\\\\epsilon))$ operations. By noticing that APPR is a local variant of Gauss-Seidel, this paper explores the question of *whether standard iterative solvers can be effectively localized*. We propose to use the *locally evolving set process*, a novel framework to characterize the algorithm locality, and demonstrate that many standard solvers can be effectively localized. Let $\\\\overline{\\\\operatorname{vol}}{ (\\\\mathcal S_t)}$ and $\\\\overline{\\\\gamma_t}$ be the running average of volume and the residual ratio of active nodes $\\\\textstyle \\\\mathcal{S_t}$ during the process. We show $\\\\overline{\\\\operatorname{vol}}{ (\\\\mathcal S_t)}/\\\\overline{\\\\gamma_t} \\\\leq 1/\\\\epsilon$ and prove APPR admits a new runtime bound $\\\\tilde{\\\\mathcal{O}}(\\\\overline{\\\\operatorname{vol}}(\\\\mathcal S_t)/(\\\\alpha\\\\overline{\\\\gamma_t}))$ mirroring the actual performance. Furthermore, when the geometric mean of residual reduction is $\\\\Theta(\\\\sqrt{\\\\alpha})$, then there exists $c \\\\in (0,2)$ such that the local Chebyshev method has runtime $\\\\tilde{\\\\mathcal{O}}(\\\\overline{\\\\operatorname{vol}}(\\\\mathcal{S_t})/(\\\\sqrt{\\\\alpha}(2-c)))$ without the monotonicity assumption. Numerical results confirm the efficiency of this novel framework and show up to a hundredfold speedup over corresponding standard solvers on real-world graphs.'}, 'pdf': {'value': '/pdf/018805bdb5e7dfb1133288f180c5012bb6b6e388.pdf'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/bbe9b3fd565b5d607aced0b1f15fafc9b5d76a21.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024iterative,\\ntitle={Iterative Methods via Locally Evolving Set Process},\\nauthor={Baojian Zhou and Yifan Sun and Reza Babanezhad Harikandeh and Xingzhi Guo and Deqing Yang and Yanghua Xiao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wT2KhEb97a}\\n}'}, 'paperhash': {'value': 'zhou|iterative_methods_via_locally_evolving_set_process'}},forum = 'wT2KhEb97a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3818/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3818/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3818/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3818/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wSqpNeMVLU',number = 19954,cdate = 1715793615509,pdate = 1727288222127,odate = 1730873998586,mdate = 1730873998607,tcdate = 1715793615509,tmdate = 1730873998607,ddate = None,content = {'title': {'value': 'A Theoretical Perspective for Speculative Decoding Algorithm'}, 'authors': {'value': ['Ming Yin', 'Minshuo Chen', 'Kaixuan Huang', 'Mengdi Wang']}, 'authorids': {'value': ['~Ming_Yin4', '~Minshuo_Chen1', '~Kaixuan_Huang1', '~Mengdi_Wang1']}, 'keywords': {'value': ['probabilistic analysis', 'theory for large language models']}, 'abstract': {'value': 'Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c4bf0178245b320f82c8dda9da595d767fd77654.pdf'}, 'supplementary_material': {'value': '/attachment/163a83c837eadac3fa32b5b9bdb52a5e2402832d.zip'}, '_bibtex': {'value': '@inproceedings{\\nyin2024a,\\ntitle={A Theoretical Perspective for Speculative Decoding Algorithm},\\nauthor={Ming Yin and Minshuo Chen and Kaixuan Huang and Mengdi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wSqpNeMVLU}\\n}'}, 'paperhash': {'value': 'yin|a_theoretical_perspective_for_speculative_decoding_algorithm'}},forum = 'wSqpNeMVLU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19954/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19954/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19954/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19954/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wSpIdUXZYX',number = 88,cdate = 1713833363060,pdate = 1727287630006,odate = 1730873838212,mdate = 1730873838230,tcdate = 1713833363060,tmdate = 1730873838230,ddate = None,content = {'title': {'value': 'Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs'}, 'authors': {'value': ['Md Ashiqur Rahman', 'Robert Joseph George', 'Mogab Elleithy', 'Daniel Leibovici', 'Zongyi Li', 'Boris Bonev', 'Colin White', 'Julius Berner', 'Raymond A. Yeh', 'Jean Kossaifi', 'Kamyar Azizzadenesheli', 'Anima Anandkumar']}, 'authorids': {'value': ['~Md_Ashiqur_Rahman2', '~Robert_Joseph_George1', '~Mogab_Elleithy1', '~Daniel_Leibovici1', '~Zongyi_Li1', '~Boris_Bonev1', '~Colin_White1', '~Julius_Berner1', '~Raymond_A._Yeh1', '~Jean_Kossaifi1', '~Kamyar_Azizzadenesheli1', '~Anima_Anandkumar1']}, 'keywords': {'value': ['Neural Operator']}, 'abstract': {'value': \"Existing neural operator architectures face challenges when solving multiphysics problems with coupled partial differential equations (PDEs) due to complex geometries, interactions between physical variables, and the limited amounts of high-resolution training data. \\nTo address these issues, we propose *Codomain Attention Neural Operator* (CoDA-NO), which tokenizes functions along the codomain or channel space, enabling self-supervised learning or pretraining of multiple PDE systems. \\nSpecifically, we extend positional encoding, self-attention, and normalization layers to function spaces. CoDA-NO can learn representations of different PDE systems with a single model. We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs over multiple systems by considering few-shot learning settings. On complex downstream tasks with limited data, such as fluid flow simulations, fluid-structure interactions, and Rayleigh-Bénard convection, we found CoDA-NO to outperform existing methods by over 36%.\"}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8f9b484442e75171a47f932c6ba656a806dac2e1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nrahman2024pretraining,\\ntitle={Pretraining Codomain Attention Neural Operators for Solving Multiphysics {PDE}s},\\nauthor={Md Ashiqur Rahman and Robert Joseph George and Mogab Elleithy and Daniel Leibovici and Zongyi Li and Boris Bonev and Colin White and Julius Berner and Raymond A. Yeh and Jean Kossaifi and Kamyar Azizzadenesheli and Anima Anandkumar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wSpIdUXZYX}\\n}'}, 'paperhash': {'value': 'rahman|pretraining_codomain_attention_neural_operators_for_solving_multiphysics_pdes'}},forum = 'wSpIdUXZYX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission88/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission88/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission88/-/Revision', 'NeurIPS.cc/2024/Conference/Submission88/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'wQpNG9JnPK',number = 10491,cdate = 1715694675218,pdate = 1727287942393,odate = 1730873929615,mdate = 1734595179325,tcdate = 1715694675218,tmdate = 1734595179325,ddate = None,content = {'title': {'value': 'Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization'}, 'authors': {'value': ['Zhikang Chen', 'Min Zhang', 'Sen Cui', 'Haoxuan Li', 'Gang Niu', 'Mingming Gong', 'Changshui Zhang', 'Kun Zhang']}, 'authorids': {'value': ['~Zhikang_Chen1', '~Min_Zhang17', '~Sen_Cui1', '~Haoxuan_Li6', '~Gang_Niu1', '~Mingming_Gong1', '~Changshui_Zhang2', '~Kun_Zhang1']}, 'keywords': {'value': ['Spurious Correlation', 'Neural Collapse', 'Out of Distribution']}, 'abstract': {'value': 'The spurious correlation between the background features of the image and its label arises due to that the samples labeled with the same class in the training set often co-occurs with a specific background, which will cause the encoder to extract non-semantic features for classification, resulting in poor out-of-distribution generalization performance. Although many studies have been proposed to address this challenge, the semantic and spurious features are still difficult to accurately decouple from the original image and fail to achieve high performance with deep learning models. This paper proposes a novel perspective inspired by neural collapse to solve the spurious correlation problem through the alternate execution of environment partitioning and learning semantic masks. Specifically, we propose to assign an environment to each sample by learning a local model for each environment and using maximum likelihood probability. At the same time, we require that the learned semantic mask neurally collapses to the same simplex equiangular tight frame (ETF) in each environment after being applied to the original input. We conduct extensive experiments on four datasets, and the results demonstrate that our method significantly improves out-of-distribution performance.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0e5a651b0723810c001303ef89ef83bf1da33e4c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024neural,\\ntitle={Neural Collapse Inspired Feature Alignment for Out-of-Distribution Generalization},\\nauthor={Zhikang Chen and Min Zhang and Sen Cui and Haoxuan Li and Gang Niu and Mingming Gong and Changshui Zhang and Kun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wQpNG9JnPK}\\n}'}, 'paperhash': {'value': 'chen|neural_collapse_inspired_feature_alignment_for_outofdistribution_generalization'}},forum = 'wQpNG9JnPK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10491/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10491/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10491/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10491/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wQiJNyPENt',number = 16846,cdate = 1715774931956,pdate = 1727288138770,odate = 1730873981825,mdate = 1730873981848,tcdate = 1715774931956,tmdate = 1730873981848,ddate = None,content = {'title': {'value': 'Batched Energy-Entropy acquisition for Bayesian Optimization'}, 'authors': {'value': ['Felix Teufel', 'Carsten Stahlhut', 'Jesper Ferkinghoff-Borg']}, 'authorids': {'value': ['~Felix_Teufel1', '~Carsten_Stahlhut1', '~Jesper_Ferkinghoff-Borg1']}, 'keywords': {'value': ['bayesian optimization', 'gaussian processes', 'acquisition', 'batch', 'parallel']}, 'TLDR': {'value': 'A novel acquisition function for batched Bayesian optimization with Gaussian processes that trades off exploration and exploitation with a controllable parameter.'}, 'abstract': {'value': 'Bayesian optimization (BO) is an attractive machine learning framework for performing sample-efficient global optimization of black-box functions. The optimization process is guided by an acquisition function that selects points to acquire in each round of BO. In batched BO, when multiple points are acquired in parallel, commonly used acquisition functions are often high-dimensional and intractable, leading to the use of sampling-based alternatives. We propose a statistical physics inspired acquisition function that can natively handle batches. Batched Energy-Entropy acquisition for BO (BEEBO) enables tight control of the explore-exploit trade-off of the optimization process and generalizes to heteroskedastic black-box problems. We demonstrate the applicability of BEEBO on a range of problems, showing competitive performance to existing acquisition functions.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/eb1379a8a03b7fd20503da9134763dc8400b8d6b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nteufel2024batched,\\ntitle={Batched Energy-Entropy acquisition for Bayesian Optimization},\\nauthor={Felix Teufel and Carsten Stahlhut and Jesper Ferkinghoff-Borg},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wQiJNyPENt}\\n}'}, 'paperhash': {'value': 'teufel|batched_energyentropy_acquisition_for_bayesian_optimization'}},forum = 'wQiJNyPENt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16846/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16846/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16846/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16846/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wN5AgP0DJ0',number = 7270,cdate = 1715616613668,pdate = 1727287841395,odate = 1730873901207,mdate = 1730873901227,tcdate = 1715616613668,tmdate = 1730873901227,ddate = None,content = {'title': {'value': 'Space-Time Continuous PDE Forecasting using Equivariant Neural Fields'}, 'authors': {'value': ['David M Knigge', 'David Wessels', 'Riccardo Valperga', 'Samuele Papa', 'Jan-Jakob Sonke', 'Erik J Bekkers', 'Stratis Gavves']}, 'authorids': {'value': ['~David_M_Knigge1', '~David_Wessels1', '~Riccardo_Valperga1', '~Samuele_Papa1', '~Jan-Jakob_Sonke2', '~Erik_J_Bekkers1', '~Stratis_Gavves1']}, 'keywords': {'value': ['pde solving', 'neural fields', 'equivariance', 'attention']}, 'TLDR': {'value': 'We introduce an equivariant continuous PDE solving method based on Equivariant Neural Fields that preserves boundary conditions and known symmetries of the PDE.'}, 'abstract': {'value': 'Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions -- such as symmetries or boundary conditions -- in favour of modelling flexibility. Instead, we propose a  space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space of the Conditional NeF - preserves known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest $G$ improves generalization and data-efficiency. Furthermore, we validate that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail -, and improve over baselines in a number of challenging geometries.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d1a0db94f57add0d588388b6446bcdc66c454e64.pdf'}, 'supplementary_material': {'value': '/attachment/c8b58ed053a19cbaa62dbbbb9cb64e5e4383c707.zip'}, '_bibtex': {'value': '@inproceedings{\\nknigge2024spacetime,\\ntitle={Space-Time Continuous {PDE} Forecasting using Equivariant Neural Fields},\\nauthor={David M Knigge and David Wessels and Riccardo Valperga and Samuele Papa and Jan-Jakob Sonke and Erik J Bekkers and Stratis Gavves},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wN5AgP0DJ0}\\n}'}, 'paperhash': {'value': 'knigge|spacetime_continuous_pde_forecasting_using_equivariant_neural_fields'}},forum = 'wN5AgP0DJ0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7270/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7270/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7270/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7270/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wK0Z49myyi',number = 3262,cdate = 1715248770332,pdate = 1727287714653,odate = 1730873864731,mdate = 1734970276992,tcdate = 1715248770332,tmdate = 1734970276992,ddate = None,content = {'title': {'value': 'CRAYM: Neural Field Optimization via Camera RAY Matching'}, 'authors': {'value': ['Liqiang Lin', 'Wenpeng Wu', 'Chi-Wing Fu', 'Hao Zhang', 'Hui Huang']}, 'authorids': {'value': ['~Liqiang_Lin1', '~Wenpeng_Wu2', '~Chi-Wing_Fu2', '~Hao_Zhang25', '~Hui_Huang3']}, 'keywords': {'value': ['Neural Implicit Fields', 'Novel View Synthesis', '3D Reconstruction']}, 'abstract': {'value': 'We introduce camera ray matching (CRAYM) into the joint optimization of camera poses and neural fields from multi-view images. The optimized field, referred to as a feature volume, can be “probed” by the camera rays for novel view synthesis (NVS) and 3D geometry reconstruction. One key reason for matching camera rays, instead of pixels as in prior works, is that the camera rays can be parameterized by the feature volume to carry both geometric and photometric information. Multi-view consistencies involving the camera rays and scene rendering can be naturally integrated into the joint optimization and network training, to impose physically meaningful constraints to improve the final quality of both the geometric reconstruction and photorealistic rendering. We formulate our per-ray optimization and matched ray coherence by focusing on camera rays passing through keypoints in the input images to elevate both the efficiency and accuracy of scene correspondences. Accumulated ray features along the feature volume provide a means to discount the coherence constraint amid erroneous ray matching. We demonstrate the effectiveness of CRAYM for both NVS and geometry reconstruction, over dense- or sparse-view settings, with qualitative and quantitative comparisons to state-of-the-art alternatives.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/168f165b72a71321d07a27b12f06fd7ac0fa9bd3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024craym,\\ntitle={{CRAYM}: Neural Field Optimization via Camera {RAY} Matching},\\nauthor={Liqiang Lin and Wenpeng Wu and Chi-Wing Fu and Hao Zhang and Hui Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wK0Z49myyi}\\n}'}, 'paperhash': {'value': 'lin|craym_neural_field_optimization_via_camera_ray_matching'}},forum = 'wK0Z49myyi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3262/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3262/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3262/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3262/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wJaCsnT9UE',number = 8107,cdate = 1715653206687,pdate = 1727287868602,odate = 1730873909497,mdate = 1730873909515,tcdate = 1715653206687,tmdate = 1730873909515,ddate = None,content = {'title': {'value': 'Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance'}, 'authors': {'value': ['Haiquan Lu', 'Xiaotian Liu', 'Yefan Zhou', 'Qunli Li', 'Kurt Keutzer', 'Michael W. Mahoney', 'Yujun Yan', 'Huanrui Yang', 'Yaoqing Yang']}, 'authorids': {'value': ['~Haiquan_Lu1', '~Xiaotian_Liu2', '~Yefan_Zhou1', '~Qunli_Li1', '~Kurt_Keutzer1', '~Michael_W._Mahoney1', '~Yujun_Yan1', '~Huanrui_Yang1', '~Yaoqing_Yang1']}, 'keywords': {'value': ['Diversity', 'loss landscape', 'deep ensemble']}, 'abstract': {'value': \"Recent studies on deep ensembles have identified the sharpness of the local minima of individual learners and the diversity of the ensemble members as key factors in improving test-time performance. Building on this, our study investigates the interplay between sharpness and diversity within deep ensembles, illustrating their crucial role in robust generalization to both in-distribution (ID) and out-of-distribution (OOD) data. We discover a trade-off between sharpness and diversity: minimizing the sharpness in the loss landscape tends to diminish the diversity of individual members within the ensemble, adversely affecting the ensemble's improvement. The trade-off is justified through our rigorous theoretical analysis and verified empirically through extensive experiments. To address the issue of reduced diversity, we introduce SharpBalance, a novel training approach that balances sharpness and diversity within ensembles. Theoretically, we show that our training strategy achieves a better sharpness-diversity trade-off. Empirically, we conducted comprehensive evaluations in various data sets (CIFAR-10, CIFAR-100, TinyImageNet) and showed that SharpBalance not only effectively improves the sharpness-diversity trade-off but also significantly improves ensemble performance in ID and OOD scenarios.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/94d83140bd5537ee6cc7018fa3fdf7107c58db7b.pdf'}, 'supplementary_material': {'value': '/attachment/0da00d42f8fe8ecc4ad08324d5b8c5d8f255afe6.zip'}, 'TLDR': {'value': 'This paper reveals a trade-off between sharpness and diversity in deep ensembles, both empirically and theoretically, and proposes SharpBalance, a novel ensemble algorithm that achieves an optimal balance between these two crucial metrics.'}, '_bibtex': {'value': '@inproceedings{\\nlu2024sharpnessdiversity,\\ntitle={Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance},\\nauthor={Haiquan Lu and Xiaotian Liu and Yefan Zhou and Qunli Li and Kurt Keutzer and Michael W. Mahoney and Yujun Yan and Huanrui Yang and Yaoqing Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wJaCsnT9UE}\\n}'}, 'paperhash': {'value': 'lu|sharpnessdiversity_tradeoff_improving_flat_ensembles_with_sharpbalance'}},forum = 'wJaCsnT9UE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8107/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8107/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8107/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8107/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'wJAF8TGVUG',number = 11558,cdate = 1715706626152,pdate = 1727287975081,odate = 1730873939844,mdate = 1730873939855,tcdate = 1715706626152,tmdate = 1730873939855,ddate = None,content = {'title': {'value': 'S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive Molecule Search'}, 'authors': {'value': ['Gengmo Zhou', 'Zhen Wang', 'Feng Yu', 'Guolin Ke', 'Zhewei Wei', 'Zhifeng Gao']}, 'authorids': {'value': ['~Gengmo_Zhou1', '~Zhen_Wang28', '~Feng_Yu6', '~Guolin_Ke3', '~Zhewei_Wei1', '~Zhifeng_Gao1']}, 'keywords': {'value': ['semi-supervised learning;  3D molecule search; contrastive learning']}, 'abstract': {'value': 'Virtual Screening is an essential technique in the early phases of drug discovery, aimed at identifying promising drug candidates from vast molecular libraries. \\nRecently, ligand-based virtual screening has garnered significant attention due to its efficacy in conducting extensive database screenings without relying on specific protein-binding site information.\\nObtaining binding affinity data for complexes is highly expensive, resulting in a limited amount of available data that covers a relatively small chemical space. Moreover, these datasets contain a significant amount of inconsistent noise. It is challenging to identify an inductive bias that consistently maintains the integrity of molecular activity during data augmentation. To tackle these challenges, we propose S-MolSearch, the first framework to our knowledge, that leverages molecular 3D information and affinity information in semi-supervised contrastive learning for ligand-based virtual screening. \\n% S-MolSearch processes both labeled and unlabeled data, trains molecular structural encoders, and generates soft labels for unlabeled data, drawing on the principles of inverse optimal transport.\\nDrawing on the principles of inverse optimal transport, S-MolSearch efficiently processes both labeled and unlabeled data, training molecular structural encoders while generating soft labels for the unlabeled data.\\nThis design allows S-MolSearch to adaptively utilize unlabeled data within the learning process.\\nEmpirically, S-MolSearch demonstrates superior performance on widely-used benchmarks LIT-PCBA and DUD-E. It surpasses both structure-based and ligand-based virtual screening methods for AUROC, BEDROC and EF.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/46d73ed2879dccfa7cc4be660af5ec5d12d274b1.pdf'}, 'TLDR': {'value': 'S-MolSearch, a semi-supervised framework for ligand-based virtual screening that leverages molecular 3D information. S-MolSearch efficiently processes labeled and unlabeled data with inverse optimal transport, achieving SOTA on LIT-PCBA and DUD-E'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024smolsearch,\\ntitle={S-MolSearch: 3D Semi-supervised Contrastive Learning for Bioactive Molecule Search},\\nauthor={Gengmo Zhou and Zhen Wang and Feng Yu and Guolin Ke and Zhewei Wei and Zhifeng Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wJAF8TGVUG}\\n}'}, 'paperhash': {'value': 'zhou|smolsearch_3d_semisupervised_contrastive_learning_for_bioactive_molecule_search'}},forum = 'wJAF8TGVUG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11558/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11558/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11558/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11558/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'wIE991zhXH',number = 7159,cdate = 1715613915903,pdate = 1727287837407,odate = 1730873899910,mdate = 1736757185917,tcdate = 1715613915903,tmdate = 1736757185917,ddate = None,content = {'title': {'value': 'Bandits with Preference Feedback: A Stackelberg Game Perspective'}, 'authors': {'value': ['Barna Pásztor', 'Parnian Kassraie', 'Andreas Krause']}, 'authorids': {'value': ['~Barna_Pásztor1', '~Parnian_Kassraie1', '~Andreas_Krause1']}, 'keywords': {'value': ['Preference Feedback', 'Duelling Bandits', 'Reproducing Kernel Hilbert Space', 'Stackelberg Games', 'Exploration-exploitation dilemma']}, 'abstract': {'value': 'Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for tuning large language models.\\nThe problem is fairly understood in toy settings with linear target functions or over finite small domains that limits practical interest.\\nTaking the next step, we consider infinite domains and kernelized rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm.\\nWe propose MaxMinLCB, which emulates this trade-off as a zero-sum Stackelberg game and chooses action pairs that are informative and have favorable reward values. MaxMinLCB consistently outperforms algorithms in the literature and satisfies an anytime-valid rate-optimal regret guarantee. This is owed to our novel preference-based confidence sequences for kernelized logistic estimators, which are of independent interest.'}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/db5295cfa310af4c44f8d5f32aaa6fcf1c92114d.pdf'}, '_bibtex': {'value': \"@inproceedings{\\np{\\\\'a}sztor2024bandits,\\ntitle={Bandits with Preference Feedback: A Stackelberg Game Perspective},\\nauthor={Barna P{\\\\'a}sztor and Parnian Kassraie and Andreas Krause},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wIE991zhXH}\\n}\"}, 'paperhash': {'value': 'pásztor|bandits_with_preference_feedback_a_stackelberg_game_perspective'}},forum = 'wIE991zhXH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7159/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7159/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7159/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7159/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'wHFaAH3E8z',number = 9005,cdate = 1715673029468,pdate = 1727287897961,odate = 1730873916815,mdate = 1730873916836,tcdate = 1715673029468,tmdate = 1730873916836,ddate = None,content = {'title': {'value': 'FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings'}, 'authors': {'value': ['Xiao Tan', 'Yiqin Wang', 'Yangyang Shen', 'Dian Shen', 'Meng Wang', 'Peibo Duan', 'Beilun Wang']}, 'authorids': {'value': ['~Xiao_Tan5', '~Yiqin_Wang2', '~Yangyang_Shen1', '~Dian_Shen1', '~Meng_Wang11', '~Peibo_Duan1', '~Beilun_Wang1']}, 'keywords': {'value': ['Precision Matrix Estimation', 'Meta Learning', 'Graphical Model', 'Small Sample']}, 'abstract': {'value': 'Precision matrix estimation is a ubiquitous task featuring numerous applications such as rare disease diagnosis and neural connectivity exploration. However, this task becomes challenging in small sample settings, where the number of samples is significantly less than the number of dimensions, leading to unreliable estimates. Previous approaches either fail to perform well in small sample settings or suffer from inefficient estimation processes, even when incorporating meta-learning techniques.\\nTo this end, we propose a novel approach FasMe for Fast and Sample-efficient Meta Precision Matrix Learning, which first extracts meta-knowledge through a multi-task learning diagram. Then, meta-knowledge constraints are applied using a maximum determinant matrix completion algorithm for the novel task. As a result, we reduce the sample size requirements to $O(\\\\log p/K)$ per meta-training task and $O(\\\\log\\\\vert \\\\mathcal{G}\\\\vert)$ for the meta-testing task. Moreover, the hereby proposed model only needs $O(p \\\\log\\\\epsilon^{-1})$ time and $O(p)$ memory for converging to an $\\\\epsilon$-accurate solution. On multiple synthetic and biomedical datasets, FasMe is at least ten times faster than the four baselines while promoting prediction accuracy in small sample settings.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7fb400b0af5f12f29a6d981142457903acaa8378.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntan2024fasme,\\ntitle={FasMe: Fast and Sample-efficient Meta Estimator for Precision Matrix Learning in Small Sample Settings},\\nauthor={Xiao Tan and Yiqin Wang and Yangyang Shen and Dian Shen and Meng Wang and Peibo Duan and Beilun Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wHFaAH3E8z}\\n}'}, 'paperhash': {'value': 'tan|fasme_fast_and_sampleefficient_meta_estimator_for_precision_matrix_learning_in_small_sample_settings'}},forum = 'wHFaAH3E8z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9005/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9005/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9005/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9005/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wGjSbaMsop',number = 15937,cdate = 1715765083971,pdate = 1727288111859,odate = 1730873976036,mdate = 1730873976053,tcdate = 1715765083971,tmdate = 1730873976053,ddate = None,content = {'title': {'value': 'Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists'}, 'authors': {'value': ['Joachim Baumann', 'Celestine Mendler-Dünner']}, 'authorids': {'value': ['~Joachim_Baumann1', '~Celestine_Mendler-Dünner1']}, 'keywords': {'value': ['collective action', 'platform power', 'sequential recommender systems', 'transformer models', 'music recommendation']}, 'TLDR': {'value': 'Small user collectives can effectively promote artists through simple reordering of playlists, leveraging the sequential nature of transformer-based recommendation systems.'}, 'abstract': {'value': 'We investigate algorithmic collective action in transformer-based recommender systems. Our use case is a collective of fans aiming to promote the visibility of an underrepresented artist by strategically placing one of their songs in the existing playlists they control. We introduce two easily implementable strategies to select the position at which to insert the song and boost recommendations at test time. The strategies exploit statistical properties of the learner to leverage discontinuities in the recommendations, and the long-tail nature of song distributions. We evaluate the efficacy of our strategies using a publicly available recommender system model released by a major music streaming platform. Our findings reveal that even small collectives (controlling less than 0.01\\\\% of the training data) can achieve up to $40\\\\times$ more test time recommendations than songs with similar training set occurrences, on average. Focusing on the externalities of the strategy, we find that the recommendations of other songs are largely preserved, and the newly gained recommendations are distributed across various artists. Together, our findings demonstrate how carefully designed collective action strategies can be effective while not necessarily being adversarial.'}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5ff95834271c7f7d9900560d509c88ba7ff05215.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nbaumann2024algorithmic,\\ntitle={Algorithmic Collective Action in Recommender Systems: Promoting Songs by Reordering Playlists},\\nauthor={Joachim Baumann and Celestine Mendler-D{\\\\\"u}nner},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wGjSbaMsop}\\n}'}, 'paperhash': {'value': 'baumann|algorithmic_collective_action_in_recommender_systems_promoting_songs_by_reordering_playlists'}},forum = 'wGjSbaMsop',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15937/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15937/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15937/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15937/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'wGP1tBCP1E',number = 827,cdate = 1714120127064,pdate = 1727287645437,odate = 1730873843347,mdate = 1730873843360,tcdate = 1714120127064,tmdate = 1730873843360,ddate = None,content = {'title': {'value': 'Diffusion Models are Certifiably Robust Classifiers'}, 'authors': {'value': ['Huanran Chen', 'Yinpeng Dong', 'Shitong Shao', 'Zhongkai Hao', 'Xiao Yang', 'Hang Su', 'Jun Zhu']}, 'authorids': {'value': ['~Huanran_Chen1', '~Yinpeng_Dong2', '~Shitong_Shao1', '~Zhongkai_Hao1', '~Xiao_Yang4', '~Hang_Su3', '~Jun_Zhu2']}, 'keywords': {'value': ['certified robustness', 'diffusion classifier', 'adversarial robustness']}, 'abstract': {'value': \"Generative learning, recognized for its effective modeling of data distributions, offers inherent advantages in handling out-of-distribution instances, especially for enhancing robustness to adversarial attacks. Among these, diffusion classifiers, utilizing powerful diffusion models, have demonstrated superior empirical robustness. However, a comprehensive theoretical understanding of their robustness is still lacking, raising concerns about their vulnerability to stronger future attacks. In this study, we prove that diffusion classifiers possess $O(1)$ Lipschitzness, and establish their certified robustness, demonstrating their inherent resilience. To achieve non-constant Lipschitzness, thereby obtaining much tighter certified robustness, we generalize diffusion classifiers to classify Gaussian-corrupted data. This involves deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. Experimental results show the superior certified robustness of these Noised Diffusion Classifiers (NDCs). Notably, we achieve over 80\\\\% and 70\\\\% certified robustness on CIFAR-10 under adversarial perturbations with \\\\(\\\\ell_2\\\\) norms less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/62eb54961c7645deb0bc1355cc5f8a275a3c9c1e.pdf'}, 'TLDR': {'value': 'We derive the certified radius and Lipschitz constant for diffusion classifiers. We also generalize diffusion classifiers to classify noisy data.'}, '_bibtex': {'value': '@inproceedings{\\nchen2024diffusion,\\ntitle={Diffusion Models are Certifiably Robust Classifiers},\\nauthor={Huanran Chen and Yinpeng Dong and Shitong Shao and Zhongkai Hao and Xiao Yang and Hang Su and Jun Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wGP1tBCP1E}\\n}'}, 'paperhash': {'value': 'chen|diffusion_models_are_certifiably_robust_classifiers'}},forum = 'wGP1tBCP1E',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission827/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission827/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission827/-/Revision', 'NeurIPS.cc/2024/Conference/Submission827/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wFzIMbTsY7',number = 5823,cdate = 1715571097377,pdate = 1727287796045,odate = 1730873888510,mdate = 1730873888521,tcdate = 1715571097377,tmdate = 1730873888521,ddate = None,content = {'title': {'value': 'Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling'}, 'authors': {'value': ['Sili Huang', 'Jifeng Hu', 'Zhejian Yang', 'Liwei Yang', 'Tao Luo', 'Hechang Chen', 'Lichao Sun', 'Bo Yang']}, 'authorids': {'value': ['~Sili_Huang1', '~Jifeng_Hu1', '~Zhejian_Yang1', '~Liwei_Yang2', '~Tao_Luo2', '~Hechang_Chen2', '~Lichao_Sun1', '~Bo_Yang6']}, 'keywords': {'value': ['In-context Reinforcement Learning', 'Mamba']}, 'abstract': {'value': 'Recent works have shown the remarkable superiority of transformer models in reinforcement learning (RL), where the decision-making problem is formulated as sequential generation. Transformer-based agents could emerge with self-improvement in online environments by providing task contexts, such as multiple trajectories, called in-context RL. However, due to the quadratic computation complexity of attention in transformers, current in-context RL methods suffer from huge computational costs as the task horizon increases. In contrast, the Mamba model is renowned for its efficient ability to process long-term dependencies, which provides an opportunity for in-context RL to solve tasks that require long-term memory. To this end, we first implement Decision Mamba (DM) by replacing the backbone of Decision Transformer (DT). Then, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers and Mamba in high-quality prediction and long-term memory. Specifically, DM-H first generates high-value sub-goals from long-term memory through the Mamba model. Then, we use sub-goals to prompt the transformer, establishing high-quality predictions. Experimental results demonstrate that DM-H achieves state-of-the-art in long and short-term tasks, such as D4RL, Grid World, and Tmaze benchmarks. Regarding efficiency, the online testing of DM-H in the long-term task is 28$\\\\times$ times faster than the transformer-based baselines.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f79cbc369ef6968176c7cc958c79839cb99e59b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024decision,\\ntitle={Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling},\\nauthor={Sili Huang and Jifeng Hu and Zhejian Yang and Liwei Yang and Tao Luo and Hechang Chen and Lichao Sun and Bo Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wFzIMbTsY7}\\n}'}, 'paperhash': {'value': 'huang|decision_mamba_reinforcement_learning_via_hybrid_selective_sequence_modeling'}},forum = 'wFzIMbTsY7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5823/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5823/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5823/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5823/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wDirCeTIoz',number = 21219,cdate = 1715800784472,pdate = 1727288250458,odate = 1730874005107,mdate = 1730874005124,tcdate = 1715800784472,tmdate = 1730874005124,ddate = None,content = {'title': {'value': 'Communication Efficient Distributed Training with Distributed Lion'}, 'authors': {'value': ['Bo Liu', 'Lemeng Wu', 'Lizhang Chen', 'Kaizhao Liang', 'Jiaxu Zhu', 'Chen Liang', 'Raghuraman Krishnamoorthi', 'qiang liu']}, 'authorids': {'value': ['~Bo_Liu13', '~Lemeng_Wu1', '~Lizhang_Chen1', '~Kaizhao_Liang1', '~Jiaxu_Zhu2', '~Chen_Liang1', '~Raghuraman_Krishnamoorthi1', '~qiang_liu4']}, 'keywords': {'value': ['Distributed Optimization']}, 'abstract': {'value': \"The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages in memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires to communicate binary or lower-precision vectors\\nbetween workers to the center server, significantly reducing the communication cost.  \\nOur theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large models. In addition, we also demonstrate that \\\\mavolion{} presents a more favorable performance-bandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients.\"}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce the distributed version of the Lion optimizer with efficient binary/low-precision communication. We provide both theoretical and empirical evidence to demonstrate it is a simple yet strong method.'}, 'pdf': {'value': '/pdf/12306ef9133c7e08f53a436d98a8c343b914f091.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024communication,\\ntitle={Communication Efficient Distributed Training with Distributed Lion},\\nauthor={Bo Liu and Lemeng Wu and Lizhang Chen and Kaizhao Liang and Jiaxu Zhu and Chen Liang and Raghuraman Krishnamoorthi and qiang liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wDirCeTIoz}\\n}'}, 'paperhash': {'value': 'liu|communication_efficient_distributed_training_with_distributed_lion'}},forum = 'wDirCeTIoz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21219/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21219/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21219/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21219/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wDDvJzvvBR',number = 20509,cdate = 1715796807924,pdate = 1727288235559,odate = 1730874001790,mdate = 1730874001804,tcdate = 1715796807924,tmdate = 1730874001804,ddate = None,content = {'title': {'value': 'Learning Spatially-Aware Language and Audio Embeddings'}, 'authors': {'value': ['Bhavika Suresh Devnani', 'Skyler Seto', 'Zakaria Aldeneh', 'Alessandro Toso', 'YELENA MENYAYLENKO', 'Barry-John Theobald', 'Jonathan Sheaffer', 'Miguel Sarabia']}, 'authorids': {'value': ['~Bhavika_Suresh_Devnani1', '~Skyler_Seto1', '~Zakaria_Aldeneh1', '~Alessandro_Toso2', '~YELENA_MENYAYLENKO1', '~Barry-John_Theobald1', '~Jonathan_Sheaffer1', '~Miguel_Sarabia1']}, 'keywords': {'value': ['multimodal embeddings', 'spatial audio', 'contrastive learning']}, 'TLDR': {'value': 'We train a model that aligns 3D Spatial Audio with open vocabulary captions.'}, 'abstract': {'value': 'Humans can picture a sound scene given an imprecise natural language description. For example, it is easy to imagine an acoustic environment given a phrase like \"the lion roar came from right behind me!\". For a machine to have the same degree of comprehension,  the machine must know what a lion is (semantic attribute), what the concept of \"behind\" is (spatial attribute) and how these pieces of linguistic information align with the semantic and spatial attributes of the sound (what a roar sounds like when its coming from behind). \\nState-of-the-art audio foundation models, such as CLAP, which learn to map between audio scenes and natural textual descriptions, are trained on non-spatial audio and text pairs, and hence lack spatial awareness. In contrast, sound event localization and detection models are limited to recognizing sounds from a fixed number of classes, and they localize the source to absolute position (e.g., 0.2m) rather than a position described using natural language (e.g., \"next to me\"). To address these gaps, we present ELSA (Embeddings for Language and Spatial Audio), a spatially aware-audio and text embedding model trained using multimodal contrastive learning. ELSA supports non-spatial audio, spatial audio, and open vocabulary text captions describing both the spatial and semantic components of sound. To train ELSA: (a) we spatially augment  the audio and captions of three open-source audio datasets totaling 4,738 hours and 890,038 samples of audio comprised from 8,972 simulated spatial configurations, and (b) we design an encoder to capture the semantics of non-spatial audio, and the semantics and spatial attributes of spatial audio using contrastive learning. ELSA is a single model that is competitive with state-of-the-art for both semantic retrieval and 3D source localization.  In particular, ELSA achieves +2.8\\\\% mean audio-to-text and text-to-audio R@1 above the LAION-CLAP baseline, and outperforms by -11.6° mean-absolute-error in 3D source localization over the SeldNET baseline on the TUT Sound Events 2018 benchmark. Moreover, we show that the representation-space of ELSA is structured, enabling swapping of direction of audio via vector arithmetic of two directional text embeddings.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a2ecfb85ce32cb9d1d5454e92a10f65a79ed4f7d.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndevnani2024learning,\\ntitle={Learning Spatially-Aware Language and Audio Embeddings},\\nauthor={Bhavika Suresh Devnani and Skyler Seto and Zakaria Aldeneh and Alessandro Toso and YELENA MENYAYLENKO and Barry-John Theobald and Jonathan Sheaffer and Miguel Sarabia},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wDDvJzvvBR}\\n}'}, 'paperhash': {'value': 'devnani|learning_spatiallyaware_language_and_audio_embeddings'}},forum = 'wDDvJzvvBR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20509/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20509/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20509/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20509/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'wBzvYh3PRA',number = 3009,cdate = 1715206247120,pdate = 1727287707697,odate = 1730873862893,mdate = 1730873862910,tcdate = 1715206247120,tmdate = 1730873862910,ddate = None,content = {'title': {'value': 'FactorSim: Generative Simulation via Factorized Representation'}, 'authors': {'value': ['Fan-Yun Sun', 'Harini S I', 'Angela Yi', 'Yihan Zhou', 'Alex Zook', 'Jonathan Tremblay', 'Logan Cross', 'Jiajun Wu', 'Nick Haber']}, 'authorids': {'value': ['~Fan-Yun_Sun1', '~Harini_S_I1', '~Angela_Yi2', '~Yihan_Zhou4', '~Alex_Zook1', '~Jonathan_Tremblay1', '~Logan_Cross1', '~Jiajun_Wu1', '~Nick_Haber1']}, 'keywords': {'value': ['generative simulation', 'POMDP', 'Large Language Models']}, 'abstract': {'value': 'Generating simulations to train intelligent agents in game-playing and robotics from natural language input, user input, or task documentation remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code’s accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose a framework for generating simulations in code to train RL agents and introduce a new benchmark to showcase its efficacy.'}, 'pdf': {'value': '/pdf/c3c4eed43ecec8fe574f69437c9137f8c41b7797.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024factorsim,\\ntitle={FactorSim: Generative Simulation via Factorized Representation},\\nauthor={Fan-Yun Sun and Harini S I and Angela Yi and Yihan Zhou and Alex Zook and Jonathan Tremblay and Logan Cross and Jiajun Wu and Nick Haber},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wBzvYh3PRA}\\n}'}, 'paperhash': {'value': 'sun|factorsim_generative_simulation_via_factorized_representation'}},forum = 'wBzvYh3PRA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3009/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3009/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3009/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3009/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wBtmN8SZ2B',number = 11374,cdate = 1715704150506,pdate = 1727287969283,odate = 1730873938187,mdate = 1736818153707,tcdate = 1715704150506,tmdate = 1736818153707,ddate = None,content = {'title': {'value': 'Learning Structured Representations with Hyperbolic Embeddings'}, 'authors': {'value': ['Aditya Sinha', 'Siqi Zeng', 'Makoto Yamada', 'Han Zhao']}, 'authorids': {'value': ['~Aditya_Sinha1', '~Siqi_Zeng1', '~Makoto_Yamada3', '~Han_Zhao1']}, 'keywords': {'value': ['hierarchical representation', 'representation learning', 'hyperbolic geometry']}, 'TLDR': {'value': 'We propose a structured regularization method for learning label-hierarchy informed representations using hyperbolic geometry.'}, 'abstract': {'value': 'Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/51b651ea18ebf5913ecbb3b9f73255a5ce047e16.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsinha2024learning,\\ntitle={Learning Structured Representations with Hyperbolic Embeddings},\\nauthor={Aditya Sinha and Siqi Zeng and Makoto Yamada and Han Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wBtmN8SZ2B}\\n}'}, 'paperhash': {'value': 'sinha|learning_structured_representations_with_hyperbolic_embeddings'}},forum = 'wBtmN8SZ2B',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11374/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11374/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11374/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11374/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'wAqdvcK1Fv',number = 12780,cdate = 1715728316586,pdate = 1727288017356,odate = 1730873951945,mdate = 1737138906402,tcdate = 1715728316586,tmdate = 1737138906402,ddate = None,content = {'title': {'value': 'Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces'}, 'authors': {'value': ['Tobias Schröder', 'Zijing Ou', 'Yingzhen Li', 'Andrew B. Duncan']}, 'authorids': {'value': ['~Tobias_Schröder2', '~Zijing_Ou1', '~Yingzhen_Li1', '~Andrew_B._Duncan1']}, 'keywords': {'value': ['Energy-based models', 'discrete probabilistic modelling', 'tabular data']}, 'TLDR': {'value': 'A MCMC-free method based on the discrete heat diffusion for the training of energy-based models on discrete and mixed data with applications in generative modelling for tabular data.'}, 'abstract': {'value': 'Energy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on data in discrete or mixed state spaces poses significant challenges due to the lack of robust and fast sampling methods. In this work, we propose to train discrete EBMs with Energy Discrepancy, a loss function which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for Markov chain Monte Carlo. We introduce perturbations of the data distribution by simulating a diffusion process on the discrete state space endowed with a graph structure. This allows us to inform the choice of perturbation from the structure of the modelled discrete variable, while the continuous time parameter enables fine-grained control of the perturbation. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including the estimation of discrete densities with non-binary vocabulary and binary image modelling. We also introduce the first application of EBMs to tabular data sets with applications in synthetic data generation and calibrated classification.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b940a6b863bcbcdb355543eae11b90bc3e6fd5e2.pdf'}, 'supplementary_material': {'value': '/attachment/4a9f159a87f431d7e18d6d9fae84cdf1bddadd30.zip'}, '_bibtex': {'value': '@inproceedings{\\nschr{\\\\\"o}der2024energybased,\\ntitle={Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces},\\nauthor={Tobias Schr{\\\\\"o}der and Zijing Ou and Yingzhen Li and Andrew B. Duncan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=wAqdvcK1Fv}\\n}'}, 'paperhash': {'value': 'schröder|energybased_modelling_for_discrete_and_mixed_data_via_heat_equations_on_structured_spaces'}},forum = 'wAqdvcK1Fv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12780/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12780/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12780/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12780/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'w6vbfSC1y0',number = 16347,cdate = 1715769531841,pdate = 1727288124121,odate = 1730873978725,mdate = 1730873978742,tcdate = 1715769531841,tmdate = 1730873978742,ddate = None,content = {'title': {'value': 'Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection'}, 'authors': {'value': ['Geng Yu', 'Jianing Zhu', 'Jiangchao Yao', 'Bo Han']}, 'authorids': {'value': ['~Geng_Yu1', '~Jianing_Zhu2', '~Jiangchao_Yao1', '~Bo_Han1']}, 'keywords': {'value': ['out-of-distribution detection', 'vision-language model', 'prompt-tuning']}, 'TLDR': {'value': 'We propose a framework, named SCT, to mitigate the problem of spurious OOD features mined from ID data in prompt-tuning based OOD detection methods.'}, 'abstract': {'value': 'Out-of-distribution (OOD) detection is crucial for deploying reliable machine learning models in open-world applications. Recent advances in CLIP-based OOD detection have shown promising results via regularizing prompt tuning with OOD features extracted from ID data. However, the irrelevant context mined from ID data can be spurious due to the inaccurate foreground-background decomposition, thus limiting the OOD detection performance. In this work, we propose a novel framework, namely, \\\\textit{Self-Calibrated Tuning (SCT)}, to mitigate this problem for effective OOD detection with only the given few-shot ID data. Specifically, SCT introduces modulating factors respectively on the two components of the original learning objective. It adaptively directs the optimization process between the two tasks during training on data with different prediction uncertainty to calibrate the influence of OOD regularization, which is compatible with many prompt tuning based OOD detection methods. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed SCT. The code is publicly available at: https://github.com/tmlr-group/SCT.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ebce8bd933169b8ae49746ddfdd82a8b0895df0d.pdf'}, 'supplementary_material': {'value': '/attachment/d88248e69c3d65a868af87a8adefc747838165bf.zip'}, '_bibtex': {'value': '@inproceedings{\\nyu2024selfcalibrated,\\ntitle={Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection},\\nauthor={Geng Yu and Jianing Zhu and Jiangchao Yao and Bo Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w6vbfSC1y0}\\n}'}, 'paperhash': {'value': 'yu|selfcalibrated_tuning_of_visionlanguage_models_for_outofdistribution_detection'}},forum = 'w6vbfSC1y0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16347/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16347/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16347/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16347/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'w6q46IslSR',number = 3324,cdate = 1715260374914,pdate = 1727287716512,odate = 1730873865178,mdate = 1730873865196,tcdate = 1715260374914,tmdate = 1730873865196,ddate = None,content = {'title': {'value': 'Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis'}, 'authors': {'value': ['Hongru Yang', 'Bhavya Kailkhura', 'Zhangyang Wang', 'Yingbin Liang']}, 'authorids': {'value': ['~Hongru_Yang1', '~Bhavya_Kailkhura1', '~Zhangyang_Wang1', '~Yingbin_Liang1']}, 'keywords': {'value': ['Transformers', 'gradient flow dynamics', 'implicit bias']}, 'abstract': {'value': 'Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. \\nIn this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed \\\\textit{automatic balancing of gradients}, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d62bb7ca05c4ddb2e68d06f57b06fae99492728a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024training,\\ntitle={Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis},\\nauthor={Hongru Yang and Bhavya Kailkhura and Zhangyang Wang and Yingbin Liang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w6q46IslSR}\\n}'}, 'paperhash': {'value': 'yang|training_dynamics_of_transformers_to_recognize_word_cooccurrence_via_gradient_flow_analysis'}},forum = 'w6q46IslSR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3324/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3324/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3324/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3324/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'w67vRHZF13',number = 72,cdate = 1713827433563,pdate = 1727287629605,odate = 1730873837943,mdate = 1730873837960,tcdate = 1713827433563,tmdate = 1730873837960,ddate = None,content = {'title': {'value': 'Unified Generative and Discriminative Training for Multi-modal Large Language Models'}, 'authors': {'value': ['Wei Chow', 'Juncheng Li', 'Qifan Yu', 'Kaihang Pan', 'Hao Fei', 'Zhiqi Ge', 'Shuai Yang', 'Siliang Tang', 'Hanwang Zhang', 'Qianru Sun']}, 'authorids': {'value': ['~Wei_Chow1', '~Juncheng_Li3', '~Qifan_Yu1', '~Kaihang_Pan1', '~Hao_Fei1', '~Zhiqi_Ge1', '~Shuai_Yang18', '~Siliang_Tang1', '~Hanwang_Zhang3', '~Qianru_Sun2']}, 'keywords': {'value': ['vision-language', 'multi-modal understanding']}, 'abstract': {'value': 'In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM’s hidden state. This approach enhances the MLLM’s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling.'}, 'pdf': {'value': '/pdf/92d9a4d22bb9998d8f043e2b98b85d4d012ff3c7.pdf'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose an structure-induced approach to unify generative and discriminative paradigms.'}, '_bibtex': {'value': '@inproceedings{\\nchow2024unified,\\ntitle={Unified Generative and Discriminative Training for Multi-modal Large Language Models},\\nauthor={Wei Chow and Juncheng Li and Qifan Yu and Kaihang Pan and Hao Fei and Zhiqi Ge and Shuai Yang and Siliang Tang and Hanwang Zhang and Qianru Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w67vRHZF13}\\n}'}, 'paperhash': {'value': 'chow|unified_generative_and_discriminative_training_for_multimodal_large_language_models'}},forum = 'w67vRHZF13',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission72/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission72/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission72/-/Revision', 'NeurIPS.cc/2024/Conference/Submission72/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'w50ICQC6QJ',number = 6701,cdate = 1715602443748,pdate = 1727287822847,odate = 1730873895371,mdate = 1730873895390,tcdate = 1715602443748,tmdate = 1730873895390,ddate = None,content = {'title': {'value': 'Discovery of the Hidden World with Large Language Models'}, 'authors': {'value': ['Chenxi Liu', 'Yongqiang Chen', 'Tongliang Liu', 'Mingming Gong', 'James Cheng', 'Bo Han', 'Kun Zhang']}, 'authorids': {'value': ['~Chenxi_Liu3', '~Yongqiang_Chen1', '~Tongliang_Liu1', '~Mingming_Gong1', '~James_Cheng2', '~Bo_Han1', '~Kun_Zhang1']}, 'keywords': {'value': ['Causal Discovery', 'Large Language Models', 'Causal Representation Learning']}, 'TLDR': {'value': 'A new framework leveraging large language models to extend the scope of causal discovery to unstructured data.'}, 'abstract': {'value': 'Revealing the underlying causal mechanisms in the real world is the key to the development of science. Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. The lack of well-defined high-level variables in many real-world applications has already been a longstanding roadblock to a broader application of CDs. To this end, this paper presents Causal representatiOn AssistanT (COAT) that introduces large language models (LLMs) to bridge the gap. LLMs are trained on massive observations of the world and have demonstrated great capability in extracting key information from unstructured data. Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements. Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors. We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal. We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT. Extensive empirical results confirm the effectiveness and reliability of COAT with significant improvements.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d7bdc070a6044df2e284ee1476561ea96fa74dae.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024discovery,\\ntitle={Discovery of the Hidden World with Large Language Models},\\nauthor={Chenxi Liu and Yongqiang Chen and Tongliang Liu and Mingming Gong and James Cheng and Bo Han and Kun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w50ICQC6QJ}\\n}'}, 'paperhash': {'value': 'liu|discovery_of_the_hidden_world_with_large_language_models'}},forum = 'w50ICQC6QJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6701/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6701/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6701/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6701/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'w4AnTVxAO9',number = 16051,cdate = 1715766030942,pdate = 1727288114866,odate = 1730873976936,mdate = 1730873976950,tcdate = 1715766030942,tmdate = 1730873976950,ddate = None,content = {'title': {'value': 'Can Language Models Learn to Skip Steps?'}, 'authors': {'value': ['Tengxiao Liu', 'Qipeng Guo', 'Xiangkun Hu', 'Cheng Jiayang', 'Yue Zhang', 'Xipeng Qiu', 'Zheng Zhang']}, 'authorids': {'value': ['~Tengxiao_Liu1', '~Qipeng_Guo1', '~Xiangkun_Hu1', '~Cheng_Jiayang1', '~Yue_Zhang7', '~Xipeng_Qiu1', '~Zheng_Zhang1']}, 'keywords': {'value': ['Large Language Models', 'Natural Language Processing', 'Reasoning', 'Human-like Abilities']}, 'abstract': {'value': 'Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and model behaviors. In this work, we study the ability to skip steps in reasoning—a hallmark of human expertise developed through practice. Unlike humans, who may skip steps to enhance efficiency or to reduce cognitive load, models do not inherently possess such motivations to minimize reasoning steps. To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. Empirical results indicate that models can develop the step skipping ability under our guidance. Moreover, after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences, the models can not only resolve tasks with increased efficiency without sacrificing accuracy, but also exhibit comparable and even enhanced generalization capabilities in out-of-domain scenarios. Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4a83957d2b46e1316f6bcdc680cbad91fa6b7a65.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024can,\\ntitle={Can Language Models Learn to Skip Steps?},\\nauthor={Tengxiao Liu and Qipeng Guo and Xiangkun Hu and Cheng Jiayang and Yue Zhang and Xipeng Qiu and Zheng Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w4AnTVxAO9}\\n}'}, 'paperhash': {'value': 'liu|can_language_models_learn_to_skip_steps'}},forum = 'w4AnTVxAO9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16051/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16051/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16051/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16051/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'w3JCTBRduf',number = 18641,cdate = 1715786555960,pdate = 1727288189471,odate = 1730873991202,mdate = 1730873991219,tcdate = 1715786555960,tmdate = 1730873991219,ddate = None,content = {'title': {'value': 'Optimization Can Learn Johnson Lindenstrauss Embeddings'}, 'authors': {'value': ['Nikos Tsikouras', 'Constantine Caramanis', 'Christos Tzamos']}, 'authorids': {'value': ['~Nikos_Tsikouras1', '~Constantine_Caramanis1', '~Christos_Tzamos1']}, 'keywords': {'value': ['Optimization', 'Non-Convex Optimization', 'Embeddings', 'Projections', 'Derandomization', 'Gradient Descent', 'Dimensionality Reduction']}, 'TLDR': {'value': 'We give a novel derandomization of JL via optimization, that avoids all bad local minima in the non-convex landscape by a diffusion-like process where we move through the space of randomized solution samplers, sequentially reducing the variance.'}, 'abstract': {'value': 'Embeddings play a pivotal role across various disciplines, offering compact representations of complex data structures. Randomized methods like Johnson-Lindenstrauss (JL) provide state-of-the-art and essentially unimprovable theoretical guarantees for achieving such representations. These guarantees are worst-case and in particular, neither the analysis, ${\\\\textit{nor the algorithm}}$, takes into account any potential structural information of the data. The natural question is: must we randomize? Could we instead use an optimization-based approach, working directly with the data? A first answer is no: as we show, the distance-preserving objective of JL has a non-convex landscape over the space of projection matrices, with many bad stationary points. But this is not the final answer. \\n\\nWe present a novel method motivated by diffusion models, that circumvents this fundamental challenge: rather than performing optimization directly over the space of projection matrices, we use optimization over the larger space of $\\\\textit{random solution samplers}$, gradually reducing the variance of the sampler. We show that by moving through this larger space, our objective converges to a deterministic (zero variance) solution, avoiding bad stationary points. \\n\\nThis method can also be seen as an optimization-based derandomization approach, and is an idea and method that we believe can be applied to many other problems.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e71a9aee432154e3433f241defc8602cd50276dd.pdf'}, 'supplementary_material': {'value': '/attachment/ee7fcb741e66d4fd07cf9aa64ad34b1ded2f142d.zip'}, '_bibtex': {'value': '@inproceedings{\\ntsikouras2024optimization,\\ntitle={Optimization Can Learn Johnson Lindenstrauss Embeddings},\\nauthor={Nikos Tsikouras and Constantine Caramanis and Christos Tzamos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w3JCTBRduf}\\n}'}, 'paperhash': {'value': 'tsikouras|optimization_can_learn_johnson_lindenstrauss_embeddings'}},forum = 'w3JCTBRduf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18641/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18641/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18641/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18641/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'w2L3Ll1jbV',number = 19715,cdate = 1715792450260,pdate = 1727288215967,odate = 1730873997345,mdate = 1730873997357,tcdate = 1715792450260,tmdate = 1730873997357,ddate = None,content = {'title': {'value': 'Adversarially Robust Multi-task Representation Learning'}, 'authors': {'value': ['Austin Watkins', 'Thanh Nguyen-Tang', 'Enayat Ullah', 'Raman Arora']}, 'authorids': {'value': ['~Austin_Watkins1', '~Thanh_Nguyen-Tang1', '~Enayat_Ullah1', '~Raman_Arora1']}, 'keywords': {'value': ['Learning Theory', 'Multi-task and Transfer Learning', 'Adversarial Robustness']}, 'abstract': {'value': 'We study adversarially robust transfer learning, wherein, given labeled data on multiple (source) tasks, the goal is to train a model with small robust error on a previously unseen (target) task.\\nIn particular, we consider a multi-task representation learning (MTRL) setting, i.e., we assume that the source and target tasks admit a simple (linear) predictor on top of a shared representation (e.g., the final hidden layer of a \\ndeep neural network).\\nIn this general setting, we provide rates on~the excess adversarial (transfer) risk for Lipschitz losses and smooth nonnegative losses.\\nThese rates show that learning a representation using adversarial training on diverse tasks  helps protect against inference-time attacks in data-scarce environments.\\nAdditionally, we provide novel rates for the single-task setting.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We give bounds for adversarially robust transfer learning.'}, 'pdf': {'value': '/pdf/7fadc60234b4e01a1ccac1ccc252854801c52c8d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwatkins2024adversarially,\\ntitle={Adversarially Robust Multi-task Representation Learning},\\nauthor={Austin Watkins and Thanh Nguyen-Tang and Enayat Ullah and Raman Arora},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w2L3Ll1jbV}\\n}'}, 'paperhash': {'value': 'watkins|adversarially_robust_multitask_representation_learning'}},forum = 'w2L3Ll1jbV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19715/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19715/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19715/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19715/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'w28i9oe9Xr',number = 6797,cdate = 1715605184198,pdate = 1727287826010,odate = 1730873896267,mdate = 1730873896279,tcdate = 1715605184198,tmdate = 1730873896279,ddate = None,content = {'title': {'value': 'High Rank Path Development: an approach to learning the filtration of stochastic processes'}, 'authors': {'value': ['Jiajie Tao', 'Hao Ni', 'Chong Liu']}, 'authorids': {'value': ['~Jiajie_Tao1', '~Hao_Ni2', '~Chong_Liu8']}, 'keywords': {'value': ['adapted weak topology; stochastic process; synthetic time series generation; path development']}, 'abstract': {'value': 'Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities, Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however, it was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems. Code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/5dcca4b22b94b2847cc0ece59e9e06e53c6636e1.zip'}, 'pdf': {'value': '/pdf/eb7aecacaf5d45f5ac40f8a3fe78d6f3122cb6e7.pdf'}, 'TLDR': {'value': 'This paper introduces the High Rank PCF Distance (HRPCFD) for metrizing extended weak convergence of stochastic processes, demonstrating its efficiency and effectiveness in numerical implementations such as conditional time series generation.'}, '_bibtex': {'value': '@inproceedings{\\ntao2024high,\\ntitle={High Rank Path Development: an approach to learning the filtration of stochastic processes},\\nauthor={Jiajie Tao and Hao Ni and Chong Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=w28i9oe9Xr}\\n}'}, 'paperhash': {'value': 'tao|high_rank_path_development_an_approach_to_learning_the_filtration_of_stochastic_processes'}},forum = 'w28i9oe9Xr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6797/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6797/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6797/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6797/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vymkuBMLlh',number = 13031,cdate = 1715733784456,pdate = 1727288026268,odate = 1730873954516,mdate = 1730873954537,tcdate = 1715733784456,tmdate = 1730873954537,ddate = None,content = {'title': {'value': 'Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand'}, 'authors': {'value': ['Md Musfiqur Rahman', 'Matt Jordan', 'Murat Kocaoglu']}, 'authorids': {'value': ['~Md_Musfiqur_Rahman1', '~Matt_Jordan1', '~Murat_Kocaoglu1']}, 'keywords': {'value': ['causal inference', 'causal graphs', 'deep generative models']}, 'TLDR': {'value': 'We propose a conditional generative model based approach to sample from any identifiable interventional or conditional interventional distribution given an arbitrary causal graph containing latent confounders.'}, 'abstract': {'value': 'Causal inference from observational data plays critical role in many applications in trustworthy machine learning.\\nWhile sound and complete algorithms exist to compute causal effects, many of them assume access to conditional likelihoods,\\n which is difficult to estimate for high-dimensional (particularly image) data. Researchers have alleviated this issue by simulating causal relations with neural models. However, when we have high-dimensional variables in the causal graph along with some unobserved confounders, no existing work can effectively sample from the un/conditional interventional distributions. In this work, we show how to sample from any identifiable interventional distribution given an arbitrary causal graph through a sequence of push-forward computations of conditional generative models, such as diffusion models. Our proposed algorithm follows the recursive steps of the existing likelihood-based identification algorithms to train a set of feed-forward models, and connect them in a specific way to sample from the desired distribution. We conduct experiments on a Colored MNIST dataset having both the treatment ($X$) and the target variables ($Y$) as images and sample from $P(y|do(x))$. Our algorithm also enables us to conduct a causal analysis to evaluate spurious correlations among input features of generative models pre-trained on the CelebA dataset. Finally, we generate high-dimensional interventional samples from the MIMIC-CXR dataset involving text and image variables.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d7a91169682e7030f7d0115904a50cf697b82461.pdf'}, 'supplementary_material': {'value': '/attachment/79fda2fc144a5e9f898e1bcc80c9c098b1661f07.zip'}, '_bibtex': {'value': '@inproceedings{\\nrahman2024conditional,\\ntitle={Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand},\\nauthor={Md Musfiqur Rahman and Matt Jordan and Murat Kocaoglu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vymkuBMLlh}\\n}'}, 'paperhash': {'value': 'rahman|conditional_generative_models_are_sufficient_to_sample_from_any_causal_effect_estimand'}},forum = 'vymkuBMLlh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13031/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13031/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13031/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13031/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vx4NgdyyVG',number = 4464,cdate = 1715416615695,pdate = 1727287751244,odate = 1730873875997,mdate = 1730873876017,tcdate = 1715416615695,tmdate = 1730873876017,ddate = None,content = {'title': {'value': 'Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation'}, 'authors': {'value': ['Jiaan Luo', 'Feng Hong', 'Jiangchao Yao', 'Bo Han', 'Ya Zhang', 'Yanfeng Wang']}, 'authorids': {'value': ['~Jiaan_Luo1', '~Feng_Hong1', '~Jiangchao_Yao1', '~Bo_Han1', '~Ya_Zhang1', '~Yanfeng_Wang1']}, 'keywords': {'value': ['machine learning', 'density ratio estimation', 'optimization']}, 'abstract': {'value': 'In deep learning, model performance often deteriorates when trained on highly imbalanced datasets, especially when evaluation metrics require robust generalization across underrepresented classes. To address the challenges posed by imbalanced data distributions, this study introduces a novel method utilizing density ratio estimation for dynamic class weight adjustment, termed as Re-weighting with Density Ratio (RDR). Our method adaptively adjusts the importance of each class during training, mitigates overfitting on dominant classes and enhances model adaptability across diverse datasets. Extensive experiments conducted on various large scale benchmark datasets validate the effectiveness of our method. Results demonstrate substantial improvements in generalization capabilities, particularly under severely imbalanced conditions.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/44e3160934c3d9637c541a15c8827e17ec5bba0e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nluo2024revive,\\ntitle={Revive Re-weighting in Imbalanced Learning by Density Ratio Estimation},\\nauthor={Jiaan Luo and Feng Hong and Jiangchao Yao and Bo Han and Ya Zhang and Yanfeng Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vx4NgdyyVG}\\n}'}, 'paperhash': {'value': 'luo|revive_reweighting_in_imbalanced_learning_by_density_ratio_estimation'}},forum = 'vx4NgdyyVG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4464/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4464/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4464/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4464/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vwgWbCxeAQ',number = 969,cdate = 1714275294222,pdate = 1727287648931,odate = 1730873844506,mdate = 1730873844523,tcdate = 1714275294222,tmdate = 1730873844523,ddate = None,content = {'title': {'value': 'Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective'}, 'authors': {'value': ['Yanan Zhang', 'Jiangmeng Li', 'Lixiang Liu', 'Wenwen Qiang']}, 'authorids': {'value': ['~Yanan_Zhang3', '~Jiangmeng_Li1', '~Lixiang_Liu1', '~Wenwen_Qiang1']}, 'keywords': {'value': ['causal', 'adaptation', 'foundational models']}, 'abstract': {'value': 'Foundational Vision-Language models such as CLIP have exhibited impressive generalization in downstream tasks. However, CLIP suffers from a two-level misalignment issue, i.e., task misalignment and data misalignment, when adapting to specific tasks. Soft prompt tuning has mitigated the task misalignment, yet the data misalignment remains a challenge. To analyze the impacts of the data misalignment, we revisit the pre-training and adaptation processes of CLIP and develop a structural causal model. We discover that while we expect to capture task-relevant information for downstream tasks accurately, the task-irrelevant knowledge impacts the prediction results and hampers the modeling of the true relationships between the images and the predicted classes. As task-irrelevant knowledge is unobservable, we leverage the front-door adjustment and propose Causality-Guided Semantic Decoupling and Classification (CDC) to mitigate the interference of task-irrelevant knowledge. Specifically, we decouple semantics contained in the data of downstream tasks and perform classification based on each semantic. Furthermore, we employ the Dempster-Shafer evidence theory to evaluate the uncertainty of each prediction generated by diverse semantics. Experiments conducted in multiple different settings have consistently demonstrated the effectiveness of CDC.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Guided by the theory of causation, we propose semantic decoupling and uncertainty modeling to conduct prompt tuning on CLIP for downstream tasks.'}, 'pdf': {'value': '/pdf/b0f849743c20730b56ef48ad02e259f767f16cf5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024rethinking,\\ntitle={Rethinking Misalignment in Vision-Language Model Adaptation from a Causal Perspective},\\nauthor={Yanan Zhang and Jiangmeng Li and Lixiang Liu and Wenwen Qiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vwgWbCxeAQ}\\n}'}, 'paperhash': {'value': 'zhang|rethinking_misalignment_in_visionlanguage_model_adaptation_from_a_causal_perspective'}},forum = 'vwgWbCxeAQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission969/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission969/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission969/-/Revision', 'NeurIPS.cc/2024/Conference/Submission969/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'vvpewjtnvm',number = 11555,cdate = 1715706588164,pdate = 1727287974958,odate = 1730873939764,mdate = 1736227811399,tcdate = 1715706588164,tmdate = 1736227811399,ddate = None,content = {'title': {'value': 'Low Precision Local Training is Enough for Federated Learning'}, 'authors': {'value': ['Zhiwei Li', 'Yiqiu LI', 'Binbin Lin', 'Zhongming Jin', 'WEIZHONG ZHANG']}, 'authorids': {'value': ['~Zhiwei_Li11', '~Yiqiu_LI1', '~Binbin_Lin3', '~Zhongming_Jin1', '~WEIZHONG_ZHANG2']}, 'keywords': {'value': ['Federated Learning', 'Low Precision Training']}, 'abstract': {'value': 'Federated Learning (FL) is a prevalent machine learning paradigm designed to address challenges posed by heterogeneous client data while preserving data privacy.\\n     Unlike distributed training, it typically orchestrates resource-constrained edge devices to communicate via a low-bandwidth communication network with a central server.  This urges the development of more computation and communication efficient training algorithms. In this paper, we propose an efficient FL paradigm,  where the local  models in the clients  are trained with  low-precision operations and communicated  with the server in low precision format, while only the model aggregation in the server is performed with  high-precision computation. We surprisingly find that   high precision models can be recovered from the  low precision local models with proper aggregation in the server. \\n     In this way, both the workload in the client-side and the communication cost can be significantly reduced.   We theoretically show that our proposed  paradigm can converge to the optimal solution as the training goes on, which demonstrates that low precision local training is enough for FL.  Our paradigm can be integrated with existing FL algorithms flexibly. Experiments across extensive benchmarks are conducted to showcase the effectiveness of our proposed method. Notably, the models trained by our method with the precision as low as 8 bits are  comparable  to those from the  full precision training. As a by-product, we show that low precision local training can relieve the over-fitting issue in local training, which under heterogeneous client data  can cause the client models drift further away from each other and lead to the failure in  model aggregation. Code is released at https://github.com/digbangbang/LPT-FL.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/aeb8909737e7debabd78bc8d5e4a4a9228bf658a.pdf'}, 'supplementary_material': {'value': '/attachment/1b5c1599f62ae74be65a7b4211e330a93706ba48.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024low,\\ntitle={Low Precision Local Training is Enough for Federated Learning},\\nauthor={Zhiwei Li and Yiqiu LI and Binbin Lin and Zhongming Jin and WEIZHONG ZHANG},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vvpewjtnvm}\\n}'}, 'paperhash': {'value': 'li|low_precision_local_training_is_enough_for_federated_learning'}},forum = 'vvpewjtnvm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11555/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11555/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11555/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11555/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vunJCq9PwU',number = 10918,cdate = 1715699052791,pdate = 1727287954863,odate = 1730873933267,mdate = 1730873933286,tcdate = 1715699052791,tmdate = 1730873933286,ddate = None,content = {'title': {'value': 'GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models'}, 'authors': {'value': ['ZAITANG LI', 'Pin-Yu Chen', 'Tsung-Yi Ho']}, 'authorids': {'value': ['~ZAITANG_LI1', '~Pin-Yu_Chen1', '~Tsung-Yi_Ho2']}, 'keywords': {'value': ['Adversarial Robustness']}, 'abstract': {'value': 'Current studies on adversarial robustness mainly focus on aggregating \\\\textit{local} robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true \\\\textit{global} robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called \\\\textit{GREAT Score}, for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In particular, we show high correlation and significantly reduced computation cost of GREAT Score when compared to the attack-based model ranking on RobustBench \\\\cite{croce2021robustbench}. (2) The use of generative models facilitates the approximation of the unknown data distribution. In our ablation study with different generative adversarial networks (GANs), we observe consistency between global robustness evaluation and the quality of GANs. (3) GREAT Score can be used for remote auditing of privacy-sensitive black-box models, as demonstrated by our robustness evaluation on several online facial recognition services.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9b455fd219afd050f8e2c5821f56718aa6c2426f.pdf'}, 'supplementary_material': {'value': '/attachment/a2a24d998ffc9934aafecc9a430c810763187870.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024great,\\ntitle={{GREAT} Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models},\\nauthor={ZAITANG LI and Pin-Yu Chen and Tsung-Yi Ho},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vunJCq9PwU}\\n}'}, 'paperhash': {'value': 'li|great_score_global_robustness_evaluation_of_adversarial_perturbation_using_generative_models'}},forum = 'vunJCq9PwU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10918/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10918/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10918/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10918/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vtRotUd539',number = 7506,cdate = 1715625495908,pdate = 1727287849056,odate = 1730873903608,mdate = 1730873903629,tcdate = 1715625495908,tmdate = 1730873903629,ddate = None,content = {'title': {'value': 'Average gradient outer product as a mechanism for deep neural collapse'}, 'authors': {'value': ['Daniel Beaglehole', 'Peter Súkeník', 'Marco Mondelli', 'Mikhail Belkin']}, 'authorids': {'value': ['~Daniel_Beaglehole1', '~Peter_Súkeník1', '~Marco_Mondelli1', '~Mikhail_Belkin1']}, 'keywords': {'value': ['Theory of deep learning', 'neural collapse', 'average gradient outer product', 'kernel methods', 'feature learning']}, 'TLDR': {'value': 'We demonstrate that feature learning through the average gradient outer product is a setting for deep neural collapse.'}, 'abstract': {'value': 'Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a variety of settings, its emergence is typically explained via data-agnostic approaches, such as the unconstrained features model. In this work, we introduce a data-dependent setting where DNC forms due to feature learning through the average gradient outer product (AGOP). The AGOP is defined with respect to a learned predictor and is equal to the uncentered covariance matrix of its input-output gradients averaged over the training dataset. Deep Recursive Feature Machines are a method that constructs a neural network by iteratively mapping the data with the AGOP and applying an untrained random feature map. We demonstrate theoretically and empirically that DNC occurs in Deep Recursive Feature Machines as a consequence of the projection with the AGOP matrix computed at each layer. We then provide evidence that this mechanism holds for neural networks more generally. We show that the right singular vectors and values of the weights can be responsible for the majority of within-class variability collapse for DNNs trained in the feature learning regime. As observed in recent work, this singular structure is highly correlated with that of the AGOP.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e6aa9a4011a802d00ba4c1202d7c30befc5c3233.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nbeaglehole2024average,\\ntitle={Average gradient outer product as a mechanism for deep neural collapse},\\nauthor={Daniel Beaglehole and Peter S{\\\\'u}ken{\\\\'\\\\i}k and Marco Mondelli and Mikhail Belkin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vtRotUd539}\\n}\"}, 'paperhash': {'value': 'beaglehole|average_gradient_outer_product_as_a_mechanism_for_deep_neural_collapse'}},forum = 'vtRotUd539',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7506/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7506/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7506/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7506/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vt2qkE1Oax',number = 11333,cdate = 1715703631958,pdate = 1727287967856,odate = 1730873937731,mdate = 1730873937745,tcdate = 1715703631958,tmdate = 1730873937745,ddate = None,content = {'title': {'value': 'Learning Segmentation from Point Trajectories'}, 'authors': {'value': ['Laurynas Karazija', 'Iro Laina', 'Christian Rupprecht', 'Andrea Vedaldi']}, 'authorids': {'value': ['~Laurynas_Karazija1', '~Iro_Laina1', '~Christian_Rupprecht1', '~Andrea_Vedaldi1']}, 'keywords': {'value': ['unsupervised segmentation', 'motion segmentation', 'point tracking']}, 'abstract': {'value': 'We consider the problem of segmenting objects in videos based on their motion and no other forms of supervision. Prior work has often approached this problem by using the principle of common fate, namely the fact that the motion of points that belong to the same object is strongly correlated. However, most authors have only considered instantaneous motion from optical flow. In this work, we present a way to train a segmentation network using long-term point trajectories as a supervisory signal to complement optical flow. The key difficulty is that long-term motion, unlike instantaneous motion, is difficult to model -- any parametric approximation is unlikely to capture complex motion patterns over long periods of time. We instead draw inspiration from subspace clustering approaches, proposing a loss function that seeks to group the trajectories into low-rank matrices where the motion of object points can be approximately explained as a linear combination of other point tracks. Our method outperforms the prior art on motion-based segmentation, which shows the utility of long-term motion and the effectiveness of our formulation.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/59c057325ef06b75796a650f052e761c2c377f1e.pdf'}, 'supplementary_material': {'value': '/attachment/368bc3f67b3140b95741558037789ba7f6f4fb96.zip'}, '_bibtex': {'value': '@inproceedings{\\nkarazija2024learning,\\ntitle={Learning Segmentation from Point Trajectories},\\nauthor={Laurynas Karazija and Iro Laina and Christian Rupprecht and Andrea Vedaldi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vt2qkE1Oax}\\n}'}, 'paperhash': {'value': 'karazija|learning_segmentation_from_point_trajectories'}},forum = 'vt2qkE1Oax',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11333/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11333/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11333/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11333/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'vpEq2bzsS0',number = 4673,cdate = 1715438621944,pdate = 1727287757447,odate = 1730873878209,mdate = 1730873878220,tcdate = 1715438621944,tmdate = 1730873878220,ddate = None,content = {'title': {'value': 'MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer'}, 'authors': {'value': ['Minghao Zhu', 'Zhengpu Wang', 'Mengxian Hu', 'Ronghao Dang', 'Xiao Lin', 'Xun Zhou', 'Chengju Liu', 'Qijun Chen']}, 'authorids': {'value': ['~Minghao_Zhu2', '~Zhengpu_Wang1', '~Mengxian_Hu1', '~Ronghao_Dang1', '~Xiao_Lin9', '~Xun_Zhou4', '~Chengju_Liu1', '~Qijun_Chen2']}, 'keywords': {'value': ['Video Recognition', 'Vision-Language Model', 'Transfer Learning']}, 'TLDR': {'value': 'This paper presents MoTE, a knowledge transfer framework that addresses the generalization and specialization trade-off problem when transferring VLMs to the video domain, achieving an optimal balance between close-set and zero-shot performance.'}, 'abstract': {'value': 'Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose Weight Merging Regularization, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 \\\\& 600, UCF, and HMDB. Code is available at https://github.com/ZMHH-H/MoTE.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/70da6cead9fc894a60ab0f52a2a9b0e9149a95a3.pdf'}, 'supplementary_material': {'value': '/attachment/5e13fb402a20c4d39b0ae911f1f589e529732d74.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024mote,\\ntitle={Mo{TE}: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer},\\nauthor={Minghao Zhu and Zhengpu Wang and Mengxian Hu and Ronghao Dang and Xiao Lin and Xun Zhou and Chengju Liu and Qijun Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vpEq2bzsS0}\\n}'}, 'paperhash': {'value': 'zhu|mote_reconciling_generalization_with_specialization_for_visuallanguage_to_video_knowledge_transfer'}},forum = 'vpEq2bzsS0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4673/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4673/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4673/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4673/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'voJCpdlw53',number = 2897,cdate = 1715179048303,pdate = 1727287704177,odate = 1730873861755,mdate = 1730873861766,tcdate = 1715179048303,tmdate = 1730873861766,ddate = None,content = {'title': {'value': 'UltraPixel: Advancing Ultra High-Resolution Image Synthesis to New Peaks'}, 'authors': {'value': ['Jingjing Ren', 'Wenbo Li', 'Haoyu Chen', 'Renjing Pei', 'Bin Shao', 'Yong Guo', 'Long Peng', 'Fenglong Song', 'Lei Zhu']}, 'authorids': {'value': ['~Jingjing_Ren1', '~Wenbo_Li6', '~Haoyu_Chen2', '~Renjing_Pei1', '~Bin_Shao2', '~Yong_Guo1', '~Long_Peng1', '~Fenglong_Song1', '~Lei_Zhu1']}, 'keywords': {'value': ['Image generation', 'Diffusion Models']}, 'abstract': {'value': 'Ultra-high-resolution image generation poses great challenges, such as increased semantic planning complexity and detail synthesis difficulties, alongside substantial training resource demands. We present UltraPixel, a novel architecture utilizing cascade diffusion models to generate high-quality images at multiple resolutions (\\\\textit{e.g.}, 1K, 2K, and 4K) within a single model, while maintaining computational efficiency. UltraPixel leverages semantics-rich representations of lower-resolution images in a later denoising stage to guide the whole generation of highly detailed high-resolution images, significantly reducing complexity. Specifically, we introduce implicit neural representations for continuous upsampling and scale-aware normalization layers adaptable to various resolutions. Notably, both low- and high-resolution processes are performed in the most compact space, sharing the majority of parameters with less than 3$\\\\%$ additional parameters for high-resolution outputs, largely enhancing training and inference efficiency. Our model achieves fast training with reduced data requirements, producing photo-realistic high-resolution images and demonstrating state-of-the-art performance in extensive experiments.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/17e36670338ced1f6346b71e16283ec8543c38f0.pdf'}, 'supplementary_material': {'value': '/attachment/71676d77d8ab54565410aff68c91516abc3b9a95.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nren2024ultrapixel,\\ntitle={UltraPixel: Advancing Ultra High-Resolution Image Synthesis to New Peaks},\\nauthor={Jingjing Ren and Wenbo Li and Haoyu Chen and Renjing Pei and Bin Shao and Yong Guo and Long Peng and Fenglong Song and Lei Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=voJCpdlw53}\\n}'}, 'paperhash': {'value': 'ren|ultrapixel_advancing_ultra_highresolution_image_synthesis_to_new_peaks'}},forum = 'voJCpdlw53',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2897/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2897/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2897/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission2897/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vo5LONGAdo',number = 1521,cdate = 1714662136582,pdate = 1727287663169,odate = 1730873849441,mdate = 1730873849452,tcdate = 1714662136582,tmdate = 1730873849452,ddate = None,content = {'title': {'value': 'Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising'}, 'authors': {'value': ['Gongfan Fang', 'Xinyin Ma', 'Xinchao Wang']}, 'authorids': {'value': ['~Gongfan_Fang2', '~Xinyin_Ma1', '~Xinchao_Wang1']}, 'keywords': {'value': ['Diffusion Models', 'Multiple Experts']}, 'abstract': {'value': 'Transformer-based diffusion models have achieved significant advancements across a variety of generative tasks. However, producing high-quality outputs typically necessitates large transformer models, which result in substantial training and inference overhead. In this work, we investigate an alternative approach involving multiple experts for denoising, and introduce RemixDiT, a novel method designed to enhance output quality at a low cost. The goal of RemixDiT is to craft N diffusion experts for different denoising timesteps, yet without the need for expensive training of N independent models. To achieve this, RemixDiT employs K basis models (where K < N) and utilizes learnable mixing coefficients to adaptively craft expert models. This design offers two significant advantages: first, although the total model size is increased, the model produced by the mixing operation shares the same architecture as a plain model, making the overall model as efficient as a standard diffusion transformer. Second, the learnable mixing adaptively allocates model capacity across timesteps, thereby effectively improving generation quality. Experiments conducted on the ImageNet dataset demonstrate that RemixDiT achieves promising results compared to standard diffusion transformers and other multiple-expert methods.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1b07fc3963a665cf6f8f91e36966e55baf261cdb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfang2024remixdit,\\ntitle={Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising},\\nauthor={Gongfan Fang and Xinyin Ma and Xinchao Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vo5LONGAdo}\\n}'}, 'paperhash': {'value': 'fang|remixdit_mixing_diffusion_transformers_for_multiexpert_denoising'}},forum = 'vo5LONGAdo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1521/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1521/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1521/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1521/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vjw4TIf8Bo',number = 9901,cdate = 1715687612800,pdate = 1727287924249,odate = 1730873924479,mdate = 1730873924499,tcdate = 1715687612800,tmdate = 1730873924499,ddate = None,content = {'title': {'value': 'PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition'}, 'authors': {'value': ['Jinghui Lu', 'Yanjie Wang', 'Ziwei Yang', 'Xuejing Liu', 'Brian Mac Namee', 'Can Huang']}, 'authorids': {'value': ['~Jinghui_Lu2', '~Yanjie_Wang2', '~Ziwei_Yang4', '~Xuejing_Liu1', '~Brian_Mac_Namee1', '~Can_Huang1']}, 'keywords': {'value': ['NER', 'inference speedup', 'LLM']}, 'abstract': {'value': 'In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets. All resources are available at https://github.com/GeorgeLuImmortal/PaDeLLM_NER.'}, 'pdf': {'value': '/pdf/d41c719a3d75bbd4f587ed89d649f8de4444d47f.pdf'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nlu2024padellmner,\\ntitle={PaDe{LLM}-{NER}: Parallel Decoding in Large Language Models for Named Entity Recognition},\\nauthor={Jinghui Lu and Yanjie Wang and Ziwei Yang and Xuejing Liu and Brian Mac Namee and Can Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vjw4TIf8Bo}\\n}'}, 'paperhash': {'value': 'lu|padellmner_parallel_decoding_in_large_language_models_for_named_entity_recognition'}},forum = 'vjw4TIf8Bo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9901/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9901/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9901/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9901/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'vjsd8Bcipv',number = 1418,cdate = 1714627253810,pdate = 1727287660405,odate = 1730873848714,mdate = 1737011817757,tcdate = 1714627253810,tmdate = 1737011817757,ddate = None,content = {'title': {'value': '$\\\\epsilon$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise'}, 'authors': {'value': ['Jialiang Wang', 'Xiong Zhou', 'Deming Zhai', 'Junjun Jiang', 'Xiangyang Ji', 'Xianming Liu']}, 'authorids': {'value': ['~Jialiang_Wang3', '~Xiong_Zhou3', '~Deming_Zhai2', '~Junjun_Jiang2', '~Xiangyang_Ji1', '~Xianming_Liu5']}, 'keywords': {'value': ['Learning with Noisy Labels', 'Robust Loss Function', 'Excess Risk Bound']}, 'TLDR': {'value': 'We propose a simple yet effective method for mitigating label noise, which can be implemented with just two lines of code.'}, 'abstract': {'value': 'Noisy labels pose a common challenge for training accurate deep neural networks. To mitigate label noise, prior studies have proposed various robust loss functions to achieve noise tolerance in the presence of label noise, particularly symmetric losses. However, they usually suffer from the underfitting issue due to the overly strict symmetric condition. In this work, we propose a simple yet effective approach for relaxing the symmetric condition, namely **$\\\\epsilon$-softmax**, which simply modifies the outputs of the softmax layer to approximate one-hot vectors with a controllable error $\\\\epsilon$. Essentially, ***$\\\\epsilon$-softmax** not only acts as an alternative for the softmax layer, but also implicitly plays the crucial role in modifying the loss function.* We  prove theoretically that **$\\\\epsilon$-softmax** can achieve noise-tolerant learning with controllable excess risk bound for almost any loss function. Recognizing that **$\\\\epsilon$-softmax**-enhanced losses may slightly reduce fitting ability on clean datasets, we further incorporate them with one symmetric loss, thereby achieving a better trade-off between robustness and effective learning. Extensive experiments demonstrate the superiority of our method in mitigating synthetic and real-world label noise.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f2d333e0e79783e10cbe29dab26d04b300ab7d1c.pdf'}, 'supplementary_material': {'value': '/attachment/3463b6976fed01ff0ba9d8e36d7454d8cea41634.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024epsilonsoftmax,\\ntitle={\\\\${\\\\textbackslash}epsilon\\\\$-Softmax: Approximating One-Hot Vectors for Mitigating Label Noise},\\nauthor={Jialiang Wang and Xiong Zhou and Deming Zhai and Junjun Jiang and Xiangyang Ji and Xianming Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vjsd8Bcipv}\\n}'}, 'paperhash': {'value': 'wang|\\\\epsilonsoftmax_approximating_onehot_vectors_for_mitigating_label_noise'}},forum = 'vjsd8Bcipv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1418/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1418/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1418/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1418/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vjCFnYTg67',number = 19466,cdate = 1715791107287,pdate = 1727288210398,odate = 1730873995752,mdate = 1730873995767,tcdate = 1715791107287,tmdate = 1730873995767,ddate = None,content = {'title': {'value': 'Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature'}, 'authors': {'value': ['Tong Zhou', 'Xuandong Zhao', 'Xiaolin Xu', 'Shaolei Ren']}, 'authorids': {'value': ['~Tong_Zhou3', '~Xuandong_Zhao1', '~Xiaolin_Xu3', '~Shaolei_Ren1']}, 'keywords': {'value': ['Large language model', 'Text Provenance', 'Spoofing attacks']}, 'abstract': {'value': 'Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6ec48e6544bb032898b579c21036f7d2dead471a.pdf'}, 'supplementary_material': {'value': '/attachment/bf094d4b2a2681c4060d01552d47e3f093379cd8.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024bileve,\\ntitle={Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature},\\nauthor={Tong Zhou and Xuandong Zhao and Xiaolin Xu and Shaolei Ren},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vjCFnYTg67}\\n}'}, 'paperhash': {'value': 'zhou|bileve_securing_text_provenance_in_large_language_models_against_spoofing_with_bilevel_signature'}},forum = 'vjCFnYTg67',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19466/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19466/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19466/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19466/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vjAORqq71s',number = 15365,cdate = 1715759541459,pdate = 1727288095791,odate = 1730873972427,mdate = 1730873972449,tcdate = 1715759541459,tmdate = 1730873972449,ddate = None,content = {'title': {'value': 'Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms'}, 'authors': {'value': ['Felix Petersen', 'Christian Borgelt', 'Tobias Sutter', 'Hilde Kuehne', 'Oliver Deussen', 'Stefano Ermon']}, 'authorids': {'value': ['~Felix_Petersen1', '~Christian_Borgelt1', '~Tobias_Sutter1', '~Hilde_Kuehne5', '~Oliver_Deussen1', '~Stefano_Ermon1']}, 'keywords': {'value': ['differentiable', 'empirical fisher', 'hessian', 'continuous', 'relaxations', 'accelerated training', \"newton's method\", 'stochastic smoothing']}, 'abstract': {'value': \"When training neural networks with custom objectives, such as ranking losses and shortest-path losses, a common problem is that they are, per se, non-differentiable. A popular approach is to continuously relax the objectives to provide gradients, enabling learning. However, such differentiable relaxations are often non-convex and can exhibit vanishing and exploding gradients, making them (already in isolation) hard to optimize. Here, the loss function poses the bottleneck when training a deep neural network. We present Newton Losses, a method for improving the performance of existing hard to optimize losses by exploiting their second-order information via their empirical Fisher and Hessian matrices. Instead of training the neural network with second-order techniques, we only utilize the loss function's second-order information to replace it by a Newton Loss, while training the network with gradient descent.  This makes our method computationally efficient. We apply Newton Losses to eight differentiable algorithms for sorting and shortest-paths, achieving significant improvements for less-optimized differentiable algorithms, and consistent improvements, even for well-optimized differentiable algorithms.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/db338206d5b05120e97346114d0d20170da316fe.pdf'}, '_bibtex': {'value': '@inproceedings{\\npetersen2024newton,\\ntitle={Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms},\\nauthor={Felix Petersen and Christian Borgelt and Tobias Sutter and Hilde Kuehne and Oliver Deussen and Stefano Ermon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vjAORqq71s}\\n}'}, 'paperhash': {'value': 'petersen|newton_losses_using_curvature_information_for_learning_with_differentiable_algorithms'}},forum = 'vjAORqq71s',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15365/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15365/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15365/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vieIamY2Gi',number = 3243,cdate = 1715246106046,pdate = 1727287714008,odate = 1730873864533,mdate = 1736761924270,tcdate = 1715246106046,tmdate = 1736761924270,ddate = None,content = {'title': {'value': 'Improved off-policy training of diffusion samplers'}, 'authors': {'value': ['Marcin Sendera', 'Minsu Kim', 'Sarthak Mittal', 'Pablo Lemos', 'Luca Scimeca', 'Jarrid Rector-Brooks', 'Alexandre Adam', 'Yoshua Bengio', 'Nikolay Malkin']}, 'authorids': {'value': ['~Marcin_Sendera1', '~Minsu_Kim2', '~Sarthak_Mittal1', '~Pablo_Lemos1', '~Luca_Scimeca1', '~Jarrid_Rector-Brooks2', '~Alexandre_Adam1', '~Yoshua_Bengio1', '~Nikolay_Malkin1']}, 'keywords': {'value': ['diffusion models', 'amortized inference', 'stochastic control', 'GFlowNets']}, 'abstract': {'value': 'We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at [this link](https://github.com/GFNOrg/gfn-diffusion) as a base for future work on diffusion models for amortized inference.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We benchmark various approaches to training neural SDEs to match a target distribution and study ways to improve training and credit assignment.'}, 'pdf': {'value': '/pdf/444c02396fa1bcdf5db528a1e89da47cdafed517.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsendera2024improved,\\ntitle={Improved off-policy training of diffusion samplers},\\nauthor={Marcin Sendera and Minsu Kim and Sarthak Mittal and Pablo Lemos and Luca Scimeca and Jarrid Rector-Brooks and Alexandre Adam and Yoshua Bengio and Nikolay Malkin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vieIamY2Gi}\\n}'}, 'paperhash': {'value': 'sendera|improved_offpolicy_training_of_diffusion_samplers'}},forum = 'vieIamY2Gi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3243/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3243/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3243/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3243/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vh9yEPLeyD',number = 2755,cdate = 1715157157198,pdate = 1727287699226,odate = 1730873860584,mdate = 1730873860595,tcdate = 1715157157198,tmdate = 1730873860595,ddate = None,content = {'title': {'value': 'Can We Leave Deepfake Data Behind in Training Deepfake Detector?'}, 'authors': {'value': ['Jikang Cheng', 'Zhiyuan Yan', 'Ying Zhang', 'Yuhao Luo', 'Zhongyuan Wang', 'Chen Li']}, 'authorids': {'value': ['~Jikang_Cheng1', '~Zhiyuan_Yan3', '~Ying_Zhang9', '~Yuhao_Luo4', '~Zhongyuan_Wang4', '~Chen_Li11']}, 'keywords': {'value': ['Deepfake Detection', 'Data Synthesis', 'Hybrid Training']}, 'abstract': {'value': 'The generalization ability of deepfake detectors is vital for their applications in real-world scenarios. One effective solution to enhance this ability is to train the models with manually-blended data, which we termed \\'\\'blendfake\\'\\', encouraging models to learn generic forgery artifacts like blending boundary. Interestingly, current SoTA methods utilize blendfake $\\\\textit{without}$ incorporating any deepfake data in their training process. This is likely because previous empirical observations suggest that vanilla hybrid training (VHT), which combines deepfake and blendfake data, results in inferior performance to methods using only blendfake data (so-called \"1+1<2\"). Therefore, a critical question arises: Can we leave deepfake behind and rely solely on blendfake data to train an effective deepfake detector? Intuitively, as deepfakes also contain additional informative forgery clues ($\\\\textit{e.g.,}$ deep generative artifacts), excluding all deepfake data in training deepfake detectors seems counter-intuitive. In this paper, we rethink the role of blendfake in detecting deepfakes and formulate the process from \"real to blendfake to deepfake\" to be a $\\\\textit{progressive transition}$. Specifically, blendfake and deepfake can be explicitly delineated as the oriented pivot anchors between \"real-to-fake\" transitions. The accumulation of forgery information should be oriented and progressively increasing during this transition process. To this end, we propose an $\\\\underline{O}$riented $\\\\underline{P}$rogressive $\\\\underline{R}$egularizor (OPR) to establish the constraints that compel the distribution of anchors to be discretely arranged. Furthermore, we introduce feature bridging to facilitate the smooth transition between adjacent anchors.  Extensive experiments confirm that our design allows leveraging forgery information from both blendfake and deepfake effectively and comprehensively. Code is available at https://github.com/beautyremain/ProDet.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c5f94fbd9d60ce8c8dc283b8970f02e3f631bd22.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ncheng2024can,\\ntitle={Can We Leave Deepfake Data Behind in Training Deepfake Detector?},\\nauthor={Jikang Cheng and Zhiyuan Yan and Ying Zhang and Yuhao Luo and Zhongyuan Wang and Chen Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vh9yEPLeyD}\\n}'}, 'paperhash': {'value': 'cheng|can_we_leave_deepfake_data_behind_in_training_deepfake_detector'}},forum = 'vh9yEPLeyD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2755/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2755/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2755/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission2755/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'veMnGKXvTx',number = 3198,cdate = 1715241700832,pdate = 1727287712665,odate = 1730873864166,mdate = 1730873864186,tcdate = 1715241700832,tmdate = 1730873864186,ddate = None,content = {'title': {'value': 'Homology Consistency Constrained Efficient Tuning for Vision-Language Models'}, 'authors': {'value': ['Huatian Zhang', 'Lei Zhang', 'Yongdong Zhang', 'Zhendong Mao']}, 'authorids': {'value': ['~Huatian_Zhang1', '~Lei_Zhang54', '~Yongdong_Zhang2', '~Zhendong_Mao1']}, 'keywords': {'value': ['Efficient Transfer Learning', 'Vision-Language Models', 'Persistent Homology', 'Topological Data Analysis']}, 'abstract': {'value': 'Efficient transfer learning has shown remarkable performance in tuning large-scale vision-language models (VLMs) toward downstream tasks with limited data resources. The key challenge of efficient transfer lies in adjusting image-text alignment to be task-specific while preserving pre-trained general knowledge. However, existing methods adjust image-text alignment merely on a set of observed samples, e.g., data set and external knowledge base, which cannot guarantee to keep the correspondence of general concepts between image and text latent manifolds without being disrupted and thereby a weak generalization of the adjusted alignment. In this work, we propose a Homology Consistency (HC) constraint for efficient transfer on VLMs, which explicitly constrains the correspondence of image and text latent manifolds through structural equivalence based on persistent homology in downstream tuning. Specifically, we build simplicial complex on the top of data to mimic the topology of latent manifolds, then track the persistence of the homology classes of topological features across multiple scales, and guide the directions of persistence tracks in image and text manifolds to coincide each other, with a deviating perturbation additionally. For practical application, we tailor the implementation of our proposed HC constraint for two main paradigms of adapter tuning. Extensive experiments on few-shot learning over 11 datasets and domain generalization demonstrate the effectiveness and robustness of our method.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5cab7ed9375bde4b2a85485640533b73a623b658.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024homology,\\ntitle={Homology Consistency Constrained Efficient Tuning for Vision-Language Models},\\nauthor={Huatian Zhang and Lei Zhang and Yongdong Zhang and Zhendong Mao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=veMnGKXvTx}\\n}'}, 'paperhash': {'value': 'zhang|homology_consistency_constrained_efficient_tuning_for_visionlanguage_models'}},forum = 'veMnGKXvTx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3198/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3198/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3198/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3198/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vcGEV6m5m2',number = 16736,cdate = 1715773878160,pdate = 1727288134837,odate = 1730873980999,mdate = 1730873981011,tcdate = 1715773878160,tmdate = 1730873981011,ddate = None,content = {'title': {'value': 'Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis'}, 'authors': {'value': ['Diwen Wan', 'Yuxiang Wang', 'Ruijie Lu', 'Gang Zeng']}, 'authorids': {'value': ['~Diwen_Wan1', '~Yuxiang_Wang4', '~Ruijie_Lu1', '~Gang_Zeng1']}, 'keywords': {'value': ['Gaussian Splatting', 'View Synthesis', 'Skeleton Discovery']}, 'abstract': {'value': 'While novel view synthesis for dynamic scenes has made significant progress, capturing skeleton models of objects and re-posing them remains a challenging task. To tackle this problem, in this paper, we propose a novel approach to automatically discover the associated skeleton model for dynamic objects from videos without the need for object-specific templates. Our approach utilizes 3D Gaussian Splatting and superpoints to reconstruct dynamic objects. Treating superpoints as rigid parts, we can discover the underlying skeleton model through intuitive cues and optimize it using the kinematic model.  Besides, an adaptive control strategy is applied to avoid the emergence of redundant superpoints.  Extensive experiments demonstrate the effectiveness and efficiency of our method in obtaining re-posable 3D objects. Not only can our approach achieve excellent visual fidelity, but it also allows for the real-time rendering of high-resolution images.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose template-free method based on 3D-GS and superpoints  to discover the skeleton model of an articulated object and achieve real-time rendering.'}, 'pdf': {'value': '/pdf/2f9205ca247e1a22333796012f9e65cf0b3497a6.pdf'}, 'supplementary_material': {'value': '/attachment/d429c7e04316cca6211055e365343b99475db2fd.zip'}, '_bibtex': {'value': '@inproceedings{\\nwan2024templatefree,\\ntitle={Template-free Articulated Gaussian Splatting for Real-time Reposable Dynamic View Synthesis},\\nauthor={Diwen Wan and Yuxiang Wang and Ruijie Lu and Gang Zeng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vcGEV6m5m2}\\n}'}, 'paperhash': {'value': 'wan|templatefree_articulated_gaussian_splatting_for_realtime_reposable_dynamic_view_synthesis'}},forum = 'vcGEV6m5m2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16736/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16736/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16736/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16736/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vYUx8j5KK2',number = 99,cdate = 1713834262381,pdate = 1727287630436,odate = 1730873838336,mdate = 1730873838353,tcdate = 1713834262381,tmdate = 1730873838353,ddate = None,content = {'title': {'value': 'Curriculum Fine-tuning of Vision Foundation Model for Medical Image Classification Under Label Noise'}, 'authors': {'value': ['Yeonguk Yu', 'Minhwan Ko', 'Sungho Shin', 'Kangmin Kim', 'Kyoobin Lee']}, 'authorids': {'value': ['~Yeonguk_Yu1', '~Minhwan_Ko1', '~Sungho_Shin3', '~Kangmin_Kim2', '~Kyoobin_Lee2']}, 'keywords': {'value': ['Learning with noisy label', 'medical image classification']}, 'abstract': {'value': 'Deep neural networks have demonstrated remarkable performance in various vision tasks, but their success heavily depends on the quality of the training data. Noisy labels are a critical issue in medical datasets and can significantly degrade model performance. Previous clean sample selection methods have not utilized the well pre-trained features of vision foundation models (VFMs) and assumed that training begins from scratch. In this paper, we propose CUFIT, a curriculum fine-tuning paradigm of VFMs for medical image classification under label noise. Our method is motivated by the fact that linear probing of VFMs is relatively unaffected by noisy samples, as it does not update the feature extractor of the VFM, thus robustly classifying the training samples. Subsequently, curriculum fine-tuning of two adapters is conducted, starting with clean sample selection from the linear probing phase. Our experimental results demonstrate that CUFIT outperforms previous methods across various medical image benchmarks. Specifically, our method surpasses previous baselines by 5.0\\\\%, 2.1\\\\%, 4.6\\\\%, and 5.8\\\\% at a 40\\\\% noise rate on the HAM10000, APTOS-2019, BloodMnist, and OrgancMnist datasets, respectively. Furthermore, we provide extensive analyses to demonstrate the impact of our method on noisy label detection. For instance, our method shows higher label precision and recall compared to previous approaches. Our work highlights the potential of leveraging VFMs in medical image classification under challenging conditions of noisy labels.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose CUFIT, a robust fine-tuning method for vision foundation models under noisy label conditions, based on the advantages of linear probing and adapters.'}, 'pdf': {'value': '/pdf/d5aa08cc841e6f20d9d81a173a38781d31ff4224.pdf'}, 'supplementary_material': {'value': '/attachment/ce53cc85637bcaefe46b51ee2e5002314c6d2f0a.zip'}, '_bibtex': {'value': '@inproceedings{\\nyu2024curriculum,\\ntitle={Curriculum Fine-tuning of Vision Foundation Model for Medical Image Classification Under Label Noise},\\nauthor={Yeonguk Yu and Minhwan Ko and Sungho Shin and Kangmin Kim and Kyoobin Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vYUx8j5KK2}\\n}'}, 'paperhash': {'value': 'yu|curriculum_finetuning_of_vision_foundation_model_for_medical_image_classification_under_label_noise'}},forum = 'vYUx8j5KK2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission99/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission99/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission99/-/Revision', 'NeurIPS.cc/2024/Conference/Submission99/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vWSll6M9pj',number = 7724,cdate = 1715635217404,pdate = 1727287856420,odate = 1730873905563,mdate = 1730873905580,tcdate = 1715635217404,tmdate = 1730873905580,ddate = None,content = {'title': {'value': 'Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs'}, 'authors': {'value': ['Alexandros Haliassos', 'Rodrigo Mira', 'Honglie Chen', 'Zoe Landgraf', 'Stavros Petridis', 'Maja Pantic']}, 'authorids': {'value': ['~Alexandros_Haliassos1', '~Rodrigo_Mira1', '~Honglie_Chen1', '~Zoe_Landgraf1', '~Stavros_Petridis1', '~Maja_Pantic2']}, 'keywords': {'value': ['Speech recognition', 'lipreading', 'self-supervised learning', 'semi-supervised learning']}, 'abstract': {'value': 'Research in auditory, visual, and audiovisual speech recognition (ASR, VSR, and AVSR, respectively) has traditionally been conducted independently. Even recent self-supervised studies addressing two or all three tasks simultaneously tend to yield separate models, leading to disjoint inference pipelines with increased memory requirements and redundancies. This paper proposes unified training strategies for these systems. We demonstrate that training a single model for all three tasks enhances VSR and AVSR performance, overcoming typical optimisation challenges when training from scratch. Moreover, we introduce a greedy pseudo-labelling approach to more effectively leverage unlabelled samples, addressing shortcomings in related self-supervised methods. Finally, we develop a self-supervised pre-training method within our framework, proving its effectiveness alongside our semi-supervised approach. Despite using a single model for all tasks, our unified approach achieves state-of-the-art performance on LRS3 for ASR, VSR, and AVSR compared to recent methods. Code will be made publicly available.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10f24815426948411d0ebd5225a50f68f81cf70b.pdf'}, 'TLDR': {'value': 'We propose a unified speech recognition method for ASR, VSR, and AVSR.'}, 'supplementary_material': {'value': '/attachment/f8a1ad275276e55234a8c57c9bf487524d451199.zip'}, '_bibtex': {'value': '@inproceedings{\\nhaliassos2024unified,\\ntitle={Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs},\\nauthor={Alexandros Haliassos and Rodrigo Mira and Honglie Chen and Zoe Landgraf and Stavros Petridis and Maja Pantic},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vWSll6M9pj}\\n}'}, 'paperhash': {'value': 'haliassos|unified_speech_recognition_a_single_model_for_auditory_visual_and_audiovisual_inputs'}},forum = 'vWSll6M9pj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7724/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7724/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7724/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7724/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'vUrOuc6NR3',number = 5516,cdate = 1715545303524,pdate = 1727287786320,odate = 1730873885317,mdate = 1730873885337,tcdate = 1715545303524,tmdate = 1730873885337,ddate = None,content = {'title': {'value': 'DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control'}, 'authors': {'value': ['Zichen Jeff Cui', 'Hengkai Pan', 'Aadhithya Iyer', 'Siddhant Haldar', 'Lerrel Pinto']}, 'authorids': {'value': ['~Zichen_Jeff_Cui1', '~Hengkai_Pan1', '~Aadhithya_Iyer1', '~Siddhant_Haldar1', '~Lerrel_Pinto1']}, 'keywords': {'value': ['Robot learning', 'representation learning', 'self-supervised learning']}, 'abstract': {'value': 'Imitation learning has proven to be a powerful tool for training complex visuo-motor policies. However, current methods often require hundreds to thousands of expert demonstrations to handle high-dimensional visual observations. A key reason for this poor data efficiency is that visual representations are predominantly either pretrained on out-of-domain data or trained directly through a behavior cloning objective. In this work, we present DynaMo, a new in-domain, self-supervised method for learning visual representations. Given a set of expert demonstrations, we jointly learn a latent inverse dynamics model and a forward dynamics model over a sequence of image embeddings, predicting the next frame in latent space, without augmentations, contrastive sampling, or access to ground truth actions. Importantly, DynaMo does not require any out-of-domain data such as Internet datasets or cross-embodied datasets. On a suite of six simulated and real environments, we show that representations learned with DynaMo significantly improve downstream imitation learning performance over prior self-supervised learning objectives, and pretrained representations. Gains from using DynaMo hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP, and nearest neighbors. Finally, we ablate over key components of DynaMo and measure its impact on downstream policy performance. Robot videos are best viewed at https://dynamo-ssl.github.io.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'DynaMo, a new self-supervised method for pretraining visual encoders for downstream visuomotor control by explicitly modeling dynamics in the demonstration observations.'}, 'pdf': {'value': '/pdf/a80285940d66984b6d99e1990c79614edb3af61b.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncui2024dynamo,\\ntitle={DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control},\\nauthor={Zichen Jeff Cui and Hengkai Pan and Aadhithya Iyer and Siddhant Haldar and Lerrel Pinto},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vUrOuc6NR3}\\n}'}, 'paperhash': {'value': 'cui|dynamo_indomain_dynamics_pretraining_for_visuomotor_control'}},forum = 'vUrOuc6NR3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5516/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5516/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5516/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5516/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vU512K8vrR',number = 7458,cdate = 1715623134830,pdate = 1727287847776,odate = 1730873903308,mdate = 1730873903326,tcdate = 1715623134830,tmdate = 1730873903326,ddate = None,content = {'title': {'value': 'Unveiling LoRA Intrinsic Ranks via Salience Analysis'}, 'authors': {'value': ['Wenjun Ke', 'Jiahao Wang', 'Peng Wang', 'Jiajun Liu', 'Dong Nie', 'Guozheng Li', 'Yining Li']}, 'authorids': {'value': ['~Wenjun_Ke1', '~Jiahao_Wang12', '~Peng_Wang11', '~Jiajun_Liu3', '~Dong_Nie1', '~Guozheng_Li3', '~Yining_Li5']}, 'keywords': {'value': ['Parameter-Efficient Fine-Tuning', 'Low-Rank Adaptation', 'LoRA']}, 'abstract': {'value': 'The immense parameter scale of large language models underscores the necessity for parameter-efficient fine-tuning methods. Methods based on Low-Rank Adaptation (LoRA) assume the low-rank characteristics of the incremental matrix and optimize the matrix obtained from low-rank decomposition. Although effective, these methods are constrained by a fixed and unalterable intrinsic rank, neglecting the variable importance of matrices. Consequently, methods for adaptive rank allocation are proposed, among which AdaLoRA demonstrates excellent fine-tuning performance. AdaLoRA conducts adaptation based on singular value decomposition (SVD), dynamically allocating intrinsic ranks according to importance. However, it still struggles to achieve a balance between fine-tuning effectiveness and efficiency, leading to limited rank allocation space. Additionally, the importance measurement focuses only on parameters with minimal impact on the loss, neglecting the dominant role of singular values in SVD-based matrices and the fluctuations during training. To address these issues, we propose SalientLoRA, which adaptively optimizes intrinsic ranks of LoRA via salience measurement. Firstly, during rank allocation, the salience measurement analyses the variation of singular value magnitudes across multiple time steps and establishes their inter-dependency relationships to assess the matrix importance. This measurement mitigates instability and randomness that may arise during importance assessment. Secondly, to achieve a balance between fine-tuning performance and efficiency, we propose an adaptive adjustment of time-series window, which adaptively controls the size of time-series for significance measurement and rank reduction during training, allowing for rapid rank allocation while maintaining training stability. This mechanism enables matrics to set a higher initial rank, thus expanding the allocation space for ranks. To evaluate the generality of our method across various tasks, we conduct experiments on natural language understanding (NLU), natural language generation (NLG), and large model instruction tuning tasks. Experimental results demonstrate the superiority of SalientLoRA, which outperforms state-of-the-art methods by 0.96\\\\%-3.56\\\\% on multiple datasets. Furthermore, as the rank allocation space expands, our method ensures fine-tuning efficiency, achieving a speed improvement of 94.5\\\\% compared to AdaLoRA. The code is publicly available at https://github.com/Heyest/SalientLoRA.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/212291a21697f8ab6df33a882c29090800c75abd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nke2024unveiling,\\ntitle={Unveiling Lo{RA} Intrinsic Ranks via Salience Analysis},\\nauthor={Wenjun Ke and Jiahao Wang and Peng Wang and Jiajun Liu and Dong Nie and Guozheng Li and Yining Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vU512K8vrR}\\n}'}, 'paperhash': {'value': 'ke|unveiling_lora_intrinsic_ranks_via_salience_analysis'}},forum = 'vU512K8vrR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7458/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7458/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7458/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7458/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vU1SiBb57j',number = 7094,cdate = 1715612482736,pdate = 1727287835183,odate = 1730873899368,mdate = 1730873899382,tcdate = 1715612482736,tmdate = 1730873899382,ddate = None,content = {'title': {'value': 'Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient'}, 'authors': {'value': ['Zechu Li', 'Rickmer Krohn', 'Tao Chen', 'Anurag Ajay', 'Pulkit Agrawal', 'Georgia Chalvatzaki']}, 'authorids': {'value': ['~Zechu_Li1', '~Rickmer_Krohn1', '~Tao_Chen1', '~Anurag_Ajay1', '~Pulkit_Agrawal1', '~Georgia_Chalvatzaki1']}, 'keywords': {'value': ['Reinforcement Learning', 'Diffusion Model', 'Multimodal Learning', 'Unsupervised Skill Discovery']}, 'TLDR': {'value': 'Deep Diffusion Policy Gradient is a novel RL algorithm that successfully trains diffusion policies online, discovering and maintaining multimodal behaviors in complex environments.'}, 'abstract': {'value': \"Deep reinforcement learning (RL) algorithms typically parameterize the policy as a deep network that outputs either a deterministic action or a stochastic one modeled as a Gaussian distribution, hence restricting learning to a single behavioral mode. Meanwhile, diffusion models emerged as a powerful framework for multimodal learning. However, the use of diffusion policies in online RL is hindered by the intractability of policy likelihood approximation, as well as the greedy objective of RL methods that can easily skew the policy to a single mode. This paper presents Deep Diffusion Policy Gradient (DDiffPG), a novel actor-critic algorithm that learns from scratch multimodal policies parameterized as diffusion models while discovering and maintaining versatile behaviors. DDiffPG explores and discovers multiple modes through off-the-shelf unsupervised clustering combined with novelty-based intrinsic motivation. DDiffPG forms a multimodal training batch and utilizes mode-specific Q-learning to mitigate the inherent greediness of the RL objective, ensuring the improvement of the diffusion policy across all modes. Our approach further allows the policy to be conditioned on mode-specific embeddings to explicitly control the learned modes. Empirical studies validate DDiffPG's capability to master multimodal behaviors in complex, high-dimensional continuous control tasks with sparse rewards, also showcasing proof-of-concept dynamic online replanning when navigating mazes with unseen obstacles. Our project page is available at https://supersglzc.github.io/projects/ddiffpg/.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c1d49e0a18ba30e91f8393bb7381467ee4dc0bc6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024learning,\\ntitle={Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient},\\nauthor={Zechu Li and Rickmer Krohn and Tao Chen and Anurag Ajay and Pulkit Agrawal and Georgia Chalvatzaki},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vU1SiBb57j}\\n}'}, 'paperhash': {'value': 'li|learning_multimodal_behaviors_from_scratch_with_diffusion_policy_gradient'}},forum = 'vU1SiBb57j',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7094/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7094/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7094/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7094/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vS5NC7jtCI',number = 9248,cdate = 1715677273928,pdate = 1727287905089,odate = 1730873919107,mdate = 1730873919125,tcdate = 1715677273928,tmdate = 1730873919125,ddate = None,content = {'title': {'value': 'AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models'}, 'authors': {'value': ['Yabin Zhang', 'Lei Zhang']}, 'authorids': {'value': ['~Yabin_Zhang2', '~Lei_Zhang2']}, 'keywords': {'value': ['Adaptive negative proxy', 'OOD detection', 'vision-language models']}, 'TLDR': {'value': 'We propose adaptive negative proxies by exploring potential OOD images during testing for OOD detection'}, 'abstract': {'value': 'Recent research has shown that pre-trained vision-language models are effective at identifying out-of-distribution (OOD) samples by using negative labels as guidance. However, employing consistent negative labels across different OOD datasets often results in semantic misalignments, as these text labels may not accurately reflect the actual space of OOD images. To overcome this issue, we introduce \\\\textit{adaptive negative proxies}, which are dynamically generated during testing by exploring actual OOD images, to align more closely with the underlying OOD label space and enhance the efficacy of negative proxy guidance. Specifically, our approach utilizes a feature memory bank to selectively cache discriminative features from test images, representing the targeted OOD distribution. This facilitates the creation of proxies that can better align with specific OOD datasets. While task-adaptive proxies average features to reflect the unique characteristics of each dataset, the sample-adaptive proxies weight features based on their similarity to individual test samples, exploring detailed sample-level nuances. The final score for identifying OOD samples integrates static negative labels with our proposed adaptive proxies, effectively combining textual and visual knowledge for enhanced performance. Our method is training-free and annotation-free, and it maintains fast testing speed. Extensive experiments across various benchmarks demonstrate the effectiveness of our approach, abbreviated as AdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg significantly outperforms existing methods, with a 2.45\\\\% increase in AUROC and a 6.48\\\\% reduction in FPR95. Codes are available at \\\\url{https://github.com/YBZh/OpenOOD-VLM}.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7d0df0f32fd41d7ace28bcfe957e1aeb0ad53117.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024adaneg,\\ntitle={AdaNeg: Adaptive Negative Proxy Guided {OOD} Detection with Vision-Language Models},\\nauthor={Yabin Zhang and Lei Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vS5NC7jtCI}\\n}'}, 'paperhash': {'value': 'zhang|adaneg_adaptive_negative_proxy_guided_ood_detection_with_visionlanguage_models'}},forum = 'vS5NC7jtCI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9248/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9248/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9248/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9248/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'vP9qAzr2Gw',number = 7225,cdate = 1715615453494,pdate = 1727287839789,odate = 1730873900540,mdate = 1730873900608,tcdate = 1715615453494,tmdate = 1730873900608,ddate = None,content = {'title': {'value': 'Supra-Laplacian Encoding for Transformer on Dynamic Graphs'}, 'authors': {'value': ['Yannis Karmim', 'Marc Lafon', \"Raphael Fournier-S'niehotta\", 'Nicolas THOME']}, 'authorids': {'value': ['~Yannis_Karmim1', '~Marc_Lafon1', \"~Raphael_Fournier-S'niehotta1\", '~Nicolas_THOME2']}, 'keywords': {'value': ['Dynamic graphs', 'Link prediction', 'Transformer', 'supra-Lapacian encoding']}, 'abstract': {'value': \"Fully connected Graph Transformers (GT) have rapidly become prominent in the static graph community as an alternative to Message-Passing models, which suffer from a lack of expressivity, oversquashing, and under-reaching.\\nHowever, in a dynamic context, by interconnecting all nodes at multiple snapshots with self-attention,GT loose both structural and temporal information. In this work, we introduce Supra-LAplacian encoding for spatio-temporal TransformErs (SLATE), a new spatio-temporal encoding to leverage the GT architecture while keeping spatio-temporal information.\\nSpecifically, we transform Discrete Time Dynamic Graphs into multi-layer graphs and take advantage of the spectral properties of their associated supra-Laplacian matrix.\\nOur second contribution explicitly model nodes' pairwise relationships with a cross-attention mechanism, providing an accurate edge representation for dynamic link prediction.\\nSLATE outperforms numerous state-of-the-art methods based on Message-Passing Graph Neural Networks combined with recurrent models (e.g, LSTM), and Dynamic Graph Transformers,\\non~9 datasets. Code is open-source and available at this link https://github.com/ykrmm/SLATE.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'New spectral spatio-temporal encoding for fully connected Dynamic Graph Transformer in dynamic link prediction'}, 'pdf': {'value': '/pdf/acb194ce31b86916495f23d4c82ee0d79949b5cb.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nkarmim2024supralaplacian,\\ntitle={Supra-Laplacian Encoding for Transformer on Dynamic Graphs},\\nauthor={Yannis Karmim and Marc Lafon and Raphael Fournier-S'niehotta and Nicolas THOME},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vP9qAzr2Gw}\\n}\"}, 'paperhash': {'value': 'karmim|supralaplacian_encoding_for_transformer_on_dynamic_graphs'}},forum = 'vP9qAzr2Gw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7225/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7225/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7225/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission7225/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vMMzjCr5Zj',number = 18023,cdate = 1715783226190,pdate = 1727288167550,odate = 1730873987367,mdate = 1730873987391,tcdate = 1715783226190,tmdate = 1730873987391,ddate = None,content = {'title': {'value': 'Large Pre-trained time series models for cross-domain Time series analysis tasks'}, 'authors': {'value': ['Harshavardhan Kamarthi', 'B. Aditya Prakash']}, 'authorids': {'value': ['~Harshavardhan_Kamarthi1', '~B._Aditya_Prakash2']}, 'keywords': {'value': ['Time-series', 'Self-supervised Learning']}, 'TLDR': {'value': 'Novel time-series segmentation and self-supervised training for building general pre-trained time-series models capable of time-series analysis across multiple domains.'}, 'abstract': {'value': 'Large pre-trained models have been vital in recent advancements in domains like language and vision, making model training for individual downstream tasks more efficient and provide superior performance. However, tackling time-series analysis tasks usually involves designing and training a separate model from scratch leveraging training data and domain expertise specific to the task. We tackle a significant challenge for pre-training a foundational time-series model from multi-domain time-series datasets: extracting semantically useful tokenized inputs to the model across heterogeneous time-series from different domains. We propose Large Pre-trained Time-series Models (LPTM) that introduces a novel method of adaptive segmentation that automatically identifies optimal dataset-specific segmentation strategy during pre-training. This enables LPTM to perform similar to or better than domain-specific state-of-art model when fine-tuned to different downstream time-series analysis tasks and under zero-shot settings. LPTM achieves superior forecasting and time-series classification results taking up to 40% less data and 50% less training time compared to state-of-art baselines.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1fe1368c9227b6a85b1429442439e39607b01c26.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkamarthi2024large,\\ntitle={Large Pre-trained time series models for cross-domain Time series analysis tasks},\\nauthor={Harshavardhan Kamarthi and B. Aditya Prakash},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vMMzjCr5Zj}\\n}'}, 'paperhash': {'value': 'kamarthi|large_pretrained_time_series_models_for_crossdomain_time_series_analysis_tasks'}},forum = 'vMMzjCr5Zj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18023/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18023/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18023/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18023/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vJSNsSFO95',number = 830,cdate = 1714120335740,pdate = 1727287645543,odate = 1730873843446,mdate = 1730873843464,tcdate = 1714120335740,tmdate = 1730873843464,ddate = None,content = {'title': {'value': 'Flaws can be Applause: Unleashing Potential of Segmenting Ambiguous Objects in SAM'}, 'authors': {'value': ['Chenxin Li', 'Yuzhihuang', 'Wuyang Li', 'Hengyu Liu', 'Xinyu Liu', 'Qing Xu', 'Zhen Chen', 'Yue Huang', 'Yixuan Yuan']}, 'authorids': {'value': ['~Chenxin_Li1', '~Yuzhihuang1', '~Wuyang_Li1', '~Hengyu_Liu2', '~Xinyu_Liu7', '~Qing_Xu4', '~Zhen_Chen9', '~Yue_Huang1', '~Yixuan_Yuan2']}, 'keywords': {'value': ['Foundation Model', 'Ambiguous Segmentation', 'Uncertainty']}, 'abstract': {'value': 'As the vision foundation models like the Segment Anything Model (SAM) demonstrate potent universality, they also present challenges in giving ambiguous and uncertain predictions. Significant variations in the model output and granularity can occur with simply subtle changes in the prompt, contradicting the consensus requirement for the robustness of a model. While some established works have been dedicated to stabilizing and fortifying the prediction of SAM, this paper takes a unique path to explore how this flaw can be inverted into an advantage when modeling inherently ambiguous data distributions. We introduce an optimization framework based on a conditional variational autoencoder, which jointly models the prompt and the granularity of the object with a latent probability distribution. This approach enables the model to adaptively perceive and represent the real ambiguous label distribution, taming SAM to produce a series of diverse, convincing, and reasonable segmentation outputs controllably. Extensive experiments on several practical deployment scenarios involving ambiguity demonstrates the exceptional performance of our framework. Project page: \\\\url{https://a-sa-m.github.io/}.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d9d0ed08e91694b0c1b594f2e8d5bece62aa7179.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024flaws,\\ntitle={Flaws can be Applause: Unleashing Potential of Segmenting Ambiguous Objects in {SAM}},\\nauthor={Chenxin Li and Yuzhihuang and Wuyang Li and Hengyu Liu and Xinyu Liu and Qing Xu and Zhen Chen and Yue Huang and Yixuan Yuan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vJSNsSFO95}\\n}'}, 'supplementary_material': {'value': '/attachment/5a5fc9bddf32e5cd5e2a12aed251543b0f3a0969.zip'}, 'paperhash': {'value': 'li|flaws_can_be_applause_unleashing_potential_of_segmenting_ambiguous_objects_in_sam'}},forum = 'vJSNsSFO95',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission830/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission830/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission830/-/Revision', 'NeurIPS.cc/2024/Conference/Submission830/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vJMMdFfL0A',number = 8269,cdate = 1715656674631,pdate = 1727287874552,odate = 1730873910767,mdate = 1736994382987,tcdate = 1715656674631,tmdate = 1736994382987,ddate = None,content = {'title': {'value': 'The Benefits of Balance: From Information Projections to Variance Reduction'}, 'authors': {'value': ['Lang Liu', 'Ronak Mehta', 'Soumik Pal', 'Zaid Harchaoui']}, 'authorids': {'value': ['~Lang_Liu1', '~Ronak_Mehta2', '~Soumik_Pal1', '~Zaid_Harchaoui1']}, 'keywords': {'value': ['regularized optimal transport', 'self-supervised learning', 'variance reduction', 'alternating projection']}, 'TLDR': {'value': 'We present a data balancing approach to distribution estimation that provides theoretical interpretations of the various self-supervised training schemes.'}, 'abstract': {'value': 'Data balancing across multiple modalities and sources appears in various forms in foundation models in machine learning and AI, e.g., in CLIP and DINO. We show that data balancing across modalities and sources actually offers an unsuspected benefit: variance reduction. We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators. Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/2c60cc6ac01579a70a7ea77c5b63e302a681e2d7.zip'}, 'pdf': {'value': '/pdf/e19715b92b9edf482506f5332dc738e0bd203da9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024the,\\ntitle={The Benefits of Balance: From Information Projections to Variance Reduction},\\nauthor={Lang Liu and Ronak Mehta and Soumik Pal and Zaid Harchaoui},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vJMMdFfL0A}\\n}'}, 'paperhash': {'value': 'liu|the_benefits_of_balance_from_information_projections_to_variance_reduction'}},forum = 'vJMMdFfL0A',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8269/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8269/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8269/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8269/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vJLTcCBZVT',number = 11410,cdate = 1715704522055,pdate = 1727287970411,odate = 1730873938323,mdate = 1730873938343,tcdate = 1715704522055,tmdate = 1730873938343,ddate = None,content = {'title': {'value': 'Improving Subgroup Robustness via Data Selection'}, 'authors': {'value': ['Saachi Jain', 'Kimia Hamidieh', 'Kristian Georgiev', 'Andrew Ilyas', 'Marzyeh Ghassemi', 'Aleksander Madry']}, 'authorids': {'value': ['~Saachi_Jain1', '~Kimia_Hamidieh1', '~Kristian_Georgiev1', '~Andrew_Ilyas1', '~Marzyeh_Ghassemi2', '~Aleksander_Madry1']}, 'keywords': {'value': ['group robustness', 'fairness', 'data attribution', 'machine learning']}, 'TLDR': {'value': 'Improving model performance on under-represented subpopulations by removing harmful training data.'}, 'abstract': {'value': \"Machine learning models can often fail on subgroups that are underrepresented\\nduring training. While dataset balancing can improve performance on\\nunderperforming groups, it requires access to training group annotations and can\\nend up removing large portions of the dataset. In this paper, we introduce\\nData Debiasing with Datamodels (D3M), a debiasing approach\\nwhich isolates and removes specific training examples that drive the model's\\nfailures on minority groups. Our approach enables us to efficiently train\\ndebiased classifiers while removing only a small number of examples, and does\\nnot require training group annotations or additional hyperparameter tuning.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a3f46e22e6e41370e2c814be79b1e92e6e971d7c.pdf'}, 'supplementary_material': {'value': '/attachment/4e7fb703da07c8282e859c6705e6beec29fa7332.zip'}, '_bibtex': {'value': '@inproceedings{\\njain2024improving,\\ntitle={Improving Subgroup Robustness via Data Selection},\\nauthor={Saachi Jain and Kimia Hamidieh and Kristian Georgiev and Andrew Ilyas and Marzyeh Ghassemi and Aleksander Madry},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vJLTcCBZVT}\\n}'}, 'paperhash': {'value': 'jain|improving_subgroup_robustness_via_data_selection'}},forum = 'vJLTcCBZVT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11410/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11410/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11410/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11410/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vIP8IWmZlN',number = 296,cdate = 1713861440473,pdate = 1727287634716,odate = 1730873839837,mdate = 1730873839848,tcdate = 1713861440473,tmdate = 1730873839848,ddate = None,content = {'title': {'value': 'Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication'}, 'authors': {'value': ['Olaf Lipinski', 'Adam Sobey', 'Federico Cerutti', 'Timothy J. Norman']}, 'authorids': {'value': ['~Olaf_Lipinski1', '~Adam_Sobey1', '~Federico_Cerutti1', '~Timothy_J._Norman1']}, 'keywords': {'value': ['Emergent Communication', 'Spatial References', 'Spatial Deixis', 'NPMI', 'NLP']}, 'abstract': {'value': 'Effective communication requires the ability to refer to specific parts of an observation in relation to others. While emergent communication literature shows success in developing various language properties, no research has shown the emergence of such positional references. This paper demonstrates how agents can communicate about spatial relationships within their observations. The results indicate that agents can develop a language capable of expressing the relationships between parts of their observation, achieving over 90% accuracy when trained in a referential game which requires such communication. Using a collocation measure, we demonstrate how the agents create such references. This analysis suggests that agents use a mixture of non-compositional and compositional messages to convey spatial relationships. We also show that the emergent language is interpretable by humans. The translation accuracy is tested by communicating with the receiver agent, where the receiver achieves over 78% accuracy using parts of this lexicon, confirming that the interpretation of the emergent language was successful.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/542546bc3b321700b242332d3fe1d91c56e85f07.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlipinski2024speaking,\\ntitle={Speaking Your Language: Spatial Relationships in Interpretable Emergent Communication},\\nauthor={Olaf Lipinski and Adam Sobey and Federico Cerutti and Timothy J. Norman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vIP8IWmZlN}\\n}'}, 'paperhash': {'value': 'lipinski|speaking_your_language_spatial_relationships_in_interpretable_emergent_communication'}},forum = 'vIP8IWmZlN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission296/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission296/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission296/-/Revision', 'NeurIPS.cc/2024/Conference/Submission296/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vIOKLMl6wu',number = 9008,cdate = 1715673084165,pdate = 1727287898158,odate = 1730873916863,mdate = 1738016897967,tcdate = 1715673084165,tmdate = 1738016897967,ddate = None,content = {'title': {'value': 'LOVA3: Learning to Visual Question Answering, Asking and Assessment'}, 'authors': {'value': ['Hengyuan Zhao', 'Pan Zhou', 'Difei Gao', 'Zechen Bai', 'Mike Zheng Shou']}, 'authorids': {'value': ['~Hengyuan_Zhao2', '~Pan_Zhou3', '~Difei_Gao1', '~Zechen_Bai1', '~Mike_Zheng_Shou1']}, 'keywords': {'value': ['Multimodal Large Language Model', 'Instruction Tuning', 'Visual Question Answering', 'Visual Question Assessment.']}, 'TLDR': {'value': \"In this work, we present  a novel ``Learning tO Visual Question Answering, Asking and Assessment'' framework that can enable the current MLLMs to obtain the additional abilities to ask and assess the question based on the visual input.\"}, 'abstract': {'value': \"Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. In this study, we introduce LOVA3, an innovative framework named ``Learning tO Visual Question Answering, Asking and Assessment,'' designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions \\nwill enhance their multimodal comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c6c15295d63406edbf7ea78fdfc7b0b77523de41.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzhao2024lova,\\ntitle={{LOVA}3: Learning to Visual Question Answering, Asking and Assessment},\\nauthor={Hengyuan Zhao and Pan Zhou and Difei Gao and Zechen Bai and Mike Zheng Shou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vIOKLMl6wu}\\n}'}, 'paperhash': {'value': 'zhao|lova3_learning_to_visual_question_answering_asking_and_assessment'}},forum = 'vIOKLMl6wu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9008/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9008/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9008/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission9008/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vI1WqFn15v',number = 6911,cdate = 1715607759295,pdate = 1727287828588,odate = 1730873897507,mdate = 1730873897520,tcdate = 1715607759295,tmdate = 1730873897520,ddate = None,content = {'title': {'value': 'Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes'}, 'authors': {'value': ['Xiaomeng Hu', 'Pin-Yu Chen', 'Tsung-Yi Ho']}, 'authorids': {'value': ['~Xiaomeng_Hu1', '~Pin-Yu_Chen1', '~Tsung-Yi_Ho2']}, 'keywords': {'value': ['Large Language Models', 'Jailbreak Detection', 'AI Alignment and Safety']}, 'abstract': {'value': \"Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the **Refusal Loss** of LLMs and then proposes a method called **Gradient Cuff** to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9781a3bd6f23c741e28496d0beae9114337a6e01.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024gradient,\\ntitle={Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes},\\nauthor={Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vI1WqFn15v}\\n}'}, 'paperhash': {'value': 'hu|gradient_cuff_detecting_jailbreak_attacks_on_large_language_models_by_exploring_refusal_loss_landscapes'}},forum = 'vI1WqFn15v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6911/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6911/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6911/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6911/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vH7GcaDhAo',number = 398,cdate = 1713885771775,pdate = 1727287636818,odate = 1730873840703,mdate = 1730873840721,tcdate = 1713885771775,tmdate = 1730873840721,ddate = None,content = {'title': {'value': 'RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions'}, 'authors': {'value': ['Ziyao Zeng', 'Yangchao Wu', 'Hyoungseob Park', 'Daniel Wang', 'Fengyu Yang', 'Stefano Soatto', 'Dong Lao', 'Byung-Woo Hong', 'Alex Wong']}, 'authorids': {'value': ['~Ziyao_Zeng2', '~Yangchao_Wu1', '~Hyoungseob_Park1', '~Daniel_Wang2', '~Fengyu_Yang1', '~Stefano_Soatto3', '~Dong_Lao1', '~Byung-Woo_Hong4', '~Alex_Wong2']}, 'keywords': {'value': ['Monocular Depth Estimation', 'Vision-Language Model', 'Multimodal Learning']}, 'abstract': {'value': 'We propose a method for metric-scale monocular depth estimation. Inferring depth from a single image is an ill-posed problem due to the loss of scale from perspective projection during the image formation process. Any scale chosen is a bias, typically stemming from training on a dataset; hence, existing works have instead opted to use relative (normalized, inverse) depth. Our goal is to recover metric-scaled depth maps through a linear transformation. The crux of our method lies in the observation that certain objects (e.g., cars, trees, street signs) are typically found or associated with certain types of scenes (e.g., outdoor). We explore whether language descriptions can be used to transform relative depth predictions to those in metric scale. Our method, RSA , takes as input a text caption describing objects present in an image and outputs the parameters of a linear transformation which can be applied globally to a relative depth map to yield metric-scaled depth predictions. We demonstrate our method on recent general-purpose monocular depth models on indoors (NYUv2, VOID) and outdoors (KITTI). When trained on multiple datasets, RSA can serve as a general alignment module in zero-shot settings. Our method improves over common practices in aligning relative to metric depth and results in predictions that are comparable to an upper bound of fitting relative depth to ground truth via a linear transformation. Code is available at: https://github.com/Adonis-galaxy/RSA.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2ada30851f1515e3885ffaa16845efacb06685cd.pdf'}, 'TLDR': {'value': 'We use language descriptions to transform relative depth to metric depth.'}, '_bibtex': {'value': '@inproceedings{\\nzeng2024rsa,\\ntitle={{RSA}: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions},\\nauthor={Ziyao Zeng and Yangchao Wu and Hyoungseob Park and Daniel Wang and Fengyu Yang and Stefano Soatto and Dong Lao and Byung-Woo Hong and Alex Wong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vH7GcaDhAo}\\n}'}, 'supplementary_material': {'value': '/attachment/145ff85c6d8eb8143719185dac54d9559b372fe0.zip'}, 'paperhash': {'value': 'zeng|rsa_resolving_scale_ambiguities_in_monocular_depth_estimators_through_language_descriptions'}},forum = 'vH7GcaDhAo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission398/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission398/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission398/-/Revision', 'NeurIPS.cc/2024/Conference/Submission398/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vDlj3veE9a',number = 5538,cdate = 1715547456474,pdate = 1727287787148,odate = 1730873885842,mdate = 1730873885857,tcdate = 1715547456474,tmdate = 1730873885857,ddate = None,content = {'title': {'value': 'The Space Complexity of Approximating Logistic Loss'}, 'authors': {'value': ['Gregory Dexter', 'Petros Drineas', 'Rajiv Khanna']}, 'authorids': {'value': ['~Gregory_Dexter1', '~Petros_Drineas1', '~Rajiv_Khanna1']}, 'keywords': {'value': ['logistic regression', 'data structure', 'space complexity', 'approximation algorithm']}, 'abstract': {'value': 'We provide space complexity lower bounds for data structures that approximate logistic loss up to $\\\\epsilon$-relative error on a logistic regression problem with data $\\\\mathbf{X} \\\\in \\\\mathbb{R}^{n \\\\times d}$ and labels $\\\\mathbf{y} \\\\in \\\\\\\\{-1,1\\\\\\\\}^d$. The space complexity of existing coreset constructions depend on a natural complexity measure $\\\\mu_\\\\mathbf{y}(\\\\mathbf{X})$. We give an $\\\\tilde{\\\\Omega}(\\\\frac{d}{\\\\epsilon^2})$ space complexity lower bound in the regime $\\\\mu_\\\\mathbf{y}(\\\\mathbf{X}) = \\\\mathcal{O}(1)$ that shows existing coresets are optimal in this regime up to lower order factors. We also prove a general $\\\\tilde{\\\\Omega}(d\\\\cdot \\\\mu_\\\\mathbf{y}(\\\\mathbf{X}))$ space lower bound when $\\\\epsilon$ is constant, showing that the dependency on $\\\\mu_\\\\mathbf{y}(\\\\mathbf{X})$ is not an artifact of mergeable coresets. Finally, we refute a prior conjecture that $\\\\mu_\\\\mathbf{y}(\\\\mathbf{X})$ is hard to compute by providing an efficient linear programming formulation, and we empirically compare our algorithm to prior approximate methods.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d07d7ff0d145d5e17efb7382670c8cce125b4acd.pdf'}, 'supplementary_material': {'value': '/attachment/3e23d03b67d834f5fc6972a78671f1a59b25f284.zip'}, '_bibtex': {'value': '@inproceedings{\\ndexter2024the,\\ntitle={The Space Complexity of Approximating Logistic Loss},\\nauthor={Gregory Dexter and Petros Drineas and Rajiv Khanna},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vDlj3veE9a}\\n}'}, 'paperhash': {'value': 'dexter|the_space_complexity_of_approximating_logistic_loss'}},forum = 'vDlj3veE9a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5538/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5538/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5538/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5538/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vCOgjBIZuL',number = 9372,cdate = 1715678922185,pdate = 1727287908723,odate = 1730873920271,mdate = 1730873920291,tcdate = 1715678922185,tmdate = 1730873920291,ddate = None,content = {'title': {'value': 'Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer'}, 'authors': {'value': ['Shuang Wu', 'Youtian Lin', 'Yifei Zeng', 'Feihu Zhang', 'Jingxi Xu', 'Philip Torr', 'Xun Cao', 'Yao Yao']}, 'authorids': {'value': ['~Shuang_Wu14', '~Youtian_Lin1', '~Yifei_Zeng1', '~Feihu_Zhang3', '~Jingxi_Xu2', '~Philip_Torr1', '~Xun_Cao1', '~Yao_Yao1']}, 'keywords': {'value': ['3D Generation', 'Diffsion Model']}, 'abstract': {'value': 'Generating high-quality 3D assets from text and images has long been challenging, primarily due to the absence of scalable 3D representations capable of capturing intricate geometry distributions. In this work, we introduce Direct3D, a native 3D generative model scalable to in-the-wild input images, without requiring a multi-view diffusion model or SDS optimization. Our approach comprises two primary components: a Direct 3D Variational Auto-Encoder (D3D-VAE) and a Direct 3D Diffusion Transformer (D3D-DiT). D3D-VAE efficiently encodes high-resolution 3D shapes into a compact and continuous latent triplane space. Notably, our method directly supervises the decoded geometry using a semi-continuous surface sampling strategy, diverging from previous methods relying on rendered images as supervision signals. D3D-DiT models the distribution of encoded 3D latents and is specifically designed to fuse positional information from the three feature maps of the triplane latent, enabling a native 3D generative model scalable to large-scale 3D datasets. Additionally, we introduce an innovative image-to-3D generation pipeline incorporating semantic and pixel-level image conditions, allowing the model to produce 3D shapes consistent with the provided conditional image input. Extensive experiments demonstrate the superiority of our large-scale pre-trained Direct3D over previous image-to-3D approaches, achieving significantly better generation quality and generalization ability, thus establishing a new state-of-the-art for 3D content creation. Project page: https://www.neural4d.com/research/direct3d.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propse a novel approach for direct 3D shape generation from a single image, bypassing the need for multi-view reconstruction.'}, 'pdf': {'value': '/pdf/99efa36a5bad48e445b21a9e053764ed5033cd4b.pdf'}, 'supplementary_material': {'value': '/attachment/ef348c73d41cd09b5eb26862dc6174b56f1b3629.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwu2024directd,\\ntitle={Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer},\\nauthor={Shuang Wu and Youtian Lin and Yifei Zeng and Feihu Zhang and Jingxi Xu and Philip Torr and Xun Cao and Yao Yao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vCOgjBIZuL}\\n}'}, 'paperhash': {'value': 'wu|direct3d_scalable_imageto3d_generation_via_3d_latent_diffusion_transformer'}},forum = 'vCOgjBIZuL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9372/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9372/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9372/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9372/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'vCIc9BXzze',number = 14846,cdate = 1715754421817,pdate = 1727288080833,odate = 1730873968640,mdate = 1730873968658,tcdate = 1715754421817,tmdate = 1730873968658,ddate = None,content = {'title': {'value': 'Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models'}, 'authors': {'value': ['Ziyi Zhou', 'Xinwei Guo', 'Jiashi Gao', 'Xiangyu Zhao', 'Shiyao Zhang', 'Xin Yao', 'Xuetao Wei']}, 'authorids': {'value': ['~Ziyi_Zhou4', '~Xinwei_Guo2', '~Jiashi_Gao1', '~Xiangyu_Zhao1', '~Shiyao_Zhang1', '~Xin_Yao1', '~Xuetao_Wei2']}, 'keywords': {'value': ['Large Language Model', 'Moral Consistency', 'Evaluation', 'Ethics']}, 'abstract': {'value': \"Large Language Models (LLMs) have demonstrated remarkable capabilities, surpassing human experts in various benchmark tests and playing a vital role in various industry sectors. Despite their effectiveness, a notable drawback of LLMs is their inconsistent moral behavior, which raises ethical concerns. This work delves into symmetric moral consistency in large language models and demonstrates that modern LLMs lack sufficient consistency ability in moral scenarios. Our extensive investigation of twelve popular LLMs reveals that their assessed consistency scores are influenced by position bias and selection bias rather than their intrinsic abilities. We propose a new framework tSMC, which gauges the effects of these biases and effectively mitigates the bias impact based on the Kullback–Leibler divergence to pinpoint LLMs' mitigated Symmetric Moral Consistency. We find that the ability of LLMs to maintain consistency varies across different moral scenarios. Specifically, LLMs show more consistency in scenarios with clear moral answers compared to those where no choice is morally perfect. The average consistency score of 12 LLMs ranges from $60.7\\\\%$ in high-ambiguity moral scenarios to $84.8\\\\%$ in low-ambiguity moral scenarios.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/c0175cacff3d23b0a19c97bed89b98a5abe896c6.zip'}, 'pdf': {'value': '/pdf/71c70424648af46a283c5a9b7e72090a29d59df7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024unveiling,\\ntitle={Unveiling the Bias Impact on Symmetric Moral Consistency of Large Language Models},\\nauthor={Ziyi Zhou and Xinwei Guo and Jiashi Gao and Xiangyu Zhao and Shiyao Zhang and Xin Yao and Xuetao Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vCIc9BXzze}\\n}'}, 'paperhash': {'value': 'zhou|unveiling_the_bias_impact_on_symmetric_moral_consistency_of_large_language_models'}},forum = 'vCIc9BXzze',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14846/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14846/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14846/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14846/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vBxeeH1X4y',number = 5531,cdate = 1715546425004,pdate = 1727287786968,odate = 1730873885669,mdate = 1736922374969,tcdate = 1715546425004,tmdate = 1736922374969,ddate = None,content = {'title': {'value': '2D-OOB: Attributing Data Contribution Through Joint Valuation Framework'}, 'authors': {'value': ['Yifan Sun', 'Jingyan Shen', 'Yongchan Kwon']}, 'authorids': {'value': ['~Yifan_Sun8', '~Jingyan_Shen1', '~Yongchan_Kwon1']}, 'keywords': {'value': ['Data valuation', 'Cell-level attribution', 'Outlier detection']}, 'TLDR': {'value': 'We introduce a joint valuation framework that attributes data contribution through assigning the valuation scores at the cell level.'}, 'abstract': {'value': \"Data valuation has emerged as a powerful framework for quantifying each datum's contribution to the training of a machine learning model. However, it is crucial to recognize that the quality of cells within a single data point can vary greatly in practice. For example, even in the case of an abnormal data point, not all cells are necessarily noisy. The single scalar score assigned by existing data valuation methods blurs the distinction between noisy and clean cells of a data point, making it challenging to interpret the data values. In this paper, we propose 2D-OOB, an out-of-bag estimation framework for jointly determining helpful (or detrimental) samples as well as the particular cells that drive them. Our comprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art performance across multiple use cases while being exponentially faster. Specifically, 2D-OOB shows promising results in detecting and rectifying fine-grained outliers at the cell level, and localizing backdoor triggers in data poisoning attacks.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bb8798d104ff0114a9b51e1c2499cdbd72d14fdd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024doob,\\ntitle={2D-{OOB}: Attributing Data Contribution Through Joint Valuation Framework},\\nauthor={Yifan Sun and Jingyan Shen and Yongchan Kwon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vBxeeH1X4y}\\n}'}, 'paperhash': {'value': 'sun|2doob_attributing_data_contribution_through_joint_valuation_framework'}},forum = 'vBxeeH1X4y',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5531/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5531/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5531/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5531/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vBlzen37i0',number = 4819,cdate = 1715463985577,pdate = 1727287761911,odate = 1730873879442,mdate = 1730873879461,tcdate = 1715463985577,tmdate = 1730873879461,ddate = None,content = {'title': {'value': 'Optimal deep learning of holomorphic operators between Banach spaces'}, 'authors': {'value': ['Ben Adcock', 'Nick Dexter', 'Sebastian Moraga']}, 'authorids': {'value': ['~Ben_Adcock1', '~Nick_Dexter1', '~Sebastian_Moraga1']}, 'keywords': {'value': ['Deep learning', 'operator learning', 'parametric PDEs', 'deep neural networks', 'generalization error', 'optimal algorithms']}, 'TLDR': {'value': 'We show that deep learning with fully-connected deep neural networks is optimal for learning holomorphic operators'}, 'abstract': {'value': \"Operator learning problems arise in many key areas of scientific computing where Partial Differential Equations (PDEs) are used to model physical systems. In such scenarios, the operators map between Banach or Hilbert spaces. In this work, we tackle the problem of learning operators between Banach spaces, in contrast to the vast majority of past works considering only Hilbert spaces. We focus on learning holomorphic operators -- an important class of problems with many applications. We combine arbitrary approximate encoders and decoders with standard feedforward Deep Neural Network (DNN) architectures -- specifically, those with constant width exceeding the depth -- under standard $\\\\ell^2$-loss minimization. We first identify a family of  DNNs such that the resulting Deep Learning (DL) procedure achieves optimal generalization bounds for such operators. For standard fully-connected architectures, we then show that there are uncountably many minimizers of the training problem that yield equivalent optimal performance. The DNN architectures we consider are `problem agnostic', with width and depth only depending on the amount of training data $m$ and not on regularity assumptions of the target operator. Next, we show that DL is optimal for this problem: no recovery procedure can surpass these generalization bounds up to log terms. Finally, we present numerical results demonstrating the practical performance on challenging problems including the parametric diffusion, Navier-Stokes-Brinkman and Boussinesq PDEs.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9cb30e6da1adb9a507589be6884af286e65e2967.pdf'}, 'supplementary_material': {'value': '/attachment/81180756a6d60d46dec1042d10fa35052948c1a6.zip'}, '_bibtex': {'value': '@inproceedings{\\nadcock2024optimal,\\ntitle={Optimal deep learning of holomorphic operators between Banach spaces},\\nauthor={Ben Adcock and Nick Dexter and Sebastian Moraga},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vBlzen37i0}\\n}'}, 'paperhash': {'value': 'adcock|optimal_deep_learning_of_holomorphic_operators_between_banach_spaces'}},forum = 'vBlzen37i0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4819/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4819/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4819/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4819/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vBah12uVbD',number = 20230,cdate = 1715795213977,pdate = 1727288228484,odate = 1730873999987,mdate = 1730874000004,tcdate = 1715795213977,tmdate = 1730874000004,ddate = None,content = {'title': {'value': 'Conformalized Credal Set Predictors'}, 'authors': {'value': ['Alireza Javanmardi', 'David Stutz', 'Eyke Hüllermeier']}, 'authorids': {'value': ['~Alireza_Javanmardi2', '~David_Stutz1', '~Eyke_Hüllermeier1']}, 'keywords': {'value': ['Conformal Prediction', 'Credal Sets', 'Imprecise Probabilities', 'Uncertainty Representation', 'Uncertainty Quantification']}, 'TLDR': {'value': 'A novel conformal prediction method is introduced to construct credal sets that are able to represent both the aleatoric and epistemic uncertainty in a prediction.'}, 'abstract': {'value': 'Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular, due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method on ambiguous classification tasks for uncertainty quantification.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3696e1bc9f5dbbdee185361a494cbb897463689d.pdf'}, '_bibtex': {'value': '@inproceedings{\\njavanmardi2024conformalized,\\ntitle={Conformalized Credal Set Predictors},\\nauthor={Alireza Javanmardi and David Stutz and Eyke H{\\\\\"u}llermeier},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vBah12uVbD}\\n}'}, 'paperhash': {'value': 'javanmardi|conformalized_credal_set_predictors'}},forum = 'vBah12uVbD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20230/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20230/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20230/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20230/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vBKoEZ1PG3',number = 2096,cdate = 1714973680500,pdate = 1727287680814,odate = 1730873855086,mdate = 1734616200345,tcdate = 1714973680500,tmdate = 1734616200345,ddate = None,content = {'title': {'value': 'HAWK: Learning to Understand Open-World Video Anomalies'}, 'authors': {'value': ['Jiaqi Tang', 'Hao LU', 'RUIZHENG WU', 'Xiaogang Xu', 'Ke Ma', 'Cheng Fang', 'Bin Guo', 'Jiangbo Lu', 'Qifeng Chen', 'Ying-Cong Chen']}, 'authorids': {'value': ['~Jiaqi_Tang1', '~Hao_LU8', '~RUIZHENG_WU1', '~Xiaogang_Xu2', '~Ke_Ma11', '~Cheng_Fang6', '~Bin_Guo3', '~Jiangbo_Lu1', '~Qifeng_Chen1', '~Ying-Cong_Chen1']}, 'keywords': {'value': ['Video Anomalies Understanding']}, 'abstract': {'value': \"Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.\\nIn this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/72f5b57d7e449d765338f2a60df13977e0717488.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntang2024hawk,\\ntitle={{HAWK}: Learning to Understand Open-World Video Anomalies},\\nauthor={Jiaqi Tang and Hao LU and RUIZHENG WU and Xiaogang Xu and Ke Ma and Cheng Fang and Bin Guo and Jiangbo Lu and Qifeng Chen and Ying-Cong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vBKoEZ1PG3}\\n}'}, 'TLDR': {'value': 'A large vision-language model for understanding open-world video anomalies.'}, 'paperhash': {'value': 'tang|hawk_learning_to_understand_openworld_video_anomalies'}},forum = 'vBKoEZ1PG3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2096/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2096/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2096/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2096/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'vBGMbFgvsX',number = 7607,cdate = 1715630209601,pdate = 1727287852839,odate = 1730873904601,mdate = 1730873904618,tcdate = 1715630209601,tmdate = 1730873904618,ddate = None,content = {'title': {'value': 'Going Beyond Heuristics by Imposing Policy Improvement as a Constraint'}, 'authors': {'value': ['Chi-Chang Lee', 'Zhang-Wei Hong', 'Pulkit Agrawal']}, 'authorids': {'value': ['~Chi-Chang_Lee1', '~Zhang-Wei_Hong1', '~Pulkit_Agrawal1']}, 'keywords': {'value': ['Deep reinforcement learning']}, 'TLDR': {'value': 'We propose a modification to existing RL algorithms to improve the performance when trained with heuristic rewards.'}, 'abstract': {'value': \"In many reinforcement learning (RL) applications, incorporating heuristic rewards alongside the task reward is crucial for achieving desirable performance. Heuristics encode prior human knowledge about how a task should be done, providing valuable hints for RL algorithms. However, such hints may not be optimal, limiting the performance of learned policies. \\nThe currently established way of using heuristics is to modify the heuristic reward in a manner that ensures that the optimal policy learned with it remains the same as the optimal policy for the task reward (i.e., optimal policy invariance). \\nHowever, these methods often fail in practical scenarios with limited training data. We found that while optimal policy invariance ensures convergence to the best policy based on task rewards, it doesn't guarantee better performance than policies trained with biased heuristics under a finite data regime, which is impractical. In this paper, we introduce a new principle tailored for finite data settings. Instead of enforcing optimal policy invariance, we train a policy that combines task and heuristic rewards and ensures it outperforms the heuristic-trained policy. As such, we prevent policies from merely exploiting heuristic rewards without improving the task reward. Our experiments on robotic locomotion, helicopter control, and manipulation tasks demonstrate that our method consistently outperforms the heuristic policy, regardless of the heuristic rewards' quality.\\nCode is available at https://github.com/Improbable-AI/hepo.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1983230de4f35516ae3297ba7a6fb88fab7fad02.pdf'}, 'supplementary_material': {'value': '/attachment/55b8dcc3d7045b9166063c32638f97a974040774.zip'}, '_bibtex': {'value': '@inproceedings{\\nlee2024going,\\ntitle={Going Beyond Heuristics by Imposing Policy Improvement as a Constraint},\\nauthor={Chi-Chang Lee and Zhang-Wei Hong and Pulkit Agrawal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vBGMbFgvsX}\\n}'}, 'paperhash': {'value': 'lee|going_beyond_heuristics_by_imposing_policy_improvement_as_a_constraint'}},forum = 'vBGMbFgvsX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7607/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7607/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7607/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7607/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'vAOgaPvgYr',number = 13804,cdate = 1715743145949,pdate = 1727288050705,odate = 1730873960637,mdate = 1730873960655,tcdate = 1715743145949,tmdate = 1730873960655,ddate = None,content = {'title': {'value': 'OccamLLM: Fast and Exact Language Model Arithmetic in a Single Step'}, 'authors': {'value': ['Owen M Dugan', 'Donato M. Jiménez Benetó', 'Charlotte Loh', 'Zhuo Chen', 'Rumen Dangovski', 'Marin Soljacic']}, 'authorids': {'value': ['~Owen_M_Dugan1', '~Donato_M._Jiménez_Benetó1', '~Charlotte_Loh1', '~Zhuo_Chen8', '~Rumen_Dangovski1', '~Marin_Soljacic1']}, 'keywords': {'value': ['LLM', 'Language Model', 'Arithmetic', 'OccamNet', 'Llama']}, 'abstract': {'value': 'Despite significant advancements in text generation and reasoning, Large Language Models (LLMs) still face challenges in accurately performing complex arithmetic operations. Language model systems often enable LLMs to generate code for arithmetic operations to achieve accurate calculations. However, this approach compromises speed and security, and fine-tuning risks the language model losing prior capabilities. We propose a framework that enables exact arithmetic in *a single autoregressive step*, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities. We use the hidden states of a LLM to control a symbolic architecture that performs arithmetic. Our implementation using Llama 3 with OccamNet as a symbolic model (OccamLlama) achieves 100\\\\% accuracy on single arithmetic operations ($+,-,\\\\times,\\\\div,\\\\sin{},\\\\cos{},\\\\log{},\\\\exp{},\\\\sqrt{}$), outperforming GPT 4o with and without a code interpreter. Furthermore, OccamLlama outperforms GPT 4o with and without a code interpreter on average across a range of mathematical problem solving benchmarks, demonstrating that OccamLLMs can excel in arithmetic tasks, even surpassing much larger models. Code is available at https://github.com/druidowm/OccamLLM.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2f805a9041d7d2e112fd00bc3259fa9079805498.pdf'}, 'TLDR': {'value': 'We propose a framework that enables exact arithmetic in a single autoregressive step, providing faster, more secure, and more interpretable LLM systems with arithmetic capabilities.'}, '_bibtex': {'value': \"@inproceedings{\\ndugan2024occamllm,\\ntitle={Occam{LLM}: Fast and Exact Language Model Arithmetic in a Single Step},\\nauthor={Owen M Dugan and Donato M. Jim{\\\\'e}nez Benet{\\\\'o} and Charlotte Loh and Zhuo Chen and Rumen Dangovski and Marin Soljacic},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vAOgaPvgYr}\\n}\"}, 'paperhash': {'value': 'dugan|occamllm_fast_and_exact_language_model_arithmetic_in_a_single_step'}},forum = 'vAOgaPvgYr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13804/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13804/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13804/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13804/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'vA4s3kN4QE',number = 2405,cdate = 1715066162252,pdate = 1727287689603,odate = 1730873857727,mdate = 1730873857738,tcdate = 1715066162252,tmdate = 1730873857738,ddate = None,content = {'title': {'value': 'LG-VQ: Language-Guided Codebook Learning'}, 'authors': {'value': ['Liang Guotao', 'Baoquan Zhang', 'Yaowei Wang', 'Yunming Ye', 'Xutao Li', 'Wanghuaibin', 'Luo Chuyao', 'kolaye', 'luolinfeng']}, 'authorids': {'value': ['~Liang_Guotao1', '~Baoquan_Zhang1', '~Yaowei_Wang1', '~Yunming_Ye1', '~Xutao_Li2', '~Wanghuaibin1', '~Luo_Chuyao1', '~kolaye1', '~luolinfeng1']}, 'keywords': {'value': ['Codebook Learing', 'VQ-GAN', 'Vector-quantized Image Modeling']}, 'TLDR': {'value': 'We utilize pre-trained text semantics to guide the codebook to learn rich multi-modal knowledge to improve the performance of multi-modal downstream tasks'}, 'abstract': {'value': 'Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. \\n  Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps.\\n  In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment.   \\n  In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/96a358e0e07a0284d43a8bd709729b6145adcd2a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguotao2024lgvq,\\ntitle={{LG}-{VQ}: Language-Guided Codebook Learning},\\nauthor={Liang Guotao and Baoquan Zhang and Yaowei Wang and Yunming Ye and Xutao Li and Wanghuaibin and Luo Chuyao and kolaye and luolinfeng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=vA4s3kN4QE}\\n}'}, 'paperhash': {'value': 'guotao|lgvq_languageguided_codebook_learning'}},forum = 'vA4s3kN4QE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2405/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2405/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2405/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2405/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v9RqRFSLQ2',number = 13744,cdate = 1715742588993,pdate = 1727288048874,odate = 1730873960170,mdate = 1730873960183,tcdate = 1715742588993,tmdate = 1730873960183,ddate = None,content = {'title': {'value': 'Learning from Uncertain Data: From Possible Worlds to Possible Models'}, 'authors': {'value': ['Jiongli Zhu', 'Su Feng', 'Boris Glavic', 'Babak Salimi']}, 'authorids': {'value': ['~Jiongli_Zhu1', '~Su_Feng1', '~Boris_Glavic1', '~Babak_Salimi1']}, 'keywords': {'value': ['data uncertainty', 'robustness verification', 'predictive multiplicity', 'abstract interpretation', 'linear regression']}, 'abstract': {'value': 'We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/87fd59808dc5ed78d3e3e6ef14d35b6e060362d8.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024learning,\\ntitle={Learning from Uncertain Data: From Possible Worlds to Possible Models},\\nauthor={Jiongli Zhu and Su Feng and Boris Glavic and Babak Salimi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v9RqRFSLQ2}\\n}'}, 'paperhash': {'value': 'zhu|learning_from_uncertain_data_from_possible_worlds_to_possible_models'}},forum = 'v9RqRFSLQ2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13744/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13744/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13744/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13744/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v8X70gTodR',number = 18851,cdate = 1715787710953,pdate = 1727288195109,odate = 1730873992219,mdate = 1737870002417,tcdate = 1715787710953,tmdate = 1737870002417,ddate = None,content = {'title': {'value': 'Analysing the Generalisation and Reliability of Steering Vectors'}, 'authors': {'value': ['Daniel Chee Hian Tan', 'David Chanin', 'Aengus Lynch', 'Brooks Paige', 'Dimitrios Kanoulas', 'Adrià Garriga-Alonso', 'Robert Kirk']}, 'authorids': {'value': ['~Daniel_Chee_Hian_Tan1', '~David_Chanin1', '~Aengus_Lynch1', '~Brooks_Paige1', '~Dimitrios_Kanoulas1', '~Adrià_Garriga-Alonso1', '~Robert_Kirk1']}, 'keywords': {'value': ['Interpretability', 'Causal Abstractions', 'Steering Vectors', 'Representation Engineering', 'Linear Representation Hypothesis', 'Contrastive Activation Addition']}, 'abstract': {'value': \"Steering vectors (SVs) are a new approach to efficiently adjust language model behaviour at inference time by intervening on intermediate model activations. They have shown promise in terms of improving both capabilities and model alignment. However, the reliability and generalisation properties of this approach are unknown. In this work, we rigorously investigate these properties, and show that steering vectors have substantial limitations both in- and out-of-distribution. In-distribution, steerability is highly variable across different inputs. Depending on the concept, spurious biases can substantially contribute to how effective steering is for each input, presenting a challenge for the widespread use of steering vectors. Out-of-distribution, while steering vectors often generalise well, for several concepts they are brittle to reasonable changes in the prompt, resulting in them failing to generalise well. Overall, our findings show that while steering can work well in the right circumstances, there remain many technical difficulties of applying steering vectors to guide models' behaviour at scale.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We evaluate steering vectors on over 100 datasets, finding that they work unreliably in-distribution and sometimes misgeneralise out-of-distribution.'}, 'pdf': {'value': '/pdf/ae36dcd5991ede8cf3524ee3dbc020207424bf16.pdf'}, 'supplementary_material': {'value': '/attachment/e00b31d6a87e2e6241e1773ffaf18c8378e0223e.zip'}, '_bibtex': {'value': '@inproceedings{\\ntan2024analysing,\\ntitle={Analysing the Generalisation and Reliability of Steering Vectors},\\nauthor={Daniel Chee Hian Tan and David Chanin and Aengus Lynch and Brooks Paige and Dimitrios Kanoulas and Adri{\\\\`a} Garriga-Alonso and Robert Kirk},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v8X70gTodR}\\n}'}, 'paperhash': {'value': 'tan|analysing_the_generalisation_and_reliability_of_steering_vectors'}},forum = 'v8X70gTodR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18851/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18851/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18851/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18851/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v8RRFNbJ43',number = 12396,cdate = 1715721335015,pdate = 1727288005396,odate = 1730873948662,mdate = 1730873948675,tcdate = 1715721335015,tmdate = 1730873948675,ddate = None,content = {'title': {'value': 'Measuring Dejavu Memorization Efficiently'}, 'authors': {'value': ['Narine Kokhlikyan', 'Bargav Jayaraman', 'Florian Bordes', 'Chuan Guo', 'Kamalika Chaudhuri']}, 'authorids': {'value': ['~Narine_Kokhlikyan1', '~Bargav_Jayaraman1', '~Florian_Bordes1', '~Chuan_Guo1', '~Kamalika_Chaudhuri1']}, 'keywords': {'value': ['memorization', 'privacy']}, 'abstract': {'value': 'Recent research has shown that representation learning models may accidentally memorize their training data. For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of he background – better than through dataset-level correlations. However, their measurement method requires training two models – one to estimate dataset-level correlations and the other to estimate memorization. This multiple model setup becomes infeasible for large open-source models. In this work, we propose alter native simple methods to estimate dataset-level correlations, and show that these can be used to approximate an off-the-shelf model’s memorization ability without any retraining. This enables, for the first time, the measurement of memorization in pre-trained open-source image representation and vision-language models. Our results show that different ways of measuring memorization yield very similar aggregate results. We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data. The code is available both for vision (https://github.com/facebookresearch/DejaVuOSS) and vision language (https://github.com/facebookresearch/VLMDejaVu) models.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'The deja vu memorization test measures training data memorization, but is inefficient because it involves training another similar model.  This work provides a simpler, more efficient way to carry out the test.'}, 'pdf': {'value': '/pdf/6f697238a026167fa803f7aeaffa5b79df2b1057.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkokhlikyan2024measuring,\\ntitle={Measuring Dejavu Memorization Efficiently},\\nauthor={Narine Kokhlikyan and Bargav Jayaraman and Florian Bordes and Chuan Guo and Kamalika Chaudhuri},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v8RRFNbJ43}\\n}'}, 'paperhash': {'value': 'kokhlikyan|measuring_dejavu_memorization_efficiently'}},forum = 'v8RRFNbJ43',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12396/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12396/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12396/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12396/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'v7vYVvmfru',number = 14409,cdate = 1715749720176,pdate = 1727288068447,odate = 1730873965643,mdate = 1736924545942,tcdate = 1715749720176,tmdate = 1736924545942,ddate = None,content = {'title': {'value': 'An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness'}, 'authors': {'value': ['Xiaochuan Gong', 'Jie Hao', 'Mingrui Liu']}, 'authorids': {'value': ['~Xiaochuan_Gong1', '~Jie_Hao3', '~Mingrui_Liu2']}, 'keywords': {'value': ['Bilevel Optimization', 'Acceleration', 'Unbounded Smoothness', 'Nonconvex Optimization']}, 'TLDR': {'value': 'This paper introduces a new algorithm and analysis to accelerate bilevel optimization under unbounded smoothness.'}, 'abstract': {'value': 'This paper investigates a class of stochastic bilevel optimization problems where the upper-level function is nonconvex with potentially unbounded smoothness and the lower-level problem is strongly convex. These problems have significant applications in sequential data learning, such as text classification using recurrent neural networks. The unbounded smoothness is characterized by the smoothness constant of the upper-level function scaling linearly with the gradient norm, lacking a uniform upper bound. Existing state-of-the-art algorithms require $\\\\widetilde{O}(\\\\epsilon^{-4})$ oracle calls of stochastic gradient or Hessian/Jacobian-vector product to find an $\\\\epsilon$-stationary point. However, it remains unclear if we can further improve the convergence rate when the assumptions for the function in the population level also hold for each random realization almost surely (e.g., Lipschitzness of each realization of the stochastic gradient). To address this issue, we propose a new Accelerated Bilevel Optimization algorithm named AccBO. The algorithm updates the upper-level variable by normalized stochastic gradient descent with recursive momentum and the lower-level variable by the stochastic Nesterov accelerated gradient descent algorithm with averaging. We prove that our algorithm achieves an oracle complexity of $\\\\widetilde{O}(\\\\epsilon^{-3})$ to find an $\\\\epsilon$-stationary point, when the lower-level stochastic gradient has a small variance $O(\\\\epsilon)$. Our proof relies on a novel lemma characterizing the dynamics of stochastic Nesterov accelerated gradient descent algorithm under distribution drift with high probability for the lower-level variable, which is of independent interest and also plays a crucial role in analyzing the hypergradient estimation error over time. Experimental results on various tasks confirm that our proposed algorithm achieves the predicted theoretical acceleration and significantly outperforms baselines in bilevel optimization.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cabee2deb31c0409877deb7e08fff4b589590116.pdf'}, 'supplementary_material': {'value': '/attachment/23d699e321d95183967c8345555d7020c7952ab2.zip'}, '_bibtex': {'value': '@inproceedings{\\ngong2024an,\\ntitle={An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness},\\nauthor={Xiaochuan Gong and Jie Hao and Mingrui Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v7vYVvmfru}\\n}'}, 'paperhash': {'value': 'gong|an_accelerated_algorithm_for_stochastic_bilevel_optimization_under_unbounded_smoothness'}},forum = 'v7vYVvmfru',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14409/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14409/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14409/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14409/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v6W55lCkhN',number = 13949,cdate = 1715744472160,pdate = 1727288055660,odate = 1730873962383,mdate = 1730873962405,tcdate = 1715744472160,tmdate = 1730873962405,ddate = None,content = {'title': {'value': 'CE-NAS: An End-to-End Carbon-Efficient Neural Architecture Search Framework'}, 'authors': {'value': ['Yiyang Zhao', 'Yunzhuo Liu', 'Bo Jiang', 'Tian Guo']}, 'authorids': {'value': ['~Yiyang_Zhao1', '~Yunzhuo_Liu1', '~Bo_Jiang2', '~Tian_Guo3']}, 'keywords': {'value': ['Neural architecture search; Carbon efficient; Optimization; Environment friendly']}, 'abstract': {'value': 'This work presents a novel approach to neural architecture search (NAS) that aims to increase carbon efficiency for the model design process. The proposed framework CE-NAS addresses the key challenge of high carbon cost associated with NAS by exploring the carbon emission variations of energy and energy differences of different NAS algorithms. At the high level, CE-NAS leverages a reinforcement-learning agent to dynamically adjust GPU resources based on carbon intensity, predicted by a time-series transformer, to balance energy-efficient sampling and energy-intensive evaluation tasks. Furthermore, CE-NAS leverages a recently proposed multi-objective optimizer to effectively reduce the NAS search space. We demonstrate the efficacy of CE-NAS in lowering carbon emissions while achieving SOTA results for both NAS datasets and open-domain NAS tasks. For example, on the HW-NasBench dataset, CE-NAS reduces carbon emissions by up to 7.22X while maintaining a search efficiency comparable to vanilla NAS. For open-domain NAS tasks, CE-NAS achieves SOTA results with 97.35% top-1 accuracy on CIFAR-10 with only 1.68M parameters and a carbon consumption of 38.53 lbs of CO2. On ImageNet, our searched model achieves 80.6% top-1 accuracy with a 0.78 ms TensorRT latency using FP16 on NVIDIA V100, consuming only 909.86 lbs of CO2, making it comparable to other one-shot-based NAS baselines. Our code is available at https://github.com/cake-lab/CE-NAS.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1e1daf62c7b574a8a94781af5ea3ed13da72701b.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzhao2024cenas,\\ntitle={{CE}-{NAS}: An End-to-End Carbon-Efficient Neural Architecture Search Framework},\\nauthor={Yiyang Zhao and Yunzhuo Liu and Bo Jiang and Tian Guo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v6W55lCkhN}\\n}'}, 'paperhash': {'value': 'zhao|cenas_an_endtoend_carbonefficient_neural_architecture_search_framework'}},forum = 'v6W55lCkhN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13949/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13949/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13949/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13949/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v5Un2QqnRf',number = 3716,cdate = 1715319236300,pdate = 1727287728621,odate = 1730873868935,mdate = 1730873868953,tcdate = 1715319236300,tmdate = 1730873868953,ddate = None,content = {'title': {'value': 'Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models'}, 'authors': {'value': ['Yang Jiao', 'Shaoxiang Chen', 'ZEQUN JIE', 'Jingjing Chen', 'Lin Ma', 'Yu-Gang Jiang']}, 'authorids': {'value': ['~Yang_Jiao5', '~Shaoxiang_Chen1', '~ZEQUN_JIE1', '~Jingjing_Chen3', '~Lin_Ma2', '~Yu-Gang_Jiang1']}, 'keywords': {'value': ['Large Multimodal Models', 'Vision-Centric Capabilities']}, 'TLDR': {'value': 'a large multimodal model with versatile vision-centric capabilities'}, 'abstract': {'value': \"Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Comprehensive experimental results on a series of vision-centric and VQA benchmarks indicate that our Lumen model not only achieves or surpasses the performance of existing LMM-based approaches in a range of vision-centric tasks while maintaining general visual understanding and instruction following capabilities.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f93765d8f6468be09c47280c032485d031a61297.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiao2024lumen,\\ntitle={Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models},\\nauthor={Yang Jiao and Shaoxiang Chen and ZEQUN JIE and Jingjing Chen and Lin Ma and Yu-Gang Jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v5Un2QqnRf}\\n}'}, 'paperhash': {'value': 'jiao|lumen_unleashing_versatile_visioncentric_capabilities_of_large_multimodal_models'}},forum = 'v5Un2QqnRf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3716/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3716/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3716/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3716/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'v4dXL3LsGX',number = 16186,cdate = 1715767543540,pdate = 1727288119114,odate = 1730873977579,mdate = 1730873977591,tcdate = 1715767543540,tmdate = 1730873977591,ddate = None,content = {'title': {'value': 'Learning to Cooperate with Humans using Generative Agents'}, 'authors': {'value': ['Yancheng Liang', 'Daphne Chen', 'Abhishek Gupta', 'Simon Shaolei Du', 'Natasha Jaques']}, 'authorids': {'value': ['~Yancheng_Liang1', '~Daphne_Chen1', '~Abhishek_Gupta1', '~Simon_Shaolei_Du1', '~Natasha_Jaques1']}, 'keywords': {'value': ['multi-agent reinforcement learning', 'human-AI cooperation']}, 'TLDR': {'value': 'We use generative model to sample partner agents to train a coordinator agent. These agents cooperate well with real human players.'}, 'abstract': {'value': \"Training agents that can coordinate zero-shot with humans is a key mission in multi-agent reinforcement learning (MARL). Current algorithms focus on training simulated human partner policies which are then used to train a Cooperator agent. The simulated human is produced either through behavior cloning over a dataset of human cooperation behavior, or by using MARL to create a population of simulated agents. However, these approaches often struggle to produce a Cooperator that can coordinate well with real humans, since the simulated humans fail to cover the diverse strategies and styles employed by people in the real world.  We show \\\\emph{learning a generative model of human partners} can effectively address this issue. Our model learns a latent variable representation of the human that can be regarded as encoding the human's unique strategy, intention, experience, or style. This generative model can be flexibly trained from any (human or neural policy) agent interaction data. By sampling from the latent space, we can use the generative model to produce different partners to train Cooperator agents. We evaluate our method---Generative Agent Modeling for Multi-agent Adaptation (GAMMA)---on Overcooked, a challenging cooperative cooking game that has become a standard benchmark for zero-shot coordination. We conduct an evaluation with real human teammates, and the results show that GAMMA consistently improves performance, whether the generative model is trained on simulated populations or human datasets. Further, we propose a method for posterior sampling from the generative model that is biased towards the human data, enabling us to efficiently improve performance with only a small amount of expensive human interaction data.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/298dd46c5abad00c26df7b356fb1f3812baaeb74.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliang2024learning,\\ntitle={Learning to Cooperate with Humans using Generative Agents},\\nauthor={Yancheng Liang and Daphne Chen and Abhishek Gupta and Simon Shaolei Du and Natasha Jaques},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v4dXL3LsGX}\\n}'}, 'paperhash': {'value': 'liang|learning_to_cooperate_with_humans_using_generative_agents'}},forum = 'v4dXL3LsGX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16186/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16186/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16186/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16186/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v416YLOQuU',number = 10882,cdate = 1715698667600,pdate = 1727287953903,odate = 1730873933029,mdate = 1730873933047,tcdate = 1715698667600,tmdate = 1730873933047,ddate = None,content = {'title': {'value': 'Adam with model exponential moving average is effective for nonconvex optimization'}, 'authors': {'value': ['Kwangjun Ahn', 'Ashok Cutkosky']}, 'authorids': {'value': ['~Kwangjun_Ahn2', '~Ashok_Cutkosky1']}, 'keywords': {'value': ['Adam', 'exponential moving average', 'EMA', 'nonconvex optimization.']}, 'abstract': {'value': 'In this work, we offer a theoretical analysis of two modern optimization techniques for training large and complex models: (i) adaptive optimization algorithms, such as Adam, and (ii) the model exponential moving average (EMA). Specifically, we demonstrate that a clipped version of Adam with model EMA achieves the optimal convergence rates in various nonconvex optimization settings, both smooth and nonsmooth. Moreover, when the scale varies significantly across different coordinates, we demonstrate that the coordinate-wise adaptivity of Adam is provably advantageous. Notably, unlike previous analyses of Adam, our analysis crucially relies on its core elements---momentum and discounting factors---as well as model EMA, motivating their wide applications in practice.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5ce039de7fa679b571456addaec6a4ae9de508a6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nahn2024adam,\\ntitle={Adam with model exponential moving average is effective for nonconvex optimization},\\nauthor={Kwangjun Ahn and Ashok Cutkosky},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v416YLOQuU}\\n}'}, 'paperhash': {'value': 'ahn|adam_with_model_exponential_moving_average_is_effective_for_nonconvex_optimization'}},forum = 'v416YLOQuU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10882/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10882/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10882/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10882/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v3y785TN7B',number = 9765,cdate = 1715685379947,pdate = 1727287920286,odate = 1730873923309,mdate = 1736947743422,tcdate = 1715685379947,tmdate = 1736947743422,ddate = None,content = {'title': {'value': 'GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields'}, 'authors': {'value': ['Weiyi Xue', 'Zehan Zheng', 'Fan Lu', 'Haiyun Wei', 'Guang Chen', 'changjun jiang']}, 'authorids': {'value': ['~Weiyi_Xue1', '~Zehan_Zheng1', '~Fan_Lu3', '~Haiyun_Wei2', '~Guang_Chen4', '~changjun_jiang2']}, 'keywords': {'value': ['Neural LiDAR Field', 'Pose-free', 'Geometry', 'Point Cloud', 'Registration']}, 'abstract': {'value': 'Although recent efforts have extended Neural Radiance Field (NeRF) into LiDAR point cloud synthesis, the majority of existing works exhibit a strong dependence on precomputed poses. However, point cloud registration methods struggle to achieve precise global pose estimation, whereas previous pose-free NeRFs overlook geometric consistency in global reconstruction. In light of this, we explore the geometric insights of point clouds, which provide explicit registration priors for reconstruction. Based on this, we propose Geometry guided Neural LiDAR Fields (GeoNLF), a hybrid framework performing alternately global neural reconstruction and pure geometric pose optimization. Furthermore, NeRFs tend to overfit individual frames and easily get stuck in local minima under sparse-view inputs. To tackle this issue, we develop a selective-reweighting strategy and introduce geometric constraints for robust optimization. Extensive experiments on NuScenes and KITTI-360 datasets demonstrate the superiority of GeoNLF in both novel view synthesis and multi-view registration of low-frequency large-scale point clouds.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/69c0cd94694fd273a2e4447f445a0484c6becf19.pdf'}, 'supplementary_material': {'value': '/attachment/927fa7bbc543aeeac7bb896d2f1bcfb98704c3b1.zip'}, '_bibtex': {'value': '@inproceedings{\\nxue2024geonlf,\\ntitle={Geo{NLF}: Geometry guided Pose-Free Neural Li{DAR} Fields},\\nauthor={Weiyi Xue and Zehan Zheng and Fan Lu and Haiyun Wei and Guang Chen and changjun jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v3y785TN7B}\\n}'}, 'paperhash': {'value': 'xue|geonlf_geometry_guided_posefree_neural_lidar_fields'}},forum = 'v3y785TN7B',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9765/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9765/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9765/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9765/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v3jHuoxMw8',number = 1291,cdate = 1714542077425,pdate = 1727287656627,odate = 1730873847074,mdate = 1734577856727,tcdate = 1714542077425,tmdate = 1734577856727,ddate = None,content = {'title': {'value': 'Vision-Language Navigation with Energy-Based Policy'}, 'authors': {'value': ['Rui Liu', 'Wenguan Wang', 'Yi Yang']}, 'authorids': {'value': ['~Rui_Liu22', '~Wenguan_Wang4', '~Yi_Yang4']}, 'keywords': {'value': ['Vision-Language Navigation; Vision-Language; Embodied vision']}, 'abstract': {'value': 'Vision-language navigation (VLN) requires an agent to execute actions following human instructions. Existing VLN models are optimized through expert demonstrations by supervised behavioural cloning or incorporating manual reward engineering. While straightforward, these efforts overlook the accumulation of errors in the Markov decision process, and struggle to match the distribution of the expert policy. Going beyond this, we propose an Energy-based Navigation Policy (ENP) to model the joint state-action distribution using an energy-based model. At each step, low energy values correspond to the state-action pairs that the expert is most likely to perform, and vice versa. Theoretically, the optimization objective is equivalent to minimizing the forward divergence between the occupancy measure of the expert and ours. Consequently, ENP learns to globally align with the expert policy by maximizing the likelihood of the actions and modeling the dynamics of the navigation states in a collaborative manner. With a variety of VLN architectures, ENP achieves promising performances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing VLN models.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3086459a83bf86ddd7c3c2a039c2f2248ea6d01f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024visionlanguage,\\ntitle={Vision-Language Navigation with Energy-Based Policy},\\nauthor={Rui Liu and Wenguan Wang and Yi Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v3jHuoxMw8}\\n}'}, 'paperhash': {'value': 'liu|visionlanguage_navigation_with_energybased_policy'}},forum = 'v3jHuoxMw8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1291/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1291/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1291/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1291/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v1kpc060aC',number = 7344,cdate = 1715618557408,pdate = 1727287843922,odate = 1730873902092,mdate = 1737021556458,tcdate = 1715618557408,tmdate = 1737021556458,ddate = None,content = {'title': {'value': 'Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML'}, 'authors': {'value': ['Tehila Dahan', 'Kfir Yehuda Levy']}, 'authorids': {'value': ['~Tehila_Dahan1', '~Kfir_Yehuda_Levy1']}, 'keywords': {'value': ['stochastic convex optimization', 'byzantine robust learning', 'online convex optimization']}, 'TLDR': {'value': 'A novel fault-tolerant framework and strategies that, for the first time, achieve optimal convergence rate in asynchronous machine learning against Byzantine failures.'}, 'abstract': {'value': 'We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous compute resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3170d2f141bca2ac2d74bb225a353d92a6d712b3.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndahan2024weight,\\ntitle={Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous {ML}},\\nauthor={Tehila Dahan and Kfir Yehuda Levy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v1kpc060aC}\\n}'}, 'paperhash': {'value': 'dahan|weight_for_robustness_a_comprehensive_approach_towards_optimal_faulttolerant_asynchronous_ml'}},forum = 'v1kpc060aC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7344/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7344/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7344/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7344/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'v1BIm8wESL',number = 8599,cdate = 1715665558050,pdate = 1727287884753,odate = 1730873913330,mdate = 1734926016172,tcdate = 1715665558050,tmdate = 1734926016172,ddate = None,content = {'title': {'value': 'Skinned Motion Retargeting with Dense Geometric Interaction Perception'}, 'authors': {'value': ['Zijie Ye', 'Jia-Wei Liu', 'Jia Jia', 'Shikun Sun', 'Mike Zheng Shou']}, 'authorids': {'value': ['~Zijie_Ye1', '~Jia-Wei_Liu1', '~Jia_Jia1', '~Shikun_Sun1', '~Mike_Zheng_Shou1']}, 'keywords': {'value': ['Neural Motion Processing', 'Motion Retargeting']}, 'abstract': {'value': 'Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected ScanRet dataset demonstrate that MeshRet achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet.'}, 'pdf': {'value': '/pdf/12ebc0df091e4b2c185faf741c87fc6937bb37c5.pdf'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/47a42cc1bec4234043baef989515007d15742b3a.zip'}, 'TLDR': {'value': 'Geometry-aware motion retargeting in a single stage with no contradiction introduced by geometry correction.'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nye2024skinned,\\ntitle={Skinned Motion Retargeting with Dense Geometric Interaction Perception},\\nauthor={Zijie Ye and Jia-Wei Liu and Jia Jia and Shikun Sun and Mike Zheng Shou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v1BIm8wESL}\\n}'}, 'paperhash': {'value': 'ye|skinned_motion_retargeting_with_dense_geometric_interaction_perception'}},forum = 'v1BIm8wESL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8599/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8599/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8599/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission8599/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'v07KRLYxDX',number = 12250,cdate = 1715719168948,pdate = 1727288000091,odate = 1730873947394,mdate = 1730873947413,tcdate = 1715719168948,tmdate = 1730873947413,ddate = None,content = {'title': {'value': 'Achieving Domain-Independent Certified Robustness via Knowledge Continuity'}, 'authors': {'value': ['Alan Sun', 'Chiyu Ma', 'Kenneth Ge', 'Soroush Vosoughi']}, 'authorids': {'value': ['~Alan_Sun1', '~Chiyu_Ma1', '~Kenneth_Ge1', '~Soroush_Vosoughi1']}, 'keywords': {'value': ['Lipschitz continuity', 'robustness', 'certified robustness', 'adversarial robustness']}, 'TLDR': {'value': 'We present knowledge continuity, a novel definition which aims to certify robustness of neural networks across continuous/discrete domains.'}, 'abstract': {'value': 'We present *knowledge continuity*, a novel definition inspired by Lipschitz continuity which aims to certify the robustness of neural networks across input domains (such as continuous and discrete domains in vision and language, respectively). Most existing approaches that seek to certify robustness, especially Lipschitz continuity, lie within the continuous domain with norm and distribution-dependent guarantees. In contrast, our proposed definition yields certification guarantees that depend only on the loss function and the intermediate learned metric spaces of the neural network. These bounds are independent of domain modality, norms, and distribution. We further demonstrate that the expressiveness of a model class is not at odds with its knowledge continuity. This implies that achieving robustness by maximizing knowledge continuity should not theoretically hinder inferential performance. Finally, to complement our theoretical results, we present several applications of knowledge continuity such as regularization, a certification algorithm, and show that knowledge continuity can be used to localize vulnerable components of a neural network.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/0b9b46ec140ee7d19a8606c9d892c0ade5e1f189.zip'}, 'pdf': {'value': '/pdf/5637147709875e41d94cb35d3fb0ef47bf7ebba5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024achieving,\\ntitle={Achieving Domain-Independent Certified Robustness via Knowledge Continuity},\\nauthor={Alan Sun and Chiyu Ma and Kenneth Ge and Soroush Vosoughi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=v07KRLYxDX}\\n}'}, 'paperhash': {'value': 'sun|achieving_domainindependent_certified_robustness_via_knowledge_continuity'}},forum = 'v07KRLYxDX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12250/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12250/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12250/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12250/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uzIWqRzjEP',number = 12800,cdate = 1715728740367,pdate = 1727288018141,odate = 1730873952243,mdate = 1730873952264,tcdate = 1715728740367,tmdate = 1730873952264,ddate = None,content = {'title': {'value': 'Learning to Edit Visual Programs with Self-Supervision'}, 'authors': {'value': ['R. Kenny Jones', 'Renhao Zhang', 'Aditya Ganeshan', 'Daniel Ritchie']}, 'authorids': {'value': ['~R._Kenny_Jones1', '~Renhao_Zhang1', '~Aditya_Ganeshan1', '~Daniel_Ritchie1']}, 'keywords': {'value': ['visual program induction', 'program synthesis', 'visual programs', 'inverse graphics']}, 'TLDR': {'value': 'For the task of visual program induction, we design a network that learns how to edit visual programs and is trained in a self-supervised bootstrapping paradigm.'}, 'abstract': {'value': 'We design a system that learns how to edit visual programs. Our edit network consumes a complete input program and a visual target. From this input, we task our network with predicting a local edit operation that could be applied to the input program to improve its similarity to the target. In order to apply this scheme for domains that lack program annotations, we develop a self-supervised learning approach that integrates this edit network into a bootstrapped finetuning loop along with a network that predicts entire programs in one-shot. Our joint finetuning scheme, when coupled with an inference procedure that initializes a population from the one-shot model and evolves members of this population with the edit network, helps to infer more accurate visual programs. Over multiple domains, we experimentally compare our method against the alternative of using only the one-shot model, and find that even under equal search-time budgets, our editing-based paradigm provides significant advantages.'}, 'pdf': {'value': '/pdf/5574437cb41abf73076c2977076bffc90f011092.pdf'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\njones2024learning,\\ntitle={Learning to Edit Visual Programs with Self-Supervision},\\nauthor={R. Kenny Jones and Renhao Zhang and Aditya Ganeshan and Daniel Ritchie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uzIWqRzjEP}\\n}'}, 'paperhash': {'value': 'jones|learning_to_edit_visual_programs_with_selfsupervision'}},forum = 'uzIWqRzjEP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12800/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12800/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12800/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12800/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uyqjpycMbU',number = 8395,cdate = 1715659182125,pdate = 1727287877938,odate = 1730873911777,mdate = 1737024665400,tcdate = 1715659182125,tmdate = 1737024665400,ddate = None,content = {'title': {'value': 'Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation'}, 'authors': {'value': ['Arvind Murari Vepa', 'ZUKANG YANG', 'Andrew Choi', 'Jungseock Joo', 'Fabien Scalzo', 'Yizhou Sun']}, 'authorids': {'value': ['~Arvind_Murari_Vepa1', '~ZUKANG_YANG2', '~Andrew_Choi2', '~Jungseock_Joo3', '~Fabien_Scalzo1', '~Yizhou_Sun1']}, 'keywords': {'value': ['Active Learning', 'Medical Imaging', 'Segmentation', 'Deep Metric Learning']}, 'TLDR': {'value': 'We propose a novel metric learning framework for Coreset for active learning in 3D medical image segmentation.'}, 'abstract': {'value': 'Deep learning has seen remarkable advancements in machine learning, yet it often demands extensive annotated data. Tasks like 3D semantic segmentation impose a substantial annotation burden, especially in domains like medicine, where expert annotations drive up the cost. Active learning (AL) holds great potential to alleviate this annotation burden in 3D medical segmentation. The majority of existing AL methods, however, are not tailored to the medical domain. While weakly-supervised methods have been explored to reduce annotation burden, the fusion of AL with weak supervision remains unexplored, despite its potential to significantly reduce annotation costs. Additionally, there is little focus on slice-based AL for 3D segmentation, which can also significantly reduce costs in comparison to conventional volume-based AL. This paper introduces a novel metric learning method for Coreset to perform slice-based active learning in 3D medical segmentation. By merging contrastive learning with inherent data groupings in medical imaging, we learn a metric that emphasizes the relevant differences in samples for training 3D medical segmentation models. We perform comprehensive evaluations using both weak and full annotations across four datasets (medical and non-medical). Our findings demonstrate that our approach surpasses existing active learning techniques on both weak and full annotations and obtains superior performance with low-annotation budgets which is crucial in medical imaging. Source code for this project is available in the supplementary materials and on GitHub: https://github.com/arvindmvepa/al-seg.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/1c3be0302ba8ec4e851ab3fc4cf4237b28cb6670.zip'}, 'pdf': {'value': '/pdf/40359e3514dc6a276f6b3a5b8a615e84483567a0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvepa2024integrating,\\ntitle={Integrating Deep Metric Learning with Coreset for Active Learning in 3D Segmentation},\\nauthor={Arvind Murari Vepa and ZUKANG YANG and Andrew Choi and Jungseock Joo and Fabien Scalzo and Yizhou Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uyqjpycMbU}\\n}'}, 'paperhash': {'value': 'vepa|integrating_deep_metric_learning_with_coreset_for_active_learning_in_3d_segmentation'}},forum = 'uyqjpycMbU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8395/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8395/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8395/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8395/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uyLtEFnpQP',number = 947,cdate = 1714232871954,pdate = 1727287648216,odate = 1730873844266,mdate = 1730873844284,tcdate = 1714232871954,tmdate = 1730873844284,ddate = None,content = {'title': {'value': 'Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals'}, 'authors': {'value': ['Hui Zheng', 'Haiteng Wang', 'Weibang Jiang', 'Zhongtao Chen', 'Li He', 'Peiyang Lin', 'Penghu Wei', 'Guoguang Zhao', 'Yunzhe Liu']}, 'authorids': {'value': ['~Hui_Zheng1', '~Haiteng_Wang1', '~Weibang_Jiang2', '~Zhongtao_Chen1', '~Li_He8', '~Peiyang_Lin1', '~Penghu_Wei1', '~Guoguang_Zhao1', '~Yunzhe_Liu2']}, 'keywords': {'value': ['neuroscience', 'sEEG', 'speech decoding', 'self-supervision', 'neuro-AI']}, 'TLDR': {'value': 'We propose  the Du-IN model, a framework for sEEG-to-Word translation task, learning contextual embeddings through discrete codebook-guided pre-training on specific brain regions.'}, 'abstract': {'value': 'Invasive brain-computer interfaces with Electrocorticography (ECoG) have shown promise for high-performance speech decoding in medical applications, but less damaging methods like intracranial stereo-electroencephalography (sEEG) remain underexplored. With rapid advances in representation learning, leveraging abundant recordings to enhance speech decoding is increasingly attractive. However, popular methods often pre-train temporal models based on brain-level tokens, overlooking that brain activities in different regions are highly desynchronized during tasks. Alternatively, they pre-train spatial-temporal models based on channel-level tokens but fail to evaluate them on challenging tasks like speech decoding, which requires intricate processing in specific language-related areas. To address this issue, we collected a well-annotated Chinese word-reading sEEG dataset targeting language-related brain networks from 12 subjects. Using this benchmark, we developed the Du-IN model, which extracts contextual embeddings based on region-level tokens through discrete codex-guided mask modeling. Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines. Model comparisons and ablation studies reveal that our design choices, including (\\\\romannumeral1) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (\\\\romannumeral2) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance. Overall, our approach -- inspired by neuroscience findings and capitalizing on region-level representations from specific brain regions -- is suitable for invasive brain modeling and represents a promising neuro-inspired AI approach in brain-computer interfaces. Code and dataset are available at https://github.com/liulab-repository/Du-IN.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ab4aacca59beba81baaa0ab0761739c5e4d46f7a.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzheng2024duin,\\ntitle={Du-{IN}: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals},\\nauthor={Hui Zheng and Haiteng Wang and Weibang Jiang and Zhongtao Chen and Li He and Peiyang Lin and Penghu Wei and Guoguang Zhao and Yunzhe Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uyLtEFnpQP}\\n}'}, 'paperhash': {'value': 'zheng|duin_discrete_unitsguided_mask_modeling_for_decoding_speech_from_intracranial_neural_signals'}},forum = 'uyLtEFnpQP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission947/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission947/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission947/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission947/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uwSaDHLlYc',number = 1946,cdate = 1714902747045,pdate = 1727287676288,odate = 1730873853567,mdate = 1731980558439,tcdate = 1714902747045,tmdate = 1731980558439,ddate = None,content = {'title': {'value': 'Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment'}, 'authors': {'value': ['Jiawei Du', 'Xin Zhang', 'Juncheng Hu', 'Wenxin Huang', 'Joey Tianyi Zhou']}, 'authorids': {'value': ['~Jiawei_Du1', '~Xin_Zhang29', '~Juncheng_Hu1', '~Wenxin_Huang1', '~Joey_Tianyi_Zhou1']}, 'keywords': {'value': ['Dataset Distillation', 'Synthetic Data', 'Diversity', 'Generatlization']}, 'abstract': {'value': 'The sharp increase in data-related expenses has motivated research into condensing datasets while retaining the most informative features. Dataset distillation has thus recently come to the fore. This paradigm generates synthetic datasets that are representative enough to replace the original dataset in training a neural network. To avoid redundancy in these synthetic datasets, it is crucial that each element contains unique features and remains diverse from others during the synthesis stage. In this paper, we provide a thorough theoretical and empirical analysis of diversity within synthesized datasets. We argue that enhancing diversity can improve the parallelizable yet isolated synthesizing approach. Specifically, we introduce a novel method that employs dynamic and directed weight adjustment techniques to modulate the synthesis process, thereby maximizing the representativeness and diversity of each synthetic instance. Our method ensures that each batch of synthetic data mirrors the characteristics of a large, varying subset of the original dataset. Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet, and ImageNet-1K, demonstrate the superior performance of our method, highlighting its effectiveness in producing diverse and representative synthetic datasets with minimal computational expense. Our code is available at https://github.com/AngusDujw/Diversity-Driven-Synthesis.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/15929c1e779391b46cb1e2a2780b2c8c2975171b.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndu2024diversitydriven,\\ntitle={Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment},\\nauthor={Jiawei Du and Xin Zhang and Juncheng Hu and Wenxin Huang and Joey Tianyi Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uwSaDHLlYc}\\n}'}, 'paperhash': {'value': 'du|diversitydriven_synthesis_enhancing_dataset_distillation_through_directed_weight_adjustment'}},forum = 'uwSaDHLlYc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1946/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1946/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1946/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1946/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'uvFDaeFR9X',number = 19778,cdate = 1715792764585,pdate = 1727288217627,odate = 1730873997607,mdate = 1730873997625,tcdate = 1715792764585,tmdate = 1730873997625,ddate = None,content = {'title': {'value': 'Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations'}, 'authors': {'value': ['Artem Agafonov', 'Petr Ostroukhov', 'Roman Mozhaev', 'Konstantin Yakovlev', 'Eduard Gorbunov', 'Martin Takáč', 'Alexander Gasnikov', 'Dmitry Kamzolov']}, 'authorids': {'value': ['~Artem_Agafonov1', '~Petr_Ostroukhov1', '~Roman_Mozhaev1', '~Konstantin_Yakovlev2', '~Eduard_Gorbunov1', '~Martin_Takáč1', '~Alexander_Gasnikov1', '~Dmitry_Kamzolov1']}, 'keywords': {'value': ['Variational Inequalities', 'Quasi-Newton Methods', 'Min-Max Problems', 'Lower Bounds', 'Optimal Methods']}, 'abstract': {'value': 'Variational inequalities represent a broad class of problems, including minimization and min-max problems, commonly found in machine learning. Existing second-order and high-order methods for variational inequalities require precise computation of derivatives, often resulting in prohibitively high iteration costs. In this work, we study the impact of Jacobian inaccuracy on second-order methods. For the smooth and monotone case, we establish a lower bound with explicit dependence on the level of Jacobian inaccuracy and propose an optimal algorithm for this key setting. When derivatives are exact, our method converges at the same rate as exact optimal second-order methods. To reduce the cost of solving the auxiliary problem, which arises in all high-order methods with global convergence, we introduce several Quasi-Newton approximations. Our method with Quasi-Newton updates achieves a global sublinear convergence rate. We extend our approach with a tensor generalization for inexact high-order derivatives and support the theory with experiments.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3fe99623de2ff3f95d6609a46442ea841749e161.pdf'}, 'supplementary_material': {'value': '/attachment/da4300a886faee222fafaa29b26ce71ccebc4832.zip'}, '_bibtex': {'value': \"@inproceedings{\\nagafonov2024exploring,\\ntitle={Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations},\\nauthor={Artem Agafonov and Petr Ostroukhov and Roman Mozhaev and Konstantin Yakovlev and Eduard Gorbunov and Martin Tak{\\\\'a}{\\\\v{c}} and Alexander Gasnikov and Dmitry Kamzolov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uvFDaeFR9X}\\n}\"}, 'paperhash': {'value': 'agafonov|exploring_jacobian_inexactness_in_secondorder_methods_for_variational_inequalities_lower_bounds_optimal_algorithms_and_quasinewton_approximations'}},forum = 'uvFDaeFR9X',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19778/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19778/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19778/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19778/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uuQQwrjMzb',number = 13864,cdate = 1715743715368,pdate = 1727288052985,odate = 1730873961346,mdate = 1736391574293,tcdate = 1715743715368,tmdate = 1736391574293,ddate = None,content = {'title': {'value': 'Adaptive Labeling for Efficient Out-of-distribution Model Evaluation'}, 'authors': {'value': ['Daksh Mittal', 'Yuanzhe Ma', 'Shalmali Joshi', 'Hongseok Namkoong']}, 'authorids': {'value': ['~Daksh_Mittal1', '~Yuanzhe_Ma1', '~Shalmali_Joshi1', '~Hongseok_Namkoong2']}, 'keywords': {'value': ['Model Evaluation', 'Uncertainty Quantification', 'Markov Decision Process', 'Policy Gradient', 'Auto-differentiation']}, 'abstract': {'value': 'Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams. To assess model performance outside the support of available data, we present a computational framework for adaptive labeling, providing cost-efficient model evaluations under severe distribution shifts. We formulate the problem as a Markov Decision Process over states defined by posterior beliefs on model performance. Each batch of new labels incurs a “state transition” to sharper beliefs, and we choose batches to minimize uncertainty on model performance at the end of the label collection process. Instead of relying on high-variance REINFORCE policy gradient estimators that do not scale, our adaptive labeling policy is optimized using path-wise policy gradients computed by auto-differentiating through simulated roll-outs. Our framework is agnostic to different uncertainty quantification approaches and highlights the virtue of planning in adaptive labeling. On synthetic and real datasets, we empirically demonstrate even a one-step lookahead policy substantially outperforms active learning-inspired heuristics.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9e66a48e6d866d69d2cb84d0ffc85fca6dc59ac5.pdf'}, 'TLDR': {'value': 'Supervised data suffers severe selection bias when labels are expensive. We formulate a MDP over posterior beliefs on model performance and solve it with pathwise policy gradients computed through an auto-differentiable pipeline.'}, '_bibtex': {'value': '@inproceedings{\\nmittal2024adaptive,\\ntitle={Adaptive Labeling for Efficient Out-of-distribution Model Evaluation},\\nauthor={Daksh Mittal and Yuanzhe Ma and Shalmali Joshi and Hongseok Namkoong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uuQQwrjMzb}\\n}'}, 'paperhash': {'value': 'mittal|adaptive_labeling_for_efficient_outofdistribution_model_evaluation'}},forum = 'uuQQwrjMzb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13864/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13864/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13864/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13864/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'utMOhsgXzB',number = 19480,cdate = 1715791185399,pdate = 1727288210776,odate = 1730873995842,mdate = 1730873995861,tcdate = 1715791185399,tmdate = 1730873995861,ddate = None,content = {'title': {'value': 'BendVLM: Test-Time Debiasing of Vision-Language Embeddings'}, 'authors': {'value': ['Walter Gerych', 'Haoran Zhang', 'Kimia Hamidieh', 'Eileen Pan', 'Maanas Sharma', 'Thomas Hartvigsen', 'Marzyeh Ghassemi']}, 'authorids': {'value': ['~Walter_Gerych2', '~Haoran_Zhang4', '~Kimia_Hamidieh1', '~Eileen_Pan1', '~Maanas_Sharma1', '~Thomas_Hartvigsen1', '~Marzyeh_Ghassemi2']}, 'keywords': {'value': ['vision language models', 'embedding models', 'multimodal models', 'debias', 'fairness']}, 'abstract': {'value': 'Vision-language (VL) embedding models have been shown to encode biases present in their training data, such as societal biases that prescribe negative characteristics to members of various racial and gender identities. Due to their wide-spread adoption for various tasks ranging from few-shot classification to text-guided image generation, debiasing VL models is crucial. Debiasing approaches that fine-tune the VL model often suffer from catastrophic forgetting. On the other hand, fine-tuning-free methods typically utilize a ``one-size-fits-all\" approach that assumes that correlation with the spurious attribute can be explained using a single linear direction across all possible inputs. In this work, we propose a nonlinear, fine-tuning-free approach for VL embedding model debiasing that tailors the debiasing operation to each unique input. This allows for a more flexible debiasing approach. Additionally, we do not require knowledge of the set of inputs a priori to inference time, making our method more appropriate for online tasks such as retrieval and text guided image generation.'}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3f3ea9e6ca357bf55565e38507b6e6b9bead00d8.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ngerych2024bendvlm,\\ntitle={Bend{VLM}: Test-Time Debiasing of Vision-Language Embeddings},\\nauthor={Walter Gerych and Haoran Zhang and Kimia Hamidieh and Eileen Pan and Maanas Sharma and Thomas Hartvigsen and Marzyeh Ghassemi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=utMOhsgXzB}\\n}'}, 'paperhash': {'value': 'gerych|bendvlm_testtime_debiasing_of_visionlanguage_embeddings'}},forum = 'utMOhsgXzB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19480/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19480/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19480/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19480/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uqxSLoCw3K',number = 12173,cdate = 1715717644569,pdate = 1727287997310,odate = 1730873946181,mdate = 1730873946194,tcdate = 1715717644569,tmdate = 1730873946194,ddate = None,content = {'title': {'value': 'Mixture of Demonstrations for In-Context Learning'}, 'authors': {'value': ['Song Wang', 'Zihan Chen', 'Chengshuai Shi', 'Cong Shen', 'Jundong Li']}, 'authorids': {'value': ['~Song_Wang6', '~Zihan_Chen5', '~Chengshuai_Shi1', '~Cong_Shen1', '~Jundong_Li2']}, 'keywords': {'value': ['In-context Learning', 'Large Language Models', 'Mixture-of-Experts']}, 'abstract': {'value': 'In-Context Learning (ICL) empowers Large Language Models (LLMs) to tackle various tasks by providing input-output examples as additional inputs, referred to as demonstrations. Nevertheless, the performance of ICL could be easily impacted by the quality of selected demonstrations. Existing efforts generally learn a retriever model to score each demonstration for selecting suitable demonstrations, however, the effect is suboptimal due to the large search space and the noise from unhelpful demonstrations. In this study, we introduce MoD, which partitions the demonstration pool into groups, each governed by an expert to reduce search space. We further design an expert-wise training strategy to alleviate the impact of unhelpful demonstrations when optimizing the retriever model. During inference, experts collaboratively retrieve demonstrations for the input query to enhance the  ICL performance. We validate MoD via experiments across a range of NLP datasets and tasks, demonstrating its state-of-the-art performance and shedding new light on the future design of retrieval methods for ICL.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fc5db0bdacac2fe51883bcf90ff63fbac5bdbf0d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024mixture,\\ntitle={Mixture of Demonstrations for In-Context Learning},\\nauthor={Song Wang and Zihan Chen and Chengshuai Shi and Cong Shen and Jundong Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uqxSLoCw3K}\\n}'}, 'paperhash': {'value': 'wang|mixture_of_demonstrations_for_incontext_learning'}},forum = 'uqxSLoCw3K',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12173/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12173/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12173/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12173/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uqWfLgZpV1',number = 8672,cdate = 1715667378679,pdate = 1727287887239,odate = 1730873913800,mdate = 1735215487523,tcdate = 1715667378679,tmdate = 1735215487523,ddate = None,content = {'title': {'value': 'On the Necessity of Collaboration for Online Model Selection with Decentralized Data'}, 'authors': {'value': ['Junfan Li', 'Zheshun Wu', 'Zenglin Xu', 'Irwin King']}, 'authorids': {'value': ['~Junfan_Li1', '~Zheshun_Wu1', '~Zenglin_Xu2', '~Irwin_King1']}, 'keywords': {'value': ['online learning', 'model selection', 'federated learning', 'kernel methods']}, 'TLDR': {'value': 'We clarify the unnecessary nature of collaboration in previous federated online model selection algorithms,  and give conditions under which collaboration is necessary.'}, 'abstract': {'value': \"We consider online model selection with decentralized data over $M$ clients, and study the necessity of collaboration among clients. Previous work proposed various federated algorithms without demonstrating their necessity, while we answer the question from a novel perspective of computational constraints. We prove lower bounds on the regret, and propose a federated algorithm and analyze the upper bound. Our results show (i) collaboration is unnecessary in the absence of computational constraints on clients; (ii) collaboration is necessary if the computational cost on each client is limited to $o(K)$, where $K$ is the number of candidate hypothesis spaces. We clarify the unnecessary nature of collaboration in previous federated algorithms for distributed online multi-kernel learning, and improve the regret bounds at a smaller computational and communication cost. Our algorithm relies on three new techniques including an improved Bernstein's inequality for martingale, a federated online mirror descent framework, and decoupling model selection and prediction, which might be of independent interest.\"}, 'pdf': {'value': '/pdf/2575e8197592ad29d2d365f3f67b34f1ae597edd.pdf'}, 'supplementary_material': {'value': '/attachment/c369f259d9dd86c4d201a5f7ef0ae7c298d4e851.zip'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nli2024on,\\ntitle={On the Necessity of Collaboration for Online Model Selection with Decentralized Data},\\nauthor={Junfan Li and Zheshun Wu and Zenglin Xu and Irwin King},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uqWfLgZpV1}\\n}'}, 'paperhash': {'value': 'li|on_the_necessity_of_collaboration_for_online_model_selection_with_decentralized_data'}},forum = 'uqWfLgZpV1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8672/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8672/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8672/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8672/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'up4tWnwRol',number = 2298,cdate = 1715030634191,pdate = 1727287686523,odate = 1730873857225,mdate = 1730873857246,tcdate = 1715030634191,tmdate = 1730873857246,ddate = None,content = {'title': {'value': 'The Fine-Grained Complexity of Gradient Computation for Training Large Language Models'}, 'authors': {'value': ['Josh Alman', 'Zhao Song']}, 'authorids': {'value': ['~Josh_Alman1', '~Zhao_Song3']}, 'keywords': {'value': ['Strong Exponential Time Hypothesis', 'Fine-grained Complexity', 'Polynomial methods', 'Gradient Complexity']}, 'abstract': {'value': \"Large language models (LLMs) have made fundamental contributions over the last a few years. To train an LLM, one needs to alternatingly run `forward' computations and backward computations. The forward computation can be viewed as attention function evaluation, and the backward computation can be viewed as a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it was proved that the forward step can be performed in almost-linear time in certain parameter regimes, but that there is no truly sub-quadratic time algorithm in the remaining parameter regimes unless the popular hypothesis $\\\\mathsf{SETH}$ is false. In this work, we show nearly identical results for the harder-seeming problem of computing the gradient of loss function of one layer attention network, and thus for the entire process of LLM training. This completely characterizes the fine-grained complexity of every step of LLM training.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A theoretical study of gradient computation, from both algorithm and hardness perspective'}, 'pdf': {'value': '/pdf/491fb9060e09eb1a90b9cd2cb253e594bf0f7c83.pdf'}, '_bibtex': {'value': '@inproceedings{\\nalman2024the,\\ntitle={The Fine-Grained Complexity of Gradient Computation for Training Large Language Models},\\nauthor={Josh Alman and Zhao Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=up4tWnwRol}\\n}'}, 'paperhash': {'value': 'alman|the_finegrained_complexity_of_gradient_computation_for_training_large_language_models'}},forum = 'up4tWnwRol',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2298/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2298/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2298/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2298/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'uoJQ9qadjY',number = 12220,cdate = 1715718623732,pdate = 1727287999025,odate = 1730873946833,mdate = 1730873946845,tcdate = 1715718623732,tmdate = 1730873946845,ddate = None,content = {'title': {'value': 'Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios'}, 'authors': {'value': ['Shantanu Jaiswal', 'Debaditya Roy', 'Basura Fernando', 'Cheston Tan']}, 'authorids': {'value': ['~Shantanu_Jaiswal1', '~Debaditya_Roy2', '~Basura_Fernando1', '~Cheston_Tan1']}, 'keywords': {'value': ['iterative and parallel computation; complex visual reasoning and question answering; neural network based reasoning architectures']}, 'abstract': {'value': 'Complex visual reasoning and question answering (VQA) is a challenging task that requires compositional multi-step processing and higher-level reasoning capabilities beyond the immediate recognition and localization of objects and events. Here, we introduce a fully neural Iterative and Parallel Reasoning Mechanism (IPRM) that combines two distinct forms of computation -- iterative and parallel -- to better address complex VQA scenarios.  Specifically, IPRM\\'s \"iterative\" computation facilitates compositional step-by-step reasoning for scenarios wherein individual operations need to be computed, stored, and recalled dynamically (e.g. when computing the query “determine the color of pen to the left of the child in red t-shirt sitting at the white table”). Meanwhile, its  \"parallel\\'\\' computation allows for the simultaneous exploration of different reasoning paths and benefits more robust and efficient execution of  operations that are mutually independent (e.g. when counting individual colors for the query: \"determine the maximum occurring color amongst all t-shirts\\'\"). We design IPRM as a lightweight and fully-differentiable neural module that can be conveniently applied to both transformer and non-transformer vision-language backbones. It notably outperforms prior task-specific methods and transformer-based attention modules across various image and video VQA benchmarks testing distinct complex reasoning capabilities such as compositional spatiotemporal reasoning (AGQA), situational reasoning (STAR), multi-hop reasoning generalization (CLEVR-Humans) and causal event linking (CLEVRER-Humans). Further, IPRM\\'s internal computations can be visualized across reasoning steps, aiding interpretability and diagnosis of its errors.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a fully neural reasoning mechanism comprising iterative & parallel computation to address complex image & video reasoning tasks such as AGQA, STAR, CLEVR-Humans and CLEVRER-Humans.'}, 'pdf': {'value': '/pdf/c993bb7e15acae1c52ff8d50243206913deb4cc9.pdf'}, 'supplementary_material': {'value': '/attachment/76811f0d00966c08128e978fac32b2841a640901.zip'}, '_bibtex': {'value': '@inproceedings{\\njaiswal2024learning,\\ntitle={Learning to Reason Iteratively and Parallelly for Complex Visual Reasoning Scenarios},\\nauthor={Shantanu Jaiswal and Debaditya Roy and Basura Fernando and Cheston Tan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uoJQ9qadjY}\\n}'}, 'paperhash': {'value': 'jaiswal|learning_to_reason_iteratively_and_parallelly_for_complex_visual_reasoning_scenarios'}},forum = 'uoJQ9qadjY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12220/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12220/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12220/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12220/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'unA5hxIn6v',number = 11764,cdate = 1715710115625,pdate = 1727287982355,odate = 1730873942245,mdate = 1736393194846,tcdate = 1715710115625,tmdate = 1736393194846,ddate = None,content = {'title': {'value': 'Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input'}, 'authors': {'value': ['Ziang Chen', 'Rong Ge']}, 'authorids': {'value': ['~Ziang_Chen1', '~Rong_Ge1']}, 'keywords': {'value': ['Subspace-sparse polynomial', 'merged-staircase property', 'algebraic independence', 'mean-field analysis', 'stochastic gradient descent']}, 'abstract': {'value': 'In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We establish a necessary condition for SGD-learnability, involving both the characteristics of the target function and the expressiveness of the activation function. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9889d1af9902051a0dba4424bf32a1e4fe24b31a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024meanfield,\\ntitle={Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input},\\nauthor={Ziang Chen and Rong Ge},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=unA5hxIn6v}\\n}'}, 'paperhash': {'value': 'chen|meanfield_analysis_for_learning_subspacesparse_polynomials_with_gaussian_input'}},forum = 'unA5hxIn6v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11764/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11764/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11764/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11764/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'umukvCdGI6',number = 2708,cdate = 1715150789610,pdate = 1727287698128,odate = 1730873860239,mdate = 1730873860257,tcdate = 1715150789610,tmdate = 1730873860257,ddate = None,content = {'title': {'value': 'DOFEN: Deep Oblivious Forest ENsemble'}, 'authors': {'value': ['Kuan-Yu Chen', 'Ping-Han Chiang', 'Hsin-Rung Chou', 'Chih-Sheng Chen', 'Tien-Hao Chang']}, 'authorids': {'value': ['~Kuan-Yu_Chen2', '~Ping-Han_Chiang1', '~Hsin-Rung_Chou2', '~Chih-Sheng_Chen1', '~Tien-Hao_Chang1']}, 'keywords': {'value': ['Tabular Data', 'Structured Data', 'Deep Neural Network', 'Architecture Design']}, 'TLDR': {'value': 'DOFEN (Deep Oblivious Forest ENsemble): a novel deep neural network architecture for tabular data, achieving sota performance compared to deep nerual network baselines and comparable performance with tree-based models.'}, 'abstract': {'value': 'Deep Neural Networks (DNNs) have revolutionized artificial intelligence, achieving impressive results on diverse data types, including images, videos, and texts. However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) on tabular data, a format extensively utilized across various domains. This paper introduces DOFEN, which stands for Deep Oblivious Forest ENsemble. DOFEN is a novel DNN architecture inspired by oblivious decision trees and achieves on-off sparse selection of columns. DOFEN surpasses other DNNs on tabular data, achieving state-of-the-art performance on the well-recognized benchmark: Tabular Benchmark, which includes 73 total datasets spanning a wide array of domains. The code of DOFEN is available at: https://github.com/Sinopac-Digital-Technology-Division/DOFEN'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/679152873f52bb3e925767f5a9c8a74e6ef96d8b.zip'}, 'pdf': {'value': '/pdf/b367f18c85c92e5b35b60e25154c9840ac8be956.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024dofen,\\ntitle={{DOFEN}: Deep Oblivious Forest {EN}semble},\\nauthor={Kuan-Yu Chen and Ping-Han Chiang and Hsin-Rung Chou and Chih-Sheng Chen and Tien-Hao Chang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=umukvCdGI6}\\n}'}, 'paperhash': {'value': 'chen|dofen_deep_oblivious_forest_ensemble'}},forum = 'umukvCdGI6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2708/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2708/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2708/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2708/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ujwIlTNrAP',number = 4533,cdate = 1715423947605,pdate = 1727287753459,odate = 1730873876937,mdate = 1740806646403,tcdate = 1715423947605,tmdate = 1740806646403,ddate = None,content = {'title': {'value': 'AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking'}, 'authors': {'value': ['Shiqi Sun', 'Yantao Lu', 'Ning Liu', 'Bo Jiang', 'Jinchao Chen', 'Ying Zhang']}, 'authorids': {'value': ['~Shiqi_Sun3', '~Yantao_Lu1', '~Ning_Liu4', '~Bo_Jiang1', '~Jinchao_Chen1', '~Ying_Zhang25']}, 'keywords': {'value': ['Network Pruning; Camera-LiDAR fusion models; Perception of Autonomous Driving;']}, 'abstract': {'value': 'Camera-LiDAR fusion models significantly enhance perception performance in autonomous driving. The fusion mechanism leverages the strengths of each modality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR fusion models utilize pre-trained backbones for efficient training. However, we argue that directly loading single-modal pre-trained camera and LiDAR backbones into camera-LiDAR fusion models introduces similar feature redundancy across modalities due to the nature of the fusion mechanism. Unfortunately, existing pruning methods are developed explicitly for single-modal models, and thus, they struggle to effectively identify these specific redundant parameters in camera-LiDAR fusion models. In this paper, to address the issue above on camera-LiDAR fusion models, we propose a novelty pruning framework Alternative Modality Masking Pruning (AlterMOMA), which employs alternative masking on each modality and identifies the redundant parameters. Specifically, when one modality parameters are masked (deactivated), the absence of features from the masked backbone compels the model to reactivate previous redundant features of the other modality backbone. Therefore, these redundant features and relevant redundant parameters can be identified via the reactivation process. The redundant parameters can be pruned by our proposed importance score evaluation function, Alternative Evaluation (AlterEva), which is based on the observation of the loss changes when certain modality parameters are activated and deactivated. Extensive experiments on the nuScene and KITTI datasets encompassing diverse tasks, baseline models, and pruning algorithms showcase that AlterMOMA outperforms existing pruning methods, attaining state-of-the-art performance.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/96b5c41a32824f29863a3aebe65b09d5c34a30f7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024altermoma,\\ntitle={Alter{MOMA}: Fusion Redundancy Pruning for Camera-Li{DAR} Fusion Models with Alternative Modality Masking},\\nauthor={Shiqi Sun and Yantao Lu and Ning Liu and Bo Jiang and Jinchao Chen and Ying Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ujwIlTNrAP}\\n}'}, 'paperhash': {'value': 'sun|altermoma_fusion_redundancy_pruning_for_cameralidar_fusion_models_with_alternative_modality_masking'}},forum = 'ujwIlTNrAP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4533/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4533/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4533/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4533/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ujk0XrNTQZ',number = 8200,cdate = 1715655507967,pdate = 1727287872264,odate = 1730873910243,mdate = 1736995768171,tcdate = 1715655507967,tmdate = 1736995768171,ddate = None,content = {'title': {'value': 'Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization'}, 'authors': {'value': ['Ronak Mehta', 'Jelena Diakonikolas', 'Zaid Harchaoui']}, 'authorids': {'value': ['~Ronak_Mehta2', '~Jelena_Diakonikolas2', '~Zaid_Harchaoui1']}, 'keywords': {'value': ['Distributionally Robust Optimization', 'Stochastic Optimization', 'Convex Optimization', 'Saddle Point']}, 'TLDR': {'value': 'A stochastic primal-dual algorithm for solving distributionally robust optimization problems. It achieves a state-of-the-art linear convergence rate and combines randomized and cyclic components.'}, 'abstract': {'value': 'We consider the penalized distributionally robust optimization (DRO) problem with a closed, convex uncertainty set, a setting that encompasses learning using $f$-DRO and spectral/$L$-risk minimization. We present Drago, a stochastic primal-dual algorithm which combines cyclic and randomized components with a carefully regularized primal update to achieve dual variance reduction. Owing to its design, Drago enjoys a state-of-the-art linear convergence rate on strongly convex-strongly concave DRO problems witha fine-grained dependency on primal and dual condition numbers. The theoretical results are supported with numerical benchmarks on regression and classification tasks.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/a94b5e960d643fba6fb198b200dfb2abcad90299.zip'}, 'pdf': {'value': '/pdf/fd87acd650f18e50e70680e19ad112b9b14352be.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmehta2024drago,\\ntitle={Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization},\\nauthor={Ronak Mehta and Jelena Diakonikolas and Zaid Harchaoui},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ujk0XrNTQZ}\\n}'}, 'paperhash': {'value': 'mehta|drago_primaldual_coupled_variance_reduction_for_faster_distributionally_robust_optimization'}},forum = 'ujk0XrNTQZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8200/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8200/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8200/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8200/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ujE83r50tR',number = 4407,cdate = 1715411350192,pdate = 1727287749892,odate = 1730873875588,mdate = 1730873875606,tcdate = 1715411350192,tmdate = 1730873875606,ddate = None,content = {'title': {'value': 'Octopus: A Multi-modal LLM with Parallel Recognition and Sequential Understanding'}, 'authors': {'value': ['Chuyang Zhao', 'YuXin Song', 'Junru Chen', 'KANG RONG', 'Haocheng Feng', 'Gang Zhang', 'Shufan Ji', 'Jingdong Wang', 'Errui Ding', 'Yifan Sun']}, 'authorids': {'value': ['~Chuyang_Zhao3', '~YuXin_Song1', '~Junru_Chen4', '~KANG_RONG1', '~Haocheng_Feng1', '~Gang_Zhang2', '~Shufan_Ji1', '~Jingdong_Wang1', '~Errui_Ding2', '~Yifan_Sun2']}, 'keywords': {'value': ['multimodal large language model', 'large language model', 'object detection']}, 'abstract': {'value': 'A mainstream of Multi-modal Large Language Models (MLLMs) have two essential functions, i.e., visual recognition (e.g., grounding) and understanding (e.g., visual question answering). Presently, all these MLLMs integrate visual recognition and understanding in a same sequential manner in the LLM head, i.e., generating the response token-by-token for both recognition and understanding. We think unifying them in the same sequential manner is not optimal for two reasons: 1) parallel recognition is more efficient than sequential recognition and is actually prevailing in deep visual recognition, and 2) the recognition results can be integrated to help high-level cognition (while the current manner does not). Such motivated, this paper proposes a novel “parallel recognition → sequential understanding” framework for MLLMs. The bottom LLM layers are utilized for parallel recognition and the recognition results are relayed into the top LLM layers for sequential understanding. Specifically, parallel recognition in the bottom LLM layers is implemented via object queries, a popular mechanism in DEtection TRansformer, which we find to harmonize well with the LLM layers. Empirical studies show our MLLM named Octopus improves accuracy on popular MLLM tasks and is up to 5× faster on visual grounding tasks.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2555221db7dc3b492d4ad067e94664955534d668.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024octopus,\\ntitle={Octopus: A Multi-modal {LLM} with Parallel Recognition and Sequential Understanding},\\nauthor={Chuyang Zhao and YuXin Song and Junru Chen and KANG RONG and Haocheng Feng and Gang Zhang and Shufan Ji and Jingdong Wang and Errui Ding and Yifan Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ujE83r50tR}\\n}'}, 'paperhash': {'value': 'zhao|octopus_a_multimodal_llm_with_parallel_recognition_and_sequential_understanding'}},forum = 'ujE83r50tR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4407/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4407/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4407/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4407/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ujDKXWTbJX',number = 19392,cdate = 1715790661372,pdate = 1727288208890,odate = 1730873995397,mdate = 1730873995416,tcdate = 1715790661372,tmdate = 1730873995416,ddate = None,content = {'title': {'value': 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models'}, 'authors': {'value': ['Kun Zhou', 'Beichen Zhang', 'jiapeng wang', 'Zhipeng Chen', 'Xin Zhao', 'Jing Sha', 'Zhichao Sheng', 'Shijin Wang', 'Ji-Rong Wen']}, 'authorids': {'value': ['~Kun_Zhou2', '~Beichen_Zhang1', '~jiapeng_wang4', '~Zhipeng_Chen2', '~Xin_Zhao10', '~Jing_Sha1', '~Zhichao_Sheng1', '~Shijin_Wang1', '~Ji-Rong_Wen1']}, 'keywords': {'value': ['Large Language Models', 'Mathematical Reasoning', 'Data Synthesis']}, 'abstract': {'value': 'Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.\\nTo enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.\\nTo reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.\\nTo achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.\\nConcretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.\\nBesides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.\\nThe both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.\\nWe leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.\\nExperimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.\\nOur code and data will be publicly released in \\\\url{https://github.com/RUCAIBox/JiuZhang3.0}.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b61328368f8231b0bef39a7ed803d8db25907822.pdf'}, 'supplementary_material': {'value': '/attachment/5a90b475df805b2586cd49013f9c59152b311009.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024jiuzhang,\\ntitle={JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models},\\nauthor={Kun Zhou and Beichen Zhang and jiapeng wang and Zhipeng Chen and Xin Zhao and Jing Sha and Zhichao Sheng and Shijin Wang and Ji-Rong Wen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ujDKXWTbJX}\\n}'}, 'paperhash': {'value': 'zhou|jiuzhang30_efficiently_improving_mathematical_reasoning_by_training_small_data_synthesis_models'}},forum = 'ujDKXWTbJX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19392/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19392/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19392/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19392/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uikhNa4wam',number = 6665,cdate = 1715601420510,pdate = 1727287821940,odate = 1730873895193,mdate = 1730873895210,tcdate = 1715601420510,tmdate = 1730873895210,ddate = None,content = {'title': {'value': 'FIFO-Diffusion: Generating Infinite Videos from Text without Training'}, 'authors': {'value': ['Jihwan Kim', 'Junoh Kang', 'Jinyoung Choi', 'Bohyung Han']}, 'authorids': {'value': ['~Jihwan_Kim5', '~Junoh_Kang2', '~Jinyoung_Choi2', '~Bohyung_Han1']}, 'keywords': {'value': ['generative models', 'diffusion', 'long video generation', 'tuning-free']}, 'TLDR': {'value': 'Infinitely long video generation technique without training based on pretrained diffusion models'}, 'abstract': {'value': 'We propose a novel inference technique based on a pretrained diffusion model for text-conditional video generation. Our approach, called FIFO-Diffusion, is conceptually capable of generating infinitely long videos without additional training. This is achieved by iteratively performing diagonal denoising, which simultaneously processes a series of consecutive frames with increasing noise levels in a queue; our method dequeues a fully denoised frame at the head while enqueuing a new random noise frame at the tail. However, diagonal denoising is a double-edged sword as the frames near the tail can take advantage of cleaner frames by forward reference but such a strategy induces the discrepancy between training and inference. Hence, we introduce latent partitioning to reduce the training-inference gap and lookahead denoising to leverage the benefit of forward referencing. Practically, FIFO-Diffusion consumes a constant amount of memory regardless of the target video length given a baseline model, while well-suited for parallel inference on multiple GPUs. We have demonstrated the promising results and effectiveness of the proposed methods on existing text-to-video generation baselines. Generated video examples and source codes are available at our project page.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/394cdbbb549f23b88a118bc60517b77b96974648.pdf'}, 'supplementary_material': {'value': '/attachment/ebc18793a282e835776f79c4a8b2c1618f01e402.zip'}, '_bibtex': {'value': '@inproceedings{\\nkim2024fifodiffusion,\\ntitle={{FIFO}-Diffusion: Generating Infinite Videos from Text without Training},\\nauthor={Jihwan Kim and Junoh Kang and Jinyoung Choi and Bohyung Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uikhNa4wam}\\n}'}, 'paperhash': {'value': 'kim|fifodiffusion_generating_infinite_videos_from_text_without_training'}},forum = 'uikhNa4wam',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6665/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6665/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6665/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6665/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'uhki1rE2NZ',number = 10515,cdate = 1715694897494,pdate = 1727287943300,odate = 1730873929923,mdate = 1730873929942,tcdate = 1715694897494,tmdate = 1730873929942,ddate = None,content = {'title': {'value': 'Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent'}, 'authors': {'value': ['Liu Ziyin', 'Mingze Wang', 'Hongchao Li', 'Lei Wu']}, 'authorids': {'value': ['~Liu_Ziyin1', '~Mingze_Wang2', '~Hongchao_Li2', '~Lei_Wu1']}, 'keywords': {'value': ['SGD', 'fixed point', 'symmetry']}, 'abstract': {'value': 'Symmetries are prevalent in deep learning and can significantly influence the learning dynamics of neural networks. In this paper, we examine how exponential symmetries -- a broad subclass of continuous symmetries present in the model architecture or loss function -- interplay with stochastic gradient descent (SGD). We first prove that gradient noise creates a systematic motion (a ``Noether flow\") of the parameters $\\\\theta$ along the degenerate direction to a unique initialization-independent fixed point $\\\\theta^*$. These points are referred to as the noise equilibria because, at these points, noise contributions from different directions are balanced and aligned. Then, we show that the balance and alignment of gradient noise can serve as a novel alternative mechanism for explaining important phenomena such as progressive sharpening/flattening and representation formation within neural networks and have practical implications for understanding techniques like representation normalization and warmup.'}, 'pdf': {'value': '/pdf/12403100cfe914c4de8db8333a848158ce24bacf.pdf'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nziyin2024parameter,\\ntitle={Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent},\\nauthor={Liu Ziyin and Mingze Wang and Hongchao Li and Lei Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uhki1rE2NZ}\\n}'}, 'paperhash': {'value': 'ziyin|parameter_symmetry_and_noise_equilibrium_of_stochastic_gradient_descent'}},forum = 'uhki1rE2NZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10515/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10515/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10515/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10515/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ugqx9tgyum',number = 7350,cdate = 1715618802837,pdate = 1727287844206,odate = 1730873902156,mdate = 1730873902167,tcdate = 1715618802837,tmdate = 1730873902167,ddate = None,content = {'title': {'value': 'Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery'}, 'authors': {'value': ['Yongwei Nie', 'Mingxian Fan', 'Chengjiang Long', 'Qing Zhang', 'Jian Zhu', 'Xuemiao Xu']}, 'authorids': {'value': ['~Yongwei_Nie1', '~Mingxian_Fan1', '~Chengjiang_Long1', '~Qing_Zhang7', '~Jian_Zhu5', '~Xuemiao_Xu1']}, 'keywords': {'value': ['Human Mesh Recovery; Meta-Learning; Test-Time Optimization; Training Optimization']}, 'abstract': {'value': 'Human Mesh Recovery (HMR) is the task of estimating a parameterized 3D human mesh from an image. There is a kind of methods first training a regression model for this problem, then further optimizing the pretrained regression model for any specific sample individually at test time. However, the pretrained model may not provide an ideal optimization starting point for the test-time optimization. Inspired by meta-learning, we incorporate the test-time optimization into training, performing a step of test-time optimization for each sample in the training batch before really conducting the training optimization over all the training samples. In this way, we obtain a meta-model, the meta-parameter of which is friendly to the test-time optimization. At test time, after several test-time optimization steps starting from the meta-parameter, we obtain much higher HMR accuracy than the test-time optimization starting from the simply pretrained regression model. Furthermore, we find test-time HMR objectives are different from training-time objectives, which reduces the effectiveness of the learning of the meta-model. To solve this problem, we propose a dual-network architecture that unifies the training-time and test-time objectives. Our method, armed with meta-learning and the dual networks, outperforms state-of-the-art regression-based and optimization-based HMR approaches, as validated by the extensive experiments. The codes are available at https://github.com/fmx789/Meta-HMR.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/49c32264f3c66302dba10c8926a06f670753fe19.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnie2024incorporating,\\ntitle={Incorporating Test-Time Optimization into Training with Dual Networks for Human Mesh Recovery},\\nauthor={Yongwei Nie and Mingxian Fan and Chengjiang Long and Qing Zhang and Jian Zhu and Xuemiao Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ugqx9tgyum}\\n}'}, 'paperhash': {'value': 'nie|incorporating_testtime_optimization_into_training_with_dual_networks_for_human_mesh_recovery'}},forum = 'ugqx9tgyum',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7350/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7350/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7350/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7350/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ugXKInqDCC',number = 12683,cdate = 1715726422855,pdate = 1727288014294,odate = 1730873950926,mdate = 1730873950944,tcdate = 1715726422855,tmdate = 1730873950944,ddate = None,content = {'title': {'value': 'AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies'}, 'authors': {'value': ['Xixi Hu', 'qiang liu', 'Xingchao Liu', 'Bo Liu']}, 'authorids': {'value': ['~Xixi_Hu2', '~qiang_liu4', '~Xingchao_Liu1', '~Bo_Liu13']}, 'keywords': {'value': ['Imitation Learning; Generative Model-Based Policy']}, 'abstract': {'value': 'Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs.\\nWith this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making\\nAdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution is uni-modal. Our comprehensive empirical evaluation shows that AdaFlow achieves high performance with fast inference speed.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fa1f545e371f428274cf16d6695ca80a78e5311d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024adaflow,\\ntitle={AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies},\\nauthor={Xixi Hu and qiang liu and Xingchao Liu and Bo Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ugXKInqDCC}\\n}'}, 'paperhash': {'value': 'hu|adaflow_imitation_learning_with_varianceadaptive_flowbased_policies'}},forum = 'ugXKInqDCC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12683/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12683/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12683/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12683/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ugL2D9idAD',number = 9533,cdate = 1715681360551,pdate = 1727287913881,odate = 1730873921569,mdate = 1730873921605,tcdate = 1715681360551,tmdate = 1730873921605,ddate = None,content = {'title': {'value': 'FilterNet: Harnessing Frequency Filters for Time Series Forecasting'}, 'authors': {'value': ['Kun Yi', 'Jingru Fei', 'Qi Zhang', 'Hui He', 'Shufeng Hao', 'Defu Lian', 'Wei Fan']}, 'authorids': {'value': ['~Kun_Yi2', '~Jingru_Fei1', '~Qi_Zhang25', '~Hui_He2', '~Shufeng_Hao1', '~Defu_Lian1', '~Wei_Fan6']}, 'keywords': {'value': ['time series forecasting', 'learning in the frequency domain']}, 'abstract': {'value': 'Given the ubiquitous presence of time series data across various domains, precise forecasting of time series holds significant importance and finds widespread real-world applications such as energy, weather, healthcare, etc. While numerous forecasters have been proposed using different network architectures, the Transformer-based models have state-of-the-art performance in time series forecasting. However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series with thousands of points. In this paper, we explore a novel perspective of enlightening signal processing for deep time series forecasting. Inspired by the filtering process, we introduce one simple yet effective network, namely FilterNet, built upon our proposed learnable frequency filters to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals. Concretely, we propose two kinds of learnable filters in the FilterNet: (i) Plain shaping filter, that adopts a universal frequency kernel for signal filtering and temporal modeling; (ii) Contextual shaping filter, that utilizes filtered frequencies examined in terms of its compatibility with input signals for\\ndependency learning. Equipped with the two filters, FilterNet can approximately surrogate the linear and attention mappings widely adopted in time series literature, while enjoying superb abilities in handling high-frequency noises and utilizing the whole frequency spectrum that is beneficial for forecasting. Finally, we conduct extensive experiments on eight time series forecasting benchmarks, and experimental results have demonstrated our superior performance in terms of both effectiveness and efficiency compared with state-of-the-art methods. Our code is available at$^1$.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/742093ccdf0fec23ba128edac060f1cd661cc400.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyi2024filternet,\\ntitle={FilterNet: Harnessing Frequency Filters for Time Series Forecasting},\\nauthor={Kun Yi and Jingru Fei and Qi Zhang and Hui He and Shufeng Hao and Defu Lian and Wei Fan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ugL2D9idAD}\\n}'}, 'paperhash': {'value': 'yi|filternet_harnessing_frequency_filters_for_time_series_forecasting'}},forum = 'ugL2D9idAD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9533/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9533/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9533/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9533/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ufPPf9ghzP',number = 12727,cdate = 1715727253425,pdate = 1727288015881,odate = 1730873951240,mdate = 1730873951259,tcdate = 1715727253425,tmdate = 1730873951259,ddate = None,content = {'title': {'value': 'A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models'}, 'authors': {'value': ['Shivvrat Arya', 'Tahrima Rahman', 'Vibhav Giridhar Gogate']}, 'authorids': {'value': ['~Shivvrat_Arya1', '~Tahrima_Rahman1', '~Vibhav_Giridhar_Gogate1']}, 'keywords': {'value': ['Most Probable Explanation', 'Probabilistic Graphical Models', 'Probabilistic Circuits', 'Neural Autoregressive Model']}, 'TLDR': {'value': 'A novel neural-based method for efficiently answering arbitrary Most Probable Explanation (any-MPE) queries in large probabilistic models.'}, 'abstract': {'value': \"We propose a novel neural networks based approach to efficiently answer arbitrary Most Probable Explanation (MPE) queries—a well-known NP-hard task—in large probabilistic models such as \\nBayesian and Markov networks, probabilistic circuits, and neural auto-regressive models. By arbitrary MPE queries, we mean that there is no predefined partition of variables into evidence and non-evidence variables. The key idea is to distill all MPE queries over a given probabilistic model into a neural network and then use the latter for answering queries, eliminating the need for time-consuming inference algorithms that operate directly on the probabilistic model. We improve upon this idea by incorporating inference-time optimization with self-supervised loss to iteratively improve the solutions and employ a teacher-student framework that provides a better initial network, which in turn, helps reduce the number of inference-time optimization steps. The teacher network utilizes a self-supervised loss function optimized for getting the exact MPE solution, while the student network learns from the teacher's near-optimal outputs through supervised loss. We demonstrate the efficacy and scalability of our approach on various datasets and a broad class of probabilistic models, showcasing its practical effectiveness.\"}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3c36eee1186a12d49d52d3cabbac6e20dfbc948a.pdf'}, 'supplementary_material': {'value': '/attachment/907343a2a8c1a52e9d91f04f0f45baf3c735cd67.zip'}, '_bibtex': {'value': '@inproceedings{\\narya2024a,\\ntitle={A Neural Network Approach for Efficiently Answering Most Probable Explanation Queries in Probabilistic Models},\\nauthor={Shivvrat Arya and Tahrima Rahman and Vibhav Giridhar Gogate},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ufPPf9ghzP}\\n}'}, 'paperhash': {'value': 'arya|a_neural_network_approach_for_efficiently_answering_most_probable_explanation_queries_in_probabilistic_models'}},forum = 'ufPPf9ghzP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12727/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12727/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12727/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12727/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ufKBRvYxtp',number = 4844,cdate = 1715468428868,pdate = 1727287762706,odate = 1730873879554,mdate = 1730873879567,tcdate = 1715468428868,tmdate = 1730873879567,ddate = None,content = {'title': {'value': 'Sample-Efficient Agnostic Boosting'}, 'authors': {'value': ['Udaya Ghai', 'Karan Singh']}, 'authorids': {'value': ['~Udaya_Ghai1', '~Karan_Singh1']}, 'keywords': {'value': ['boosting; sample complexity; learning theory; reinforcement learning']}, 'TLDR': {'value': 'Improved sample complexity for agnostic boosting'}, 'abstract': {'value': 'The theory of boosting provides a computational framework for aggregating approximate weak learning algorithms, which perform marginally better than a random predictor, into an accurate strong learner. In the realizable case, the success of the boosting approach is underscored by a remarkable fact that the resultant sample complexity matches that of a computationally demanding alternative, namely Empirical Risk Minimization (ERM). This in particular implies that the realizable boosting methodology has the potential to offer computational relief without compromising on sample efficiency.\\n\\nDespite recent progress, in agnostic boosting, where assumptions on the conditional distribution of labels given feature descriptions are absent, ERM outstrips the agnostic boosting methodology in being quadratically more sample efficient than all known agnostic boosting algorithms. In this paper, we make progress on closing this gap, and give a substantially more sample efficient agnostic boosting algorithm than those known, without compromising on the computational (or oracle) complexity. A key feature of our algorithm is that it leverages the ability to reuse samples across multiple rounds of boosting, while guaranteeing a generalization error strictly better than those obtained by blackbox applications of uniform convergence arguments. We also apply our approach to other previously studied learning problems, including boosting for reinforcement learning, and demonstrate improved results.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6e20c0464b80b05ec2b8673322c045da67b6811d.pdf'}, 'supplementary_material': {'value': '/attachment/0ae5e8b871b4947c7ad8bb9c08a2731d64317f19.zip'}, '_bibtex': {'value': '@inproceedings{\\nghai2024sampleefficient,\\ntitle={Sample-Efficient Agnostic Boosting},\\nauthor={Udaya Ghai and Karan Singh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ufKBRvYxtp}\\n}'}, 'paperhash': {'value': 'ghai|sampleefficient_agnostic_boosting'}},forum = 'ufKBRvYxtp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4844/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4844/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4844/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4844/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'udZKVMPf3S',number = 3890,cdate = 1715334082112,pdate = 1727287733711,odate = 1730873870204,mdate = 1737121501572,tcdate = 1715334082112,tmdate = 1737121501572,ddate = None,content = {'title': {'value': 'Calibrating Reasoning in Language Models with Internal Consistency'}, 'authors': {'value': ['Zhihui Xie', 'Jizhou Guo', 'Tong Yu', 'Shuai Li']}, 'authorids': {'value': ['~Zhihui_Xie2', '~Jizhou_Guo1', '~Tong_Yu3', '~Shuai_Li3']}, 'keywords': {'value': ['Large language models', 'reasoning', 'faithfulness', 'internal consistency']}, 'abstract': {'value': 'Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model’s internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model’s confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We observe internal inconsistency in chain-of-thought reasoning and propose a new method to mitigate it for enhanced reasoning performance of language models.'}, 'pdf': {'value': '/pdf/f671ccf7e167e0fa20272ac829e683807e78b82e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxie2024calibrating,\\ntitle={Calibrating Reasoning in Language Models with Internal Consistency},\\nauthor={Zhihui Xie and Jizhou Guo and Tong Yu and Shuai Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=udZKVMPf3S}\\n}'}, 'paperhash': {'value': 'xie|calibrating_reasoning_in_language_models_with_internal_consistency'}},forum = 'udZKVMPf3S',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3890/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3890/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3890/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3890/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'udTwwF7tks',number = 20476,cdate = 1715796620946,pdate = 1727288234378,odate = 1730874001430,mdate = 1730874001454,tcdate = 1715796620946,tmdate = 1730874001454,ddate = None,content = {'title': {'value': 'Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval'}, 'authors': {'value': ['Ashwin Ramachandran', 'Vaibhav Raj', 'Indradyumna Roy', 'Soumen Chakrabarti', 'Abir De']}, 'authorids': {'value': ['~Ashwin_Ramachandran1', '~Vaibhav_Raj1', '~Indradyumna_Roy1', '~Soumen_Chakrabarti1', '~Abir_De1']}, 'keywords': {'value': ['Graph Neural Networks', 'Graph Retrieval']}, 'TLDR': {'value': 'Improved neural interaction modelling of graph pairs for subgraph matching based retrieval.'}, 'abstract': {'value': 'Graph retrieval based on subgraph isomorphism has several real-world applications such as scene graph retrieval, molecular fingerprint detection and circuit design. Roy et al. [35] proposed IsoNet, a late interaction model for subgraph matching, which first computes the node and edge embeddings of each graph independently of paired graph  and then computes a trainable alignment map. Here, we present $\\\\texttt{IsoNet++}$, an early interaction graph neural network (GNN), based on several technical innovations. First, we compute embeddings of all nodes by passing messages within and across the two input graphs, guided by an *injective alignment* between their nodes. Second, we update this alignment in a lazy fashion over multiple *rounds*. Within each round, we run a layerwise GNN from scratch, based on the current state of the alignment. After the completion of one round of GNN, we use the last-layer embeddings to update the alignments, and proceed to the next round. Third, $\\\\texttt{IsoNet++}$ incorporates a novel notion of node-pair partner interaction. Traditional early interaction computes attention between a node and its potential partners in the other graph, the attention then controlling messages passed across graphs. We consider *node pairs* (not single nodes) as potential partners. Existence of an edge between the nodes in one graph and non-existence in the other provide vital signals for refining the alignment. Our experiments on several datasets show that the alignments get progressively refined with successive rounds,\\nresulting in significantly better retrieval performance than existing methods. We demonstrate that all three innovations contribute to the enhanced accuracy. Our code and datasets are publicly available at https://github.com/structlearning/isonetpp.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4ae7afbdb746c97e2bfe511705355b40c1bce0e3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nramachandran2024iteratively,\\ntitle={Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval},\\nauthor={Ashwin Ramachandran and Vaibhav Raj and Indradyumna Roy and Soumen Chakrabarti and Abir De},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=udTwwF7tks}\\n}'}, 'paperhash': {'value': 'ramachandran|iteratively_refined_early_interaction_alignment_for_subgraph_matching_based_graph_retrieval'}},forum = 'udTwwF7tks',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20476/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20476/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20476/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20476/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ud0RBkdBfE',number = 8426,cdate = 1715660144656,pdate = 1727287879157,odate = 1730873912039,mdate = 1736422127962,tcdate = 1715660144656,tmdate = 1736422127962,ddate = None,content = {'title': {'value': 'Convergence Analysis of Split Federated Learning on Heterogeneous Data'}, 'authors': {'value': ['Pengchao Han', 'Chao Huang', 'Geng Tian', 'Ming Tang', 'Xin Liu']}, 'authorids': {'value': ['~Pengchao_Han1', '~Chao_Huang9', '~Geng_Tian1', '~Ming_Tang5', '~Xin_Liu6']}, 'keywords': {'value': ['split federated learning', 'distributed learning', 'convergence analysis', 'machine learning']}, 'TLDR': {'value': 'Convergence analysis of split federated learning'}, 'abstract': {'value': 'Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\\\\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during training. Numerical experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/4410fbc945e685e1dc0b9bc2086f22c433f95bb9.zip'}, 'pdf': {'value': '/pdf/0b3df95b77b81524235a7ed6b82b52aae602430c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhan2024convergence,\\ntitle={Convergence Analysis of Split Federated Learning on Heterogeneous Data},\\nauthor={Pengchao Han and Chao Huang and Geng Tian and Ming Tang and Xin Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ud0RBkdBfE}\\n}'}, 'paperhash': {'value': 'han|convergence_analysis_of_split_federated_learning_on_heterogeneous_data'}},forum = 'ud0RBkdBfE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8426/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8426/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8426/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8426/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ucxQrked0d',number = 3784,cdate = 1715325330661,pdate = 1727287730754,odate = 1730873869491,mdate = 1730873869513,tcdate = 1715325330661,tmdate = 1730873869513,ddate = None,content = {'title': {'value': 'Making Offline RL Online: Collaborative World Models for Offline Visual Reinforcement Learning'}, 'authors': {'value': ['Qi Wang', 'Junming Yang', 'Yunbo Wang', 'Xin Jin', 'Wenjun Zeng', 'Xiaokang Yang']}, 'authorids': {'value': ['~Qi_Wang26', '~Junming_Yang1', '~Yunbo_Wang2', '~Xin_Jin8', '~Wenjun_Zeng3', '~Xiaokang_Yang1']}, 'keywords': {'value': ['World models', 'reinforcement learning', 'visual control']}, 'abstract': {'value': 'Training offline RL models using visual inputs poses two significant challenges, *i.e.*, the overfitting problem in representation learning and the overestimation bias for expected future rewards. Recent work has attempted to alleviate the overestimation bias by encouraging conservative behaviors. This paper, in contrast, tries to build more flexible constraints for value estimation without impeding the exploration of potential advantages. The key idea is to leverage off-the-shelf RL simulators, which can be easily interacted with in an online manner, as the “*test bed*” for offline policies. To enable effective online-to-offline knowledge transfer, we introduce CoWorld, a model-based RL approach that mitigates cross-domain discrepancies in state and reward spaces. Experimental results demonstrate the effectiveness of CoWorld, outperforming existing RL approaches by large margins.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5a411d1f7badc798a114ee857720735fdee8d9ac.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024making,\\ntitle={Making Offline {RL} Online: Collaborative World Models for Offline Visual Reinforcement Learning},\\nauthor={Qi Wang and Junming Yang and Yunbo Wang and Xin Jin and Wenjun Zeng and Xiaokang Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ucxQrked0d}\\n}'}, 'paperhash': {'value': 'wang|making_offline_rl_online_collaborative_world_models_for_offline_visual_reinforcement_learning'}},forum = 'ucxQrked0d',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3784/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3784/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3784/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3784/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'ucXUtMPWhv',number = 15613,cdate = 1715762099576,pdate = 1727288102214,odate = 1730873973688,mdate = 1730873973705,tcdate = 1715762099576,tmdate = 1730873973705,ddate = None,content = {'title': {'value': 'ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer'}, 'authors': {'value': ['Jiawen Zhang', 'Shun Zheng', 'Xumeng Wen', 'Xiaofang Zhou', 'Jiang Bian', 'Jia Li']}, 'authorids': {'value': ['~Jiawen_Zhang1', '~Shun_Zheng1', '~Xumeng_Wen1', '~Xiaofang_Zhou3', '~Jiang_Bian1', '~Jia_Li4']}, 'keywords': {'value': ['time-series forecasting', 'arbitrary-horizon forecasting', 'transformer']}, 'abstract': {'value': \"Numerous industrial sectors necessitate models capable of providing robust forecasts across various horizons. Despite the recent strides in crafting specific architectures for time-series forecasting and developing pre-trained universal models, a comprehensive examination of their capability in accommodating varied-horizon forecasting during inference is still lacking. This paper bridges this gap through the design and evaluation of the Elastic Time-Series Transformer (ElasTST). The ElasTST model incorporates a non-autoregressive design with placeholders and structured self-attention masks, warranting future outputs that are invariant to adjustments in inference horizons. A tunable version of rotary position embedding is also integrated into ElasTST to capture time-series-specific periods and enhance adaptability to different horizons. Additionally, ElasTST employs a multi-scale patch design, effectively integrating both fine-grained and coarse-grained information.  During the training phase, ElasTST uses a horizon reweighting strategy that approximates the effect of random sampling across multiple horizons with a single fixed horizon setting. Through comprehensive experiments and comparisons with state-of-the-art time-series architectures and contemporary foundation models, we demonstrate the efficacy of ElasTST's unique design elements. Our findings position ElasTST as a robust solution for the practical necessity of varied-horizon forecasting.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4747a9cde6909a55d4179490101ff0d5f36daf83.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024elastst,\\ntitle={Elas{TST}: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer},\\nauthor={Jiawen Zhang and Shun Zheng and Xumeng Wen and Xiaofang Zhou and Jiang Bian and Jia Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ucXUtMPWhv}\\n}'}, 'paperhash': {'value': 'zhang|elastst_towards_robust_variedhorizon_forecasting_with_elastic_timeseries_transformer'}},forum = 'ucXUtMPWhv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15613/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15613/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15613/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15613/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uatPOPWzzU',number = 16798,cdate = 1715774474219,pdate = 1727288137175,odate = 1730873981354,mdate = 1730873981374,tcdate = 1715774474219,tmdate = 1730873981374,ddate = None,content = {'title': {'value': 'Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles'}, 'authors': {'value': ['Rui Duan', 'Mingjian Guang', 'Junli Wang', 'Chungang Yan', 'Hongda Qi', 'Wenkang Su', 'Can Tian', 'Haoran Yang']}, 'authorids': {'value': ['~Rui_Duan4', '~Mingjian_Guang2', '~Junli_Wang1', '~Chungang_Yan1', '~Hongda_Qi1', '~Wenkang_Su1', '~Can_Tian1', '~Haoran_Yang8']}, 'keywords': {'value': ['graph neural networks; filter ensemble; homophily and heterophily']}, 'TLDR': {'value': 'Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles'}, 'abstract': {'value': 'Polynomial-based learnable spectral graph neural networks (GNNs) utilize polynomial to approximate graph convolutions and have achieved impressive performance on graphs. Nevertheless, there are three progressive problems to be solved. Some models use polynomials with better approximation for approximating filters, yet perform worse on real-world graphs. Carefully crafted graph learning methods, sophisticated polynomial approximations, and refined coefficient constraints leaded to overfitting, which diminishes the generalization of the models. How to design a model that retains the ability of polynomial-based spectral GNNs to approximate filters while it possesses higher generalization and performance? In this paper, we propose a spectral GNN with triple filter ensemble (TFE-GNN), which extracts homophily and heterophily from graphs with different levels of homophily adaptively while utilizing the initial features. Specifically, the first and second ensembles are combinations of a set of base low-pass and high-pass filters, respectively, after which the third ensemble combines them with two learnable coefficients and yield a graph convolution (TFE-Conv). Theoretical analysis shows that the approximation ability of TFE-GNN is consistent with that of ChebNet under certain conditions, namely it can learn arbitrary filters. TFE-GNN can be viewed as a reasonable combination of two unfolded and integrated excellent spectral GNNs, which motivates it to perform well. Experiments show that TFE-GNN achieves high generalization and new state-of-the-art performance on various real-world datasets.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c48983151fb063fc13fbbf2529feadc3d40a3cf2.pdf'}, 'supplementary_material': {'value': '/attachment/7a564db332225eeeee3ce77a9499cd4aa4245742.zip'}, '_bibtex': {'value': '@inproceedings{\\nduan2024unifying,\\ntitle={Unifying Homophily and Heterophily for Spectral Graph Neural Networks via Triple Filter Ensembles},\\nauthor={Rui Duan and Mingjian Guang and Junli Wang and Chungang Yan and Hongda Qi and Wenkang Su and Can Tian and Haoran Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uatPOPWzzU}\\n}'}, 'paperhash': {'value': 'duan|unifying_homophily_and_heterophily_for_spectral_graph_neural_networks_via_triple_filter_ensembles'}},forum = 'uatPOPWzzU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16798/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16798/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16798/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16798/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uaNZvF1VFe',number = 16336,cdate = 1715769386045,pdate = 1727288123842,odate = 1730873978688,mdate = 1734585854266,tcdate = 1715769386045,tmdate = 1734585854266,ddate = None,content = {'title': {'value': 'Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction'}, 'authors': {'value': ['Wei Jiang', 'Sifan Yang', 'Wenhao Yang', 'Lijun Zhang']}, 'authorids': {'value': ['~Wei_Jiang8', '~Sifan_Yang2', '~Wenhao_Yang3', '~Lijun_Zhang1']}, 'keywords': {'value': ['SignSGD', 'majority vote', 'sign-based method', 'variance reduction']}, 'abstract': {'value': 'Sign stochastic gradient descent (signSGD) is a communication-efficient method that transmits only the sign of stochastic gradients for parameter updating. Existing literature has demonstrated that signSGD can achieve a convergence rate of $\\\\mathcal{O}(d^{1/2}T^{-1/4})$, where $d$ represents the dimension and $T$ is the iteration number. In this paper, we improve this convergence rate to $\\\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the Sign-based Stochastic Variance Reduction (SSVR) method, which employs variance reduction estimators to track gradients and leverages their signs to update. For finite-sum problems, our method can be further enhanced to achieve a convergence rate of $\\\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$, where $m$ denotes the number of component functions. Furthermore, we investigate the heterogeneous majority vote in distributed settings and introduce two novel algorithms that attain improved convergence rates of $\\\\mathcal{O}(d^{1/2}T^{-1/2} + dn^{-1/2})$ and $\\\\mathcal{O}(d^{1/4}T^{-1/4})$ respectively, outperforming the previous results of $\\\\mathcal{O}(dT^{-1/4} + dn^{-1/2})$ and $\\\\mathcal{O}(d^{3/8}T^{-1/8})$, where $n$ represents the number of nodes. Numerical experiments across different tasks validate the effectiveness of our proposed methods.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9b43a99cf25d1e35bb50a6d5c29beae9ed50c900.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiang2024efficient,\\ntitle={Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction},\\nauthor={Wei Jiang and Sifan Yang and Wenhao Yang and Lijun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uaNZvF1VFe}\\n}'}, 'paperhash': {'value': 'jiang|efficient_signbased_optimization_accelerating_convergence_via_variance_reduction'}},forum = 'uaNZvF1VFe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16336/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16336/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16336/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16336/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uZi7H5Ac0X',number = 13952,cdate = 1715744492698,pdate = 1727288055768,odate = 1730873962391,mdate = 1730873962412,tcdate = 1715744492698,tmdate = 1730873962412,ddate = None,content = {'title': {'value': 'A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints'}, 'authors': {'value': ['Liuyuan Jiang', 'Quan Xiao', 'Victor M. Tenorio', 'Fernando Real-Rojas', 'Antonio Marques', 'Tianyi Chen']}, 'authorids': {'value': ['~Liuyuan_Jiang1', '~Quan_Xiao1', '~Victor_M._Tenorio1', '~Fernando_Real-Rojas1', '~Antonio_Marques1', '~Tianyi_Chen5']}, 'keywords': {'value': ['Bilevel Optimization', 'Constrained Optimization', 'Hessian-free', 'Convergence Analysis', 'Penalty Based', 'Primal Dual']}, 'abstract': {'value': 'Interest in bilevel optimization has grown in recent years, partially due to its relevance for challenging machine-learning problems. Several exciting recent works have been centered around developing efficient gradient-based algorithms that can solve bilevel optimization problems with provable guarantees. However, the existing literature mainly focuses on bilevel problems either without constraints, or featuring only simple constraints that do not couple variables across the upper and lower levels, excluding a range of complex applications. Our paper studies this challenging but less explored scenario and develops a (fully) first-order algorithm, which we term BLOCC, to tackle BiLevel Optimization problems with Coupled  Constraints.  We establish rigorous convergence theory for the proposed algorithm and demonstrate its effectiveness on two well-known real-world applications - support vector machine (SVM) - based model training and infrastructure planning in transportation networks.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/13a0f27075bedab8b79d901ed72ef74c635ac09c.pdf'}, 'supplementary_material': {'value': '/attachment/3594f83f2ce3da0c3774d7a0290ff05ff81c2536.zip'}, '_bibtex': {'value': '@inproceedings{\\njiang2024a,\\ntitle={A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints},\\nauthor={Liuyuan Jiang and Quan Xiao and Victor M. Tenorio and Fernando Real-Rojas and Antonio Marques and Tianyi Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uZi7H5Ac0X}\\n}'}, 'paperhash': {'value': 'jiang|a_primaldualassisted_penalty_approach_to_bilevel_optimization_with_coupled_constraints'}},forum = 'uZi7H5Ac0X',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13952/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13952/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13952/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13952/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uYZTzcHaQB',number = 14573,cdate = 1715751337887,pdate = 1727288073368,odate = 1730873966827,mdate = 1736989146216,tcdate = 1715751337887,tmdate = 1736989146216,ddate = None,content = {'title': {'value': 'Motif-oriented influence maximization for viral marketing in large-scale social networks'}, 'authors': {'value': ['Mingyang Zhou', 'Weiji Cao', 'Hao Liao', 'Rui Mao']}, 'authorids': {'value': ['~Mingyang_Zhou2', '~Weiji_Cao1', '~Hao_Liao1', '~Rui_Mao2']}, 'keywords': {'value': ['influence maximization', 'influential node', 'viral marketing']}, 'abstract': {'value': 'The influence maximization (IM) problem aims to identify a budgeted set of nodes with the highest potential to influence the largest number of users in a cascade model, a key challenge in viral marketing. Traditional \\\\emph{IM} approaches consider each user/node independently as a potential target customer. However, in many scenarios, the target customers comprise motifs, where activating only one or a few users within a motif is insufficient for effective viral marketing, which, nevertheless, receives little attention. For instance, if a motif of three friends planning to dine together, targeting all three simultaneously is crucial for a restaurant advertisement to succeed.\\nIn this paper, we address the motif-oriented influence maximization problem under the linear threshold model. We prove that the motif-oriented IM problem is NP-hard and that the influence function is neither supermodular nor submodular, in contrast to the classical \\\\emph{IM} setting.\\nTo simplify the problem, we establish the submodular upper and lower bounds for the influence function. By leveraging the submodular property, we propose a natural greedy strategy that simultaneously maximizes both bounds. Our algorithm has an approximation ratio of  $\\\\tau\\\\cdot (1-1/e-\\\\varepsilon)$ and a near-linear time complexity of $O((k+l)(m+\\\\eta)\\\\log \\\\eta/\\\\varepsilon^2)$.\\nExperimental results on diverse datasets confirm the effectiveness of our approach in motif maximization.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4a0b2002dc5b4f6b4f90ecee051a0010a0aecb0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024motiforiented,\\ntitle={Motif-oriented influence maximization for viral marketing in large-scale social networks},\\nauthor={Mingyang Zhou and Weiji Cao and Hao Liao and Rui Mao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uYZTzcHaQB}\\n}'}, 'paperhash': {'value': 'zhou|motiforiented_influence_maximization_for_viral_marketing_in_largescale_social_networks'}},forum = 'uYZTzcHaQB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14573/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14573/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14573/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14573/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'uXuObobJHO',number = 10374,cdate = 1715693481033,pdate = 1727287937545,odate = 1730873928323,mdate = 1734555204815,tcdate = 1715693481033,tmdate = 1734555204815,ddate = None,content = {'title': {'value': 'Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models'}, 'authors': {'value': ['Jinlin Lai', 'Justin Domke', 'Daniel Sheldon']}, 'authorids': {'value': ['~Jinlin_Lai2', '~Justin_Domke1', '~Daniel_Sheldon1']}, 'keywords': {'value': ['Hamiltonian Monte Carlo', 'Bayesian inference', 'hierarchical models']}, 'abstract': {'value': 'Bayesian reasoning in linear mixed-effects models (LMMs) is challenging and often requires advanced sampling techniques like Markov chain Monte Carlo (MCMC).\\nA common approach is to write the model in a probabilistic programming language and then sample via Hamiltonian Monte Carlo (HMC).\\nHowever, there are many ways a user can transform a model that make inference more or less efficient.\\nIn particular, marginalizing some variables can greatly improve inference but is difficult for users to do manually.\\nWe develop an algorithm to easily marginalize random effects in LMMs.\\nA naive approach introduces cubic time operations within an inference algorithm like HMC, but we reduce the running time to linear using fast linear algebra techniques.\\nWe show that marginalization is always beneficial when applicable and highlight improvements in various models, especially ones from cognitive sciences.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/378559ad4751205920b97814178bfb59814e5aaa.pdf'}, 'supplementary_material': {'value': '/attachment/18e3eaec43ff39372f5f5b4bcab3ff932a73deb9.zip'}, '_bibtex': {'value': '@inproceedings{\\nlai2024hamiltonian,\\ntitle={Hamiltonian Monte Carlo Inference of Marginalized Linear Mixed-Effects Models},\\nauthor={Jinlin Lai and Daniel Sheldon and Justin Domke},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uXuObobJHO}\\n}'}, 'paperhash': {'value': 'lai|hamiltonian_monte_carlo_inference_of_marginalized_linear_mixedeffects_models'}},forum = 'uXuObobJHO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10374/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10374/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10374/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10374/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uXJlgkWdcI',number = 472,cdate = 1713930479432,pdate = 1727287638506,odate = 1730873841224,mdate = 1730873841241,tcdate = 1713930479432,tmdate = 1730873841241,ddate = None,content = {'title': {'value': 'PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices'}, 'authors': {'value': ['Hanqing Zhu', 'Wenyan Cong', 'Guojin Chen', 'Shupeng Ning', 'Ray Chen', 'Jiaqi Gu', 'David Z. Pan']}, 'authorids': {'value': ['~Hanqing_Zhu1', '~Wenyan_Cong1', '~Guojin_Chen1', '~Shupeng_Ning1', '~Ray_Chen2', '~Jiaqi_Gu3', '~David_Z._Pan1']}, 'keywords': {'value': ['AI for Scienece', 'Optical simulation', 'Neural opeartor', 'AI for PDE']}, 'abstract': {'value': 'Electromagnetic field simulation is central to designing, optimizing, and validating photonic devices and circuits. \\nHowever, costly computation associated with numerical simulation poses a significant bottleneck, hindering scalability and turnaround time in the photonic circuit design process.\\nNeural operators offer a promising alternative, but existing SOTA approaches, Neurolight, struggle with predicting high-fidelity fields for real-world complicated photonic devices, with the best reported 0.38 normalized mean absolute error in Neurolight.\\nThe interplays of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers.\\nIn this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges.\\nWe propose a novel cross-axis factorized PACE operator with a strong long-distance modeling capacity to connect the full-domain complex field pattern with local device structures.\\nInspired by human learning, we further divide and conquer the simulation task for extremely hard cases into two progressively easy tasks, with a first-stage model learning an initial solution refined by a second model.\\nOn various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers.\\nThe two-stage setup further advances high-fidelity simulation for even more intricate cases.\\nIn terms of runtime, \\nPACE demonstrates 154-577x and 11.8-12x simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively.\\nWe open-sourced the code and *complicated* optical device dataset at [PACE-Light](https://github.com/zhuhanqing/PACE-Light).'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10b54377d0cf5b57481e75058fec3c1d6858aede.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024pace,\\ntitle={{PACE}: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices},\\nauthor={Hanqing Zhu and Wenyan Cong and Guojin Chen and Shupeng Ning and Ray Chen and Jiaqi Gu and David Z. Pan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uXJlgkWdcI}\\n}'}, 'paperhash': {'value': 'zhu|pace_pacing_operator_learning_to_accurate_optical_field_simulation_for_complicated_photonic_devices'}},forum = 'uXJlgkWdcI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission472/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission472/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission472/-/Revision', 'NeurIPS.cc/2024/Conference/Submission472/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'uSKzEaj9zJ',number = 21440,cdate = 1715801828327,pdate = 1727288255522,odate = 1730874005775,mdate = 1730874005786,tcdate = 1715801828327,tmdate = 1730874005786,ddate = None,content = {'title': {'value': 'Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery'}, 'authors': {'value': ['Yue Yu', 'Ning Liu', 'Fei Lu', 'Tian Gao', 'Siavash Jafarzadeh', 'Stewart A Silling']}, 'authorids': {'value': ['~Yue_Yu3', '~Ning_Liu6', '~Fei_Lu2', '~Tian_Gao1', '~Siavash_Jafarzadeh1', '~Stewart_A_Silling1']}, 'keywords': {'value': ['Foundation Model', 'Neural Operators', 'Inverse PDE Problems', 'Physical Modeling']}, 'abstract': {'value': 'Despite recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physical model. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. Lastly, we empirically demonstrate the advantages of NAO over baseline neural models in terms of the generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning an interpretable foundation model of physical systems, but also offers a new perspective towards understanding the attention mechanism.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6cac3b157a00647405bf6878194d4c24024eac22.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyu2024nonlocal,\\ntitle={Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery},\\nauthor={Yue Yu and Ning Liu and Fei Lu and Tian Gao and Siavash Jafarzadeh and Stewart A Silling},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uSKzEaj9zJ}\\n}'}, 'paperhash': {'value': 'yu|nonlocal_attention_operator_materializing_hidden_knowledge_towards_interpretable_physics_discovery'}},forum = 'uSKzEaj9zJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21440/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21440/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21440/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21440/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'uS0PwIBzC0',number = 5572,cdate = 1715550108160,pdate = 1727287788200,odate = 1730873886135,mdate = 1730873886156,tcdate = 1715550108160,tmdate = 1730873886156,ddate = None,content = {'title': {'value': 'SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors'}, 'authors': {'value': ['Vijay Lingam', 'Atula Tejaswi Neerkaje', 'Aditya Vavre', 'Aneesh Shetty', 'Gautham Krishna Gudur', 'Joydeep Ghosh', 'Eunsol Choi', 'Alex Dimakis', 'Aleksandar Bojchevski', 'sujay sanghavi']}, 'authorids': {'value': ['~Vijay_Lingam1', '~Atula_Tejaswi_Neerkaje1', '~Aditya_Vavre1', '~Aneesh_Shetty1', '~Gautham_Krishna_Gudur1', '~Joydeep_Ghosh1', '~Eunsol_Choi1', '~Alex_Dimakis1', '~Aleksandar_Bojchevski1', '~sujay_sanghavi1']}, 'keywords': {'value': ['Parameter Efficient Fine Tuning', 'Large Language Models', 'Deep Learning']}, 'abstract': {'value': 'Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights $\\\\(\\\\mathbf{W}\\\\)$ and inject learnable matrices $\\\\(\\\\mathbf{\\\\Delta W}\\\\)$. These $\\\\(\\\\mathbf{\\\\Delta W}\\\\)$ matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically exhibit a performance gap compared to full fine-tuning. While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters. We propose SVFT, a *simple* approach that structures $\\\\(\\\\mathbf{\\\\Delta W}\\\\)$ based on the specific weight matrix $\\\\(\\\\mathbf{W}\\\\)$. SVFT updates $\\\\(\\\\mathbf{W}\\\\)$ as a sparse combination $\\\\(M\\\\)$ of outer products of its singular vectors, training only the coefficients of these combinations. Crucially, we make additional off-diagonal elements in $M$ learnable, enabling a smooth trade-off between trainable parameters and expressivity—an aspect that distinctly sets our approach apart from previous works leveraging singular values. Extensive experiments on language and vision benchmarks show that SVFT recovers up to **96%** of full fine-tuning performance while training only **0.006 to 0.25%** of parameters, outperforming existing methods that achieve only up to **{85\\\\%}** performance with **0.03 to 0.8%** of the trainable parameter budget.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/adde9792a32c217eb77036b356c635e95d846915.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlingam2024svft,\\ntitle={{SVFT}: Parameter-Efficient Fine-Tuning with Singular Vectors},\\nauthor={Vijay Lingam and Atula Tejaswi Neerkaje and Aditya Vavre and Aneesh Shetty and Gautham Krishna Gudur and Joydeep Ghosh and Eunsol Choi and Alex Dimakis and Aleksandar Bojchevski and sujay sanghavi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uS0PwIBzC0}\\n}'}, 'paperhash': {'value': 'lingam|svft_parameterefficient_finetuning_with_singular_vectors'}},forum = 'uS0PwIBzC0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5572/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5572/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5572/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5572/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'uRnTYPkF3V',number = 11803,cdate = 1715710966647,pdate = 1727287984255,odate = 1730873942837,mdate = 1730873942859,tcdate = 1715710966647,tmdate = 1730873942859,ddate = None,content = {'title': {'value': 'Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood'}, 'authors': {'value': ['Ziyi Liu', 'Idan Attias', 'Daniel M. Roy']}, 'authorids': {'value': ['~Ziyi_Liu7', '~Idan_Attias1', '~Daniel_M._Roy1']}, 'keywords': {'value': ['online learning', 'log loss', 'probabilistic forecasting']}, 'abstract': {'value': 'We study the fundamental problem of sequential probability assignment, also known as online learning with logarithmic loss, with respect to an arbitrary, possibly nonparametric hypothesis class. Our goal is to obtain a complexity measure for the hypothesis class that characterizes the minimax regret and to determine a general, minimax optimal algorithm. Notably, the sequential $\\\\ell_{\\\\infty}$ entropy, extensively studied in the literature (Rakhlin and Sridharan, 2015, Bilodeau et al., 2020, Wu et al., 2023), was shown to not characterize minimax regret in general. Inspired by the seminal work of Shtarkov (1987)\\n    and Rakhlin, Sridharan, and Tewari (2010), we introduce a novel complexity measure, the \\\\emph{contextual Shtarkov sum}, corresponding to the Shtarkov sum after projection onto a multiary context tree, and show that the worst case log contextual Shtarkov sum equals the minimax regret. Using the contextual Shtarkov sum, we derive the minimax optimal strategy, dubbed \\\\emph{contextual Normalized Maximum Likelihood} (cNML). Our results hold for sequential experts, beyond binary labels, which are settings rarely considered in prior work. \\n    To illustrate the utility of this characterization, we provide a short proof of a new regret upper bound in terms of sequential $\\\\ell_{\\\\infty}$ entropy, unifying and sharpening state-of-the-art bounds by Bilodeau et al. (2020) and Wu et al. (2023).'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We define a notion of complexity for a class of probability kernels and show it is the minimax regret of sequential probability assignment, and allows us to describe the minimax algorithm.'}, 'pdf': {'value': '/pdf/45cc567d0c06061737d622879a849ccfc81d9c05.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024sequential,\\ntitle={Sequential Probability Assignment with Contexts: Minimax Regret, Contextual Shtarkov Sums, and Contextual Normalized Maximum Likelihood},\\nauthor={Ziyi Liu and Idan Attias and Daniel M. Roy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uRnTYPkF3V}\\n}'}, 'paperhash': {'value': 'liu|sequential_probability_assignment_with_contexts_minimax_regret_contextual_shtarkov_sums_and_contextual_normalized_maximum_likelihood'}},forum = 'uRnTYPkF3V',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11803/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11803/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11803/-/Revision', 'NeurIPS.cc/2024/Conference/Submission11803/-/Deletion', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11803/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uOvrwVW1yA',number = 20168,cdate = 1715794850058,pdate = 1727288227038,odate = 1730873999673,mdate = 1730873999690,tcdate = 1715794850058,tmdate = 1730873999690,ddate = None,content = {'title': {'value': 'Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut'}, 'authors': {'value': ['Hongyu Cheng', 'Sammy Khalife', 'Barbara Fiedorowicz', 'Amitabh Basu']}, 'authorids': {'value': ['~Hongyu_Cheng1', '~Sammy_Khalife1', '~Barbara_Fiedorowicz1', '~Amitabh_Basu1']}, 'keywords': {'value': ['Integer programming', 'branch-and-cut', 'branch-and-bound', 'sample complexity', 'neural networks', 'learning theory', 'data-driven algorithm design']}, 'abstract': {'value': 'Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by considering the setup where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved, using neural networks. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm *for that instance*. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization (e.g., which cut to add?). In other words, the neural network will take as input a mixed-integer optimization instance and output a decision that will result in a small branch-and-cut tree for that instance. Our computational results provide evidence that our particular way of using neural networks for cut selection can make a significant impact in reducing branch-and-cut tree sizes, compared to previous data-driven approaches.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b7e10c38cee539331a5186b1d97e6369cb37e69b.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024sample,\\ntitle={Sample Complexity of Algorithm Selection Using Neural Networks and Its Applications to Branch-and-Cut},\\nauthor={Hongyu Cheng and Sammy Khalife and Barbara Fiedorowicz and Amitabh Basu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uOvrwVW1yA}\\n}'}, 'paperhash': {'value': 'cheng|sample_complexity_of_algorithm_selection_using_neural_networks_and_its_applications_to_branchandcut'}},forum = 'uOvrwVW1yA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20168/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20168/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20168/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20168/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uO53206oLJ',number = 11869,cdate = 1715712208549,pdate = 1727287986235,odate = 1730873943307,mdate = 1730873943319,tcdate = 1715712208549,tmdate = 1730873943319,ddate = None,content = {'title': {'value': 'Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data'}, 'authors': {'value': ['Jiaojiao Zhang', 'Jiang Hu', 'Anthony Man-Cho So', 'Mikael Johansson']}, 'authorids': {'value': ['~Jiaojiao_Zhang3', '~Jiang_Hu2', '~Anthony_Man-Cho_So1', '~Mikael_Johansson3']}, 'keywords': {'value': ['Federated learning', 'manifold optimization', 'heterogeneous data']}, 'abstract': {'value': 'Many machine learning tasks, such as principal component analysis and low-rank matrix completion, give rise to manifold optimization problems. Although there is a large body of work studying the design and analysis of algorithms for manifold optimization in the centralized setting, there are currently very few works addressing the federated setting. In this paper, we consider nonconvex federated learning\\nover a compact smooth submanifold in the setting of heterogeneous client data. We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift. Theoretically, we show that our proposed algorithm converges sub-linearly to a neighborhood of a first-order optimal solution by using a novel analysis that jointly exploits the manifold structure and properties of the loss functions. Numerical experiments demonstrate that our algorithm has significantly smaller computational and communication overhead than existing methods.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper proposes a computation- and communication-efficient algorithm for federated learning over manifolds with heterogeneous data.'}, 'pdf': {'value': '/pdf/0a6ebdd721a6f62f7e75a245c3c48a52794ba892.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024nonconvex,\\ntitle={Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data},\\nauthor={Jiaojiao Zhang and Jiang Hu and Anthony Man-Cho So and Mikael Johansson},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uO53206oLJ}\\n}'}, 'paperhash': {'value': 'zhang|nonconvex_federated_learning_on_compact_smooth_submanifolds_with_heterogeneous_data'}},forum = 'uO53206oLJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11869/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11869/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11869/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11869/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uNKlTQ8mBD',number = 14424,cdate = 1715749848858,pdate = 1727288069029,odate = 1730873965691,mdate = 1730873965709,tcdate = 1715749848858,tmdate = 1730873965709,ddate = None,content = {'title': {'value': 'Learning Formal Mathematics From Intrinsic Motivation'}, 'authors': {'value': ['Gabriel Poesia', 'David Broman', 'Nick Haber', 'Noah Goodman']}, 'authorids': {'value': ['~Gabriel_Poesia1', '~David_Broman1', '~Nick_Haber1', '~Noah_Goodman1']}, 'keywords': {'value': ['reasoning', 'reinforcement learning', 'formal mathematics', 'logic']}, 'TLDR': {'value': 'We jointly learn to prove formal mathematical theorems and to propose harder provable conjectures in a self-improving loop'}, 'abstract': {'value': \"How did humanity coax mathematics from the aether? We explore the Platonic view that mathematics can be discovered from its axioms---a game of conjecture and proof. We describe an agent that jointly learns to pose challenging problems for itself (conjecturing) and solve them (theorem proving). Given a mathematical domain axiomatized in dependent type theory, we first combine methods for constrained decoding and type-directed synthesis to sample valid conjectures from a language model. Our method guarantees well-formed conjectures by construction, even as we start with a randomly initialized model. We use the same model to represent a policy and value function for guiding proof search. Our agent targets generating hard but provable conjectures --- a moving target, since its own theorem proving ability also improves as it trains. We propose novel methods for hindsight relabeling on proof search trees to significantly improve the agent's sample efficiency in both tasks. Experiments on 3 axiomatic domains (propositional logic, arithmetic and group theory) demonstrate that our agent can bootstrap from only the axioms, self-improving in generating true and challenging conjectures and in finding proofs.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/42d3b14720041d447c657071a08de640733954a0.pdf'}, 'supplementary_material': {'value': '/attachment/d581092ecc885d5149dd8002d3958c36e11fdef5.zip'}, '_bibtex': {'value': '@inproceedings{\\npoesia2024learning,\\ntitle={Learning Formal Mathematics From Intrinsic Motivation},\\nauthor={Gabriel Poesia and David Broman and Nick Haber and Noah Goodman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uNKlTQ8mBD}\\n}'}, 'paperhash': {'value': 'poesia|learning_formal_mathematics_from_intrinsic_motivation'}},forum = 'uNKlTQ8mBD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14424/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14424/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14424/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14424/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uM3rQ14iex',number = 19422,cdate = 1715790820332,pdate = 1727288209379,odate = 1730873995506,mdate = 1730873995525,tcdate = 1715790820332,tmdate = 1730873995525,ddate = None,content = {'title': {'value': 'Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits'}, 'authors': {'value': ['Muhammad Qasim Elahi', 'Mahsa Ghasemi', 'Murat Kocaoglu']}, 'authorids': {'value': ['~Muhammad_Qasim_Elahi1', '~Mahsa_Ghasemi1', '~Murat_Kocaoglu1']}, 'keywords': {'value': ['Causal Bandits', 'No-regret Learning', 'Causal Discovery']}, 'TLDR': {'value': \"We propose a two-phase algorithm for causal bandits with unknown causal graphs: Phase one learns the subgraph on reward's ancestors to identify all possibly optimal arms, followed by a standard bandit algorithm with analysis of the cumulative regret.\"}, 'abstract': {'value': 'Causal knowledge about the relationships among decision variables and a reward variable in a bandit setting can accelerate the learning of an optimal decision. Current works often assume the causal graph is known, which may not always be available a priori. Motivated by this challenge, we focus on the causal bandit problem in scenarios where the underlying causal graph is unknown and may include latent confounders. While intervention on the parents of the reward node is optimal in the absence of latent confounders, this is not necessarily the case in general. Instead, one must consider a set of possibly optimal arms/interventions, each being a special subset of the ancestors of the reward node, making causal discovery beyond the parents of the reward node essential. For regret minimization, we identify that discovering the full causal structure is unnecessary; however, no existing work provides the necessary and sufficient components of the causal graph. We formally characterize the set of necessary and sufficient latent confounders one needs to detect or learn to ensure that all possibly optimal arms are identified correctly. We also propose a randomized algorithm for learning the causal graph with a limited number of samples, providing a sample complexity guarantee for any desired confidence level. In the causal bandit setup, we propose a two-stage approach. In the first stage, we learn the induced subgraph on ancestors of the reward, along with a necessary and sufficient subset of latent confounders, to construct the set of possibly optimal arms. We show that for our proposed algorithm, the number of intervention samples required to learn the set of possibly optimal arms scales polynomially with respect to the number of nodes. The second phase involves the application of a standard bandit algorithm, such as the UCB algorithm. We also establish a regret bound for our two-phase approach, which is sublinear in the number of rounds.'}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d0114b817255971d1cf775616aeaace91ed92a7b.pdf'}, 'supplementary_material': {'value': '/attachment/0da32fc5f5e8e084de3deb6b8d1acc8f5233bf59.zip'}, '_bibtex': {'value': '@inproceedings{\\nelahi2024partial,\\ntitle={Partial Structure Discovery is Sufficient for No-regret Learning in Causal Bandits},\\nauthor={Muhammad Qasim Elahi and Mahsa Ghasemi and Murat Kocaoglu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uM3rQ14iex}\\n}'}, 'paperhash': {'value': 'elahi|partial_structure_discovery_is_sufficient_for_noregret_learning_in_causal_bandits'}},forum = 'uM3rQ14iex',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19422/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19422/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19422/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19422/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uLGyoBn7hm',number = 13821,cdate = 1715743248199,pdate = 1727288051272,odate = 1730873960754,mdate = 1730873960769,tcdate = 1715743248199,tmdate = 1730873960769,ddate = None,content = {'title': {'value': 'Disentangled Representation Learning in Non-Markovian Causal Systems'}, 'authors': {'value': ['Adam Li', 'Yushu Pan', 'Elias Bareinboim']}, 'authorids': {'value': ['~Adam_Li1', '~Yushu_Pan1', '~Elias_Bareinboim2']}, 'keywords': {'value': ['causal representation learning', 'disentanglement', 'nonlinear ICA']}, 'TLDR': {'value': 'We introduce a causal representation identifiability algorithm for determining which latent variables are disentangleable given a homogenous set of input distributions from multiple domains and knowledge of the latent variable causal structure.'}, 'abstract': {'value': 'Considering various data modalities, such as images, videos, and text, humans perform causal reasoning using high-level causal variables, as opposed to operating at the low, pixel level from which the data comes. \\nIn practice, most causal reasoning methods assume that the data is described as granular as the underlying causal generative factors, which is often violated in various AI tasks. \\nThis mismatch translates into a lack of guarantees in various tasks such as generative modeling, decision-making, fairness, and generalizability, to cite a few. \\nIn this paper, we acknowledge this issue and study the problem of causal disentangled representation learning from a combination of data gathered from various heterogeneous domains and assumptions in the form of a latent causal graph. To the best of our knowledge, the proposed work is the first to consider i) non-Markovian causal settings, where there may be unobserved confounding, ii) arbitrary distributions that arise from multiple domains, and iii) a relaxed version of disentanglement. Specifically, we introduce graphical criteria that allow for disentanglement under various conditions. Building on these results, we develop an algorithm that returns a causal disentanglement map, highlighting which latent variables can be disentangled given the combination of data and assumptions. The theory is corroborated by experiments.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8350116f8253990dda7ce413729df73f9a61f109.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024disentangled,\\ntitle={Disentangled Representation Learning in Non-Markovian Causal Systems},\\nauthor={Adam Li and Yushu Pan and Elias Bareinboim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uLGyoBn7hm}\\n}'}, 'paperhash': {'value': 'li|disentangled_representation_learning_in_nonmarkovian_causal_systems'}},forum = 'uLGyoBn7hm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13821/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13821/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13821/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13821/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uHs6RJFDsg',number = 3706,cdate = 1715318093055,pdate = 1727287728427,odate = 1730873868876,mdate = 1734607566308,tcdate = 1715318093055,tmdate = 1734607566308,ddate = None,content = {'title': {'value': 'MoVA: Adapting Mixture of Vision Experts to Multimodal Context'}, 'authors': {'value': ['Zhuofan Zong', 'Bingqi Ma', 'Dazhong Shen', 'Guanglu Song', 'Hao Shao', 'Dongzhi Jiang', 'Hongsheng Li', 'Yu Liu']}, 'authorids': {'value': ['~Zhuofan_Zong1', '~Bingqi_Ma1', '~Dazhong_Shen1', '~Guanglu_Song2', '~Hao_Shao1', '~Dongzhi_Jiang1', '~Hongsheng_Li3', '~Yu_Liu2']}, 'keywords': {'value': ['Multimodal large language model', 'Vision encoder', 'Mixture-of-expert']}, 'abstract': {'value': \"As the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pretrained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, e.g., the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/85eac16c2d58fa7601c94c13ed540dcb3e97d7c3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzong2024mova,\\ntitle={Mo{VA}: Adapting Mixture of Vision Experts to Multimodal Context},\\nauthor={Zhuofan Zong and Bingqi Ma and Dazhong Shen and Guanglu Song and Hao Shao and Dongzhi Jiang and Hongsheng Li and Yu Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uHs6RJFDsg}\\n}'}, 'paperhash': {'value': 'zong|mova_adapting_mixture_of_vision_experts_to_multimodal_context'}},forum = 'uHs6RJFDsg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3706/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3706/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3706/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3706/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uHml6eyoVF',number = 17639,cdate = 1715781016449,pdate = 1727288158487,odate = 1730873985317,mdate = 1730873985337,tcdate = 1715781016449,tmdate = 1730873985337,ddate = None,content = {'title': {'value': 'Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks'}, 'authors': {'value': ['Eszter Szekely', 'Lorenzo Bardone', 'Federica Gerace', 'Sebastian Goldt']}, 'authorids': {'value': ['~Eszter_Szekely1', '~Lorenzo_Bardone1', '~Federica_Gerace1', '~Sebastian_Goldt1']}, 'keywords': {'value': ['higher-order cumulant', 'hypothesis test', 'neural network', 'random features', 'low-degree method']}, 'abstract': {'value': 'Neural networks excel at discovering statistical patterns in\\nhigh-dimensional data sets. In practice, higher-order cumulants, which quantify\\nthe non-Gaussian correlations between three or more variables, are particularly\\nimportant for the performance of neural networks. But how efficient are neural\\nnetworks at extracting features from higher-order cumulants? We study this\\nquestion in the spiked cumulant model, where the statistician needs to recover a\\nprivileged direction or \"spike\\'\\' from the order-$p\\\\ge 4$ cumulants\\nof $d$-dimensional inputs. \\nWe first discuss the fundamental statistical and\\ncomputational limits of recovering the spike by analysing the number of\\n samples $n$ required to strongly distinguish between inputs from the spiked\\ncumulant model and isotropic Gaussian inputs. \\nExisting literature established the presence of a wide statistical-to-computational gap in this problem. We deepen this line of work by finding an exact formula for the likelihood ratio norm which proves that statistical\\ndistinguishability requires $n\\\\gtrsim d$ samples, while distinguishing the two\\ndistributions in polynomial time requires $n \\\\gtrsim d^2$ samples for a wide\\nclass of algorithms, i.e. those covered by the low-degree conjecture. \\nNumerical experiments show that neural networks do indeed learn to distinguish\\nthe two distributions with quadratic sample complexity, while ``lazy\\'\\' methods\\nlike random features are not better than random guessing in this regime. Our\\nresults show that neural networks extract information from higher-order\\ncorrelations in the spiked cumulant model efficiently, and reveal a large gap in\\nthe amount of data required by neural networks and random features to learn from\\nhigher-order cumulants.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We analyse the statistical-to-computational gap in learning from higher-order data correlations and show that neural networks learn these correlations more efficiently than kernel methods.'}, 'pdf': {'value': '/pdf/3458a2f739a06864f346678b6092e63a157de8b5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nszekely2024learning,\\ntitle={Learning from higher-order correlations, efficiently: hypothesis tests, random features, and neural networks},\\nauthor={Eszter Szekely and Lorenzo Bardone and Federica Gerace and Sebastian Goldt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uHml6eyoVF}\\n}'}, 'paperhash': {'value': 'szekely|learning_from_higherorder_correlations_efficiently_hypothesis_tests_random_features_and_neural_networks'}},forum = 'uHml6eyoVF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17639/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17639/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17639/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17639/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uHcG5Y6fdB',number = 13615,cdate = 1715741383051,pdate = 1727288044904,odate = 1730873959188,mdate = 1730873959256,tcdate = 1715741383051,tmdate = 1730873959256,ddate = None,content = {'title': {'value': 'Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context'}, 'authors': {'value': ['Kazusato Oko', 'Yujin Song', 'Taiji Suzuki', 'Denny Wu']}, 'authorids': {'value': ['~Kazusato_Oko1', '~Yujin_Song1', '~Taiji_Suzuki1', '~Denny_Wu2']}, 'keywords': {'value': ['Transformer', 'in-context learning', 'feature learning', 'single-index models']}, 'abstract': {'value': 'Transformers can efficiently learn in-context from example demonstrations. Most existing theoretical analyses studied the in-context learning (ICL) ability of transformers for linear function classes, where it is typically shown that the minimizer of the pretraining loss implements one gradient descent step on the least squares objective. However, this simplified linear setting arguably does not demonstrate the statistical efficiency of ICL, since the pretrained transformer does not outperform directly solving linear regression on the test prompt. \\nIn this paper, we study ICL of a nonlinear function class via transformer with nonlinear MLP layer: given a class of \\\\textit{single-index} target functions $f_*(\\\\boldsymbol{x}) = \\\\sigma_*(\\\\langle\\\\boldsymbol{x},\\\\boldsymbol{\\\\beta}\\\\rangle)$, where the index features $\\\\boldsymbol{\\\\beta}\\\\in\\\\mathbb{R}^d$ are drawn from a $r$-dimensional subspace, we show that a nonlinear transformer optimized by gradient descent (with a pretraining sample complexity that depends on the \\\\textit{information exponent} of the link functions $\\\\sigma_*$) learns $f_*$ in-context with a prompt length that only depends on the dimension of the distribution of target functions $r$; in contrast, any algorithm that directly learns $f_*$ on test prompt yields a statistical complexity that scales with the ambient dimension $d$.  Our result highlights the adaptivity of the pretrained transformer to low-dimensional structures of the function class, which enables sample-efficient ICL that outperforms estimators that only have access to the in-context data.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ebe3fdc5e357d327b920801545a353f902eefb86.pdf'}, '_bibtex': {'value': '@inproceedings{\\noko2024pretrained,\\ntitle={Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context},\\nauthor={Kazusato Oko and Yujin Song and Taiji Suzuki and Denny Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uHcG5Y6fdB}\\n}'}, 'paperhash': {'value': 'oko|pretrained_transformer_efficiently_learns_lowdimensional_target_functions_incontext'}},forum = 'uHcG5Y6fdB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13615/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13615/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13615/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13615/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uFXGsiYkkX',number = 2623,cdate = 1715129970967,pdate = 1727287695834,odate = 1730873859612,mdate = 1730873859626,tcdate = 1715129970967,tmdate = 1730873859626,ddate = None,content = {'title': {'value': 'BAKU: An Efficient Transformer for Multi-Task Policy Learning'}, 'authors': {'value': ['Siddhant Haldar', 'Zhuoran Peng', 'Lerrel Pinto']}, 'authorids': {'value': ['~Siddhant_Haldar1', '~Zhuoran_Peng1', '~Lerrel_Pinto1']}, 'keywords': {'value': ['Robot learning', 'Imitation Learning', 'Multitask Learning']}, 'abstract': {'value': 'Training generalist agents capable of solving diverse tasks is challenging, often requiring large datasets of expert demonstrations. This is particularly problematic in robotics, where each data point requires physical execution of actions in the real world. Thus, there is a pressing need for architectures that can effectively leverage the available training data. In this work, we present BAKU, a simple transformer architecture that enables efficient learning of multi-task robot policies. BAKU builds upon recent advancements in offline imitation learning and meticulously combines observation trunks, action chunking, multi-sensory observations, and action heads to substantially improve upon prior work. Our experiments on 129 simulated tasks across LIBERO, Meta-World suite, and the Deepmind Control suite exhibit an overall 18% absolute improvement over RT-1 and MT-ACT, with a 36% improvement on the harder LIBERO benchmark. On 30 real-world manipulation tasks, given an average of just 17 demonstrations per task, BAKU achieves a 91% success rate. Videos of the robot are best viewed at baku-robot.github.io.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/71b48662ea04f445f5d15dd51227f43a7ca9b49c.pdf'}, 'TLDR': {'value': 'We present BAKU, a simple architecture for multi-task policy learning that provides highly efficient training, particularly in data-scarce problems such as robotics.'}, 'supplementary_material': {'value': '/attachment/462f081e6994a2869793fb010c78c528e425f229.zip'}, '_bibtex': {'value': '@inproceedings{\\nhaldar2024baku,\\ntitle={{BAKU}: An Efficient Transformer for Multi-Task Policy Learning},\\nauthor={Siddhant Haldar and Zhuoran Peng and Lerrel Pinto},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uFXGsiYkkX}\\n}'}, 'paperhash': {'value': 'haldar|baku_an_efficient_transformer_for_multitask_policy_learning'}},forum = 'uFXGsiYkkX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2623/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2623/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2623/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2623/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uDxhMgjVJB',number = 19615,cdate = 1715791960706,pdate = 1727288213870,odate = 1730873996358,mdate = 1730873996378,tcdate = 1715791960706,tmdate = 1730873996378,ddate = None,content = {'title': {'value': 'Automatic Outlier Rectification via Optimal Transport'}, 'authors': {'value': ['Jose Blanchet', 'Jiajin Li', 'Markus Pelger', 'Greg Zanotti']}, 'authorids': {'value': ['~Jose_Blanchet1', '~Jiajin_Li2', '~Markus_Pelger1', '~Greg_Zanotti1']}, 'keywords': {'value': ['Outlier Rectification; Optimal Transport; Statistically Robust']}, 'abstract': {'value': 'In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize the optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator effectively identify the outlier during the optimization process. We demonstrate the effectiveness of our approach over conventional approaches in simulations and empirical analyses for mean estimation, least absolute regression, and the fitting of option implied volatility surfaces.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper introduces a novel framework for outlier detection using optimal transport with a concave cost function, integrating outlier rectification and estimation into a single optimization process.'}, 'pdf': {'value': '/pdf/3eff26a95982127bc320a89099d7f6d2b1cbe74e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nblanchet2024automatic,\\ntitle={Automatic Outlier Rectification via Optimal Transport},\\nauthor={Jose Blanchet and Jiajin Li and Markus Pelger and Greg Zanotti},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uDxhMgjVJB}\\n}'}, 'paperhash': {'value': 'blanchet|automatic_outlier_rectification_via_optimal_transport'}},forum = 'uDxhMgjVJB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19615/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19615/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19615/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19615/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'uDD44NROOt',number = 14143,cdate = 1715746586563,pdate = 1727288061282,odate = 1730873963673,mdate = 1735344097094,tcdate = 1715746586563,tmdate = 1735344097094,ddate = None,content = {'title': {'value': 'SPRINQL: Sub-optimal Demonstrations driven Offline Imitation Learning'}, 'authors': {'value': ['Huy Hoang', 'Tien Anh Mai', 'Pradeep Varakantham']}, 'authorids': {'value': ['~Huy_Hoang1', '~Tien_Anh_Mai1', '~Pradeep_Varakantham1']}, 'keywords': {'value': ['imitation learning', 'offline imitation learning', 'reference reward', 'supplementary data', 'ranked dataset']}, 'TLDR': {'value': 'We develop a novel inverse soft-Q learning for offline imitation learning with expert and non-expert demonstrations.'}, 'abstract': {'value': \"We focus on offline imitation learning (IL), which aims to mimic an expert's behavior using demonstrations without any interaction with the environment. One of the main challenges in offline IL is the limited support of expert demonstrations, which typically cover only a small fraction of the state-action space. While it may not be feasible to obtain numerous expert demonstrations, it is often possible to gather a larger set of sub-optimal demonstrations. For example, in treatment optimization problems, there are varying levels of doctor treatments available for different chronic conditions. These range from treatment specialists and experienced general practitioners to less experienced general practitioners. Similarly, when robots are trained to imitate humans in routine tasks, they might learn from individuals with different levels of expertise and efficiency. \\n\\nIn this paper, we propose an offline IL approach that leverages the larger set of sub-optimal demonstrations while effectively mimicking expert trajectories. Existing offline IL methods based on behavior cloning or distribution matching often face issues such as overfitting to the limited set of expert demonstrations or inadvertently imitating sub-optimal trajectories from the larger dataset. Our approach, which is based on inverse soft-Q learning, learns from both expert and sub-optimal demonstrations. It assigns higher importance (through learned weights) to aligning with expert demonstrations and lower importance to aligning with sub-optimal ones. A key contribution of our approach, called SPRINQL, is transforming the offline IL problem into a convex optimization over the space of Q functions. Through comprehensive experimental evaluations, we demonstrate that the SPRINQL algorithm achieves state-of-the-art (SOTA) performance on offline IL benchmarks. Code is available at https://github.com/hmhuy0/SPRINQL .\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/21f890aa8acefa4c5640a534a16533bb251a5681.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhoang2024sprinql,\\ntitle={{SPRINQL}: Sub-optimal Demonstrations driven Offline Imitation Learning},\\nauthor={Huy Hoang and Tien Anh Mai and Pradeep Varakantham},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uDD44NROOt}\\n}'}, 'paperhash': {'value': 'hoang|sprinql_suboptimal_demonstrations_driven_offline_imitation_learning'}},forum = 'uDD44NROOt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14143/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14143/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14143/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14143/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uCvdw0IOuU',number = 18136,cdate = 1715783906712,pdate = 1727288171857,odate = 1730873988177,mdate = 1730873988189,tcdate = 1715783906712,tmdate = 1730873988189,ddate = None,content = {'title': {'value': 'Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation'}, 'authors': {'value': ['Wenfang Yao', 'Chen Liu', 'Kejing Yin', 'William K. Cheung', 'Jing Qin']}, 'authorids': {'value': ['~Wenfang_Yao1', '~Chen_Liu25', '~Kejing_Yin1', '~William_K._Cheung1', '~Jing_Qin3']}, 'keywords': {'value': ['Multi-modal clinical data', 'latent diffusion model', 'chest X-ray image', 'eletronic health records.']}, 'TLDR': {'value': 'We propose DDL-CXR to address the asynchronicity in clinical multimodal fusion by generating individualized up-to-date CXR images. Cross-modal interactions are captured by the generation process, leading to improved prediction performance.'}, 'abstract': {'value': 'Integrating multi-modal clinical data, such as electronic health records (EHR) and chest X-ray images (CXR), is particularly beneficial for clinical prediction tasks. However, in a temporal setting, multi-modal data are often inherently asynchronous. EHR can be continuously collected but CXR is generally taken with a much longer interval due to its high cost and radiation dose. When clinical prediction is needed, the last available CXR image might have been outdated, leading to suboptimal predictions. To address this challenge, we propose DDL-CXR, a method that dynamically generates an up-to-date latent representation of the individualized CXR images. Our approach leverages latent diffusion models for patient-specific generation strategically conditioned on a previous CXR image and EHR time series, providing information regarding anatomical structures and disease progressions, respectively. In this way, the interaction across modalities could be better captured by the latent CXR generation process, ultimately improving the prediction performance. Experiments using MIMIC datasets show that the proposed model could effectively address asynchronicity in multimodal fusion and consistently outperform existing methods.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/86eb5a07aded079c269761cb445a63593d75e74d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyao2024addressing,\\ntitle={Addressing Asynchronicity in Clinical Multimodal Fusion via Individualized Chest X-ray Generation},\\nauthor={Wenfang Yao and Chen Liu and Kejing Yin and William K. Cheung and Jing Qin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uCvdw0IOuU}\\n}'}, 'paperhash': {'value': 'yao|addressing_asynchronicity_in_clinical_multimodal_fusion_via_individualized_chest_xray_generation'}},forum = 'uCvdw0IOuU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18136/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18136/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18136/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18136/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'uCgFk8nP0Z',number = 2978,cdate = 1715198128943,pdate = 1727287706750,odate = 1730873862445,mdate = 1730873862469,tcdate = 1715198128943,tmdate = 1730873862469,ddate = None,content = {'title': {'value': 'DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation'}, 'authors': {'value': ['Felipe Garrido', 'Benjamin Heymann', 'Maxime Vono', 'Patrick Loiseau', 'Vianney Perchet']}, 'authorids': {'value': ['~Felipe_Garrido1', '~Benjamin_Heymann1', '~Maxime_Vono1', '~Patrick_Loiseau1', '~Vianney_Perchet3']}, 'keywords': {'value': ['data valuation', 'shapley value', 'approximation']}, 'abstract': {'value': 'We consider the dataset valuation problem, that is the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others.\\nThe Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d5040e60c0b03762c252d1fcdf5dbbeeba1f0efe.pdf'}, 'supplementary_material': {'value': '/attachment/40ee1c4ff6cdfefda28420346d021a68c7edb70a.zip'}, '_bibtex': {'value': '@inproceedings{\\ngarrido2024dushapley,\\ntitle={{DU}-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation},\\nauthor={Felipe Garrido and Benjamin Heymann and Maxime Vono and Patrick Loiseau and Vianney Perchet},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uCgFk8nP0Z}\\n}'}, 'paperhash': {'value': 'garrido|dushapley_a_shapley_value_proxy_for_efficient_dataset_valuation'}},forum = 'uCgFk8nP0Z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2978/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2978/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2978/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2978/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uCZI8gSfD4',number = 14879,cdate = 1715754882670,pdate = 1727288081650,odate = 1730873968809,mdate = 1734577810927,tcdate = 1715754882670,tmdate = 1734577810927,ddate = None,content = {'title': {'value': 'Training Compute-Optimal Protein Language Models'}, 'authors': {'value': ['Xingyi Cheng', 'Bo Chen', 'Pan Li', 'Jing Gong', 'Jie Tang', 'Le Song']}, 'authorids': {'value': ['~Xingyi_Cheng3', '~Bo_Chen11', '~Pan_Li11', '~Jing_Gong1', '~Jie_Tang1', '~Le_Song1']}, 'keywords': {'value': ['Protein Language Model', 'Scaling Law']}, 'TLDR': {'value': 'We explore optimally training protein language models and propose the scaling law, an area of significant interest in biological research with limited guidance on best practices.'}, 'abstract': {'value': 'We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited.\\nMost models are trained with extensive compute resources until performance gains plateau, focusing primarily on increasing model sizes rather than optimizing the efficient compute frontier that balances performance and compute budgets.\\nOur investigation is grounded in a massive dataset consisting of 939 million protein sequences. \\nWe trained over 300 models ranging from 3.5 million to 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate the relations between model sizes, training token numbers, and objectives.\\nFirst, we observed the effect of diminishing returns for the Causal Language Model (CLM) and that of overfitting for Masked Language Model (MLM) when repeating the commonly used Uniref database. To address this, we included metagenomic protein sequences in the training set to increase the diversity and avoid the plateau or overfitting effects. \\nSecond, we obtained the scaling laws of CLM and MLM on Transformer, tailored to the specific characteristics of protein sequence data. \\nThird, we observe a transfer scaling phenomenon from CLM to MLM, further demonstrating the effectiveness of transfer through scaling behaviors based on estimated Effectively Transferred Tokens.\\nFinally, to validate our scaling laws, we compare the large-scale versions of ESM-2 and PROGEN2 on downstream tasks, encompassing evaluations of protein generation as well as structure- and function-related tasks, all within less or equivalent pre-training compute budgets.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5b6ca1e41c84b9d1e3cae244f0de73392b8e685d.pdf'}, 'supplementary_material': {'value': '/attachment/4f0dae142729844f2917f820d589d8f539c2dac0.zip'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024training,\\ntitle={Training Compute-Optimal Protein Language Models},\\nauthor={Xingyi Cheng and Bo Chen and Pan Li and Jing Gong and Jie Tang and Le Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uCZI8gSfD4}\\n}'}, 'paperhash': {'value': 'cheng|training_computeoptimal_protein_language_models'}},forum = 'uCZI8gSfD4',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14879/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14879/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14879/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14879/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uBVCPAMDGk',number = 7058,cdate = 1715611786872,pdate = 1727287833845,odate = 1730873898982,mdate = 1730873899058,tcdate = 1715611786872,tmdate = 1730873899058,ddate = None,content = {'title': {'value': 'Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination'}, 'authors': {'value': ['Shelly Golan', 'Roy Ganz', 'Michael Elad']}, 'authorids': {'value': ['~Shelly_Golan1', '~Roy_Ganz1', '~Michael_Elad1']}, 'keywords': {'value': ['Adversarial Training; Robustness; Energy-Based Models; Classification;']}, 'abstract': {'value': 'The recently introduced Consistency models pose an efficient alternative to diffusion algorithms, enabling rapid and good quality image synthesis. These methods overcome the slowness of diffusion models by directly mapping noise to data, while maintaining a (relatively) simpler training. Consistency models enable a fast one- or few-step generation, but they typically fall somewhat short in sample quality when compared to their diffusion origins. \\nIn this work we propose a novel and highly effective technique for post-processing Consistency-based generated images, enhancing their perceptual quality. Our approach utilizes a joint classifier-discriminator model, in which both portions are trained adversarially. While the classifier aims to grade an image based on its assignment to a designated class, the discriminator portion of the very same network leverages the softmax values to assess the proximity of the input image to the targeted data manifold, thereby serving as an Energy-based Model. By employing example-specific projected gradient iterations under the guidance of this joint machine, we refine synthesized images and achieve an improved FID scores on the ImageNet 64x64 dataset for both Consistency-Training and Consistency-Distillation techniques.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d198174754c207b8bf18c138254bbe39902f45e4.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngolan2024enhancing,\\ntitle={Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination},\\nauthor={Shelly Golan and Roy Ganz and Michael Elad},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uBVCPAMDGk}\\n}'}, 'paperhash': {'value': 'golan|enhancing_consistencybased_image_generation_via_adversarialytrained_classification_and_energybased_discrimination'}},forum = 'uBVCPAMDGk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7058/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7058/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7058/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7058/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'uAzhODjALU',number = 5543,cdate = 1715547897197,pdate = 1727287787220,odate = 1730873885845,mdate = 1736378400969,tcdate = 1715547897197,tmdate = 1736378400969,ddate = None,content = {'title': {'value': 'The Mamba in the Llama: Distilling and Accelerating Hybrid Models'}, 'authors': {'value': ['Junxiong Wang', 'Daniele Paliotta', 'Avner May', 'Alexander M Rush', 'Tri Dao']}, 'authorids': {'value': ['~Junxiong_Wang1', '~Daniele_Paliotta1', '~Avner_May1', '~Alexander_M_Rush1', '~Tri_Dao1']}, 'keywords': {'value': ['Mamba', 'Transformer', 'Knowledge Distillation', 'Speculative Decoding']}, 'abstract': {'value': 'Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. \\nWe demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. \\nOur top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model.  We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at [MambaInLlama](https://github.com/jxiw/MambaInLlama) for distillation and [SpeculativeMamba](https://github.com/itsdaniele/speculative\\\\_mamba) for speculative decoding.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/669243b56f6a425c55c92536184abde35c26027c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024the,\\ntitle={The Mamba in the Llama: Distilling and Accelerating Hybrid Models},\\nauthor={Junxiong Wang and Daniele Paliotta and Avner May and Alexander M Rush and Tri Dao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=uAzhODjALU}\\n}'}, 'paperhash': {'value': 'wang|the_mamba_in_the_llama_distilling_and_accelerating_hybrid_models'}},forum = 'uAzhODjALU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5543/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5543/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5543/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5543/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u9ShP64FJV',number = 9571,cdate = 1715681912510,pdate = 1727287915074,odate = 1730873921932,mdate = 1730873921949,tcdate = 1715681912510,tmdate = 1730873921949,ddate = None,content = {'title': {'value': 'Protecting Your LLMs with Information Bottleneck'}, 'authors': {'value': ['Zichuan Liu', 'Zefan Wang', 'Linjie Xu', 'Jinyu Wang', 'Lei Song', 'Tianchun Wang', 'Chunlin Chen', 'Wei Cheng', 'Jiang Bian']}, 'authorids': {'value': ['~Zichuan_Liu3', '~Zefan_Wang2', '~Linjie_Xu1', '~Jinyu_Wang1', '~Lei_Song3', '~Tianchun_Wang1', '~Chunlin_Chen1', '~Wei_Cheng1', '~Jiang_Bian1']}, 'keywords': {'value': ['Defense', 'Information Bottleneck', 'Jailbreaking', 'Large Language Models']}, 'TLDR': {'value': 'Our protector efficiently defends against adversarial prompts without losing key information'}, 'abstract': {'value': 'The advent of large language models (LLMs) has revolutionized the field of natural language processing, yet they might be attacked to produce harmful content.\\nDespite efforts to ethically align LLMs, these are often fragile and can be circumvented by jailbreaking attacks through optimized or manual adversarial prompts.\\nTo address this, we introduce the Information Bottleneck Protector (IBProtector), a defense mechanism grounded in the information bottleneck principle, and we modify the objective to avoid trivial solutions.\\nThe IBProtector selectively compresses and perturbs prompts, facilitated by a lightweight and trainable extractor, preserving only essential information for the target LLMs to respond with the expected answer.\\nMoreover, we further consider a situation where the gradient is not visible to be compatible with any LLM.\\nOur empirical evaluations show that IBProtector outperforms current defense methods in mitigating jailbreak attempts, without overly affecting response quality or inference speed. \\nIts effectiveness and adaptability across various attack methods and target LLMs underscore the potential of IBProtector as a novel, transferable defense that bolsters the security of LLMs without requiring modifications to the underlying models.'}, 'pdf': {'value': '/pdf/9e0d322217158c45f3a1f006a62564210eda4dc6.pdf'}, 'supplementary_material': {'value': '/attachment/2346dc5b20b2c7e48de484ebe3ab8cc8b1553b4c.zip'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nliu2024protecting,\\ntitle={Protecting Your {LLM}s with Information Bottleneck},\\nauthor={Zichuan Liu and Zefan Wang and Linjie Xu and Jinyu Wang and Lei Song and Tianchun Wang and Chunlin Chen and Wei Cheng and Jiang Bian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u9ShP64FJV}\\n}'}, 'paperhash': {'value': 'liu|protecting_your_llms_with_information_bottleneck'}},forum = 'u9ShP64FJV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9571/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9571/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9571/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'u7okTt4ZyE',number = 2110,cdate = 1714977559855,pdate = 1727287681181,odate = 1730873855314,mdate = 1735527690123,tcdate = 1714977559855,tmdate = 1735527690123,ddate = None,content = {'title': {'value': 'Taming Diffusion Prior for Image Super-Resolution with Domain Shift SDEs'}, 'authors': {'value': ['Qinpeng Cui', \"Yi'xuan Liu\", 'Xinyi Zhang', 'Qiqi Bao', 'Qingmin Liao', 'liwang Amd', 'Lu Tian', 'Zicheng Liu', 'Zhongdao Wang', 'Emad Barsoum']}, 'authorids': {'value': ['~Qinpeng_Cui1', \"~Yi'xuan_Liu1\", '~Xinyi_Zhang10', '~Qiqi_Bao1', '~Qingmin_Liao1', '~liwang_Amd1', '~Lu_Tian3', '~Zicheng_Liu1', '~Zhongdao_Wang2', '~Emad_Barsoum1']}, 'keywords': {'value': ['Diffusion Models; Super-Resolution']}, 'abstract': {'value': 'Diffusion-based image super-resolution (SR) models have attracted substantial interest due to their powerful image restoration capabilities. However, prevailing diffusion models often struggle to strike an optimal balance between efficiency and performance. Typically, they either neglect to exploit the potential of existing extensive pretrained models, limiting their generative capacity, or they necessitate a dozens of forward passes starting from random noises, compromising inference efficiency. In this paper, we present DoSSR, a $\\\\textbf{Do}$main $\\\\textbf{S}$hift diffusion-based SR model that capitalizes on the generative powers of pretrained diffusion models while significantly enhancing efficiency by initiating the diffusion process with low-resolution (LR) images. At the core of our approach is a domain shift equation that integrates seamlessly with existing diffusion models. This integration not only improves the use of diffusion prior but also boosts inference efficiency. Moreover, we advance our method by transitioning the discrete shift process to a continuous formulation, termed as DoS-SDEs. This advancement leads to the fast and customized solvers that further enhance sampling efficiency. Empirical results demonstrate that our proposed method achieves state-of-the-art performance on synthetic and real-world datasets, while notably requiring $\\\\textbf{\\\\emph{only 5 sampling steps}}$. Compared to previous diffusion prior based methods, our approach achieves a remarkable speedup of 5-7 times, demonstrating its superior efficiency.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fe140549f7d235b4d8486b6f7227fafebbdde760.pdf'}, '_bibtex': {'value': \"@inproceedings{\\ncui2024taming,\\ntitle={Taming Diffusion Prior for Image Super-Resolution with Domain Shift {SDE}s},\\nauthor={Qinpeng Cui and Yi'xuan Liu and Xinyi Zhang and Qiqi Bao and Qingmin Liao and liwang Amd and Lu Tian and Zicheng Liu and Zhongdao Wang and Emad Barsoum},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u7okTt4ZyE}\\n}\"}, 'paperhash': {'value': 'cui|taming_diffusion_prior_for_image_superresolution_with_domain_shift_sdes'}},forum = 'u7okTt4ZyE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2110/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2110/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2110/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2110/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u7JRmrGutT',number = 19013,cdate = 1715788718915,pdate = 1727288199450,odate = 1730873993073,mdate = 1730873993098,tcdate = 1715788718915,tmdate = 1730873993098,ddate = None,content = {'title': {'value': 'Graph Edit Distance with General Costs Using Neural Set Divergence'}, 'authors': {'value': ['Eeshaan Jain', 'Indradyumna Roy', 'Saswat Meher', 'Soumen Chakrabarti', 'Abir De']}, 'authorids': {'value': ['~Eeshaan_Jain1', '~Indradyumna_Roy1', '~Saswat_Meher1', '~Soumen_Chakrabarti1', '~Abir_De1']}, 'keywords': {'value': ['graph neural network', 'graph edit distance']}, 'abstract': {'value': 'Graph Edit Distance (GED) measures the (dis-)similarity between two given graphs in terms of the minimum-cost edit sequence, which transforms one graph to the other.\\nGED is related to other notions of graph similarity, such as graph and subgraph isomorphism, maximum common subgraph, etc. However, the computation of exact GED is NP-Hard, which has recently motivated the design of neural models for GED estimation.\\nHowever, they do not explicitly account for edit operations with different costs. In response, we propose $\\\\texttt{GraphEdX}$, a neural GED estimator that can work with general costs specified for the four edit operations, viz., edge deletion, edge addition, node deletion, and node addition.\\nWe first present GED as a quadratic assignment problem (QAP) that incorporates these four costs.\\nThen, we represent each graph as a set of node and edge embeddings and use them to design a family of neural set divergence surrogates. We replace the QAP terms corresponding to each operation with their surrogates. \\nComputing such neural set divergence requires aligning nodes and edges of the two graphs.\\nWe learn these alignments using a Gumbel-Sinkhorn permutation generator, additionally ensuring that the node and edge alignments are consistent with each other. Moreover, these alignments are cognizant of both the presence and absence of edges between node pairs.\\nThrough extensive experiments on several datasets, along with a variety of edit cost settings, we show that $\\\\texttt{GraphEdX}$ consistently outperforms state-of-the-art methods and heuristics in terms of prediction error. The code is available at https://github.com/structlearning/GraphEdX.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/405e32e6f9667c1d8b6932a8b230ad111ba7d111.pdf'}, '_bibtex': {'value': '@inproceedings{\\njain2024graph,\\ntitle={Graph Edit Distance with General Costs Using Neural Set Divergence},\\nauthor={Eeshaan Jain and Indradyumna Roy and Saswat Meher and Soumen Chakrabarti and Abir De},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u7JRmrGutT}\\n}'}, 'TLDR': {'value': 'Neural graph edit distance network with uniform and non-uniform costs'}, 'paperhash': {'value': 'jain|graph_edit_distance_with_general_costs_using_neural_set_divergence'}},forum = 'u7JRmrGutT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19013/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u6XxyuD3Ro',number = 7317,cdate = 1715617786746,pdate = 1727287842877,odate = 1730873901687,mdate = 1736857179195,tcdate = 1715617786746,tmdate = 1736857179195,ddate = None,content = {'title': {'value': 'Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously'}, 'authors': {'value': ['Stephen Pasteris', 'Chris Hicks', 'Vasilios Mavroudis', 'Mark Herbster']}, 'authorids': {'value': ['~Stephen_Pasteris1', '~Chris_Hicks1', '~Vasilios_Mavroudis1', '~Mark_Herbster1']}, 'keywords': {'value': ['Online Convex Optimisation', 'Non-stationary Learning']}, 'abstract': {'value': 'We consider the classic problem of online convex optimisation. Whereas the notion of static regret is relevant for stationary problems, the notion of switching regret is more appropriate for non-stationary problems. A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment. In this paper we show that, perhaps surprisingly, we can achieve the asymptotically optimal switching regret on every possible segmentation simultaneously. Our algorithm for doing so is very efficient: having a space and per-trial time complexity that is logarithmic in the time-horizon. Our algorithm also obtains novel bounds on its dynamic regret: being adaptive to variations in the rate of change of the comparator sequence.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1d79cf793394d552ca09f0c84043a66df15140b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\npasteris2024online,\\ntitle={Online Convex Optimisation: The Optimal Switching Regret for all Segmentations Simultaneously},\\nauthor={Stephen Pasteris and Chris Hicks and Vasilios Mavroudis and Mark Herbster},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u6XxyuD3Ro}\\n}'}, 'paperhash': {'value': 'pasteris|online_convex_optimisation_the_optimal_switching_regret_for_all_segmentations_simultaneously'}},forum = 'u6XxyuD3Ro',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7317/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7317/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7317/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7317/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u6FuiKzT1K',number = 4514,cdate = 1715420638346,pdate = 1727287752938,odate = 1730873876834,mdate = 1735971054825,tcdate = 1715420638346,tmdate = 1735971054825,ddate = None,content = {'title': {'value': 'Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers'}, 'authors': {'value': ['Jinsong Chen', 'Hanpeng Liu', 'John E. Hopcroft', 'Kun He']}, 'authorids': {'value': ['~Jinsong_Chen2', '~Hanpeng_Liu2', '~John_E._Hopcroft1', '~Kun_He1']}, 'keywords': {'value': ['Node classification', 'Graph Transformer', 'Positive Token Sequence', 'Negative Token Sequence', 'Contrastive Learning']}, 'TLDR': {'value': 'This paper enhances node classification in graph Transformers by leveraging contrastive learning and a hybrid token generator to capture diverse graph information.'}, 'abstract': {'value': 'While tokenized graph Transformers have demonstrated strong performance in node classification tasks, their reliance on a limited subset of nodes with high similarity scores for constructing token sequences overlooks valuable information from other nodes, hindering their ability to fully harness graph information for learning optimal node representations. To address this limitation, we propose a novel graph Transformer called GCFormer. Unlike previous approaches, GCFormer develops a hybrid token generator to create two types of token sequences, positive and negative, to capture diverse graph information. And a tailored Transformer-based backbone is adopted to learn meaningful node representations from these generated token sequences. Additionally, GCFormer introduces contrastive learning to extract valuable information from both positive and negative token sequences, enhancing the quality of learned node representations. Extensive experimental results across various datasets, including homophily and heterophily graphs, demonstrate the superiority of GCFormer in node classification, when compared to representative graph neural networks (GNNs) and graph Transformers.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9f0c38f076c089cd29634dab2e092a81cad9505d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024leveraging,\\ntitle={Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers},\\nauthor={Jinsong Chen and Hanpeng Liu and John E. Hopcroft and Kun He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u6FuiKzT1K}\\n}'}, 'paperhash': {'value': 'chen|leveraging_contrastive_learning_for_enhanced_node_representations_in_tokenized_graph_transformers'}},forum = 'u6FuiKzT1K',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4514/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4514/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4514/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4514/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u5enPCwaLt',number = 17254,cdate = 1715778401974,pdate = 1727288148553,odate = 1730873983592,mdate = 1730873983604,tcdate = 1715778401974,tmdate = 1730873983604,ddate = None,content = {'title': {'value': 'Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding'}, 'authors': {'value': ['Alexis Bellot', 'Silvia Chiappa']}, 'authorids': {'value': ['~Alexis_Bellot1', '~Silvia_Chiappa1']}, 'keywords': {'value': ['causal inference', 'estimation', 'bounding', 'unobserved confounding']}, 'TLDR': {'value': 'We introduce bounds and estimators for the effect of policies given observational data and a causal graph of the system.'}, 'abstract': {'value': 'As many practical fields transition to provide personalized decisions, data is increasingly relevant to support the evaluation of candidate plans and policies (e.g., guidelines for the treatment of disease, government directives, etc.). In the machine learning literature, significant efforts have been put into developing machinery to predict the effectiveness of policies efficiently. The challenge is that, in practice, the effectiveness of a candidate policy is not always identifiable, i.e., not uniquely estimable from the combination of the available data and assumptions about the domain at hand (e.g., encoded in a causal graph). In this paper, we develop graphical characterizations and estimation tools to bound the effect of policies given a causal graph and observational data collected in non-identifiable settings. Specifically, our contributions are two-fold: (1) we derive analytical bounds for general probabilistic and conditional policies that are tighter than existing results, (2) we develop an estimation framework to estimate bounds from finite samples, applicable in higher-dimensional spaces and continuously-valued data. We further show that the resulting estimators have favourable statistical properties such as fast convergence and robustness to model misspecification.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/56a61bc5e839e1ea16506727e798899070f3754b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbellot2024towards,\\ntitle={Towards Estimating Bounds on the Effect of Policies under Unobserved Confounding},\\nauthor={Alexis Bellot and Silvia Chiappa},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u5enPCwaLt}\\n}'}, 'paperhash': {'value': 'bellot|towards_estimating_bounds_on_the_effect_of_policies_under_unobserved_confounding'}},forum = 'u5enPCwaLt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17254/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17254/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17254/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17254/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u5BkOgWWZW',number = 13087,cdate = 1715734716244,pdate = 1727288028208,odate = 1730873955019,mdate = 1730873955039,tcdate = 1715734716244,tmdate = 1730873955039,ddate = None,content = {'title': {'value': 'Explaining Datasets in Words: Statistical Models with Natural Language Parameters'}, 'authors': {'value': ['Ruiqi Zhong', 'Heng Wang', 'Dan Klein', 'Jacob Steinhardt']}, 'authorids': {'value': ['~Ruiqi_Zhong1', '~Heng_Wang10', '~Dan_Klein1', '~Jacob_Steinhardt1']}, 'keywords': {'value': ['language model; explainability; exploratory analysis; data science; explainable modeling']}, 'TLDR': {'value': 'We explain datasets by modeling with natural language parameters (e.g. clustering where each cluster is associated with an explanation). We propose an algorithm based on continuous relaxation and iterative refinement to learn these models.'}, 'abstract': {'value': \"To make sense of massive data, we often first fit simplified models and then interpret the parameters; for example, we cluster the text embeddings and then interpret the mean parameters of each cluster.\\nHowever, these parameters are often high-dimensional and hard to interpret.\\nTo make model parameters directly interpretable, we introduce a family of statistical models---including clustering, time series, and classification models---parameterized by *natural language predicates*. \\nFor example, a cluster of text about COVID could be parameterized by the predicate ``*discusses COVID*''.\\nTo learn these statistical models effectively, we develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models (LMs).\\nFinally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images.\\nOur framework is highly versatile, applicable to both textual and visual domains, can be easily steered to focus on specific properties (e.g. subareas), and explains sophisticated concepts that classical methods (e.g. n-gram analysis) struggle to produce.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6bf57dd60959ba5605f358a7a7b884d7a32c7de5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhong2024explaining,\\ntitle={Explaining Datasets in Words: Statistical Models with Natural Language Parameters},\\nauthor={Ruiqi Zhong and Heng Wang and Dan Klein and Jacob Steinhardt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u5BkOgWWZW}\\n}'}, 'paperhash': {'value': 'zhong|explaining_datasets_in_words_statistical_models_with_natural_language_parameters'}},forum = 'u5BkOgWWZW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13087/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13087/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13087/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13087/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u3mZzd0Pdx',number = 9487,cdate = 1715680564098,pdate = 1727287912537,odate = 1730873921169,mdate = 1734832082325,tcdate = 1715680564098,tmdate = 1734832082325,ddate = None,content = {'title': {'value': 'Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization'}, 'authors': {'value': ['Rongzhen Wang', 'Chenyu Zheng', 'Guoqiang Wu', 'Xu Min', 'Xiaolu Zhang', 'JUN ZHOU', 'Chongxuan Li']}, 'authorids': {'value': ['~Rongzhen_Wang1', '~Chenyu_Zheng1', '~Guoqiang_Wu2', '~Xu_Min1', '~Xiaolu_Zhang2', '~JUN_ZHOU6', '~Chongxuan_Li1']}, 'keywords': {'value': ['Uniform stability', 'Lower bound', 'Hyperparameter optimization', 'Bilevel programming']}, 'TLDR': {'value': 'We establish  uniform stability lower bounds for representative gradient-based bilevel hyperparameter optimization algorithms.'}, 'abstract': {'value': 'Gradient-based bilevel programming leverages unrolling differentiation (UD) or implicit function theorem (IFT) to solve hyperparameter optimization (HO) problems, and is proven effective and scalable in practice. \\nTo understand their generalization behavior, existing works establish upper bounds on the uniform stability of these algorithms, while their tightness is still unclear. \\nTo this end, this paper attempts to establish stability lower bounds for UD-based and IFT-based algorithms. \\nA central technical challenge arises from the dependency of each outer-level update on the concurrent stage of inner optimization in bilevel programming. \\nTo address this problem, we introduce lower-bounded expansion properties to characterize the instability in update rules which can serve as general tools for lower-bound analysis. \\nThese properties guarantee the hyperparameter divergence at the outer level and the Lipschitz constant of inner output at the inner level in the context of HO.\\nGuided by these insights, we construct a quadratic example that yields tight lower bounds for the UD-based algorithm and meaningful bounds for a representative IFT-based algorithm.\\nOur tight result indicates that uniform stability has reached its limit in stability analysis for the UD-based algorithm.'}, 'pdf': {'value': '/pdf/9805ac8ebc0d7d218ea746ba7ae5f031fac46932.pdf'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/92295d0fa594686522770859ea8236a14fbe0393.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024lower,\\ntitle={Lower Bounds of Uniform Stability in Gradient-Based Bilevel Algorithms for Hyperparameter Optimization},\\nauthor={Rongzhen Wang and Chenyu Zheng and Guoqiang Wu and Xu Min and Xiaolu Zhang and JUN ZHOU and Chongxuan Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u3mZzd0Pdx}\\n}'}, 'paperhash': {'value': 'wang|lower_bounds_of_uniform_stability_in_gradientbased_bilevel_algorithms_for_hyperparameter_optimization'}},forum = 'u3mZzd0Pdx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9487/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9487/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9487/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9487/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u2gzfXRLaN',number = 11887,cdate = 1715712604014,pdate = 1727287986789,odate = 1730873943472,mdate = 1730873943488,tcdate = 1715712604014,tmdate = 1730873943488,ddate = None,content = {'title': {'value': 'Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization'}, 'authors': {'value': ['Omar Montasser', 'Han Shao', 'Emmanuel Abbe']}, 'authorids': {'value': ['~Omar_Montasser1', '~Han_Shao4', '~Emmanuel_Abbe1']}, 'keywords': {'value': ['pac learning guarantees', 'theory for distribution shifts', 'sample complexity', 'ood generalization', 'vc dimension']}, 'TLDR': {'value': 'We initiate a theoretical study for learning under distribution shifts when the shifts are captured by a collection of data transformation maps.'}, 'abstract': {'value': 'Learning with identical train and test distributions has been extensively investigated both practically and theoretically. Much remains to be understood, however, in statistical learning under distribution shifts. This paper focuses on a distribution shift setting where train and test distributions can be related by classes of (data) transformation maps. We initiate a theoretical study for this framework, investigating learning scenarios where the target class of transformations is either known or unknown. We establish learning rules and algorithmic reductions to Empirical Risk Minimization (ERM), accompanied with  learning guarantees. We obtain upper bounds on the sample complexity in terms of the VC dimension of the class composing predictors with transformations, which we show in many cases is not much larger than the VC dimension of the class of predictors. We highlight that the learning rules we derive offer a game-theoretic viewpoint on distribution shift: a learner searching for predictors and an adversary searching for transformation maps to respectively minimize and maximize the worst-case loss.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6df5ab210a13b0c96e72de1101a3d2ac1755a6fd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmontasser2024transformationinvariant,\\ntitle={Transformation-Invariant Learning and Theoretical Guarantees for {OOD} Generalization},\\nauthor={Omar Montasser and Han Shao and Emmanuel Abbe},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u2gzfXRLaN}\\n}'}, 'paperhash': {'value': 'montasser|transformationinvariant_learning_and_theoretical_guarantees_for_ood_generalization'}},forum = 'u2gzfXRLaN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11887/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11887/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11887/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11887/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u1mNGLYN74',number = 250,cdate = 1713856276836,pdate = 1727287633710,odate = 1730873839374,mdate = 1735884704369,tcdate = 1713856276836,tmdate = 1735884704369,ddate = None,content = {'title': {'value': 'DRACO: A Denoising-Reconstruction Autoencoder for Cryo-EM'}, 'authors': {'value': ['YingJun Shen', 'Haizhao Dai', 'Qihe Chen', 'Yan Zeng', 'Jiakai Zhang', 'Yuan Pei', 'Jingyi Yu']}, 'authorids': {'value': ['~YingJun_Shen1', '~Haizhao_Dai1', '~Qihe_Chen2', '~Yan_Zeng3', '~Jiakai_Zhang3', '~Yuan_Pei2', '~Jingyi_Yu5']}, 'keywords': {'value': ['masked image modeling', 'foundation model', 'cryo-EM', 'denoising autoencoder']}, 'abstract': {'value': \"Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods. However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises. We introduce DRACO, a Denoising-Reconstruction Autoencoder for CryO-EM, inspired by the Noise2Noise (N2N) approach. By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme. We mask both images to create denoising and reconstruction tasks. For DRACO's pre-training, the quality of the dataset is essential, we hence build a high-quality, diverse dataset from an uncurated public database, including over 270,000 movies or micrographs. After pre-training, DRACO naturally serves as a generalizable cryo-EM image denoiser and a foundation model for various cryo-EM downstream tasks. DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/40929f3ca7d47d1d178027cd1953fdb7039fb933.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshen2024draco,\\ntitle={{DRACO}: A Denoising-Reconstruction Autoencoder for Cryo-{EM}},\\nauthor={YingJun Shen and Haizhao Dai and Qihe Chen and Yan Zeng and Jiakai Zhang and Yuan Pei and Jingyi Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u1mNGLYN74}\\n}'}, 'paperhash': {'value': 'shen|draco_a_denoisingreconstruction_autoencoder_for_cryoem'}},forum = 'u1mNGLYN74',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission250/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission250/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission250/-/Revision', 'NeurIPS.cc/2024/Conference/Submission250/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'u1Z3HWz4VJ',number = 4232,cdate = 1715391487595,pdate = 1727287744583,odate = 1730873873880,mdate = 1730873873919,tcdate = 1715391487595,tmdate = 1730873873919,ddate = None,content = {'title': {'value': 'RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations for Universal Robustness'}, 'authors': {'value': ['Enyi Jiang', 'Gagandeep Singh']}, 'authorids': {'value': ['~Enyi_Jiang1', '~Gagandeep_Singh1']}, 'keywords': {'value': ['Adversarial Robustness', 'Pre-training and Fine-tuning', 'Distribution Shifts']}, 'abstract': {'value': \"Most existing works focus on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, these AT models' multiple-norm robustness (union accuracy) is still low, which is crucial since in the real-world an adversary is not necessarily bounded by a single norm. The tradeoffs among robustness against multiple $l_p$ perturbations and accuracy/robustness make obtaining good union and clean accuracy challenging. We design a logit pairing loss to improve the union accuracy by analyzing the tradeoffs from the lens of distribution shifts. We connect natural training (NT) with AT via gradient projection, to incorporate useful information from NT into AT, where we empirically and theoretically show it moderates the accuracy/robustness tradeoff. We propose a novel training framework \\\\textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. \\\\textbf{RAMP} can be easily adapted for robust fine-tuning and full AT. For robust fine-tuning, \\\\textbf{RAMP} obtains a union accuracy up to $53.3\\\\%$ on CIFAR-10, and $29.1\\\\%$ on ImageNet. For training from scratch, \\\\textbf{RAMP} achieves a union accuracy of $44.6\\\\%$ and good clean accuracy of $81.2\\\\%$ on ResNet-18 against AutoAttack on CIFAR-10. Beyond multi-norm robustness \\\\textbf{RAMP}-trained models achieve superior \\\\textit{universal robustness}, effectively generalizing against a range of unseen adversaries and natural corruptions.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/545fc3560c1ccd73e712583aba057a4b6cd52393.pdf'}, 'supplementary_material': {'value': '/attachment/60efc40c9debd534b538669d46cd9a02162c541d.zip'}, 'TLDR': {'value': 'We design a logit pairing loss and connect natural training with adversarial training via gradient projection to improve the multi-norm robustness while maintaining good clean accuracy.'}, '_bibtex': {'value': '@inproceedings{\\njiang2024ramp,\\ntitle={{RAMP}: Boosting Adversarial Robustness Against Multiple \\\\$l\\\\_p\\\\$ Perturbations for Universal Robustness},\\nauthor={Enyi Jiang and Gagandeep Singh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=u1Z3HWz4VJ}\\n}'}, 'paperhash': {'value': 'jiang|ramp_boosting_adversarial_robustness_against_multiple_l_p_perturbations_for_universal_robustness'}},forum = 'u1Z3HWz4VJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4232/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4232/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4232/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4232/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tz83Nyb71l',number = 2456,cdate = 1715074975268,pdate = 1727287690685,odate = 1730873858127,mdate = 1730873858140,tcdate = 1715074975268,tmdate = 1730873858140,ddate = None,content = {'title': {'value': 'YOLOv10: Real-Time End-to-End Object Detection'}, 'authors': {'value': ['Ao Wang', 'Hui Chen', 'Lihao Liu', 'Kai CHEN', 'Zijia Lin', 'Jungong Han', 'Guiguang Ding']}, 'authorids': {'value': ['~Ao_Wang2', '~Hui_Chen7', '~Lihao_Liu2', '~Kai_CHEN17', '~Zijia_Lin1', '~Jungong_Han1', '~Guiguang_Ding1']}, 'keywords': {'value': ['YOLO', 'object detection', 'computer vision']}, 'abstract': {'value': \"Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and the model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings the competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both the efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves the state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8$\\\\times$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8$\\\\times$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\\\% less latency and 25\\\\% fewer parameters for the same performance. Code and models are available at https://github.com/THU-MIG/yolov10.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A YOLO model for object detection'}, 'pdf': {'value': '/pdf/6fb5f9fb30b35e9848efa2418cd263f561d2e3a5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024yolov,\\ntitle={{YOLO}v10: Real-Time End-to-End Object Detection},\\nauthor={Ao Wang and Hui Chen and Lihao Liu and Kai CHEN and Zijia Lin and Jungong Han and Guiguang Ding},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tz83Nyb71l}\\n}'}, 'paperhash': {'value': 'wang|yolov10_realtime_endtoend_object_detection'}},forum = 'tz83Nyb71l',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2456/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2456/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2456/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2456/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tyPcIETPWM',number = 7243,cdate = 1715616079338,pdate = 1727287840386,odate = 1730873900827,mdate = 1730873900847,tcdate = 1715616079338,tmdate = 1730873900847,ddate = None,content = {'title': {'value': 'Conditional Outcome Equivalence: A Quantile Alternative to CATE'}, 'authors': {'value': ['Josh Givens', 'Henry Reeve', 'Song Liu', 'Katarzyna Reluga']}, 'authorids': {'value': ['~Josh_Givens1', '~Henry_Reeve1', '~Song_Liu1', '~Katarzyna_Reluga1']}, 'keywords': {'value': ['Heteregenous Treatment Effect', 'Conditional Average Treatment Effect', 'Conditional Quantile Treatment Effect', 'Quantile Regression']}, 'abstract': {'value': 'The conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes the CQTE especially valuable in cases where the effect of a treatment is not well-modelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of the CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to the CATE where it is possible to obtain high-quality estimates  which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of the CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to the CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a new estimand, the conditional quantile comparator to compete with both the CATE and the CQTE.'}, 'pdf': {'value': '/pdf/a63da2a2fa88b2892641ef33b8f9ba615e7dc057.pdf'}, 'supplementary_material': {'value': '/attachment/88829afaccc68d7a922d48d48410abd43fa22bab.zip'}, '_bibtex': {'value': '@inproceedings{\\ngivens2024conditional,\\ntitle={Conditional Outcome Equivalence: A Quantile Alternative to {CATE}},\\nauthor={Josh Givens and Henry Reeve and Song Liu and Katarzyna Reluga},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tyPcIETPWM}\\n}'}, 'paperhash': {'value': 'givens|conditional_outcome_equivalence_a_quantile_alternative_to_cate'}},forum = 'tyPcIETPWM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7243/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7243/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7243/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7243/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'twpPD9UMUN',number = 181,cdate = 1713843449546,pdate = 1727287632033,odate = 1730873838840,mdate = 1736559010315,tcdate = 1713843449546,tmdate = 1736559010315,ddate = None,content = {'title': {'value': 'Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering'}, 'authors': {'value': ['Jie Ma', 'Min Hu', 'Pinghui Wang', 'Wangchun Sun', 'Lingyun Song', 'Hongbin Pei', 'Jun Liu', 'Youtian Du']}, 'authorids': {'value': ['~Jie_Ma1', '~Min_Hu7', '~Pinghui_Wang1', '~Wangchun_Sun1', '~Lingyun_Song1', '~Hongbin_Pei1', '~Jun_Liu10', '~Youtian_Du1']}, 'keywords': {'value': ['audio-visual question answering', 'bias elimination', 'debiasing', 'multimodality learning']}, 'abstract': {'value': 'Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, *MUSIC-AVQA-R*, crafted in two steps: rephrasing questions within the test split of a public dataset (*MUSIC-AVQA*) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\\\\%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at <https://github.com/reml-group/MUSIC-AVQA-R>.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We systematically explore the bias in the AVQA task from the persepctive of model designs and model evaluations.'}, 'pdf': {'value': '/pdf/8521874c9bf1c81111a7fbd4c31cf236ed40619d.pdf'}, 'supplementary_material': {'value': '/attachment/a8e33ad7b038e7c5bf90bcc2fc52442f88c94721.zip'}, '_bibtex': {'value': '@inproceedings{\\nma2024look,\\ntitle={Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering},\\nauthor={Jie Ma and Min Hu and Pinghui Wang and Wangchun Sun and Lingyun Song and Hongbin Pei and Jun Liu and Youtian Du},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=twpPD9UMUN}\\n}'}, 'paperhash': {'value': 'ma|look_listen_and_answer_overcoming_biases_for_audiovisual_question_answering'}},forum = 'twpPD9UMUN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission181/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission181/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission181/-/Revision', 'NeurIPS.cc/2024/Conference/Submission181/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'twYE75Mnkt',number = 20345,cdate = 1715795881155,pdate = 1727288231829,odate = 1730874000854,mdate = 1730874000875,tcdate = 1715795881155,tmdate = 1730874000875,ddate = None,content = {'title': {'value': 'Derandomizing Multi-Distribution Learning'}, 'authors': {'value': ['Kasper Green Larsen', 'Omar Montasser', 'Nikita Zhivotovskiy']}, 'authorids': {'value': ['~Kasper_Green_Larsen1', '~Omar_Montasser1', '~Nikita_Zhivotovskiy1']}, 'keywords': {'value': ['pac learning', 'multi-distribution', 'derandomization', 'computational efficiency', 'discrepancy minimization']}, 'TLDR': {'value': 'We show that it is computationally hard to derandomize multi-distribution learning algorithms. We also show this hardness can be alleviated with a structural condition that we identify.'}, 'abstract': {'value': 'Multi-distribution or collaborative learning involves learning a single predictor that works well across multiple data distributions, using samples from each during training. Recent research on multi-distribution learning, focusing on binary loss and finite VC dimension classes, has shown near-optimal sample complexity that is achieved with oracle efficient algorithms. That is, these algorithms are computationally efficient given an efficient ERM for the class. Unlike in classical PAC learning, where the optimal sample complexity is achieved with deterministic predictors, current multi-distribution learning algorithms output randomized predictors. This raises the question: can these algorithms be derandomized to produce a deterministic predictor for multiple distributions? Through a reduction to discrepancy minimization, we show that derandomizing multi-distribution learning is computationally hard, even when ERM is computationally efficient. On the positive side, we identify a structural condition enabling an efficient black-box reduction, converting existing randomized multi-distribution predictors into deterministic ones.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f67adbada6b20732846d792b8b89fdf428f4f20e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlarsen2024derandomizing,\\ntitle={Derandomizing Multi-Distribution Learning},\\nauthor={Kasper Green Larsen and Omar Montasser and Nikita Zhivotovskiy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=twYE75Mnkt}\\n}'}, 'paperhash': {'value': 'larsen|derandomizing_multidistribution_learning'}},forum = 'twYE75Mnkt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20345/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20345/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20345/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20345/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tvQ3XCKWbB',number = 7188,cdate = 1715614612245,pdate = 1727287838357,odate = 1730873900313,mdate = 1730873900330,tcdate = 1715614612245,tmdate = 1730873900330,ddate = None,content = {'title': {'value': 'Enriching Disentanglement: From Logical Definitions to Quantitative Metrics'}, 'authors': {'value': ['Yivan Zhang', 'Masashi Sugiyama']}, 'authorids': {'value': ['~Yivan_Zhang1', '~Masashi_Sugiyama1']}, 'keywords': {'value': ['Disentanglement', 'Representation Learning', 'Logic', 'Metric', 'Algebra', 'Category Theory', 'Topos Theory']}, 'abstract': {'value': 'Disentangling the explanatory factors in complex data is a promising approach for generalizable and data-efficient representation learning. While a variety of quantitative metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what properties these metrics truly quantify. In this work, we establish algebraic relationships between logical definitions and quantitative metrics to derive theoretically grounded disentanglement metrics. Concretely, we introduce a compositional approach for converting a higher-order predicate into a real-valued quantity by replacing (i) equality with a strict premetric, (ii) the Heyting algebra of binary truth values with a quantale of continuous values, and (iii) quantifiers with aggregators. The metrics induced by logical definitions have strong theoretical guarantees, and some of them are easily differentiable and can be used as learning objectives directly. Finally, we empirically demonstrate the effectiveness of the proposed metrics by isolating different aspects of disentangled representations.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1c6c34020b56e235277af4b47085e69bbf792fea.pdf'}, 'supplementary_material': {'value': '/attachment/2c5f0e66abe3f9c564a820daf79932c4b5639221.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024enriching,\\ntitle={Enriching Disentanglement: From Logical Definitions to Quantitative Metrics},\\nauthor={Yivan Zhang and Masashi Sugiyama},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tvQ3XCKWbB}\\n}'}, 'paperhash': {'value': 'zhang|enriching_disentanglement_from_logical_definitions_to_quantitative_metrics'}},forum = 'tvQ3XCKWbB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7188/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7188/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7188/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7188/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tuiqq1G8I5',number = 12076,cdate = 1715715931377,pdate = 1727287993736,odate = 1730873945186,mdate = 1736973045270,tcdate = 1715715931377,tmdate = 1736973045270,ddate = None,content = {'title': {'value': 'DisCEdit: Model Editing by Identifying Discriminative Components'}, 'authors': {'value': ['Chaitanya Murti', 'Chiranjib Bhattacharyya']}, 'authorids': {'value': ['~Chaitanya_Murti1', '~Chiranjib_Bhattacharyya1']}, 'keywords': {'value': ['model editing', 'selective forgetting', 'structured pruning', 'total variation distance']}, 'abstract': {'value': 'Model editing is a growing area of research that is particularly valuable in contexts where modifying key model components, like neurons or filters, can significantly impact the model’s performance. The key challenge lies in identifying important components useful to the model’s predictions. We apply model editing to address two active areas of research, Structured Pruning, and Selective Class Forgetting. In this work, we adopt a distributional approach to the problem of identifying important components, leveraging the recently proposed discriminative filters hypothesis, which states that well-trained (convolutional) models possess discriminative filters that are essential to prediction. To do so, we define discriminative ability in terms of the Bayes error rate associated with the feature distributions, which is equivalent to computing the Total Variation (TV) distance between the distributions. However, computing the TV distance is intractable, motivating us to derive novel witness function-based lower bounds on the TV distance that require no assumptions on the underlying distributions; using this bound generalizes prior work such as Murti et al. [39] that relied on unrealistic Gaussianity assumptions on the feature distributions. With these bounds, we are able to discover critical subnetworks responsible for classwise predictions, and derive DISCEDIT-SP and DISCEDIT-U , algorithms for structured pruning requiring no access to the training data and loss function, and selective forgetting respectively. We apply DISCEDIT-U to selective class forgetting on models trained on CIFAR10 and CIFAR100, and we show that on average, we can reduce accuracy on a single class by over 80% with a minimal reduction in test accuracy on the remaining classes. Similarly, on Structured pruning problems, we obtain 40.8% sparsity on ResNet50 on Imagenet, with only a 2.6% drop in accuracy with minimal fine-tuning.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We use new lower bounds on the TV distance to identify discriminative network components for structured pruning and selective forgetting.'}, 'pdf': {'value': '/pdf/c7df4a98dbe1cbf0192e59c5f5fa58b1b3a7693e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmurti2024discedit,\\ntitle={Dis{CE}dit: Model Editing by Identifying Discriminative Components},\\nauthor={Chaitanya Murti and Chiranjib Bhattacharyya},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tuiqq1G8I5}\\n}'}, 'paperhash': {'value': 'murti|discedit_model_editing_by_identifying_discriminative_components'}},forum = 'tuiqq1G8I5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12076/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12076/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12076/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12076/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tu1oC7zHGW',number = 2506,cdate = 1715085438124,pdate = 1727287692022,odate = 1730873858573,mdate = 1735532980390,tcdate = 1715085438124,tmdate = 1735532980390,ddate = None,content = {'title': {'value': 'Unveiling the Tapestry of Consistency in Large Vision-Language Models'}, 'authors': {'value': ['Yuan Zhang', 'Fei xiao', 'Tao Huang', 'Chun-Kai Fan', 'Hongyuan Dong', 'Jiawen Li', 'Jiacong Wang', 'Kuan Cheng', 'Shanghang Zhang', 'Haoyuan Guo']}, 'authorids': {'value': ['~Yuan_Zhang20', '~Fei_xiao8', '~Tao_Huang5', '~Chun-Kai_Fan1', '~Hongyuan_Dong2', '~Jiawen_Li6', '~Jiacong_Wang1', '~Kuan_Cheng1', '~Shanghang_Zhang4', '~Haoyuan_Guo1']}, 'keywords': {'value': ['Consistency', 'ConBench', 'Large Vision-Language Models', 'Analysis']}, 'abstract': {'value': 'Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. \\n(2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose a Consistency benchmark, get an in-depth analysis and design a simple method to improve VLMs.'}, 'supplementary_material': {'value': '/attachment/6af31d1ca7f8caf635dea91c0c149ba97dd9083a.zip'}, 'pdf': {'value': '/pdf/83dfd150b1948e538576ef1e057ad1a668b1ca3e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024unveiling,\\ntitle={Unveiling the Tapestry of Consistency in Large Vision-Language Models},\\nauthor={Yuan Zhang and Fei xiao and Tao Huang and Chun-Kai Fan and Hongyuan Dong and Jiawen Li and Jiacong Wang and Kuan Cheng and Shanghang Zhang and Haoyuan Guo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tu1oC7zHGW}\\n}'}, 'paperhash': {'value': 'zhang|unveiling_the_tapestry_of_consistency_in_large_visionlanguage_models'}},forum = 'tu1oC7zHGW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2506/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2506/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2506/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2506/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'ttUXtV2YrA',number = 9993,cdate = 1715688866016,pdate = 1727287926958,odate = 1730873925242,mdate = 1730873925259,tcdate = 1715688866016,tmdate = 1730873925259,ddate = None,content = {'title': {'value': 'Revisiting the Integration of Convolution and Attention for Vision Backbone'}, 'authors': {'value': ['Lei Zhu', 'Xinjiang Wang', 'Wayne Zhang', 'Rynson W. H. Lau']}, 'authorids': {'value': ['~Lei_Zhu12', '~Xinjiang_Wang1', '~Wayne_Zhang2', '~Rynson_W._H._Lau1']}, 'keywords': {'value': ['convolution', 'attention', 'vision backbone']}, 'abstract': {'value': 'Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically considered alternatives to each other for building vision backbones. Although some works try to integrate both, they apply the two operators simultaneously at the finest pixel granularity. With Convs responsible for per-pixel feature extraction already, the question is whether we still need to include the heavy MHSAs at such a fine-grained level. In fact, this is the root cause of the scalability issue w.r.t. the input resolution for vision transformers. To address this important problem, we propose in this work to use MSHAs and Convs in parallel \\\\textbf{at different granularity levels} instead. Specifically, in each layer, we use two different ways to represent an image: a fine-grained regular grid and a coarse-grained set of semantic slots. We apply different operations to these two representations: Convs to the grid for local features, and MHSAs to the slots for global features. A pair of fully differentiable soft clustering and dispatching modules is introduced to bridge the grid and set representations, thus \\nenabling local-global fusion. Through extensive experiments on various vision tasks, we empirically verify the potential of the proposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of fine-grained features to light-weight Convs, it is sufficient to use MHSAs in a few (e.g., 64) semantic slots to match the performance of recent state-of-the-art backbones, while being more efficient. Our visualization results also demonstrate that the soft clustering module produces a meaningful semantic grouping effect with only IN1k classification supervision, which may induce better interpretability and inspire new weakly-supervised semantic segmentation approaches. Code will be available at \\\\url{https://github.com/rayleizhu/GLMix}.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1bec4c01160f57a5560a6532b6328cafc3f4324e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024revisiting,\\ntitle={Revisiting the Integration of Convolution and Attention for Vision Backbone},\\nauthor={Lei Zhu and Xinjiang Wang and Wayne Zhang and Rynson W. H. Lau},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ttUXtV2YrA}\\n}'}, 'TLDR': {'value': 'Apply attention and convolution at different granularity levels for more efficient representation learning.'}, 'paperhash': {'value': 'zhu|revisiting_the_integration_of_convolution_and_attention_for_vision_backbone'}},forum = 'ttUXtV2YrA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9993/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9993/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9993/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9993/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ttLcbEkaj6',number = 1700,cdate = 1714751770937,pdate = 1727287668788,odate = 1730873851166,mdate = 1730873851184,tcdate = 1714751770937,tmdate = 1730873851184,ddate = None,content = {'title': {'value': 'AirSketch: Generative Motion to Sketch'}, 'authors': {'value': ['Hui Xian Grace Lim', 'Xuanming Cui', 'Yogesh S Rawat', 'Ser-Nam Lim']}, 'authorids': {'value': ['~Hui_Xian_Grace_Lim1', '~Xuanming_Cui1', '~Yogesh_S_Rawat1', '~Ser-Nam_Lim3']}, 'keywords': {'value': ['Generative', 'Hand Motion', 'Sketch']}, 'abstract': {'value': 'Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7b8baae72e6db07ea36b5aadd9c4552c0bb6eeec.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlim2024airsketch,\\ntitle={AirSketch: Generative Motion to Sketch},\\nauthor={Hui Xian Grace Lim and Xuanming Cui and Yogesh S Rawat and Ser-Nam Lim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ttLcbEkaj6}\\n}'}, 'paperhash': {'value': 'lim|airsketch_generative_motion_to_sketch'}},forum = 'ttLcbEkaj6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1700/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1700/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1700/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1700/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tsIKrvexBd',number = 16013,cdate = 1715765744766,pdate = 1727288113785,odate = 1730873976594,mdate = 1736835286714,tcdate = 1715765744766,tmdate = 1736835286714,ddate = None,content = {'title': {'value': 'Leveraging Tumor Heterogeneity: Heterogeneous Graph Representation Learning for Cancer Survival Prediction in Whole Slide Images'}, 'authors': {'value': ['Junxian Wu', 'Xinyi Ke', 'Xiaoming Jiang', 'Huanwen Wu', 'Youyong Kong', 'Lizhi Shao']}, 'authorids': {'value': ['~Junxian_Wu3', '~Xinyi_Ke1', '~Xiaoming_Jiang2', '~Huanwen_Wu1', '~Youyong_Kong1', '~Lizhi_Shao1']}, 'keywords': {'value': ['Whole Slide Image', 'Survival Prediction', 'Tumor Heterogeneity', 'Heterogeneous\\xa0Graph', 'Graph Convolutional Network']}, 'abstract': {'value': 'Survival prediction is a significant challenge in cancer management. Tumor micro-environment is a highly sophisticated ecosystem consisting of cancer cells, immune cells, endothelial cells, fibroblasts, nerves and extracellular matrix. The intratumor heterogeneity and the interaction across multiple tissue types profoundly impacts the prognosis. However, current methods often neglect the fact that the contribution to prognosis differs with tissue types. In this paper, we propose ProtoSurv, a novel heterogeneous graph model for WSI survival prediction. The learning process of ProtoSurv is not only driven by data but also incorporates pathological domain knowledge, including the awareness of tissue heterogeneity, the emphasis on prior knowledge of prognostic-related tissues, and the depiction of spatial interaction across multiple tissues. We validate ProtoSurv across five different cancer types from TCGA (i.e., BRCA, LGG, LUAD, COAD and PAAD), and demonstrate the superiority of our method over the state-of-the-art methods.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6fd5cf89ec3de3b1b6a77c18c0c9e62d0ac0b563.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024leveraging,\\ntitle={Leveraging Tumor Heterogeneity: Heterogeneous Graph Representation Learning for Cancer Survival Prediction in Whole Slide Images},\\nauthor={Junxian Wu and Xinyi Ke and Xiaoming Jiang and Huanwen Wu and Youyong Kong and Lizhi Shao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tsIKrvexBd}\\n}'}, 'paperhash': {'value': 'wu|leveraging_tumor_heterogeneity_heterogeneous_graph_representation_learning_for_cancer_survival_prediction_in_whole_slide_images'}},forum = 'tsIKrvexBd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16013/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tnh4LK72yj',number = 2077,cdate = 1714967259278,pdate = 1727287680168,odate = 1730873854766,mdate = 1734751186833,tcdate = 1714967259278,tmdate = 1734751186833,ddate = None,content = {'title': {'value': 'Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework'}, 'authors': {'value': ['Zhongchao Yi', 'Zhengyang Zhou', 'Qihe Huang', 'Yanjiang Chen', 'Liheng Yu', 'Xu Wang', 'Yang Wang']}, 'authorids': {'value': ['~Zhongchao_Yi1', '~Zhengyang_Zhou1', '~Qihe_Huang2', '~Yanjiang_Chen1', '~Liheng_Yu1', '~Xu_Wang16', '~Yang_Wang32']}, 'keywords': {'value': ['continuous multi-task learning', 'spatio-temporal forecasting', 'urban intelligence']}, 'TLDR': {'value': 'Breaking free from isolation, this work presents an innovative multi-task spatio-temporal modeling approach, fostering interconnectedness among diverse data sources for enhanced prediction accuracy and adaptability in urban forecasting'}, 'abstract': {'value': 'Spatiotemporal learning has become a pivotal technique to enable urban intelligence. Traditional spatiotemporal models mostly focus on a specific task by assuming a same distribution between training and testing sets. However, given that urban systems are usually dynamic, multi-sourced with imbalanced data distributions, current specific task-specific models fail to generalize to new urban conditions and adapt to new domains without explicitly modeling interdependencies across various dimensions and types of urban data. To this end, we argue that there is an essential to propose a Continuous Multi-task Spatio-Temporal learning framework (CMuST) to empower  collective urban intelligence, which  reforms the urban spatiotemporal learning from single-domain  to cooperatively multi-dimensional and multi-task learning. Specifically, CMuST proposes a new multi-dimensional spatiotemporal interaction network (MSTI) to allow cross-interactions between context and main observations as well as  self-interactions within spatial and temporal aspects  to be  exposed, which is also the core for capturing task-level commonality and personalization. To ensure continuous task learning, a novel Rolling Adaptation training scheme (RoAda) is devised, which not only preserves task uniqueness by constructing data summarization-driven task prompts, but also harnesses correlated patterns among tasks  by iterative model behavior modeling. We further establish a benchmark of three cities for multi-task spatiotemporal learning, and empirically demonstrate the superiority of CMuST via extensive evaluations on these datasets. The impressive improvements on both few-shot streaming data and new domain tasks against existing SOAT methods are achieved. Code is available at https://github.com/DILab-USTCSZ/CMuST.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/97148ef3439d4c09aeb2847ed85a61ab7bd105d9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyi2024get,\\ntitle={Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework},\\nauthor={Zhongchao Yi and Zhengyang Zhou and Qihe Huang and Yanjiang Chen and Liheng Yu and Xu Wang and Yang Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tnh4LK72yj}\\n}'}, 'paperhash': {'value': 'yi|get_rid_of_isolation_a_continuous_multitask_spatiotemporal_learning_framework'}},forum = 'tnh4LK72yj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2077/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2077/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2077/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2077/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tnQbciDjVf',number = 8452,cdate = 1715660822100,pdate = 1727287879890,odate = 1730873912293,mdate = 1734839167527,tcdate = 1715660822100,tmdate = 1734839167527,ddate = None,content = {'title': {'value': 'TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration'}, 'authors': {'value': ['Yiwei Guo', 'Shaobin Zhuang', 'Kunchang Li', 'Yu Qiao', 'Yali Wang']}, 'authorids': {'value': ['~Yiwei_Guo2', '~Shaobin_Zhuang1', '~Kunchang_Li1', '~Yu_Qiao1', '~Yali_Wang1']}, 'keywords': {'value': ['Vision-Language Models', 'Few-shot Transfer Learning', 'Heterogeneous Agent Collaboration']}, 'abstract': {'value': 'Vision-language foundation models (such as CLIP) have recently shown their power in transfer learning, owing to large-scale image-text pre-training. However, target domain data in the downstream tasks can be highly different from the pre-training phase, which makes it hard for such a single model to generalize well. Alternatively, there exists a wide range of expert models that contain diversified vision and/or language knowledge pre-trained on different modalities, tasks, networks, and datasets. Unfortunately, these models are \"isolated agents\" with heterogeneous structures, and how to integrate their knowledge for generalizing CLIP-like models has not been fully explored. To bridge this gap, we propose a general and concise TransAgent framework, which transports the knowledge of the isolated agents in a unified manner, and effectively guides CLIP to generalize with multi-source knowledge distillation. With such a distinct framework, we flexibly collaborate with 11 heterogeneous agents to empower vision-language foundation models, without further cost in the inference phase. Finally, our TransAgent achieves state-of-the-art performance on 11 visual recognition datasets. Under the same low-shot setting, it outperforms the popular CoOp with around 10\\\\% on average, and 20\\\\% on EuroSAT which contains large domain shifts.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/49216ae30f248ec7be10e4a2d12eb5a0235dee9b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguo2024transagent,\\ntitle={TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration},\\nauthor={Yiwei Guo and Shaobin Zhuang and Kunchang Li and Yu Qiao and Yali Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tnQbciDjVf}\\n}'}, 'paperhash': {'value': 'guo|transagent_transfer_visionlanguage_foundation_models_with_heterogeneous_agent_collaboration'}},forum = 'tnQbciDjVf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8452/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8452/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8452/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8452/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tmX1AUmkl6',number = 1585,cdate = 1714703680020,pdate = 1727287665400,odate = 1730873850117,mdate = 1730873850135,tcdate = 1714703680020,tmdate = 1730873850135,ddate = None,content = {'title': {'value': 'Evaluation of Text-to-Video Generation Models: A Dynamics Perspective'}, 'authors': {'value': ['Mingxiang Liao', 'Hannan Lu', 'Qixiang Ye', 'Wangmeng Zuo', 'Fang Wan', 'Tianyu Wang', 'Yuzhong Zhao', 'Jingdong Wang', 'Xinyu Zhang']}, 'authorids': {'value': ['~Mingxiang_Liao1', '~Hannan_Lu1', '~Qixiang_Ye1', '~Wangmeng_Zuo3', '~Fang_Wan1', '~Tianyu_Wang12', '~Yuzhong_Zhao1', '~Jingdong_Wang1', '~Xinyu_Zhang3']}, 'keywords': {'value': ['Video generation model', 'dynamics evalution']}, 'abstract': {'value': 'Comprehensive and constructive evaluation protocols play an important role when developing sophisticated text-to-video (T2V) generation models. Existing evaluation protocols primarily focus on temporal consistency and content continuity, yet largely ignore dynamics of video content. Such dynamics is an essential dimension measuring the visual vividness and the honesty of video content to text prompts. In this study, we propose an effective evaluation protocol, termed DEVIL, which centers on the dynamics dimension to evaluate T2V generation models, as well as improving existing evaluation metrics. In practice, we define a set of dynamics scores corresponding to multiple temporal granularities, and a new benchmark of text prompts under multiple dynamics grades. Upon the text prompt benchmark, we assess the generation capacity of T2V models, characterized by metrics of dynamics ranges and T2V alignment. Moreover, we analyze the relevance of existing metrics to dynamics metrics, improving them from the perspective of dynamics. Experiments show that DEVIL evaluation metrics enjoy up to about 90\\\\% consistency with human ratings, demonstrating the potential to advance T2V generation models.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/9e07a137aa1c283d99a0a83033a635cba2ccb4c4.zip'}, 'pdf': {'value': '/pdf/f5d221285c7e41a1254cb05401775dc6bf587637.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliao2024evaluation,\\ntitle={Evaluation of Text-to-Video Generation Models: A Dynamics Perspective},\\nauthor={Mingxiang Liao and Hannan Lu and Qixiang Ye and Wangmeng Zuo and Fang Wan and Tianyu Wang and Yuzhong Zhao and Jingdong Wang and Xinyu Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tmX1AUmkl6}\\n}'}, 'paperhash': {'value': 'liao|evaluation_of_texttovideo_generation_models_a_dynamics_perspective'}},forum = 'tmX1AUmkl6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1585/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1585/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1585/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1585/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tmQH8prqLc',number = 16352,cdate = 1715769584956,pdate = 1727288124298,odate = 1730873978807,mdate = 1730873978826,tcdate = 1715769584956,tmdate = 1730873978826,ddate = None,content = {'title': {'value': 'Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions'}, 'authors': {'value': ['Wei Jiang', 'Sifan Yang', 'Yibo Wang', 'Lijun Zhang']}, 'authorids': {'value': ['~Wei_Jiang8', '~Sifan_Yang2', '~Yibo_Wang2', '~Lijun_Zhang1']}, 'keywords': {'value': ['Adaptive methods', 'variance reduction', 'finite-sum optimization', 'stochastic compositional optimization']}, 'abstract': {'value': 'This paper explores adaptive variance reduction methods for stochastic optimization based on the STORM technique. Existing adaptive extensions of STORM rely on strong assumptions like bounded gradients and bounded function values, or suffer an additional $\\\\mathcal{O}(\\\\log T)$ term in the convergence rate. To address these limitations, we introduce a novel adaptive STORM method that achieves an optimal convergence rate of $\\\\mathcal{O}(T^{-1/3})$ for non-convex functions with our newly designed learning rate strategy. Compared with existing approaches, our method requires weaker assumptions and attains the optimal convergence rate without the additional $\\\\mathcal{O}(\\\\log T)$ term. We also extend the proposed technique to stochastic compositional optimization, obtaining the same optimal rate of $\\\\mathcal{O}(T^{-1/3})$. Furthermore, we investigate the non-convex finite-sum problem and develop another innovative adaptive variance reduction method that achieves an optimal convergence rate of $\\\\mathcal{O}(n^{1/4} T^{-1/2} )$, where $n$ represents the number of component functions. Numerical experiments across various tasks validate the effectiveness of our method.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/07069315f4a7232a3a174e03fffd71b20d273da3.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiang2024adaptive,\\ntitle={Adaptive Variance Reduction for Stochastic Optimization under Weaker Assumptions},\\nauthor={Wei Jiang and Sifan Yang and Yibo Wang and Lijun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tmQH8prqLc}\\n}'}, 'paperhash': {'value': 'jiang|adaptive_variance_reduction_for_stochastic_optimization_under_weaker_assumptions'}},forum = 'tmQH8prqLc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16352/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16352/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16352/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16352/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tk0uaRynhH',number = 11358,cdate = 1715703892827,pdate = 1727287968726,odate = 1730873937879,mdate = 1730873937899,tcdate = 1715703892827,tmdate = 1730873937899,ddate = None,content = {'title': {'value': 'Dynamic Conditional Optimal Transport through Simulation-Free Flows'}, 'authors': {'value': ['Gavin Kerrigan', 'Giosue Migliorini', 'Padhraic Smyth']}, 'authorids': {'value': ['~Gavin_Kerrigan1', '~Giosue_Migliorini1', '~Padhraic_Smyth1']}, 'keywords': {'value': ['flow matching', 'optimal transport', 'generative models', 'conditional generation']}, 'TLDR': {'value': 'We study the geometry of conditional optimal transport from a dynamical perspective, and use our theory to build conditional generative models.'}, 'abstract': {'value': 'We study the geometry of conditional optimal transport (COT) and prove a dynamic formulation which generalizes the Benamou-Brenier Theorem. Equipped with these tools, we propose a simulation-free flow-based method for conditional generative modeling. Our method couples an arbitrary source distribution to a specified target distribution through a triangular COT plan, and a conditional generative model is obtained by approximating the geodesic path of measures induced by this COT plan. Our theory and methods are applicable in infinite-dimensional settings, making them well suited for a wide class of Bayesian inverse problems. Empirically, we demonstrate that our method is competitive on several challenging conditional generation tasks, including an infinite-dimensional inverse problem.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d4e9108f3aa259861f75b34a8398e06ef828b822.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkerrigan2024dynamic,\\ntitle={Dynamic Conditional Optimal Transport through Simulation-Free Flows},\\nauthor={Gavin Kerrigan and Giosue Migliorini and Padhraic Smyth},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tk0uaRynhH}\\n}'}, 'paperhash': {'value': 'kerrigan|dynamic_conditional_optimal_transport_through_simulationfree_flows'}},forum = 'tk0uaRynhH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11358/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11358/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11358/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11358/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tj8nsfxi5r',number = 5288,cdate = 1715521132812,pdate = 1727287779145,odate = 1730873883617,mdate = 1730873883628,tcdate = 1715521132812,tmdate = 1730873883628,ddate = None,content = {'title': {'value': 'From News to Forecast: Integrating Event Analysis in LLM-Based Time Series Forecasting with Reflection'}, 'authors': {'value': ['Xinlei Wang', 'Maike Feng', 'Jing Qiu', 'Jinjin Gu', 'Junhua Zhao']}, 'authorids': {'value': ['~Xinlei_Wang3', '~Maike_Feng1', '~Jing_Qiu3', '~Jinjin_Gu1', '~Junhua_Zhao1']}, 'keywords': {'value': ['Large Language Model', 'Time Series Forecasting', 'AI Agent']}, 'abstract': {'value': \"This paper introduces a novel approach that leverages Large Language Models (LLMs) and Generative Agents to enhance time series forecasting by reasoning across both text and time series data. With language as a medium, our method adaptively integrates social events into forecasting models, aligning news content with time series fluctuations to provide richer insights. Specifically, we utilize LLM-based agents to iteratively filter out irrelevant news and employ human-like reasoning to evaluate predictions. This enables the model to analyze complex events, such as unexpected incidents and shifts in social behavior, and continuously refine the selection logic of news and the robustness of the agent's output. By integrating selected news events with time series data, we fine-tune a pre-trained LLM to predict sequences of digits in time series. The results demonstrate significant improvements in forecasting accuracy, suggesting a potential paradigm shift in time series forecasting through the effective utilization of unstructured news data.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ee49cee9662dd2a8cae6583a9b82aff4cb4b5298.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024from,\\ntitle={From News to Forecast: Integrating Event Analysis in {LLM}-Based Time Series Forecasting with Reflection},\\nauthor={Xinlei Wang and Maike Feng and Jing Qiu and Jinjin Gu and Junhua Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tj8nsfxi5r}\\n}'}, 'paperhash': {'value': 'wang|from_news_to_forecast_integrating_event_analysis_in_llmbased_time_series_forecasting_with_reflection'}},forum = 'tj8nsfxi5r',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5288/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5288/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5288/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5288/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'thUf6ZBlPp',number = 8022,cdate = 1715650660307,pdate = 1727287865928,odate = 1730873908689,mdate = 1730873908708,tcdate = 1715650660307,tmdate = 1730873908708,ddate = None,content = {'title': {'value': 'EigenVI: score-based variational inference with orthogonal function expansions'}, 'authors': {'value': ['Diana Cai', 'Chirag Modi', 'Charles Margossian', 'Robert M. Gower', 'David Blei', 'Lawrence K. Saul']}, 'authorids': {'value': ['~Diana_Cai1', '~Chirag_Modi1', '~Charles_Margossian1', '~Robert_M._Gower1', '~David_Blei2', '~Lawrence_K._Saul3']}, 'keywords': {'value': ['variational inference', 'black-box variational inference', 'Bayesian inference', 'probabilistic modeling', 'score-based divergence', 'score matching', 'non-Gaussian variational families']}, 'abstract': {'value': 'We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over $\\\\mathbb{R}^D$, the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriordb. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'EigenVI uses score matching with flexible yet tractable variational families built using orthogonal function expansions and reduces to an eigenvalue problem.'}, 'pdf': {'value': '/pdf/3a5a8652ca4682261561100d0bf891d9086e2f30.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncai2024eigenvi,\\ntitle={Eigen{VI}: score-based variational inference with orthogonal function expansions},\\nauthor={Diana Cai and Chirag Modi and Charles Margossian and Robert M. Gower and David Blei and Lawrence K. Saul},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=thUf6ZBlPp}\\n}'}, 'paperhash': {'value': 'cai|eigenvi_scorebased_variational_inference_with_orthogonal_function_expansions'}},forum = 'thUf6ZBlPp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8022/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8022/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8022/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8022/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'teVxVdy8R2',number = 15870,cdate = 1715764430274,pdate = 1727288109728,odate = 1730873975324,mdate = 1730873975396,tcdate = 1715764430274,tmdate = 1730873975396,ddate = None,content = {'title': {'value': 'Prediction with Action: Visual Policy Learning via Joint Denoising Process'}, 'authors': {'value': ['Yanjiang Guo', 'Yucheng Hu', 'Jianke Zhang', 'Yen-Jen Wang', 'Xiaoyu Chen', 'Chaochao Lu', 'Jianyu Chen']}, 'authorids': {'value': ['~Yanjiang_Guo1', '~Yucheng_Hu1', '~Jianke_Zhang1', '~Yen-Jen_Wang1', '~Xiaoyu_Chen4', '~Chaochao_Lu1', '~Jianyu_Chen1']}, 'keywords': {'value': ['Visual policy learning', 'diffusion', 'image generation']}, 'TLDR': {'value': 'We propose a novel framework to predict images and robot actions through joint dinoising process.'}, 'abstract': {'value': 'Diffusion models have demonstrated remarkable capabilities in image generation tasks, including image editing and video creation, representing a good understanding of the physical world. On the other line, diffusion models have also shown promise in robotic control tasks by denoising actions, known as diffusion policy. Although the diffusion generative model and diffusion policy exhibit distinct capabilities—image prediction and robotic action, respectively—they technically follow similar denoising process. In robotic tasks, the ability to predict future images and generate actions is highly correlated since they share the same underlying dynamics of the physical world. Building on this insight, we introduce \\\\textbf{PAD}, a novel visual policy learning framework that unifies image \\\\textbf{P}rediction and robot \\\\textbf{A}ction within a joint \\\\textbf{D}enoising process.  Specifically, PAD utilizes Diffusion Transformers (DiT) to seamlessly integrate images and robot states, enabling the simultaneous prediction of future images and robot actions. Additionally, PAD supports co-training on both robotic demonstrations and large-scale video datasets and can be easily extended to other robotic modalities, such as depth images. \\nPAD outperforms previous methods, achieving a significant 38.9\\\\% relative improvement on the full Metaworld benchmark, by utilizing a single text-conditioned visual policy within a data-efficient imitation learning setting. Furthermore, PAD demonstrates superior generalization to unseen tasks in real-world robot manipulation settings with 28.0\\\\% success rate increase compared to the strongest baseline. \\nVideos of PAD can be found at https://sites.google.com/view/pad-paper'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/27b4f1716e2587fbc410f94a23ff1e25367e737e.pdf'}, 'supplementary_material': {'value': '/attachment/b56068659dea16ffee93984832caa308416da444.zip'}, '_bibtex': {'value': '@inproceedings{\\nguo2024prediction,\\ntitle={Prediction with Action: Visual Policy Learning via Joint Denoising Process},\\nauthor={Yanjiang Guo and Yucheng Hu and Jianke Zhang and Yen-Jen Wang and Xiaoyu Chen and Chaochao Lu and Jianyu Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=teVxVdy8R2}\\n}'}, 'paperhash': {'value': 'guo|prediction_with_action_visual_policy_learning_via_joint_denoising_process'}},forum = 'teVxVdy8R2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15870/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15870/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15870/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15870/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'te6VagJf6G',number = 21651,cdate = 1715802893137,pdate = 1727288259425,odate = 1730874006522,mdate = 1730874006535,tcdate = 1715802893137,tmdate = 1730874006535,ddate = None,content = {'title': {'value': 'Learning to Reason via Program Generation, Emulation, and Search'}, 'authors': {'value': ['Nathaniel Weir', 'Muhammad Khalifa', 'Linlu Qiu', 'Orion Weller', 'Peter Clark']}, 'authorids': {'value': ['~Nathaniel_Weir1', '~Muhammad_Khalifa2', '~Linlu_Qiu1', '~Orion_Weller1', '~Peter_Clark1']}, 'keywords': {'value': ['language models', 'instruction tuning', 'code generation', 'reasoning', 'program search', 'program emulation']}, 'TLDR': {'value': 'We show that fine-tuning LMs to generate and then emulate the execution of programs creates models can learn new tasks via program search.'}, 'abstract': {'value': 'Program synthesis with language models (LMs) has unlocked a large set of reasoning abilities; code-tuned LMs have proven adept at generating programs that solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word concatenation). However, not all reasoning tasks are easily expressible as code, e.g. tasks involving commonsense reasoning, moral decision-making, and sarcasm understanding. Our goal is to extend a LM’s program synthesis skills to such tasks and evaluate the results via pseudo-programs, namely Python programs where some leaf function calls are left undefined. To that end, we propose, Code Generation and Emulated EXecution (COGEX). COGEX works by (1) training LMs to generate pseudo-programs and (2) teaching them to emulate their generated program’s execution, including those leaf functions, allowing the LM’s knowledge to fill in the execution gaps; and (3) using them to search over many programs to find an optimal one. To adapt the COGEX model to a new task, we introduce a method for performing program search to find a single program whose pseudo-execution yields optimal performance when applied to all the instances of a given dataset. We show that our approach yields large improvements compared to standard in-context learning approaches on a battery of tasks, both algorithmic and soft reasoning. This result thus demonstrates that code synthesis can be applied to a much broader class of problems than previously considered.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3a26df6098d24990525e293f7f00b96ed7ee7344.pdf'}, '_bibtex': {'value': '@inproceedings{\\nweir2024learning,\\ntitle={Learning to Reason via Program Generation, Emulation, and Search},\\nauthor={Nathaniel Weir and Muhammad Khalifa and Linlu Qiu and Orion Weller and Peter Clark},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=te6VagJf6G}\\n}'}, 'paperhash': {'value': 'weir|learning_to_reason_via_program_generation_emulation_and_search'}},forum = 'te6VagJf6G',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21651/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21651/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21651/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21651/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tdZLKY9usl',number = 15303,cdate = 1715758933631,pdate = 1727288094090,odate = 1730873971882,mdate = 1730873971902,tcdate = 1715758933631,tmdate = 1730873971902,ddate = None,content = {'title': {'value': 'Reimagining Mutual Information for Enhanced Defense against Data Leakage in Collaborative Inference'}, 'authors': {'value': ['Lin Duan', 'Jingwei Sun', 'Jinyuan Jia', 'Yiran Chen', 'Maria Gorlatova']}, 'authorids': {'value': ['~Lin_Duan1', '~Jingwei_Sun2', '~Jinyuan_Jia2', '~Yiran_Chen1', '~Maria_Gorlatova1']}, 'keywords': {'value': ['Collaborative inference']}, 'abstract': {'value': \"Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus protecting user's data. Nevertheless, prior research has shown that collaborative inference still results in the exposure of input and predictions from edge devices. To defend against such data leakage in collaborative inference, we introduce InfoScissors, a defense strategy designed to reduce the mutual information between a model's intermediate outcomes and the device's input and predictions. We evaluate our defense on several datasets in the context of diverse attacks. Besides the empirical comparison, we provide a theoretical analysis of the inadequacies of recent defense strategies that also utilize mutual information, particularly focusing on those based on the Variational Information Bottleneck (VIB) approach. We illustrate the superiority of our method and offer a theoretical analysis of it.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e57c7c895e4a68d89ceac1afb6bd7412acb4f12a.pdf'}, 'supplementary_material': {'value': '/attachment/37b2d1f6804c1fc7af9dec4b15d87ed28bb4fb3a.zip'}, '_bibtex': {'value': '@inproceedings{\\nduan2024reimagining,\\ntitle={Reimagining Mutual Information for Enhanced Defense against Data Leakage in Collaborative Inference},\\nauthor={Lin Duan and Jingwei Sun and Jinyuan Jia and Yiran Chen and Maria Gorlatova},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tdZLKY9usl}\\n}'}, 'paperhash': {'value': 'duan|reimagining_mutual_information_for_enhanced_defense_against_data_leakage_in_collaborative_inference'}},forum = 'tdZLKY9usl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15303/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15303/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15303/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15303/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tb1MlJCY5g',number = 502,cdate = 1713941326596,pdate = 1727287638928,odate = 1730873841255,mdate = 1736823561235,tcdate = 1713941326596,tmdate = 1736823561235,ddate = None,content = {'title': {'value': 'KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts'}, 'authors': {'value': ['Jing-Cheng Pang', 'Si-Hang Yang', 'Kaiyuan Li', 'Jiaji Zhang', 'Xiong-Hui Chen', 'Nan Tang', 'Yang Yu']}, 'authorids': {'value': ['~Jing-Cheng_Pang1', '~Si-Hang_Yang1', '~Kaiyuan_Li2', '~Jiaji_Zhang1', '~Xiong-Hui_Chen1', '~Nan_Tang4', '~Yang_Yu5']}, 'keywords': {'value': ['reinforcement learning', 'large language models', 'knowledgeable agents']}, 'abstract': {'value': 'Reinforcement learning (RL) traditionally trains agents using interaction data, which limits their capabilities to the scope of the training data. To create more knowledgeable agents, leveraging knowledge from large language models (LLMs) has shown a promising way. Despite various attempts to combine LLMs with RL, there is commonly a semantic gap between action signals and LLM tokens, which hinders their integration. This paper introduces a novel approach, KALM (Knowledgeable Agents from Language Model Rollouts), to learn knowledgeable agents by bridging this gap. KALM extracts knowledge from LLMs in the form of imaginary rollouts, which agents can learn through offline RL. To overcome the limitation that LLMs are inherently text-based and may be incompatible with numerical environmental data, KALM fine-tunes the LLM to perform bidirectional translation between textual goals and rollouts. This process enables the LLM to understand the environment better, facilitating the generation of meaningful rollouts. Experiments on robotic manipulation tasks demonstrate that KALM allows agents to rephrase complex goals and tackle novel tasks requiring new optimal behaviors. KALM achieves a 46% success rate in completing 1400 various novel goals, significantly outperforming the 26% success rate of baseline methods. Project homepage: https://kalmneurips2024.github.io.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This study investigates developing knowledgeable agents with RL and LLMs, which achieve low-level control and adapt to novel situations.'}, 'pdf': {'value': '/pdf/82a85efda24bbab8b3b9acb9f056b2105ab5be11.pdf'}, '_bibtex': {'value': '@inproceedings{\\npang2024kalm,\\ntitle={{KALM}: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts},\\nauthor={Jing-Cheng Pang and Si-Hang Yang and Kaiyuan Li and Jiaji Zhang and Xiong-Hui Chen and Nan Tang and Yang Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tb1MlJCY5g}\\n}'}, 'paperhash': {'value': 'pang|kalm_knowledgeable_agents_by_offline_reinforcement_learning_from_large_language_model_rollouts'}},forum = 'tb1MlJCY5g',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission502/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission502/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission502/-/Revision', 'NeurIPS.cc/2024/Conference/Submission502/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tacb2bFZcm',number = 3703,cdate = 1715317709368,pdate = 1727287728364,odate = 1730873868840,mdate = 1730873868886,tcdate = 1715317709368,tmdate = 1730873868886,ddate = None,content = {'title': {'value': 'UPS: Unified Projection Sharing for Lightweight Single-Image Super-resolution and Beyond'}, 'authors': {'value': ['Kun Zhou', 'Xinyu Lin', 'Zhonghang LIU', 'Xiaoguang Han', 'Jiangbo Lu']}, 'authorids': {'value': ['~Kun_Zhou3', '~Xinyu_Lin2', '~Zhonghang_LIU1', '~Xiaoguang_Han2', '~Jiangbo_Lu1']}, 'keywords': {'value': ['Lightweight SISR', 'Projection Space']}, 'abstract': {'value': 'To date, transformer-based frameworks have demonstrated impressive results in single-image super-resolution (SISR). However, under practical lightweight scenarios, the complex interaction of deep image feature extraction and similarity modeling limits the performance of these methods, since they require simultaneous layer-specific optimization of both two tasks. In this work, we introduce a novel Unified Projection Sharing algorithm(UPS) to decouple the feature extraction and similarity modeling, achieving notable performance. To do this, we establish a unified projection space defined by a learnable projection matrix, for similarity calculation across all self-attention layers. As a result, deep image feature extraction remains a per-layer optimization manner, while similarity modeling is carried out by projecting these image features onto the shared projection space. Extensive experiments demonstrate that our proposed UPS achieves state-of-the-art performance relative to leading lightweight SISR methods, as verified by various popular benchmarks. Moreover, our unified optimized projection space exhibits encouraging robustness performance for unseen data (degraded and depth images). Finally, UPS also demonstrates promising results across various image restoration tasks, including real-world and classic SISR, image denoising, and image deblocking.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/23b5ecf1afe998ecdd8d69c48a52d8ef2e144490.pdf'}, 'supplementary_material': {'value': '/attachment/c7c7687db5e1b9cab16d91c1b7e3c55e5a680d6d.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024ups,\\ntitle={{UPS}: Unified Projection Sharing for Lightweight Single-Image Super-resolution and Beyond},\\nauthor={Kun Zhou and Xinyu Lin and Zhonghang LIU and Xiaoguang Han and Jiangbo Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tacb2bFZcm}\\n}'}, 'paperhash': {'value': 'zhou|ups_unified_projection_sharing_for_lightweight_singleimage_superresolution_and_beyond'}},forum = 'tacb2bFZcm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3703/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3703/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3703/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3703/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'taI8M5DiXj',number = 16888,cdate = 1715775247886,pdate = 1727288140114,odate = 1730873982194,mdate = 1730873982207,tcdate = 1715775247886,tmdate = 1730873982207,ddate = None,content = {'title': {'value': 'When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding'}, 'authors': {'value': ['Marah Ghoummaid', 'Uri Shalit']}, 'authorids': {'value': ['~Marah_Ghoummaid1', '~Uri_Shalit1']}, 'keywords': {'value': ['policy learning', 'causal inference', 'sensitivity analysis', 'human-algorithm collaboration']}, 'TLDR': {'value': 'Learning a treatment recommendation policy under hidden confounding, with the option of deferring the decision to an expert'}, 'abstract': {'value': 'We consider the task of learning how to act in collaboration with a human expert based on observational data. The task is motivated by high-stake scenarios such as healthcare and welfare where algorithmic action recommendations are made to a human expert, opening the option of deferring making a recommendation in cases where the human might act better on their own.\\n    This task is especially challenging when dealing with observational data, as using such data runs the risk of hidden confounders whose existence can lead to biased and harmful policies. However, unlike standard policy learning, the presence of a human expert can mitigate some of these risks. We build on the work of Mozannar and Sontag (2020) on consistent surrogate loss for learning with the option of deferral to an expert, where they solve a cost-sensitive supervised classification problem. Since we are solving a causal problem, where labels don’t exist, we use a causal model to learn costs which are robust to a bounded degree of hidden confounding.\\n    We prove that our approach can take advantage of the strengths of both the model and the expert to obtain a better policy than either. We demonstrate our results by conducting experiments on synthetic and semi-synthetic data and show the advantages of our method compared to baselines.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/f4989a50707e637d261905f2ed4edcca62ad108d.zip'}, 'pdf': {'value': '/pdf/6fae7b33c6be48b9db0784a050cc4bc4d4f3c1b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nghoummaid2024when,\\ntitle={When to Act and When to Ask: Policy Learning With Deferral Under Hidden Confounding},\\nauthor={Marah Ghoummaid and Uri Shalit},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=taI8M5DiXj}\\n}'}, 'paperhash': {'value': 'ghoummaid|when_to_act_and_when_to_ask_policy_learning_with_deferral_under_hidden_confounding'}},forum = 'taI8M5DiXj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16888/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16888/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16888/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16888/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'tZtepJBtHg',number = 2135,cdate = 1714982487676,pdate = 1727287681949,odate = 1730873855627,mdate = 1730873855650,tcdate = 1714982487676,tmdate = 1730873855650,ddate = None,content = {'title': {'value': 'Transductive Active Learning: Theory and Applications'}, 'authors': {'value': ['Jonas Hübotter', 'Bhavya Sukhija', 'Lenart Treven', 'Yarden As', 'Andreas Krause']}, 'authorids': {'value': ['~Jonas_Hübotter1', '~Bhavya_Sukhija1', '~Lenart_Treven1', '~Yarden_As1', '~Andreas_Krause1']}, 'keywords': {'value': ['active learning', 'experimental design', 'bandits', 'Bayesian optimization', 'neural networks', 'deep learning', 'fine-tuning', 'transfer learning', 'transductive learning', 'generalization', 'extrapolation']}, 'abstract': {'value': 'We study a generalization of classical active learning to real-world settings with concrete prediction targets where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region.\\nWe analyze a family of decision rules that sample adaptively to minimize uncertainty about prediction targets.\\nWe are the first to show, under general regularity assumptions, that such decision rules converge uniformly to the smallest possible uncertainty obtainable from the accessible data.\\nWe demonstrate their strong sample efficiency in two key applications: active fine-tuning of large neural networks and safe Bayesian optimization, where they achieve state-of-the-art performance.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4d2fe62468dee7d8a39e169ecd8c2cfd078a896.pdf'}, 'supplementary_material': {'value': '/attachment/a9eec6b4c7d0fa9e7795b4fdd37068d3aa59aa2f.zip'}, '_bibtex': {'value': '@inproceedings{\\nh{\\\\\"u}botter2024transductive,\\ntitle={Transductive Active Learning: Theory and Applications},\\nauthor={Jonas H{\\\\\"u}botter and Bhavya Sukhija and Lenart Treven and Yarden As and Andreas Krause},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tZtepJBtHg}\\n}'}, 'TLDR': {'value': 'We develop a theory for automatic data selection when you know what you want to learn. We show that knowing what you want a model to learn can be leveraged to learn much more efficiently than just trying to learn \"everything\".'}, 'paperhash': {'value': 'hübotter|transductive_active_learning_theory_and_applications'}},forum = 'tZtepJBtHg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2135/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2135/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2135/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2135/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tZRpvLXevU',number = 15418,cdate = 1715760055235,pdate = 1727288097493,odate = 1730873972846,mdate = 1734594923822,tcdate = 1715760055235,tmdate = 1734594923822,ddate = None,content = {'title': {'value': 'Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks'}, 'authors': {'value': ['Victor Boutin', 'Rishav Mukherji', 'Aditya Agrawal', 'Sabine Muzellec', 'Thomas FEL', 'Thomas Serre', 'Rufin VanRullen']}, 'authorids': {'value': ['~Victor_Boutin2', '~Rishav_Mukherji1', '~Aditya_Agrawal3', '~Sabine_Muzellec1', '~Thomas_FEL1', '~Thomas_Serre1', '~Rufin_VanRullen1']}, 'keywords': {'value': ['Neuroscience', 'Cognitive Science', 'One-Shot Generative Models', 'Latent Diffusion Models', 'Human Machine alignment', 'Human-Machine comparison']}, 'TLDR': {'value': 'Here, we demonstrate that brain-inspired regularizations in latent diffusion models close the gap between humans and machines on the one-shot drawing task.'}, 'abstract': {'value': \"Humans can effortlessly draw new categories from a single exemplar, a feat that has long posed a challenge for generative models. However, this gap has started to close with recent advances in diffusion models. This one-shot drawing task requires powerful inductive biases that have not been systematically investigated. Here, we study how different inductive biases shape the latent space of Latent Diffusion Models (LDMs). Along with standard LDM regularizers (KL and vector quantization), we explore supervised regularizations (including classification and prototype-based representation) and contrastive inductive biases (using  SimCLR and redundancy reduction objectives). We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) -- better mimicking human perception (as evaluated psychophysically). Overall, our results suggest that the gap between humans and machines in one-shot drawings is almost closed.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/260aead53bf9f89aad29e7ba04bfc14fb441a286.pdf'}, '_bibtex': {'value': '@inproceedings{\\nboutin2024latent,\\ntitle={Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks},\\nauthor={Victor Boutin and Rishav Mukherji and Aditya Agrawal and Sabine Muzellec and Thomas FEL and Thomas Serre and Rufin VanRullen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tZRpvLXevU}\\n}'}, 'paperhash': {'value': 'boutin|latent_representation_matters_humanlike_sketches_in_oneshot_drawing_tasks'}},forum = 'tZRpvLXevU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15418/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15418/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15418/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15418/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tYdR1lTWqh',number = 1338,cdate = 1714586259202,pdate = 1727287658131,odate = 1730873847707,mdate = 1730873847726,tcdate = 1714586259202,tmdate = 1730873847726,ddate = None,content = {'title': {'value': 'Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference'}, 'authors': {'value': ['Jiabao Ji', 'Yujian Liu', 'Yang Zhang', 'Gaowen Liu', 'Ramana Rao Kompella', 'Sijia Liu', 'Shiyu Chang']}, 'authorids': {'value': ['~Jiabao_Ji1', '~Yujian_Liu1', '~Yang_Zhang3', '~Gaowen_Liu4', '~Ramana_Rao_Kompella1', '~Sijia_Liu1', '~Shiyu_Chang2']}, 'keywords': {'value': ['Large Language Model', 'LLM Unlearn']}, 'abstract': {'value': 'As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents; and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives – maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM’s overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0e313ac1fca466e62184f6e0baa9b91859992cea.pdf'}, 'supplementary_material': {'value': '/attachment/166810ed19da5e422ecd9984f94e5eb6fe3944ab.zip'}, '_bibtex': {'value': '@inproceedings{\\nji2024reversing,\\ntitle={Reversing the Forget-Retain Objectives: An Efficient {LLM} Unlearning Framework from Logit Difference},\\nauthor={Jiabao Ji and Yujian Liu and Yang Zhang and Gaowen Liu and Ramana Rao Kompella and Sijia Liu and Shiyu Chang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tYdR1lTWqh}\\n}'}, 'paperhash': {'value': 'ji|reversing_the_forgetretain_objectives_an_efficient_llm_unlearning_framework_from_logit_difference'}},forum = 'tYdR1lTWqh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1338/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1338/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1338/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1338/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tWkL7k1u5v',number = 20386,cdate = 1715796093135,pdate = 1727288232565,odate = 1730874001048,mdate = 1730874001069,tcdate = 1715796093135,tmdate = 1730874001069,ddate = None,content = {'title': {'value': 'Improving Equivariant Model Training via Constraint Relaxation'}, 'authors': {'value': ['Stefanos Pertigkiozoglou', 'Evangelos Chatzipantazis', 'Shubhendu Trivedi', 'Kostas Daniilidis']}, 'authorids': {'value': ['~Stefanos_Pertigkiozoglou1', '~Evangelos_Chatzipantazis1', '~Shubhendu_Trivedi2', '~Kostas_Daniilidis1']}, 'keywords': {'value': ['Equivariant Neural Networks', 'Symmetries', 'Approximate Equivariance', 'Optimization']}, 'TLDR': {'value': 'Improving the optimization of equivariant neural networks by relaxing the equivariant constraint during training'}, 'abstract': {'value': \"Equivariant neural networks have been widely used in a variety of applications due to their ability to generalize well in tasks where the underlying data symmetries are known. Despite their successes, such networks can be difficult to optimize and require careful hyperparameter tuning to train successfully. In this work, we propose a novel framework for improving the optimization of such models by relaxing the hard equivariance constraint during training: We relax the equivariance constraint of the network's intermediate layers by introducing an additional non-equivariant term that we progressively constrain until we arrive at an equivariant solution. By controlling the magnitude of the activation of the additional relaxation term, we allow the model to optimize over a larger hypothesis space containing approximate equivariant networks and converge back to an equivariant solution at the end of training. We provide experimental results on different state-of-the-art network architectures, demonstrating how this training framework can result in equivariant models with improved generalization performance. Our code is available at https://github.com/StefanosPert/Equivariant_Optimization_CR\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bc08cca49893134348537df9e6335e1e32619ffc.pdf'}, '_bibtex': {'value': '@inproceedings{\\npertigkiozoglou2024improving,\\ntitle={Improving Equivariant Model Training via Constraint Relaxation},\\nauthor={Stefanos Pertigkiozoglou and Evangelos Chatzipantazis and Shubhendu Trivedi and Kostas Daniilidis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tWkL7k1u5v}\\n}'}, 'paperhash': {'value': 'pertigkiozoglou|improving_equivariant_model_training_via_constraint_relaxation'}},forum = 'tWkL7k1u5v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20386/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20386/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20386/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20386/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'tVConYid20',number = 12566,cdate = 1715724279656,pdate = 1727288010492,odate = 1730873950113,mdate = 1730873950130,tcdate = 1715724279656,tmdate = 1730873950130,ddate = None,content = {'title': {'value': 'FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision'}, 'authors': {'value': ['Jay Shah', 'Ganesh Bikshandi', 'Ying Zhang', 'Vijay Thakkar', 'Pradeep Ramani', 'Tri Dao']}, 'authorids': {'value': ['~Jay_Shah2', '~Ganesh_Bikshandi1', '~Ying_Zhang34', '~Vijay_Thakkar1', '~Pradeep_Ramani1', '~Tri_Dao1']}, 'keywords': {'value': ['attention', 'hardware-aware algorithms', 'H100']}, 'abstract': {'value': 'Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU.\\nWe develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\\\times$ with BF16 reaching up to 840 TFLOPs/s (85\\\\% utilization), and with FP8 reaching 1.3 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\\\times$ lower numerical error than a baseline FP8 attention.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We speed up FlashAttention on modern GPUs (Hopper) with asynchrony and low-precision'}, 'pdf': {'value': '/pdf/b6740309eaf98aa3b013a5269784842f1debc392.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshah2024flashattention,\\ntitle={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},\\nauthor={Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tVConYid20}\\n}'}, 'paperhash': {'value': 'shah|flashattention3_fast_and_accurate_attention_with_asynchrony_and_lowprecision'}},forum = 'tVConYid20',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12566/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12566/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12566/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12566/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tUpcRQNvVM',number = 14619,cdate = 1715751838811,pdate = 1727288074778,odate = 1730873967224,mdate = 1730873967242,tcdate = 1715751838811,tmdate = 1730873967242,ddate = None,content = {'title': {'value': 'Deep Submodular Peripteral Networks'}, 'authors': {'value': ['Gantavya Bhatt', 'Arnav Mohanty Das', 'Jeff Bilmes']}, 'authorids': {'value': ['~Gantavya_Bhatt1', '~Arnav_Mohanty_Das1', '~Jeff_Bilmes1']}, 'keywords': {'value': ['Submodular Optimization', 'Learning Set Functions', 'Experimental Design', 'Streaming Summarization', 'Data subset selection', 'Knowledge Distillation']}, 'TLDR': {'value': 'This work proposes a new expressive parametric family of submodular functions and new graded-pairwise comparison (GPC) loss functions for their learning from expensive teachers.'}, 'abstract': {'value': \"Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition.  Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a GPC-based strategy to connect and then tackle both of the above challenges.  We introduce newly devised GPC-style ``peripteral'' loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, or RHLF preference ranking, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback.  We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function and demonstrate its superiority both for experimental design and online streaming applications.\"}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/172483363c72a00d7bbe1d70ca8e1b45baeb3889.pdf'}, 'supplementary_material': {'value': '/attachment/b9ef153c4555f3b8585d58be39995638189da743.zip'}, '_bibtex': {'value': '@inproceedings{\\nbhatt2024deep,\\ntitle={Deep Submodular Peripteral Networks},\\nauthor={Gantavya Bhatt and Arnav Mohanty Das and Jeff Bilmes},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tUpcRQNvVM}\\n}'}, 'paperhash': {'value': 'bhatt|deep_submodular_peripteral_networks'}},forum = 'tUpcRQNvVM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14619/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14619/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14619/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14619/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tUHABDZP0Q',number = 2700,cdate = 1715149365924,pdate = 1727287697931,odate = 1730873860174,mdate = 1730873860193,tcdate = 1715149365924,tmdate = 1730873860193,ddate = None,content = {'title': {'value': 'Reinforced Cross-Domain Knowledge Distillation on Time Series Data'}, 'authors': {'value': ['QING XU', 'Min Wu', 'Xiaoli Li', 'Kezhi Mao', 'Zhenghua Chen']}, 'authorids': {'value': ['~QING_XU5', '~Min_Wu2', '~Xiaoli_Li1', '~Kezhi_Mao1', '~Zhenghua_Chen2']}, 'keywords': {'value': ['Knowledge distillation', 'Domain adaptation', 'Time series']}, 'abstract': {'value': \"Unsupervised domain adaptation methods have demonstrated superior capabilities in handling the domain shift issue which widely exists in various time series tasks. However, their prominent adaptation performances heavily rely on complex model architectures, posing an unprecedented challenge in deploying them on resource-limited devices for real-time monitoring. Existing approaches, which integrates knowledge distillation into domain adaptation frameworks to simultaneously address domain shift and model complexity, often neglect network capacity gap between teacher and student and just coarsely align their outputs over all source and target samples, resulting in poor distillation efficiency. Thus, in this paper, we propose an innovative framework named Reinforced Cross-Domain Knowledge Distillation (RCD-KD) which can effectively adapt to student's network capability via dynamically selecting suitable target domain samples for knowledge transferring. Particularly, a reinforcement learning-based module with a novel reward function is proposed to learn optimal target sample selection policy based on student's capacity. Meanwhile, a domain discriminator is designed to transfer the domain invariant knowledge. Empirical experimental results and analyses on four public time series datasets demonstrate the effectiveness of our proposed method over other state-of-the-art benchmarks.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/61019d031c791930dc4224fd05e54124ae9c4697.pdf'}, 'supplementary_material': {'value': '/attachment/7933254b42012be2f041fcec929d133726974ca6.zip'}, '_bibtex': {'value': '@inproceedings{\\nxu2024reinforced,\\ntitle={Reinforced Cross-Domain Knowledge Distillation on Time Series Data},\\nauthor={QING XU and Min Wu and Xiaoli Li and Kezhi Mao and Zhenghua Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tUHABDZP0Q}\\n}'}, 'paperhash': {'value': 'xu|reinforced_crossdomain_knowledge_distillation_on_time_series_data'}},forum = 'tUHABDZP0Q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2700/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2700/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2700/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2700/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tTpVHsqTKf',number = 10058,cdate = 1715689746959,pdate = 1727287928600,odate = 1730873925723,mdate = 1730873925737,tcdate = 1715689746959,tmdate = 1730873925737,ddate = None,content = {'title': {'value': 'SyncVIS: Synchronized Video Instance Segmentation'}, 'authors': {'value': ['rongkun Zheng', 'Lu Qi', 'Xi Chen', 'Yi Wang', 'Kun Wang', 'Yu Qiao', 'Hengshuang Zhao']}, 'authorids': {'value': ['~rongkun_Zheng1', '~Lu_Qi1', '~Xi_Chen30', '~Yi_Wang19', '~Kun_Wang8', '~Yu_Qiao1', '~Hengshuang_Zhao2']}, 'keywords': {'value': ['video instance segmentation', 'video-frame synchronization']}, 'TLDR': {'value': 'We propose to conduct synchronized modeling explicitly introduces video-level query embeddings and designs two key modules to synchronize video-level query with frame-level query embeddings'}, 'abstract': {'value': \"Recent DETR-based methods have advanced the development of Video Instance Segmentation (VIS) through transformers' efficiency and capability in modeling spatial and temporal information. Despite harvesting remarkable progress, existing works follow asynchronous designs, which model video sequences via either video-level queries only or adopting query-sensitive cascade structures, resulting in difficulties when handling complex and challenging video scenarios. In this work, we analyze the cause of this phenomenon and the limitations of the current solutions, and propose to conduct synchronized modeling via a new framework named SyncVIS. Specifically, SyncVIS explicitly introduces video-level query embeddings and designs two key modules to synchronize video-level query with frame-level query embeddings: a synchronized video-frame modeling paradigm and a synchronized embedding optimization strategy. The former attempts to promote the mutual learning of frame- and video-level embeddings with each other and the latter divides large video sequences into small clips for easier optimization. Extensive experimental evaluations are conducted on the challenging YouTube-VIS 2019 & 2021 & 2022, and OVIS benchmarks, and SyncVIS achieves state-of-the-art results, which demonstrates the effectiveness and generality of the proposed approach. The code is available at https://github.com/rkzheng99/SyncVIS.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8dabb4fe1f28a068b801955fb703d8b9826f8a1d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024syncvis,\\ntitle={Sync{VIS}: Synchronized Video Instance Segmentation},\\nauthor={rongkun Zheng and Lu Qi and Xi Chen and Yi Wang and Kun Wang and Yu Qiao and Hengshuang Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tTpVHsqTKf}\\n}'}, 'paperhash': {'value': 'zheng|syncvis_synchronized_video_instance_segmentation'}},forum = 'tTpVHsqTKf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10058/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10058/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10058/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10058/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tTnFH7D1h4',number = 3230,cdate = 1715244501208,pdate = 1727287713707,odate = 1730873864397,mdate = 1730873864424,tcdate = 1715244501208,tmdate = 1730873864424,ddate = None,content = {'title': {'value': 'Out-of-Distribution Detection with a Single Unconditional Diffusion Model'}, 'authors': {'value': ['Alvin Heng', 'Alexandre H. Thiery', 'Harold Soh']}, 'authorids': {'value': ['~Alvin_Heng1', '~Alexandre_H._Thiery1', '~Harold_Soh1']}, 'keywords': {'value': ['out-of-distribution detection', 'anomaly detection', 'diffusion model']}, 'TLDR': {'value': 'We propose to perform unsupervised out-of-distribution detection using a single unconditional diffusion model by characterizing properties of the diffusion path.'}, 'abstract': {'value': 'Out-of-distribution (OOD) detection is a critical task in machine learning that seeks to identify abnormal samples. Traditionally, unsupervised methods utilize a deep generative model for OOD detection. However, such approaches require a new model to be trained for each inlier dataset. This paper explores whether a single model can  perform OOD detection across diverse tasks. To that end, we introduce  Diffusion Paths (DiffPath), which  uses a single diffusion model originally trained to perform unconditional generation for OOD detection. We introduce a novel technique of measuring the rate-of-change and curvature of the diffusion paths connecting samples to the standard normal. Extensive experiments show that with a single model, DiffPath is competitive with prior work using individual models on a variety of OOD tasks involving different distributions. Our code is publicly available at https://github.com/clear-nus/diffpath.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/38fb26df19bbe7033e041b9e7356933d0c59aeb7.pdf'}, 'supplementary_material': {'value': '/attachment/5bb473a015627dee5ce956eaeda839a0bbf4f832.zip'}, '_bibtex': {'value': '@inproceedings{\\nheng2024outofdistribution,\\ntitle={Out-of-Distribution Detection with a Single Unconditional Diffusion Model},\\nauthor={Alvin Heng and Alexandre H. Thiery and Harold Soh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tTnFH7D1h4}\\n}'}, 'paperhash': {'value': 'heng|outofdistribution_detection_with_a_single_unconditional_diffusion_model'}},forum = 'tTnFH7D1h4',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3230/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3230/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3230/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3230/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tSWoT8ttkO',number = 16392,cdate = 1715769911325,pdate = 1727288125469,odate = 1730873979080,mdate = 1736842295651,tcdate = 1715769911325,tmdate = 1736842295651,ddate = None,content = {'title': {'value': 'Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate'}, 'authors': {'value': ['Fan-Ming Luo', 'Zuolin Tu', 'Zefang Huang', 'Yang Yu']}, 'authorids': {'value': ['~Fan-Ming_Luo1', '~Zuolin_Tu1', '~Zefang_Huang1', '~Yang_Yu5']}, 'keywords': {'value': ['Reinforcement Learning', 'Recurrent Off-Policy Reinforcement Learning', 'Partially Observable Markov Decision Process']}, 'abstract': {'value': 'Real-world decision-making tasks are usually partially observable Markov decision processes (POMDPs), where the state is not fully observable. Recent progress has demonstrated that recurrent reinforcement learning (RL), which consists of a context encoder based on recurrent neural networks (RNNs) for unobservable state prediction and a multilayer perceptron (MLP) policy for decision making, can mitigate partial observability and serve as a robust baseline for POMDP tasks. However, prior recurrent RL algorithms have faced issues with training instability. In this paper, we find that this instability stems from the autoregressive nature of RNNs, which causes even small changes in RNN parameters to produce large output variations over long trajectories. Therefore, we propose **R**ecurrent Off-policy RL with Context-**E**ncoder-**S**p**e**cific **L**earning Rate (RESeL) to tackle this issue. Specifically, RESeL uses a lower learning rate for context encoder than other MLP layers to ensure the stability of the former while maintaining the training efficiency of the latter. We integrate this technique into existing off-policy RL methods, resulting in the RESeL algorithm. We evaluated RESeL in 18 POMDP tasks, including classic, meta-RL, and credit assignment scenarios, as well as five MDP locomotion tasks. The experiments demonstrate significant improvements in training stability with RESeL. Comparative results show that RESeL achieves notable performance improvements over previous recurrent RL baselines in POMDP tasks, and is competitive with or even surpasses state-of-the-art methods in MDP tasks. Further ablation studies highlight the necessity of applying a distinct learning rate for the context encoder. Code is available at https://github.com/FanmingL/Recurrent-Offpolicy-RL.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/df1ec1f407d2298066a0f1ea391fcbcb9ad49e6c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nluo2024efficient,\\ntitle={Efficient Recurrent Off-Policy {RL} Requires a Context-Encoder-Specific Learning Rate},\\nauthor={Fan-Ming Luo and Zuolin Tu and Zefang Huang and Yang Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tSWoT8ttkO}\\n}'}, 'paperhash': {'value': 'luo|efficient_recurrent_offpolicy_rl_requires_a_contextencoderspecific_learning_rate'}},forum = 'tSWoT8ttkO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16392/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16392/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16392/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16392/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tRRWoa9e80',number = 1763,cdate = 1714795104878,pdate = 1727287670645,odate = 1730873851715,mdate = 1730873851732,tcdate = 1714795104878,tmdate = 1730873851732,ddate = None,content = {'title': {'value': 'Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis'}, 'authors': {'value': ['taihang Hu', 'Linxuan Li', 'Joost van de Weijer', 'Hongcheng Gao', 'Fahad Khan', 'Jian Yang', 'Ming-Ming Cheng', 'Kai Wang', 'Yaxing Wang']}, 'authorids': {'value': ['~taihang_Hu1', '~Linxuan_Li2', '~Joost_van_de_Weijer5', '~Hongcheng_Gao1', '~Fahad_Khan1', '~Jian_Yang1', '~Ming-Ming_Cheng3', '~Kai_Wang7', '~Yaxing_Wang3']}, 'keywords': {'value': ['Diffusion model', 'Attribute binding', 'Text Embedding']}, 'abstract': {'value': 'Although text-to-image (T2I) models exhibit remarkable generation capabilities,\\nthey frequently fail to accurately bind semantically related objects or attributes\\nin the input prompts; a challenge termed semantic binding. Previous approaches\\neither involve intensive fine-tuning of the entire T2I model or require users or\\nlarge language models to specify generation layouts, adding complexity. In this\\npaper, we define semantic binding as the task of associating a given object with its\\nattribute, termed attribute binding, or linking it to other related sub-objects, referred\\nto as object binding. We introduce a novel method called Token Merging (ToMe),\\nwhich enhances semantic binding by aggregating relevant tokens into a single\\ncomposite token. This ensures that the object, its attributes and sub-objects all share\\nthe same cross-attention map. Additionally, to address potential confusion among\\nmain objects with complex textual prompts, we propose end token substitution as\\na complementary strategy. To further refine our approach in the initial stages of\\nT2I generation, where layouts are determined, we incorporate two auxiliary losses,\\nan entropy loss and a semantic binding loss, to iteratively update the composite\\ntoken to improve the generation integrity. We conducted extensive experiments to\\nvalidate the effectiveness of ToMe, comparing it against various existing methods\\non the T2I-CompBench and our proposed GPT-4o object binding benchmark. Our\\nmethod is particularly effective in complex scenarios that involve multiple objects\\nand attributes, which previous methods often fail to address. The code will be\\n publicly available at https://github.com/hutaihang/ToMe'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f83aad7dcfb8bc237df660f4fe4b024935d4b8cb.pdf'}, 'supplementary_material': {'value': '/attachment/1a588a5f93b246e82128b278d13bb0a35780762d.zip'}, '_bibtex': {'value': '@inproceedings{\\nhu2024token,\\ntitle={Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis},\\nauthor={taihang Hu and Linxuan Li and Joost van de Weijer and Hongcheng Gao and Fahad Khan and Jian Yang and Ming-Ming Cheng and Kai Wang and Yaxing Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tRRWoa9e80}\\n}'}, 'paperhash': {'value': 'hu|token_merging_for_trainingfree_semantic_binding_in_texttoimage_synthesis'}},forum = 'tRRWoa9e80',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1763/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1763/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1763/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1763/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tQukGCDaNT',number = 1257,cdate = 1714501856701,pdate = 1727287655901,odate = 1730873846826,mdate = 1737004017406,tcdate = 1714501856701,tmdate = 1737004017406,ddate = None,content = {'title': {'value': 'Improved Distribution Matching Distillation for Fast Image Synthesis'}, 'authors': {'value': ['Tianwei Yin', 'Michaël Gharbi', 'Taesung Park', 'Richard Zhang', 'Eli Shechtman', 'Fredo Durand', 'William T. Freeman']}, 'authorids': {'value': ['~Tianwei_Yin1', '~Michaël_Gharbi1', '~Taesung_Park2', '~Richard_Zhang1', '~Eli_Shechtman3', '~Fredo_Durand1', '~William_T._Freeman1']}, 'keywords': {'value': ['Image Generation', 'diffusion based models', 'model distillation']}, 'abstract': {'value': 'Recent approaches have shown promises distilling expensive diffusion models into efficient one-step generators.\\nAmongst them, Distribution Matching Distillation (DMD) produces one-step generators that match their teacher in distribution, i.e., the distillation process does not enforce a one-to-one correspondence with the sampling trajectories of their teachers.\\nHowever, to ensure stable training in practice, DMD requires an additional regression loss computed using a large set of noise--image pairs, generated by the teacher with many steps of a deterministic sampler.\\nThis is not only computationally expensive for large-scale text-to-image synthesis, but it also limits the student\\'s quality, tying it too closely to the teacher\\'s original sampling paths.\\nWe introduce DMD2, a set of techniques that lift this limitation and improve DMD training.\\nFirst, we eliminate the regression loss and the need for expensive dataset construction.\\nWe show that the resulting instability is due to the \"fake\" critic not estimating the distribution \\nof generated samples with sufficient accuracy and propose a two time-scale update rule as a remedy.\\nSecond, we integrate a GAN loss into the distillation procedure, discriminating between generated samples and real images.\\nThis lets us train the student model on real data, thus mitigating the imperfect \"real\" score estimation from the teacher model, and thereby enhancing quality.\\nThird, we introduce a new training procedure that enables multi-step sampling in the student, and\\naddresses the training--inference input mismatch of previous work, by simulating inference-time generator samples during training. \\nTaken together, our improvements set new benchmarks in one-step image generation, with FID scores of 1.28 on ImageNet-64×64 and 8.35 on zero-shot COCO 2014, surpassing the original teacher despite a 500X reduction in inference cost.\\nFurther, we show our approach can generate megapixel images by distilling SDXL, demonstrating exceptional visual quality among few-step methods, and surpassing the teacher. \\nWe release our code and pretrained models.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We distill diffusion models into few-step generators that produce images with superior quality.'}, 'pdf': {'value': '/pdf/1905628c3311a975f3893addcfe05ba10aa58153.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyin2024improved,\\ntitle={Improved Distribution Matching Distillation for Fast Image Synthesis},\\nauthor={Tianwei Yin and Micha{\\\\\"e}l Gharbi and Taesung Park and Richard Zhang and Eli Shechtman and Fredo Durand and William T. Freeman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tQukGCDaNT}\\n}'}, 'paperhash': {'value': 'yin|improved_distribution_matching_distillation_for_fast_image_synthesis'}},forum = 'tQukGCDaNT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1257/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1257/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1257/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1257/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tPgagXpvcV',number = 15793,cdate = 1715763675870,pdate = 1727288107124,odate = 1730873974672,mdate = 1730873974689,tcdate = 1715763675870,tmdate = 1730873974689,ddate = None,content = {'title': {'value': 'Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss'}, 'authors': {'value': ['Paul KRZAKALA', 'Junjie Yang', 'Rémi Flamary', \"Florence d'Alché-Buc\", 'Charlotte Laclau', 'Matthieu Labeau']}, 'authorids': {'value': ['~Paul_KRZAKALA1', '~Junjie_Yang3', '~Rémi_Flamary1', \"~Florence_d'Alché-Buc2\", '~Charlotte_Laclau2', '~Matthieu_Labeau2']}, 'keywords': {'value': ['Optimal Transport', 'Graph Prediction', 'Structured Prediction', 'Graph', 'Deep Learning']}, 'TLDR': {'value': 'We introduce Any2Graph a framework for deep end-to-end supervised graph prediction. The key components of the framework is PMFGW, an Optimal Transport Loss.'}, 'abstract': {'value': 'We propose Any2graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e. a deep learning model that predicts an entire graph for any kind of input. The framework is built on a novel Optimal Transport loss, the Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary properties (permutation invariance, differentiability and scalability) and is designed to handle any-sized graphs. Numerical experiments showcase the versatility of the approach that outperform existing competitors on a novel challenging synthetic dataset and a variety of real-world tasks such as map construction from satellite image (Sat2Graph) or molecule prediction from fingerprint (Fingerprint2Graph).'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2da8ac1a2d0e07ab727eb467d8d3907addafb2ca.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nkrzakala2024anygraph,\\ntitle={Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss},\\nauthor={Paul KRZAKALA and Junjie Yang and R{\\\\'e}mi Flamary and Florence d'Alch{\\\\'e}-Buc and Charlotte Laclau and Matthieu Labeau},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tPgagXpvcV}\\n}\"}, 'paperhash': {'value': 'krzakala|any2graph_deep_endtoend_supervised_graph_prediction_with_an_optimal_transport_loss'}},forum = 'tPgagXpvcV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15793/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15793/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15793/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15793/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tPdJ2qHkOB',number = 20740,cdate = 1715798114540,pdate = 1727288240510,odate = 1730874003078,mdate = 1730874003096,tcdate = 1715798114540,tmdate = 1730874003096,ddate = None,content = {'title': {'value': 'Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing'}, 'authors': {'value': ['Ye Tian', 'Baolin Peng', 'Linfeng Song', 'Lifeng Jin', 'Dian Yu', 'Lei Han', 'Haitao Mi', 'Dong Yu']}, 'authorids': {'value': ['~Ye_Tian1', '~Baolin_Peng2', '~Linfeng_Song1', '~Lifeng_Jin1', '~Dian_Yu3', '~Lei_Han1', '~Haitao_Mi1', '~Dong_Yu2']}, 'keywords': {'value': ['self-improving', 'search', 'large language models']}, 'abstract': {'value': 'Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b2c3d9c63136264c9f2fbafd2a34aa244f407bc5.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntian2024toward,\\ntitle={Toward Self-Improvement of {LLM}s via Imagination, Searching, and Criticizing},\\nauthor={Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Lei Han and Haitao Mi and Dong Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tPdJ2qHkOB}\\n}'}, 'paperhash': {'value': 'tian|toward_selfimprovement_of_llms_via_imagination_searching_and_criticizing'}},forum = 'tPdJ2qHkOB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20740/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20740/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20740/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20740/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tOXoQPRzPL',number = 1265,cdate = 1714511277706,pdate = 1727287656024,odate = 1730873846986,mdate = 1730873846999,tcdate = 1714511277706,tmdate = 1730873846999,ddate = None,content = {'title': {'value': 'An Image is Worth 32 Tokens for Reconstruction and Generation'}, 'authors': {'value': ['Qihang Yu', 'Mark Weber', 'Xueqing Deng', 'Xiaohui Shen', 'Daniel Cremers', 'Liang-Chieh Chen']}, 'authorids': {'value': ['~Qihang_Yu1', '~Mark_Weber1', '~Xueqing_Deng2', '~Xiaohui_Shen2', '~Daniel_Cremers1', '~Liang-Chieh_Chen1']}, 'keywords': {'value': ['image tokenization', 'image generation']}, 'abstract': {'value': 'Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce **T**ransformer-based 1-D**i**mensional **Tok**enizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a 256 × 256 × 3 image can be reduced to just **32** discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains **1.97** gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet 256 × 256 benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet 512 × 512 benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 vs. 3.04), but also reduces the image tokens by 64×, leading to **410× faster** generation process. Our best-performing variant can significantly surpasses DiT-XL/2 (gFID **2.13** vs. 3.04) while still generating high-quality samples **74× faster**. Codes and models are available at https://github.com/bytedance/1d-tokenizer'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8d26fff4931350ad8c13726dc061ecd5349c8fd6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyu2024an,\\ntitle={An Image is Worth 32 Tokens for Reconstruction and Generation},\\nauthor={Qihang Yu and Mark Weber and Xueqing Deng and Xiaohui Shen and Daniel Cremers and Liang-Chieh Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tOXoQPRzPL}\\n}'}, 'paperhash': {'value': 'yu|an_image_is_worth_32_tokens_for_reconstruction_and_generation'}},forum = 'tOXoQPRzPL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1265/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1265/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1265/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1265/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tNhwg9U767',number = 4790,cdate = 1715460109421,pdate = 1727287760888,odate = 1730873879166,mdate = 1730873879187,tcdate = 1715460109421,tmdate = 1730873879187,ddate = None,content = {'title': {'value': 'Microstructures and Accuracy of Graph Recall by Large Language Models'}, 'authors': {'value': ['Yanbang Wang', 'Hejie Cui', 'Jon Kleinberg']}, 'authorids': {'value': ['~Yanbang_Wang1', '~Hejie_Cui1', '~Jon_Kleinberg3']}, 'keywords': {'value': ['large language models', 'graph recall', 'human cognition', 'sociology', 'network motif']}, 'abstract': {'value': 'Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structural patterns) in their recall. We find that LLMs not only underperform often in graph recall, but also tend to favor more triangles and alternating 2-paths. Moreover, we find that more advanced LLMs have a striking dependence on the domain that a real-world graph comes from --- by yielding the best recall accuracy when the graph is narrated in a language style consistent with its original domain.'}, 'primary_area': {'value': 'machine_learning_for_social_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local subgraph patterns) in the recalled graphs.'}, 'pdf': {'value': '/pdf/9a7fc5c6bae6f8bb957b0be6f8041b144393455e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024microstructures,\\ntitle={Microstructures and Accuracy of Graph Recall by Large Language Models},\\nauthor={Yanbang Wang and Hejie Cui and Jon Kleinberg},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tNhwg9U767}\\n}'}, 'paperhash': {'value': 'wang|microstructures_and_accuracy_of_graph_recall_by_large_language_models'}},forum = 'tNhwg9U767',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4790/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4790/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4790/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4790/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tN0xnYPLt6',number = 13317,cdate = 1715738293821,pdate = 1727288035535,odate = 1730873957150,mdate = 1735894285540,tcdate = 1715738293821,tmdate = 1735894285540,ddate = None,content = {'title': {'value': 'TinyLUT: Tiny Look-Up Table for Efficient Image Restoration at the Edge'}, 'authors': {'value': ['Huanan LI', 'Juntao Guan', 'Lai Rui', 'Sijun Ma', 'Lin Gu', 'Zhangming Zhu']}, 'authorids': {'value': ['~Huanan_LI1', '~Juntao_Guan1', '~Lai_Rui1', '~Sijun_Ma1', '~Lin_Gu4', '~Zhangming_Zhu1']}, 'keywords': {'value': ['Image restoration', 'Deep Learning', 'Look-up Table', 'Super Resolution', 'Image Denoising']}, 'abstract': {'value': 'Look-up tables(LUTs)-based methods have recently shown enormous potential in image restoration tasks, which are capable of significantly accelerating the inference. However, the size of LUT exhibits exponential growth with the convolution kernel size, creating a storage bottleneck for its broader application on edge devices. Here, we address the storage explosion challenge to promote the capacity of mapping the complex CNN models by LUT. We introduce an innovative separable mapping strategy to achieve over $7\\\\times$ storage reduction, transforming the storage from exponential dependence on kernel size to a linear relationship. Moreover, we design a dynamic discretization mechanism to decompose the activation and compress the quantization scale that further shrinks the LUT storage by $4.48\\\\times$. As a result, the storage requirement of our proposed TinyLUT is around 4.1\\\\% of MuLUT-SDY-X2 and amenable to on-chip cache, yielding competitive accuracy with over $5\\\\times$ lower inference latency on Raspberry 4B than FSRCNN. Our proposed TinyLUT enables superior inference speed on edge devices with new state-of-the-art accuracy on both of image super-resolution and denoising, showcasing the potential of applying this method to various image restoration tasks at the edge. The codes are available at: https://github.com/Jonas-KD/TinyLUT.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1df5ba9d709ef47fd00e399db6114e07034d4339.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nli2024tinylut,\\ntitle={Tiny{LUT}: Tiny Look-Up Table for Efficient Image Restoration at the Edge},\\nauthor={Huanan LI and Juntao Guan and Lai Rui and Sijun Ma and Lin Gu and Zhangming Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tN0xnYPLt6}\\n}'}, 'paperhash': {'value': 'li|tinylut_tiny_lookup_table_for_efficient_image_restoration_at_the_edge'}},forum = 'tN0xnYPLt6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13317/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13317/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13317/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13317/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tLXgzQ5WZl',number = 1766,cdate = 1714797420028,pdate = 1727287670891,odate = 1730873851832,mdate = 1731582666838,tcdate = 1714797420028,tmdate = 1731582666838,ddate = None,content = {'title': {'value': 'SCube: Instant Large-Scale Scene Reconstruction using VoxSplats'}, 'authors': {'value': ['Xuanchi Ren', 'Yifan Lu', 'hanxue liang', 'Jay Zhangjie Wu', 'Huan Ling', 'Mike Chen', 'Sanja Fidler', 'Francis Williams', 'Jiahui Huang']}, 'authorids': {'value': ['~Xuanchi_Ren1', '~Yifan_Lu1', '~hanxue_liang1', '~Jay_Zhangjie_Wu1', '~Huan_Ling1', '~Mike_Chen5', '~Sanja_Fidler1', '~Francis_Williams1', '~Jiahui_Huang3']}, 'keywords': {'value': ['Large-scale Scene Reconstruction', 'Sparse-view Reconstruction', 'Sparse Voxels', 'Gaussian Splatting', 'Diffusion Models']}, 'abstract': {'value': 'We present SCube, a novel method for reconstructing large-scale 3D scenes (geometry, appearance, and semantics) from a sparse set of posed images. Our method encodes reconstructed scenes using a novel representation VoxSplat, which is a set of 3D Gaussians supported on a high-resolution sparse-voxel scaffold. To reconstruct a VoxSplat from images, we employ a hierarchical voxel latent diffusion model conditioned on the input images followed by a feedforward appearance prediction model. The diffusion model generates high-resolution grids progressively in a coarse-to-fine manner, and the appearance network predicts a set of Gaussians within each voxel. From as few as 3 non-overlapping input images, SCube can generate millions of Gaussians with a 10243 voxel grid spanning hundreds of meters in 20 seconds. Past works tackling scene reconstruction from images either rely on per-scene optimization and fail to reconstruct the scene away from input views (thus requiring dense view coverage as input) or leverage geometric priors based on low-resolution models, which produce blurry results. In contrast, SCube leverages high-resolution sparse networks and produces sharp outputs from few views. We show the superiority of SCube compared to prior art using the Waymo self-driving dataset on 3D reconstruction and demonstrate its applications, such as LiDAR simulation and text-to-scene generation.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'SCube reconstructs large-scale 3D scenes represented with voxel-supported Gasussian splats from non-overlapping sparse views in 20 seconds.'}, 'pdf': {'value': '/pdf/92cbfcd4a6ff12dcf1feabe0149772877fc6b95d.pdf'}, 'supplementary_material': {'value': '/attachment/05f2e8f0ce84d13f4cbd95968336bafdd7d24f09.zip'}, '_bibtex': {'value': '@inproceedings{\\nren2024scube,\\ntitle={{SC}ube: Instant Large-Scale Scene Reconstruction using VoxSplats},\\nauthor={Xuanchi Ren and Yifan Lu and hanxue liang and Jay Zhangjie Wu and Huan Ling and Mike Chen and Sanja Fidler and Francis Williams and Jiahui Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tLXgzQ5WZl}\\n}'}, 'paperhash': {'value': 'ren|scube_instant_largescale_scene_reconstruction_using_voxsplats'}},forum = 'tLXgzQ5WZl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1766/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1766/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1766/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1766/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tLWoxftJVh',number = 14381,cdate = 1715749338773,pdate = 1727288067799,odate = 1730873965435,mdate = 1730873965461,tcdate = 1715749338773,tmdate = 1730873965461,ddate = None,content = {'title': {'value': 'Consistency Purification: Effective and Efficient Diffusion Purification towards Certified Robustness'}, 'authors': {'value': ['Yiquan Li', 'Zhongzhu Chen', 'Kun Jin', 'Jiongxiao Wang', 'Jiachen Lei', 'Bo Li', 'Chaowei Xiao']}, 'authorids': {'value': ['~Yiquan_Li2', '~Zhongzhu_Chen1', '~Kun_Jin1', '~Jiongxiao_Wang1', '~Jiachen_Lei1', '~Bo_Li19', '~Chaowei_Xiao2']}, 'keywords': {'value': ['Consistency Model', 'Diffusion Purification', 'Certified Robustness', 'Randomized Smoothing']}, 'abstract': {'value': 'Diffusion Purification, purifying noised images with diffusion models, has been widely used for enhancing certified robustness via randomized smoothing. However, existing frameworks often grapple with the balance between efficiency and effectiveness. While the Denoising Diffusion Probabilistic Model (DDPM) offers an efficient single-step purification, it falls short in ensuring purified images reside on the data manifold. Conversely, the Stochastic Diffusion Model effectively places purified images on the data manifold but demands solving cumbersome stochastic differential equations, while its derivative, the Probability Flow Ordinary Differential Equation (PF-ODE), though solving simpler ordinary differential equations, still requires multiple computational steps. In this work, we demonstrated that an ideal purification pipeline should generate the purified images on the data manifold that are as much semantically aligned to the original images for effectiveness in one step for efficiency. Therefore, we introduced Consistency Purification, an efficiency-effectiveness Pareto superior purifier compared to the previous work. Consistency Purification employs the consistency model, a one-step generative model distilled from PF-ODE, thus can generate on-manifold purified images with a single network evaluation. However, the consistency model is designed not for purification thus it does not inherently ensure semantic alignment between purified and original images. To resolve this issue, we further refine it through Consistency Fine-tuning with LPIPS loss, which enables more aligned semantic meaning while keeping the purified images on data manifold. Our comprehensive experiments demonstrate that our Consistency Purification framework achieves state-of-the-art certified robustness and efficiency compared to baseline methods.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d7c8211f8cf4d2c17a20f9438490973b05cef931.pdf'}, 'supplementary_material': {'value': '/attachment/e2eb7ed57ea8a403fde6e7d7a94d7101c5f41ad4.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024consistency,\\ntitle={Consistency Purification: Effective and Efficient Diffusion Purification towards Certified Robustness},\\nauthor={Yiquan Li and Zhongzhu Chen and Kun Jin and Jiongxiao Wang and Jiachen Lei and Bo Li and Chaowei Xiao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tLWoxftJVh}\\n}'}, 'paperhash': {'value': 'li|consistency_purification_effective_and_efficient_diffusion_purification_towards_certified_robustness'}},forum = 'tLWoxftJVh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14381/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14381/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14381/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission14381/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tKuLgnDWWN',number = 3804,cdate = 1715327210146,pdate = 1727287731348,odate = 1730873869670,mdate = 1735544221430,tcdate = 1715327210146,tmdate = 1735544221430,ddate = None,content = {'title': {'value': 'SILENCE: Protecting privacy in offloaded speech understanding on resource-constrained devices'}, 'authors': {'value': ['DONGQI CAI', 'Shangguang Wang', 'Zeling Zhang', 'Felix Xiaozhu Lin', 'Mengwei Xu']}, 'authorids': {'value': ['~DONGQI_CAI2', '~Shangguang_Wang1', '~Zeling_Zhang1', '~Felix_Xiaozhu_Lin1', '~Mengwei_Xu1']}, 'keywords': {'value': ['spoken language understanding', 'resource-constrained devices', 'privacy-preserving']}, 'TLDR': {'value': 'We provide a lightweight, privacy-preserving encoder that can be efficiently embedded into low-power audio devices.'}, 'abstract': {'value': 'Speech serves as a ubiquitous input interface for embedded mobile devices. \\nCloud-based solutions, while offering powerful speech understanding services, raise significant concerns regarding user privacy. \\nTo address this, disentanglement-based encoders have been proposed to remove sensitive information from speech signals without compromising the speech understanding functionality. \\nHowever, these encoders demand high memory usage and computation complexity, making them impractical for resource-constrained wimpy devices.\\nOur solution is based on a key observation that speech understanding hinges on long-term dependency knowledge of the entire utterance, in contrast to privacy-sensitive elements that are short-term dependent. \\nExploiting this observation, we propose SILENCE, a lightweight system that selectively obscuring short-term details, without damaging the long-term dependent speech understanding performance.\\nThe crucial part of SILENCE is a differential mask generator derived from interpretable learning to \\nautomatically configure the masking process.\\nWe have implemented SILENCE on the STM32H7 microcontroller and evaluate its efficacy under different attacking scenarios. \\nOur results demonstrate that SILENCE offers speech understanding performance and privacy protection capacity comparable to existing encoders, while achieving up to 53.3$\\\\times$ speedup and 134.1$\\\\times$ reduction in memory footprint.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f916b7a4e107eaa97efdcf615a87e1d4a63f8d66.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncai2024silence,\\ntitle={{SILENCE}: Protecting privacy in offloaded speech understanding on resource-constrained devices},\\nauthor={DONGQI CAI and Shangguang Wang and Zeling Zhang and Felix Xiaozhu Lin and Mengwei Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tKuLgnDWWN}\\n}'}, 'supplementary_material': {'value': '/attachment/91e88b5077583bc26f3acf862b95d11ae3ca2875.zip'}, 'paperhash': {'value': 'cai|silence_protecting_privacy_in_offloaded_speech_understanding_on_resourceconstrained_devices'}},forum = 'tKuLgnDWWN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3804/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3804/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3804/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3804/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tJGX7tpGO8',number = 4383,cdate = 1715409772560,pdate = 1727287749014,odate = 1730873875499,mdate = 1730873875512,tcdate = 1715409772560,tmdate = 1730873875512,ddate = None,content = {'title': {'value': 'What Matters in Graph Class Incremental Learning? An Information Preservation Perspective'}, 'authors': {'value': ['Jialu Li', 'Yu Wang', 'Pengfei Zhu', 'Wanyu Lin', 'Qinghua Hu']}, 'authorids': {'value': ['~Jialu_Li4', '~Yu_Wang33', '~Pengfei_Zhu1', '~Wanyu_Lin1', '~Qinghua_Hu1']}, 'keywords': {'value': ['Graph neural network', 'Class incremental learning']}, 'abstract': {'value': 'Graph class incremental learning (GCIL) requires the model to classify emerging nodes of new classes while remembering old classes. Existing methods are designed to preserve effective information of old models or graph data to alleviate forgetting, but there is no clear theoretical understanding of what matters in information preservation. In this paper, we consider that present practice suffers from high semantic and structural shifts assessed by two devised shift metrics. We provide insights into information preservation in GCIL and find that maintaining graph information can preserve information of old models in theory to calibrate node semantic and graph structure shifts. We correspond graph information into low-frequency local-global information and high-frequency information in spatial domain. Based on the analysis, we propose a framework, Graph Spatial Information Preservation (GSIP). Specifically, for low-frequency information preservation, the old node representations obtained by inputting replayed nodes into the old model are aligned with the outputs of the node and its neighbors in the new model, and then old and new outputs are globally matched after pooling. For high-frequency information preservation, the new node representations are encouraged to imitate the near-neighbor pair similarity of old node representations. GSIP achieves a 10\\\\% increase in terms of the forgetting metric compared to prior methods on large-scale datasets. Our framework can also seamlessly integrate existing replay designs. The code is available through https://github.com/Jillian555/GSIP.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f16531b97e4d2d6593f7273b8e6bb7292070ff71.pdf'}, 'supplementary_material': {'value': '/attachment/17d6496e4ecda764de12eb5e35ee9c8d1ec37188.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024what,\\ntitle={What Matters in Graph Class Incremental Learning? An Information Preservation Perspective},\\nauthor={Jialu Li and Yu Wang and Pengfei Zhu and Wanyu Lin and Qinghua Hu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tJGX7tpGO8}\\n}'}, 'paperhash': {'value': 'li|what_matters_in_graph_class_incremental_learning_an_information_preservation_perspective'}},forum = 'tJGX7tpGO8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4383/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4383/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4383/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4383/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tIzW3l2uaN',number = 14653,cdate = 1715752251343,pdate = 1727288075730,odate = 1730873967535,mdate = 1736931605410,tcdate = 1715752251343,tmdate = 1736931605410,ddate = None,content = {'title': {'value': 'One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection'}, 'authors': {'value': ['Yiyue Li', 'Shaoting Zhang', 'Kang Li', 'Qicheng Lao']}, 'authorids': {'value': ['~Yiyue_Li1', '~Shaoting_Zhang4', '~Kang_Li9', '~Qicheng_Lao2']}, 'keywords': {'value': ['Few-shot anomaly detection', 'Diffusion models', 'Image personalization']}, 'abstract': {'value': \"Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query image's features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domains—an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6b5458fb6e6670cbf2bbd4ae0055cecc96ff8043.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024onetonormal,\\ntitle={One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection},\\nauthor={Yiyue Li and Shaoting Zhang and Kang Li and Qicheng Lao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tIzW3l2uaN}\\n}'}, 'paperhash': {'value': 'li|onetonormal_anomaly_personalization_for_fewshot_anomaly_detection'}},forum = 'tIzW3l2uaN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14653/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14653/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14653/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14653/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tGozvLTDY3',number = 4587,cdate = 1715430682877,pdate = 1727287754796,odate = 1730873877314,mdate = 1730873877333,tcdate = 1715430682877,tmdate = 1730873877333,ddate = None,content = {'title': {'value': 'DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization'}, 'authors': {'value': ['Yueming Xu', 'Haochen Jiang', 'Zhongyang Xiao', 'Jianfeng Feng', 'Li Zhang']}, 'authorids': {'value': ['~Yueming_Xu1', '~Haochen_Jiang1', '~Zhongyang_Xiao1', '~Jianfeng_Feng2', '~Li_Zhang5']}, 'keywords': {'value': ['SLAM', 'Deep learning method', 'Gaussian splatting', 'Hybrid pose estimation', 'Dynamic environment']}, 'abstract': {'value': 'Achieving robust and precise pose estimation in dynamic scenes is a significant research challenge in Visual Simultaneous Localization and Mapping (SLAM). Recent advancements integrating Gaussian Splatting into SLAM systems have proven effective in creating high-quality renderings using explicit 3D Gaussian models, significantly improving environmental reconstruction fidelity. However, these approaches depend on a static environment assumption and face challenges in dynamic environments due to inconsistent observations of geometry and photometry. To address this problem, we propose DG-SLAM, the first robust dynamic visual SLAM system grounded in 3D Gaussians, which provides precise camera pose estimation alongside high-fidelity reconstructions. Specifically, we propose effective strategies, including motion mask generation, adaptive Gaussian point management, and a hybrid camera tracking algorithm to improve the accuracy and robustness of pose estimation. Extensive experiments demonstrate that DG-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and novel-view synthesis in dynamic scenes, outperforming existing methods meanwhile preserving real-time rendering ability.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/003a4a26d3ccd1f63f63eb5a3f55b8f83ce8f36b.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nxu2024dgslam,\\ntitle={{DG}-{SLAM}: Robust Dynamic Gaussian Splatting {SLAM} with Hybrid Pose Optimization},\\nauthor={Yueming Xu and Haochen Jiang and Zhongyang Xiao and Jianfeng Feng and Li Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tGozvLTDY3}\\n}'}, 'paperhash': {'value': 'xu|dgslam_robust_dynamic_gaussian_splatting_slam_with_hybrid_pose_optimization'}},forum = 'tGozvLTDY3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4587/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4587/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4587/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission4587/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tGDUDKirAy',number = 4835,cdate = 1715466961766,pdate = 1727287762410,odate = 1730873879477,mdate = 1737006298429,tcdate = 1715466961766,tmdate = 1737006298429,ddate = None,content = {'title': {'value': 'Verified Safe Reinforcement Learning  for Neural Network Dynamic Models'}, 'authors': {'value': ['Junlin Wu', 'Huan Zhang', 'Yevgeniy Vorobeychik']}, 'authorids': {'value': ['~Junlin_Wu2', '~Huan_Zhang1', '~Yevgeniy_Vorobeychik1']}, 'keywords': {'value': ['neural network', 'formal verification', 'safe reinforcement learning']}, 'abstract': {'value': 'Learning reliably safe autonomous control is one of the core problems in trustworthy autonomy. However, training a controller that can be formally verified to be safe remains a major challenge. We introduce a novel approach for learning verified safe control policies in nonlinear neural dynamical systems while maximizing overall performance. Our approach aims to achieve safety in the sense of finite-horizon reachability proofs, and is comprised of three key parts. The first is a novel curriculum learning scheme that iteratively increases the verified safe horizon. The second leverages the iterative nature of gradient-based learning to leverage incremental verification, reusing information from prior verification runs. Finally, we learn multiple verified initial-state-dependent controllers, an idea that is especially valuable for more complex domains where learning a single universal verified safe controller is extremely challenging. Our experiments on five safe control problems demonstrate that our trained controllers can achieve verified safety over horizons that are as much as an order of magnitude longer than state-of-the-art baselines, while maintaining high reward, as well as a perfect safety record over entire episodes. Our code is available at https://github.com/jlwu002/VSRL.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/80e30450b128fb51039b52fb98607fc612b1c4bf.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024verified,\\ntitle={Verified Safe Reinforcement Learning  for Neural Network Dynamic Models},\\nauthor={Junlin Wu and Huan Zhang and Yevgeniy Vorobeychik},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tGDUDKirAy}\\n}'}, 'paperhash': {'value': 'wu|verified_safe_reinforcement_learning_for_neural_network_dynamic_models'}},forum = 'tGDUDKirAy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4835/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4835/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4835/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4835/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tFB5SsabVb',number = 8746,cdate = 1715668781396,pdate = 1727287889704,odate = 1730873914410,mdate = 1730873914428,tcdate = 1715668781396,tmdate = 1730873914428,ddate = None,content = {'title': {'value': 'Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series'}, 'authors': {'value': ['Giangiacomo Mercatali', 'Andre Freitas', 'Jie Chen']}, 'authorids': {'value': ['~Giangiacomo_Mercatali1', '~Andre_Freitas1', '~Jie_Chen1']}, 'keywords': {'value': ['Graph learning', 'neural flows', 'time series']}, 'abstract': {'value': 'Interacting systems are prevalent in nature. It is challenging to accurately predict the dynamics of the system if its constituent components are analyzed independently. We develop a graph-based model that unveils the systemic interactions of time series observed at irregular time points, by using a directed acyclic graph to model the conditional dependencies (a form of causal notation) of the system components and learning this graph in tandem with a continuous-time model that parameterizes the solution curves of ordinary differential equations (ODEs). Our technique, a graph neural flow, leads to substantial enhancements over non-graph-based methods, as well as graph-based methods without the modeling of conditional dependencies. We validate our approach on several tasks, including time series classification and forecasting, to demonstrate its efficacy.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fed6b3dc2dd66de999874c711e0ab177f0c6f469.pdf'}, 'TLDR': {'value': 'A graph-based continuous-time model is proposed to unveil systemic interactions and improve time series tasks such as classification and forecasting.'}, '_bibtex': {'value': '@inproceedings{\\nmercatali2024graph,\\ntitle={Graph Neural Flows for Unveiling Systemic Interactions Among Irregularly Sampled Time Series},\\nauthor={Giangiacomo Mercatali and Andre Freitas and Jie Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tFB5SsabVb}\\n}'}, 'paperhash': {'value': 'mercatali|graph_neural_flows_for_unveiling_systemic_interactions_among_irregularly_sampled_time_series'}},forum = 'tFB5SsabVb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8746/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8746/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8746/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8746/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tEEpVPDaRf',number = 3942,cdate = 1715341872680,pdate = 1727287735222,odate = 1730873870607,mdate = 1730873870624,tcdate = 1715341872680,tmdate = 1730873870624,ddate = None,content = {'title': {'value': 'Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models'}, 'authors': {'value': ['Sangwon Jang', 'Jaehyeong Jo', 'Kimin Lee', 'Sung Ju Hwang']}, 'authorids': {'value': ['~Sangwon_Jang1', '~Jaehyeong_Jo1', '~Kimin_Lee1', '~Sung_Ju_Hwang1']}, 'keywords': {'value': ['Text-to-Image Diffusion Models', 'Multi-subject personalization']}, 'TLDR': {'value': 'MuDI enables multi-subject personalization by effectively decoupling identities from multiple subjects.'}, 'abstract': {'value': 'Text-to-image diffusion models have shown remarkable success in generating personalized subjects based on a few reference images. However, current methods often fail when generating multiple subjects simultaneously, resulting in mixed\\nidentities with combined attributes from different subjects. In this work, we present MuDI, a novel framework that enables multi-subject personalization by effectively decoupling identities from multiple subjects. Our main idea is to utilize segmented subjects generated by a foundation model for segmentation (Segment Anything) for both training and inference, as a form of data augmentation for training and initialization for the generation process. Moreover, we further introduce a new metric to better evaluate the performance of our method on multi-subject personalization. Experimental results show that our MuDI can produce high-quality personalized images without identity mixing, even for highly similar subjects as shown in Figure 1. Specifically, in human evaluation, MuDI obtains twice the success rate for personalizing multiple subjects without identity mixing over existing baselines and is preferred over 70% against the strongest baseline.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/76a3c0edf9676a9a55092b29e47710fc0b9fcc5d.pdf'}, 'supplementary_material': {'value': '/attachment/8395b1483dd930c45db620562d3894ead51062fc.zip'}, '_bibtex': {'value': '@inproceedings{\\njang2024identity,\\ntitle={Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models},\\nauthor={Sangwon Jang and Jaehyeong Jo and Kimin Lee and Sung Ju Hwang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tEEpVPDaRf}\\n}'}, 'paperhash': {'value': 'jang|identity_decoupling_for_multisubject_personalization_of_texttoimage_models'}},forum = 'tEEpVPDaRf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3942/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3942/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3942/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3942/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'tDvFa5OJyS',number = 12248,cdate = 1715719136158,pdate = 1727287999956,odate = 1730873947318,mdate = 1736871309022,tcdate = 1715719136158,tmdate = 1736871309022,ddate = None,content = {'title': {'value': 'Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference'}, 'authors': {'value': ['Jonathan Wenger', 'Kaiwen Wu', 'Philipp Hennig', 'Jacob R. Gardner', 'Geoff Pleiss', 'John Patrick Cunningham']}, 'authorids': {'value': ['~Jonathan_Wenger1', '~Kaiwen_Wu2', '~Philipp_Hennig1', '~Jacob_R._Gardner1', '~Geoff_Pleiss1', '~John_Patrick_Cunningham1']}, 'keywords': {'value': ['Gaussian Processes', 'Model Selection', 'Approximate Inference', 'Variational Inference', 'Probabilistic Numerics']}, 'TLDR': {'value': 'We demonstrate how to perform model selection for computation-aware Gaussian processes enabling training on over a million data points on a single GPU.'}, 'abstract': {'value': 'Model selection in Gaussian processes scales prohibitively with the size of the training dataset, both in time and memory.\\nWhile many approximations exist, all incur inevitable approximation error.\\nRecent work accounts for this error in the form of computational uncertainty, which enables---at the cost of quadratic complexity---an explicit tradeoff between computational efficiency and precision.\\nHere we extend this development to model selection, which requires significant enhancements to the existing approach, including linear-time scaling in the size of the dataset.\\nWe propose a novel training loss for hyperparameter optimization and demonstrate empirically that the resulting method can outperform SGPR, CGGP and SVGP, state-of-the-art methods for GP model selection, on medium to large-scale datasets.\\nOur experiments show that model selection for computation-aware GPs trained on 1.8 million data points can be done within a few hours on a single GPU.\\nAs a result of this work, Gaussian processes can be trained on large-scale datasets without significantly compromising their ability to quantify uncertainty---a fundamental prerequisite for optimal decision-making.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ecb0fb74bb68be860850bbec1e4e3168232d0d24.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwenger2024computationaware,\\ntitle={Computation-Aware Gaussian Processes: Model Selection And Linear-Time Inference},\\nauthor={Jonathan Wenger and Kaiwen Wu and Philipp Hennig and Jacob R. Gardner and Geoff Pleiss and John Patrick Cunningham},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tDvFa5OJyS}\\n}'}, 'paperhash': {'value': 'wenger|computationaware_gaussian_processes_model_selection_and_lineartime_inference'}},forum = 'tDvFa5OJyS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12248/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12248/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12248/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12248/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tDMTwto6jv',number = 12433,cdate = 1715722015196,pdate = 1727288006461,odate = 1730873949033,mdate = 1736843269295,tcdate = 1715722015196,tmdate = 1736843269295,ddate = None,content = {'title': {'value': 'SEL-BALD: Deep Bayesian Active Learning with Selective Labels'}, 'authors': {'value': ['Ruijiang Gao', 'Mingzhang Yin', 'Maytal Saar-Tsechansky']}, 'authorids': {'value': ['~Ruijiang_Gao2', '~Mingzhang_Yin1', '~Maytal_Saar-Tsechansky1']}, 'keywords': {'value': ['Bayesian Active Learning with Disagreement; Selective Labels;']}, 'abstract': {'value': 'Machine learning systems are widely used in many high-stakes contexts in which experimental designs for assigning treatments are infeasible. When evaluating decisions is costly, such as investigating fraud cases, or evaluating biopsy decisions, a sample-efficient strategy is needed. However, while existing active learning methods assume humans will always label the instances selected by the machine learning model, in many critical applications, humans may decline to label  instances selected by the machine learning model due to reasons such as regulation constraint, domain knowledge, or algorithmic aversion, thus not sample efficient. \\nIn this paper, we study the Active Learning with Instance Rejection (ALIR) problem, which considers the human discretion behavior for high-stakes decision making problems. We propose new active learning algorithms under deep bayesian active learning for selective labeling (SEL-BALD) to address the ALIR problem. Our algorithms consider how to acquire information for both the machine learning model and the human discretion model. We conduct experiments on both synthetic and real-world datasets to demonstrate the effectiveness of our proposed algorithms.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose novel methods for active learning under the selective labels problem.'}, 'pdf': {'value': '/pdf/ea44cb24d4d45cbc9e4fd0819527e6c39cb4b1d4.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngao2024selbald,\\ntitle={{SEL}-{BALD}: Deep Bayesian Active Learning for Selective Labeling with Instance Rejection},\\nauthor={Ruijiang Gao and Mingzhang Yin and Maytal Saar-Tsechansky},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tDMTwto6jv}\\n}'}, 'paperhash': {'value': 'gao|selbald_deep_bayesian_active_learning_with_selective_labels'}},forum = 'tDMTwto6jv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12433/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12433/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12433/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12433/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tCf7S75xFa',number = 12520,cdate = 1715723409601,pdate = 1727288009011,odate = 1730873949739,mdate = 1736854711252,tcdate = 1715723409601,tmdate = 1736854711252,ddate = None,content = {'title': {'value': 'Physics-Informed Variational State-Space Gaussian Processes'}, 'authors': {'value': ['Oliver Hamelijnck', 'Arno Solin', 'Theodoros Damoulas']}, 'authorids': {'value': ['~Oliver_Hamelijnck1', '~Arno_Solin1', '~Theodoros_Damoulas1']}, 'keywords': {'value': ['gaussian processes', 'variational approximations', 'state space gaussian processes', 'physics informed gaussian processes']}, 'abstract': {'value': 'Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4424cd43913ea9035300e356e962178e273d1e61.pdf'}, 'supplementary_material': {'value': '/attachment/9461298fce1715a838e9758280749bfe546e0f0a.zip'}, '_bibtex': {'value': '@inproceedings{\\nhamelijnck2024physicsinformed,\\ntitle={Physics-Informed Variational State-Space Gaussian Processes},\\nauthor={Oliver Hamelijnck and Arno Solin and Theodoros Damoulas},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tCf7S75xFa}\\n}'}, 'paperhash': {'value': 'hamelijnck|physicsinformed_variational_statespace_gaussian_processes'}},forum = 'tCf7S75xFa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12520/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12520/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12520/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12520/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tBRNC6YemY',number = 21559,cdate = 1715802493278,pdate = 1727288257902,odate = 1730874006201,mdate = 1730874006214,tcdate = 1715802493278,tmdate = 1730874006214,ddate = None,content = {'title': {'value': 'Gorilla: Large Language Model Connected with Massive APIs'}, 'authors': {'value': ['Shishir G Patil', 'Tianjun Zhang', 'Xin Wang', 'Joseph E. Gonzalez']}, 'authorids': {'value': ['~Shishir_G_Patil1', '~Tianjun_Zhang1', '~Xin_Wang1', '~Joseph_E._Gonzalez1']}, 'keywords': {'value': ['LLM', 'Tool Use', 'APIs', 'Function Calling']}, 'abstract': {'value': 'Large Language Models (LLMs) have seen an impressive wave of advances, with\\nmodels now excelling in a variety of tasks, such as mathematical reasoning and\\nprogram synthesis. However, their potential to effectively use tools via API calls\\nremains unfulfilled. This is a challenging task even for today’s state-of-the-art\\nLLMs such as GPT-4 largely due to their unawareness of what APIs are available\\nand how to use them in a frequently updated tool set. We develop Gorilla, a\\nfinetuned LLaMA model that surpasses the performance of GPT-4 on writing API\\ncalls. Trained with the novel Retriever Aware Training (RAT), when combined\\nwith a document retriever, Gorilla demonstrates a strong capability to adapt to\\ntest-time document changes, allowing flexible user updates or version changes.\\nIt also substantially mitigates the issue of hallucination, commonly encountered\\nwhen prompting LLMs directly. To evaluate the model’s ability, we introduce\\nAPIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and\\nTensorHub APIs. The successful integration of the retrieval system with Gorilla\\ndemonstrates the potential for LLMs to use tools more accurately, keep up with\\nfrequently updated documentation, and consequently increase the reliability and\\napplicability of their outputs. Gorilla’s code, model, data, and demo are available\\nat: https://gorilla.cs.berkeley.edu'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Teaching LLMs to use tools at scale with innvoations in finetuning (RAT) and a novel way to mesasure hallucination using AST.'}, 'pdf': {'value': '/pdf/18a6a0d15f2abb68c940776e23e520a6e0894c93.pdf'}, '_bibtex': {'value': '@inproceedings{\\npatil2024gorilla,\\ntitle={Gorilla: Large Language Model Connected with Massive {API}s},\\nauthor={Shishir G Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tBRNC6YemY}\\n}'}, 'paperhash': {'value': 'patil|gorilla_large_language_model_connected_with_massive_apis'}},forum = 'tBRNC6YemY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21559/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21559/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21559/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21559/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tAlMAcqK9s',number = 14585,cdate = 1715751478164,pdate = 1727288073674,odate = 1730873966974,mdate = 1730873966995,tcdate = 1715751478164,tmdate = 1730873966995,ddate = None,content = {'title': {'value': 'Optimal Algorithms for Augmented Testing of Discrete Distributions'}, 'authors': {'value': ['Maryam Aliakbarpour', 'Piotr Indyk', 'Ronitt Rubinfeld', 'Sandeep Silwal']}, 'authorids': {'value': ['~Maryam_Aliakbarpour1', '~Piotr_Indyk1', '~Ronitt_Rubinfeld1', '~Sandeep_Silwal1']}, 'keywords': {'value': ['distribution testing', 'learning-augmented algorithms', 'data driven algorithm', 'hypothesis testing', 'hypothesis selection', 'distribution learning']}, 'TLDR': {'value': 'We study hypothesis testing of distributions in an augmented setting where learned information about the underlying distribution is available.'}, 'abstract': {'value': 'We consider the problem of hypothesis testing for discrete distributions. In the standard model, where we have sample access to an underlying distribution $p$, extensive research has established optimal bounds for uniformity testing,  identity testing (goodness of fit), and closeness testing (equivalence or two-sample testing). We explore these problems in a setting where a predicted data distribution, possibly derived from historical data or predictive machine learning models, is available. We demonstrate that such a predictor can indeed reduce the number of samples required for all three property testing tasks. The reduction in sample complexity depends directly on the predictor’s quality, measured by its total variation distance from $p$. A key advantage of our algorithms is their adaptability to the precision of the prediction. Specifically, our algorithms can self-adjust their sample complexity based on the accuracy of the available prediction, operating without any prior knowledge of the estimation’s accuracy (i.e. they are consistent). Additionally, we never use more samples than the standard approaches require, even if the predictions provide no meaningful information (i.e. they are also robust). We provide lower bounds to indicate that the improvements in sample complexity achieved by our algorithms are information-theoretically optimal. Furthermore, experimental results show that the performance of our algorithms on real data significantly exceeds our worst-case guarantees for sample complexity, demonstrating the practicality of our approach.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/22800f4412a7e4114550343bfb521e8eced7a1fc.pdf'}, 'supplementary_material': {'value': '/attachment/933b6c0b581e7616de8f90ae6197e3326555bb85.zip'}, '_bibtex': {'value': '@inproceedings{\\naliakbarpour2024optimal,\\ntitle={Optimal Algorithms for Augmented Testing of Discrete Distributions},\\nauthor={Maryam Aliakbarpour and Piotr Indyk and Ronitt Rubinfeld and Sandeep Silwal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tAlMAcqK9s}\\n}'}, 'paperhash': {'value': 'aliakbarpour|optimal_algorithms_for_augmented_testing_of_discrete_distributions'}},forum = 'tAlMAcqK9s',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14585/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14585/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14585/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14585/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'tAOg1HdvGy',number = 12428,cdate = 1715721960760,pdate = 1727288006295,odate = 1730873948998,mdate = 1736234975793,tcdate = 1715721960760,tmdate = 1736234975793,ddate = None,content = {'title': {'value': 'Interpolating Item and User Fairness in Multi-Sided Recommendations'}, 'authors': {'value': ['Qinyi Chen', 'Jason Cheuk Nam Liang', 'Negin Golrezaei', 'Djallel Bouneffouf']}, 'authorids': {'value': ['~Qinyi_Chen1', '~Jason_Cheuk_Nam_Liang1', '~Negin_Golrezaei1', '~Djallel_Bouneffouf2']}, 'keywords': {'value': ['fair recommendation', 'multi-sided platform', 'multi-stakeholder fairness', 'recommendation system', 'online learning algorithms']}, 'abstract': {'value': \"Today's online platforms heavily lean on algorithmic recommendations for bolstering user engagement and driving revenue. However, these recommendations can impact multiple stakeholders simultaneously---the platform, items (sellers), and users (customers)---each with their unique objectives, making it difficult to find the right middle ground that accommodates all stakeholders. To address this, we introduce a novel fair recommendation framework, Problem (FAIR), that flexibly balances multi-stakeholder interests via a constrained optimization formulation. We next explore Problem (FAIR) in a dynamic online setting where data uncertainty further adds complexity, and propose a low-regret algorithm FORM that concurrently performs real-time learning and fair recommendations, two tasks that are often at odds. Via both theoretical analysis and a numerical case study on real-world data, we demonstrate the efficacy of our framework and method in maintaining platform revenue while ensuring desired levels of fairness for both items and users.\"}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present a fair recommendation framework that balances platform revenue and item/user fairness in multi-sided platforms, along with a low-regret algorithm that ensures fair recommendations in an online setting where user data must be learned.'}, 'pdf': {'value': '/pdf/eef61d78b1ce22da208cfbe28f199bd976afc1c5.pdf'}, 'supplementary_material': {'value': '/attachment/edf8c40f8223db588a4efb4398a97c088ada2eca.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024interpolating,\\ntitle={Interpolating Item and User Fairness in Multi-Sided Recommendations},\\nauthor={Qinyi Chen and Jason Cheuk Nam Liang and Negin Golrezaei and Djallel Bouneffouf},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=tAOg1HdvGy}\\n}'}, 'paperhash': {'value': 'chen|interpolating_item_and_user_fairness_in_multisided_recommendations'}},forum = 'tAOg1HdvGy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12428/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12428/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12428/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12428/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't9gNEhreht',number = 12237,cdate = 1715718939140,pdate = 1727287999697,odate = 1730873947119,mdate = 1730873947134,tcdate = 1715718939140,tmdate = 1730873947134,ddate = None,content = {'title': {'value': 'SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data'}, 'authors': {'value': ['Jialu Li', 'Jaemin Cho', 'Yi-Lin Sung', 'Jaehong Yoon', 'Mohit Bansal']}, 'authorids': {'value': ['~Jialu_Li2', '~Jaemin_Cho1', '~Yi-Lin_Sung1', '~Jaehong_Yoon1', '~Mohit_Bansal2']}, 'keywords': {'value': ['Text-to-Image Generation; LoRA Learning and Merging; Skill-based Experts; Auto-Generated Data']}, 'abstract': {'value': 'Recent text-to-image (T2I) generation models have demonstrated impressive capabilities in creating images from text descriptions. However, these T2I generation models often fail to generate images that precisely match the details of the text inputs, such as incorrect spatial relationship or missing objects. In this paper, we introduce SELMA: Skill-Specific Expert Learning and Merging with Auto-Generated Data, a novel paradigm to improve the faithfulness of T2I models by fine-tuning models on automatically generated, multi-skill image-text datasets, with skill-specific expert learning and merging. First, SELMA leverages an LLM’s in-context learning capability to generate multiple datasets of text prompts that can teach different skills, and then generates the images with a T2I model based on the prompts. Next, SELMA adapts the T2I model to the new skills by learning multiple single-skill LoRA (low-rank adaptation) experts followed by expert merging. Our independent expert fine-tuning specializes multiple models for different skills, and expert merging helps build a joint multi-skill T2I model that can generate faithful images given diverse text prompts, while mitigating the knowledge conflict from different datasets. We empirically demonstrate that SELMA significantly improves the semantic alignment and text faithfulness of state-of-the-art T2I diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human preference metrics (PickScore, ImageReward, and HPS), as well as human evaluation. Moreover, fine-tuning with image-text pairs auto-collected via SELMA shows comparable performance to fine-tuning with ground truth data. Lastly, we show that fine-tuning with images from a weaker T2I model can help improve the generation quality of a stronger T2I model, suggesting promising weak-to-strong generalization in T2I models. We provide code in the supplementary materials.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9c5145da5a9316d95276533fdb05fffd3d83b851.pdf'}, 'supplementary_material': {'value': '/attachment/db0909502db2e235af104301cd6ed369a8d3b85d.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024selma,\\ntitle={{SELMA}: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data},\\nauthor={Jialu Li and Jaemin Cho and Yi-Lin Sung and Jaehong Yoon and Mohit Bansal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t9gNEhreht}\\n}'}, 'paperhash': {'value': 'li|selma_learning_and_merging_skillspecific_texttoimage_experts_with_autogenerated_data'}},forum = 't9gNEhreht',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12237/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12237/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12237/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12237/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't8iosEWoyd',number = 13811,cdate = 1715743189337,pdate = 1727288050861,odate = 1730873960671,mdate = 1734628537350,tcdate = 1715743189337,tmdate = 1734628537350,ddate = None,content = {'title': {'value': 'Stochastic contextual bandits with graph feedback: from independence number to MAS number'}, 'authors': {'value': ['Yuxiao Wen', 'Yanjun Han', 'Zhengyuan Zhou']}, 'authorids': {'value': ['~Yuxiao_Wen1', '~Yanjun_Han1', '~Zhengyuan_Zhou2']}, 'keywords': {'value': ['contextual bandits', 'graph feedback', 'minimax rate']}, 'TLDR': {'value': 'We (1) prove a regret lower bound through a novel graph quantity that increases with the number of contexts and (2) propose algorithms that achieve tight upper bound under reasonable assumptions and a weaker one in general.'}, 'abstract': {'value': 'We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. Unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. In this paper, we make inroads into this inquiry by establishing a regret lower bound $\\\\Omega(\\\\sqrt{\\\\beta_M(G) T})$, where $M$ is the number of contexts, $G$ is the feedback graph, and $\\\\beta_M(G)$ is our proposed graph-theoretic quantity that characterizes the fundamental learning limit for this class of problems. Interestingly, $\\\\beta_M(G)$ interpolates between $\\\\alpha(G)$ (the independence number of the graph) and $\\\\mathsf{m}(G)$ (the maximum acyclic subgraph (MAS) number of the graph) as the number of contexts $M$ varies. We also provide algorithms that achieve near-optimal regret for important classes of context sequences and/or feedback graphs, such as transitively closed graphs that find applications in auctions and inventory control. In particular, with many contexts, our results show that the MAS number essentially characterizes the statistical complexity for contextual bandits, as opposed to the independence number in multi-armed bandits.'}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/513ccbfe70b63a8134c688af5c125c0ddad739c2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwen2024stochastic,\\ntitle={Stochastic contextual bandits with graph feedback: from independence number to {MAS} number},\\nauthor={Yuxiao Wen and Yanjun Han and Zhengyuan Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t8iosEWoyd}\\n}'}, 'paperhash': {'value': 'wen|stochastic_contextual_bandits_with_graph_feedback_from_independence_number_to_mas_number'}},forum = 't8iosEWoyd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13811/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13811/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13811/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13811/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't7wvJstsiV',number = 4508,cdate = 1715420420327,pdate = 1727287752792,odate = 1730873876714,mdate = 1737804291527,tcdate = 1715420420327,tmdate = 1737804291527,ddate = None,content = {'title': {'value': 'SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models'}, 'authors': {'value': ['Jianyi Zhang', 'Da-Cheng Juan', 'Cyrus Rashtchian', 'Chun-Sung Ferng', 'Heinrich Jiang', 'Yiran Chen']}, 'authorids': {'value': ['~Jianyi_Zhang1', '~Da-Cheng_Juan1', '~Cyrus_Rashtchian1', '~Chun-Sung_Ferng1', '~Heinrich_Jiang1', '~Yiran_Chen1']}, 'keywords': {'value': ['Large Language Models']}, 'abstract': {'value': 'Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20\\\\% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Self Logits Evolution Decoding (SLED) improves LLM factuality through better decoding.'}, 'pdf': {'value': '/pdf/9dafcb7ab68ae1bc5ba37491547967b4c961b3aa.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024sled,\\ntitle={{SLED}: Self Logits Evolution Decoding for Improving Factuality in Large Language Models},\\nauthor={Jianyi Zhang and Da-Cheng Juan and Cyrus Rashtchian and Chun-Sung Ferng and Heinrich Jiang and Yiran Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t7wvJstsiV}\\n}'}, 'paperhash': {'value': 'zhang|sled_self_logits_evolution_decoding_for_improving_factuality_in_large_language_models'}},forum = 't7wvJstsiV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4508/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4508/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4508/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4508/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 't7euV5dl5M',number = 12818,cdate = 1715729070531,pdate = 1727288018571,odate = 1730873952359,mdate = 1736948322019,tcdate = 1715729070531,tmdate = 1736948322019,ddate = None,content = {'title': {'value': 'Approximation-Aware Bayesian Optimization'}, 'authors': {'value': ['Natalie Maus', 'Kyurae Kim', 'David Eriksson', 'Geoff Pleiss', 'John Patrick Cunningham', 'Jacob R. Gardner']}, 'authorids': {'value': ['~Natalie_Maus1', '~Kyurae_Kim1', '~David_Eriksson2', '~Geoff_Pleiss1', '~John_Patrick_Cunningham1', '~Jacob_R._Gardner1']}, 'keywords': {'value': ['Bayesian optimization', 'variational inference', 'Gaussian processes', 'utility maximization', 'expected improvement', 'knowledge gradient', 'black-box optimization']}, 'abstract': {'value': 'High-dimensional Bayesian optimization (BO) tasks such as molecular design often require $>10,$$000$ function evaluations before obtaining meaningful results. While methods like sparse variational Gaussian processes (SVGPs) reduce computational requirements in these settings, the underlying approximations result in suboptimal data acquisitions that slow the progress of optimization. In this paper we modify SVGPs to better align with the goals of BO: targeting informed data acquisition over global posterior fidelity. Using the framework of utility-calibrated variational inference (Lacoste–Julien et al., 2011), we unify GP approximation and data acquisition into a joint optimization problem, thereby ensuring optimal decisions under a limited computational budget. Our approach can be used with any decision-theoretic acquisition function and is readily compatible with trust region methods like TuRBO (Eriksson et al., 2019). We derive efficient joint objectives for the expected improvement (EI) and knowledge gradient (KG) acquisition functions in both the standard and batch BO settings. On a variety of recent high dimensional benchmark tasks in control and molecular design, our approach significantly outperforms standard SVGPs and is capable of achieving comparable rewards with up to $10\\\\times$ fewer function evaluations.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2ca4a8f0e8762d4239df7e343b5fac191fa32a86.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmaus2024approximationaware,\\ntitle={Approximation-Aware Bayesian Optimization},\\nauthor={Natalie Maus and Kyurae Kim and David Eriksson and Geoff Pleiss and John Patrick Cunningham and Jacob R. Gardner},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t7euV5dl5M}\\n}'}, 'paperhash': {'value': 'maus|approximationaware_bayesian_optimization'}},forum = 't7euV5dl5M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12818/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12818/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12818/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12818/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't7SGOv5W5z',number = 18425,cdate = 1715785429924,pdate = 1727288183339,odate = 1730873989623,mdate = 1730873989641,tcdate = 1715785429924,tmdate = 1730873989641,ddate = None,content = {'title': {'value': 'UQE: A Query Engine for Unstructured Databases'}, 'authors': {'value': ['Hanjun Dai', 'Bethany Yixin Wang', 'Xingchen Wan', 'Bo Dai', 'Sherry Yang', 'Azade Nova', 'Pengcheng Yin', 'Phitchaya Mangpo Phothilimthana', 'Charles Sutton', 'Dale Schuurmans']}, 'authorids': {'value': ['~Hanjun_Dai1', '~Bethany_Yixin_Wang1', '~Xingchen_Wan1', '~Bo_Dai1', '~Sherry_Yang1', '~Azade_Nova1', '~Pengcheng_Yin1', '~Phitchaya_Mangpo_Phothilimthana1', '~Charles_Sutton1', '~Dale_Schuurmans1']}, 'keywords': {'value': ['unstructured data', 'sampling and optimization', 'database', 'language models']}, 'TLDR': {'value': 'a query engine that leverages advances in sampling, online learning and foundation models for effective unstructured data analytics'}, 'abstract': {'value': 'Analytics on structured data is a mature field with many successful methods.\\nHowever, most real world data exists in unstructured form, such as images and conversations.\\nWe investigate the potential of Large Language Models (LLMs) to enable unstructured data analytics.\\nIn particular, we propose a new Universal Query Engine (UQE) that directly interrogates and draws insights from unstructured data collections.\\nThis engine accepts queries in a Universal Query Language (UQL), a dialect of SQL that provides full natural language flexibility in specifying conditions and operators.\\nThe new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution.\\nIn addition, we borrow techniques from classical compiler theory to better orchestrate the workflow between sampling methods and foundation model calls.\\nWe demonstrate the efficiency of UQE on data analytics across different modalities, including images, dialogs and reviews, across a range of useful query types, including conditional aggregation, semantic retrieval and abstraction aggregation.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5ad4bc9d9ad47371210cf048a5e7ce29ad2f3bf7.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ndai2024uqe,\\ntitle={{UQE}: A Query Engine for Unstructured Databases},\\nauthor={Hanjun Dai and Bethany Yixin Wang and Xingchen Wan and Bo Dai and Sherry Yang and Azade Nova and Pengcheng Yin and Phitchaya Mangpo Phothilimthana and Charles Sutton and Dale Schuurmans},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t7SGOv5W5z}\\n}'}, 'paperhash': {'value': 'dai|uqe_a_query_engine_for_unstructured_databases'}},forum = 't7SGOv5W5z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18425/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18425/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18425/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18425/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't4VwoIYBf0',number = 5905,cdate = 1715575674283,pdate = 1727287798559,odate = 1730873889032,mdate = 1730873889050,tcdate = 1715575674283,tmdate = 1730873889050,ddate = None,content = {'title': {'value': 'C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory'}, 'authors': {'value': ['Tianjiao Luo', 'Tim Pearce', 'Huayu Chen', 'Jianfei Chen', 'Jun Zhu']}, 'authorids': {'value': ['~Tianjiao_Luo1', '~Tim_Pearce1', '~Huayu_Chen1', '~Jianfei_Chen1', '~Jun_Zhu2']}, 'keywords': {'value': ['Reinforcement Learning', 'Control Theory', 'Stability Analysis']}, 'abstract': {'value': 'Generative Adversarial Imitation Learning (GAIL) provides a promising approach to training a generative policy to imitate a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from an adversarial discriminator. However, optimizing GAIL is difficult in practise, with the training loss oscillating during training, slowing convergence. This optimization instability can prevent GAIL from finding a good policy, harming its final performance. In this paper, we study GAIL’s optimization from a control-theoretic perspective. We show that GAIL cannot converge to the desired equilibrium. In response, we analyze the training dynamics of GAIL in function space and design a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a simplified “one-step” setting. Going from theory to practice, we propose Controlled-GAIL (C-GAIL), which adds a differentiable regularization term on the GAIL objective to stabilize training. Empirically, the C-GAIL regularizer improves the training of various existing GAIL methods, including the popular GAIL-DAC, by speeding up the convergence, reducing the range of oscillation, and matching the expert distribution more closely.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/82916812fa37eed846d5121ee9e03fd6d11bf34c.pdf'}, 'supplementary_material': {'value': '/attachment/4b31b83c9ba7d4064ab08cd0c37fcba63ca92fbe.zip'}, '_bibtex': {'value': '@inproceedings{\\nluo2024cgail,\\ntitle={C-{GAIL}: Stabilizing Generative Adversarial Imitation Learning with Control Theory},\\nauthor={Tianjiao Luo and Tim Pearce and Huayu Chen and Jianfei Chen and Jun Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t4VwoIYBf0}\\n}'}, 'paperhash': {'value': 'luo|cgail_stabilizing_generative_adversarial_imitation_learning_with_control_theory'}},forum = 't4VwoIYBf0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5905/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5905/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5905/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5905/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 't3BhmwAzhv',number = 1498,cdate = 1714656732479,pdate = 1727287662484,odate = 1730873849252,mdate = 1730873849269,tcdate = 1714656732479,tmdate = 1730873849269,ddate = None,content = {'title': {'value': 'Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers'}, 'authors': {'value': ['Haifeng Huang', 'Yilun Chen', 'Zehan Wang', 'Rongjie Huang', 'Runsen Xu', 'Tai Wang', 'Luping Liu', 'Xize Cheng', 'Yang Zhao', 'Jiangmiao Pang', 'Zhou Zhao']}, 'authorids': {'value': ['~Haifeng_Huang3', '~Yilun_Chen1', '~Zehan_Wang2', '~Rongjie_Huang1', '~Runsen_Xu1', '~Tai_Wang2', '~Luping_Liu2', '~Xize_Cheng1', '~Yang_Zhao14', '~Jiangmiao_Pang1', '~Zhou_Zhao3']}, 'keywords': {'value': ['3D Scene Understanding', 'Multi-modal Large Language Model']}, 'abstract': {'value': 'Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations. By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A 3D large language model unifying referencing and grounding capabilities.'}, 'pdf': {'value': '/pdf/a0e287203349727d214afc84803750516b90b274.pdf'}, 'supplementary_material': {'value': '/attachment/6348068a3c035d7af393ad39a6bdf0a6851eacee.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024chatscene,\\ntitle={Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers},\\nauthor={Haifeng Huang and Yilun Chen and Zehan Wang and Rongjie Huang and Runsen Xu and Tai Wang and Luping Liu and Xize Cheng and Yang Zhao and Jiangmiao Pang and Zhou Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=t3BhmwAzhv}\\n}'}, 'paperhash': {'value': 'huang|chatscene_bridging_3d_scene_and_large_language_models_with_object_identifiers'}},forum = 't3BhmwAzhv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1498/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1498/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1498/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1498/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'sy7eSEXdPC',number = 12656,cdate = 1715725932235,pdate = 1727288013523,odate = 1730873950814,mdate = 1730873950837,tcdate = 1715725932235,tmdate = 1730873950837,ddate = None,content = {'title': {'value': 'Multi-LLM Debate: Framework, Principals, and Interventions'}, 'authors': {'value': ['Andrew Estornell', 'Yang Liu']}, 'authorids': {'value': ['~Andrew_Estornell1', '~Yang_Liu3']}, 'keywords': {'value': ['multi-agent debate', 'LLM']}, 'abstract': {'value': 'The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains.\\nMuch like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality.\\nWe first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathematically examined.\\nBuilding on this framework, we provide several theoretical results for multi-agent debate.\\nIn particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. \\nWhen this majority opinion is the result of a common misconception (ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception.\\nUsing insights from our theoretical results we then propose three interventions which improve the efficacy of debate. \\nFor each intervention, we provide theoretical results demonstrating how debate is improved.\\nWe also demonstrate that these interventions result in better performance on four common benchmark tasks.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ae3a0032b5023848f8c865ef47d515acf58cb84f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nestornell2024multillm,\\ntitle={Multi-{LLM} Debate: Framework, Principals, and Interventions},\\nauthor={Andrew Estornell and Yang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sy7eSEXdPC}\\n}'}, 'paperhash': {'value': 'estornell|multillm_debate_framework_principals_and_interventions'}},forum = 'sy7eSEXdPC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12656/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12656/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12656/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12656/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sy2SmstDOB',number = 19335,cdate = 1715790398734,pdate = 1727288207163,odate = 1730873995020,mdate = 1737121925984,tcdate = 1715790398734,tmdate = 1737121925984,ddate = None,content = {'title': {'value': 'UniFL: Improve Latent Diffusion Model via Unified Feedback Learning'}, 'authors': {'value': ['Jiacheng Zhang', 'Jie Wu', 'Yuxi Ren', 'Xin Xia', 'Huafeng Kuang', 'Pan Xie', 'Jiashi Li', 'Xuefeng Xiao', 'Weilin Huang', 'Shilei Wen', 'Lean Fu', 'Guanbin Li']}, 'authorids': {'value': ['~Jiacheng_Zhang5', '~Jie_Wu8', '~Yuxi_Ren1', '~Xin_Xia1', '~Huafeng_Kuang1', '~Pan_Xie1', '~Jiashi_Li1', '~Xuefeng_Xiao1', '~Weilin_Huang1', '~Shilei_Wen1', '~Lean_Fu1', '~Guanbin_Li2']}, 'keywords': {'value': ['Diffusion Model', 'Feedback Learning', 'Acceleration']}, 'TLDR': {'value': 'A unified feedback learning framework to enhance the performance of text-to-image diffusion model'}, 'abstract': {'value': 'Latent diffusion models (LDM) have revolutionized text-to-image generation, leading to the proliferation of various advanced models and diverse downstream applications. However, despite these significant advancements, current diffusion models still suffer from several limitations, including inferior visual quality, inadequate aesthetic appeal, and inefficient inference, without a comprehensive solution in sight. To address these challenges, we present **UniFL**, a unified framework that leverages feedback learning to enhance diffusion models comprehensively. UniFL stands out as a universal, effective, and generalizable solution applicable to various diffusion models, such as SD1.5 and SDXL.\\nNotably, UniFL consists of three key components: perceptual feedback learning, which enhances visual quality; decoupled feedback learning, which improves aesthetic appeal; and adversarial feedback learning, which accelerates inference.\\nIn-depth experiments and extensive user studies validate the superior performance of our method in enhancing generation quality and inference acceleration. For instance, UniFL surpasses ImageReward by 17\\\\% user preference in terms of generation quality and outperforms LCM and SDXL Turbo by 57\\\\% and 20\\\\% general preference with 4-step inference.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ccfb16db47fdbb6715d56673827f52bce423db2b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024unifl,\\ntitle={Uni{FL}: Improve Latent Diffusion Model via Unified Feedback Learning},\\nauthor={Jiacheng Zhang and Jie Wu and Yuxi Ren and Xin Xia and Huafeng Kuang and Pan Xie and Jiashi Li and Xuefeng Xiao and Weilin Huang and Shilei Wen and Lean Fu and Guanbin Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sy2SmstDOB}\\n}'}, 'paperhash': {'value': 'zhang|unifl_improve_latent_diffusion_model_via_unified_feedback_learning'}},forum = 'sy2SmstDOB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19335/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19335/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19335/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19335/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'swp3lPDmZe',number = 2308,cdate = 1715033807900,pdate = 1727287686747,odate = 1730873857344,mdate = 1736201352921,tcdate = 1715033807900,tmdate = 1736201352921,ddate = None,content = {'title': {'value': 'Off-Policy Selection for Initiating Human-Centric Experimental Design'}, 'authors': {'value': ['Ge Gao', 'Xi Yang', 'Qitong Gao', 'Song Ju', 'Miroslav Pajic', 'Min Chi']}, 'authorids': {'value': ['~Ge_Gao4', '~Xi_Yang11', '~Qitong_Gao1', '~Song_Ju1', '~Miroslav_Pajic2', '~Min_Chi1']}, 'keywords': {'value': ['Off-policy selection (OPS)', 'Offline reinforcement learning and OPS for human-centric experimental design', 'intelligent tutoring', 'sepsis treatments']}, 'abstract': {'value': 'In human-centric applications like healthcare and education, the \\\\textit{heterogeneity} among patients and students necessitates personalized treatments and instructional interventions. While reinforcement learning (RL) has been utilized in those tasks, off-policy selection (OPS) is pivotal to close the loop by offline evaluating and selecting policies without online interactions, yet current OPS methods often overlook the heterogeneity among participants. Our work is centered on resolving a \\\\textit{pivotal challenge} in human-centric systems (HCSs): \\\\textbf{\\\\textit{how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant?}} We introduce First-Glance Off-Policy Selection (FPS), a novel approach that systematically addresses participant heterogeneity through sub-group segmentation and tailored OPS criteria to each sub-group. By grouping individuals with similar traits, FPS facilitates personalized policy selection aligned with unique characteristics of each participant or group of participants. FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention. FPS presents significant advancement in enhancing learning outcomes of students and in-hospital care outcomes.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8d345a8973e3f20dedb3ffedeb4680bd783a2b3e.pdf'}, 'supplementary_material': {'value': '/attachment/920c1edfa3b3591233cdac766a10e647b3529334.zip'}, '_bibtex': {'value': '@inproceedings{\\ngao2024offpolicy,\\ntitle={Off-Policy Selection for Initiating Human-Centric Experimental Design},\\nauthor={Ge Gao and Xi Yang and Qitong Gao and Song Ju and Miroslav Pajic and Min Chi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=swp3lPDmZe}\\n}'}, 'paperhash': {'value': 'gao|offpolicy_selection_for_initiating_humancentric_experimental_design'}},forum = 'swp3lPDmZe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2308/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2308/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2308/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2308/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'suYAAOI5bd',number = 12239,cdate = 1715718956279,pdate = 1727287999795,odate = 1730873947190,mdate = 1730873947211,tcdate = 1715718956279,tmdate = 1730873947211,ddate = None,content = {'title': {'value': 'On the Expressive Power of Tree-Structured Probabilistic Circuits'}, 'authors': {'value': ['Lang Yin', 'Han Zhao']}, 'authorids': {'value': ['~Lang_Yin1', '~Han_Zhao1']}, 'keywords': {'value': ['Probabilistic circuits', 'Circuit complexities', 'Network polynomials', 'Probabilistic models']}, 'TLDR': {'value': 'Our paper proves a universal upper bound and a conditional lower bound for the expressive power of tree-structured probabilistic circuits.'}, 'abstract': {'value': 'Probabilistic circuits (PCs) have emerged as a powerful framework compactly representing probability distributions for efficient and exact probabilistic inference. It has been shown that PCs with general directed acyclic graph (DAG) structure can be understood as a mixture of exponentially (in its height) many components, each of which is a product distributions over univariate marginals. However, existing structure learning algorithms for PCs often generate tree-structured circuits, or using tree-structured circuits as intermediate steps to compress them into DAG-structured circuits. This leads to an intriguing question on whether there exists an exponential gap between DAGs and trees for the PC structure.\\n\\nIn this paper, we provide a negative answer to this conjecture by proving that, for $n$ variables, there is a quasi-polynomial upper bound $n^{O(\\\\log n)}$ on the size of an equivalent tree computing the same probability distribution. On the other hand, we will also show that given a depth restriction on the tree, there is a super-polynomial separation between tree and DAG-structured PCs. Our work takes an important step towards understanding the expressive power of tree-structured PCs, and our techniques may be of independent interest in the study of structure learning algorithms for PCs.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b43ed6380196199080503de5e5cbac4b6464df1a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyin2024on,\\ntitle={On the Expressive Power of Tree-Structured Probabilistic Circuits},\\nauthor={Lang Yin and Han Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=suYAAOI5bd}\\n}'}, 'paperhash': {'value': 'yin|on_the_expressive_power_of_treestructured_probabilistic_circuits'}},forum = 'suYAAOI5bd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12239/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12239/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12239/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12239/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'stY80vVBS8',number = 6939,cdate = 1715608335554,pdate = 1727287829654,odate = 1730873897786,mdate = 1734719678875,tcdate = 1715608335554,tmdate = 1734719678875,ddate = None,content = {'title': {'value': 'Learning-Augmented Dynamic Submodular Maximization'}, 'authors': {'value': ['Arpit Agarwal', 'Eric Balkanski']}, 'authorids': {'value': ['~Arpit_Agarwal2', '~Eric_Balkanski2']}, 'keywords': {'value': ['Submodular maximization', 'algorithms with predictions', 'dynamic algorithms']}, 'TLDR': {'value': 'We study dynamic submodular maximization in the algorithms with predictions framework.'}, 'abstract': {'value': 'In dynamic submodular maximization, the goal is to maintain a high-value solution over a sequence of element insertions and deletions with a fast update time. Motivated by large-scale applications and the fact that dynamic data often exhibits patterns, we ask the following question: can predictions be used to accelerate the update time of dynamic submodular maximization algorithms? \\n\\nWe consider the model for dynamic algorithms with predictions where predictions regarding the insertion and deletion times of elements can be used for preprocessing. Our main result is an algorithm with an $O(\\\\text{poly}(\\\\log \\\\eta, \\\\log w, \\\\log k))$ amortized update time over the sequence of updates that achieves a $1/2 - \\\\epsilon$ approximation for dynamic monotone submodular maximization under a cardinality constraint $k$, where the prediction error $\\\\eta$ is the number of elements that are not inserted and deleted within $w$ time steps of their predicted insertion and deletion times. This amortized update time is independent of the length of the stream and instead depends on the prediction error.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/49936dbcd6981eeb4a3a45c32cd25eb31bb1cee1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nagarwal2024learningaugmented,\\ntitle={Learning-Augmented Dynamic Submodular Maximization},\\nauthor={Arpit Agarwal and Eric Balkanski},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=stY80vVBS8}\\n}'}, 'paperhash': {'value': 'agarwal|learningaugmented_dynamic_submodular_maximization'}},forum = 'stY80vVBS8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6939/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6939/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6939/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6939/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'stXtBqyTWX',number = 13941,cdate = 1715744432823,pdate = 1727288055487,odate = 1730873962303,mdate = 1734734823283,tcdate = 1715744432823,tmdate = 1734734823283,ddate = None,content = {'title': {'value': 'Toward Efficient Inference for Mixture of Experts'}, 'authors': {'value': ['Haiyang Huang', 'Newsha Ardalani', 'Anna Sun', 'Liu Ke', 'Shruti Bhosale', 'Hsien-Hsin S. Lee', 'Carole-Jean Wu', 'Benjamin Lee']}, 'authorids': {'value': ['~Haiyang_Huang2', '~Newsha_Ardalani1', '~Anna_Sun1', '~Liu_Ke1', '~Shruti_Bhosale1', '~Hsien-Hsin_S._Lee1', '~Carole-Jean_Wu2', '~Benjamin_Lee3']}, 'keywords': {'value': ['Mixture-of-Experts', 'inference']}, 'abstract': {'value': 'Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment. We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves maximum throughput by 6.21-11.55$\\\\times$ for LM, 5.75-10.98$\\\\times$ for MT Encoder and 2.58-5.71$\\\\times$ for MT Decoder.\\nIt also reduces memory usage by up to 1.36$\\\\times$ for LM and up to 1.1$\\\\times$ for MT. We further propose Expert Buffering, a new caching mechanism that only keeps hot, active experts in GPU memory while buffering the rest in CPU memory. This reduces static memory allocation by 1.47$\\\\times$. Finally, we propose a load balancing methodology that provides additional robustness to the workload. Our code is available at https://github.com/hyhuang00/moe_inference.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper addresses deployment challenges of Mixture-of-Experts models in Language Modeling and Machine Translation by introducing dynamic gating, expert buffering, and load balancing to improve throughput, memory usage, and robustness.'}, 'pdf': {'value': '/pdf/b9888255233cbfec88dd7c0bc9b48c48b33bf0ec.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024toward,\\ntitle={Toward Efficient Inference for Mixture of Experts},\\nauthor={Haiyang Huang and Newsha Ardalani and Anna Sun and Liu Ke and Shruti Bhosale and Hsien-Hsin S. Lee and Carole-Jean Wu and Benjamin Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=stXtBqyTWX}\\n}'}, 'paperhash': {'value': 'huang|toward_efficient_inference_for_mixture_of_experts'}},forum = 'stXtBqyTWX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13941/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13941/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13941/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13941/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'srQxkSPJLW',number = 4899,cdate = 1715481840585,pdate = 1727287764210,odate = 1730873880015,mdate = 1735004110957,tcdate = 1715481840585,tmdate = 1735004110957,ddate = None,content = {'title': {'value': 'DI-MaskDINO: A Joint Object Detection and Instance Segmentation Model'}, 'authors': {'value': ['Zhixiong Nan', 'Xianghong Li', 'Tao Xiang', 'Jifeng Dai']}, 'authorids': {'value': ['~Zhixiong_Nan2', '~Xianghong_Li1', '~Tao_Xiang2', '~Jifeng_Dai1']}, 'keywords': {'value': ['object detection', 'instance segmentation']}, 'abstract': {'value': 'This paper is motivated by an interesting phenomenon: the performance of object detection lags behind that of instance segmentation (i.e., performance imbalance) when investigating the intermediate results from the beginning transformer decoder layer of MaskDINO (i.e., the SOTA model for joint detection and segmentation). This phenomenon inspires us to think about a question: will the performance imbalance at the beginning layer of transformer decoder constrain the upper bound of the final performance? With this question in mind, we further conduct qualitative and quantitative pre-experiments, which validate the negative impact of detection-segmentation imbalance issue on the model performance. To address this issue, this paper proposes DI-MaskDINO model, the core idea of which is to improve the final performance by alleviating the detection-segmentation imbalance. DI-MaskDINO is implemented by configuring our proposed De-Imbalance (DI) module and Balance-Aware Tokens Optimization (BATO) module to MaskDINO. DI is responsible for generating balance-aware query, and BATO uses the balance-aware query to guide the optimization of the initial feature tokens. The balance-aware query and optimized feature tokens are respectively taken as the Query and Key&Value of transformer decoder to perform joint object detection and instance segmentation. DI-MaskDINO outperforms existing joint object detection and instance segmentation models on COCO and BDD100K benchmarks, achieving +1.2 $AP^{box}$ and +0.9 $AP^{mask}$ improvements compared to SOTA joint detection and segmentation model MaskDINO. In addition, DI-MaskDINO also obtains +1.0 $AP^{box}$ improvement compared to SOTA object detection model DINO and +3.0 $AP^{mask}$ improvement compared to SOTA segmentation model Mask2Former.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4368bec2b96c2307f38e80b692b13071c2affb4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnan2024dimaskdino,\\ntitle={{DI}-Mask{DINO}: A Joint Object Detection and Instance Segmentation Model},\\nauthor={Zhixiong Nan and Xianghong Li and Tao Xiang and Jifeng Dai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=srQxkSPJLW}\\n}'}, 'paperhash': {'value': 'nan|dimaskdino_a_joint_object_detection_and_instance_segmentation_model'}},forum = 'srQxkSPJLW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4899/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4899/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4899/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4899/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'spwE9sLrfg',number = 19924,cdate = 1715793468755,pdate = 1727288221193,odate = 1730873998344,mdate = 1736920205871,tcdate = 1715793468755,tmdate = 1736920205871,ddate = None,content = {'title': {'value': 'Verified Code Transpilation with LLMs'}, 'authors': {'value': ['Sahil Bhatia', 'Jie Qiu', 'Niranjan Hasabnis', 'Sanjit A. Seshia', 'Alvin Cheung']}, 'authorids': {'value': ['~Sahil_Bhatia3', '~Jie_Qiu2', '~Niranjan_Hasabnis1', '~Sanjit_A._Seshia1', '~Alvin_Cheung2']}, 'keywords': {'value': ['Program Synthesis', 'Compilers', 'Formal Methods', 'LLMs']}, 'abstract': {'value': \"Domain-specific languages (DSLs) have become integral to various software workflows. Such languages offer domain-specific optimizations and abstractions that improve code readability and maintainability.  However, leveraging these languages requires developers to rewrite existing code using the specific DSL's API. While large language models (LLMs) have shown some success in automatic code transpilation, none of them provide any functional correctness guarantees on the rewritten code. Another approach for automating this task is verified lifting, which relies on program synthesis to find programs in the target language that are functionally equivalent to the source language program. While several verified lifting tools have been developed for various application domains, they are specialized for specific source-target languages or require significant expertise in domain knowledge to make the search efficient. In this paper, leveraging recent advances in LLMs, we propose an LLM-based approach (LLMLift) to building verified lifting tools. We use the LLM's capabilities to reason about programs to translate a given program into its corresponding equivalent in the target language. Additionally, we use LLMs to generate proofs for functional equivalence. We develop lifting-based compilers for four DSLs targeting different application domains. Our approach not only outperforms previous symbolic-based tools in number of benchmarks transpiled and transpilation time, but also requires significantly less effort to build.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/012b57c59a8ded100759796c4b88fe44ae7f67bb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbhatia2024verified,\\ntitle={Verified Code Transpilation with {LLM}s},\\nauthor={Sahil Bhatia and Jie Qiu and Niranjan Hasabnis and Sanjit A. Seshia and Alvin Cheung},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=spwE9sLrfg}\\n}'}, 'paperhash': {'value': 'bhatia|verified_code_transpilation_with_llms'}},forum = 'spwE9sLrfg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19924/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19924/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19924/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19924/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sp8wHIsnu9',number = 14454,cdate = 1715750132600,pdate = 1727288070134,odate = 1730873965978,mdate = 1730873965993,tcdate = 1715750132600,tmdate = 1730873965993,ddate = None,content = {'title': {'value': 'Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models'}, 'authors': {'value': ['Yeming Wen', 'Swarat Chaudhuri']}, 'authorids': {'value': ['~Yeming_Wen1', '~Swarat_Chaudhuri1']}, 'keywords': {'value': ['diverse generation', 'instruction fine-tuning', 'synthetic dataset']}, 'abstract': {'value': 'Presenting users with diverse responses from foundation models is crucial for enhancing user experience and accommodating varying preferences. \\nHowever, generating multiple high-quality and diverse responses without sacrificing accuracy remains a challenge, especially when using greedy sampling. \\nIn this work, we propose a novel framework, Synthesize-Partition-Adapt (SPA), that leverages the abundant synthetic data available in many domains to elicit diverse responses from foundation models.\\nBy leveraging signal provided by data attribution methods such as influence functions, SPA partitions data into subsets, each targeting unique aspects of the data, and trains multiple model adaptations optimized for these subsets.\\nExperimental results demonstrate the effectiveness of our approach in diversifying foundation model responses while maintaining high quality, showcased through the HumanEval and MBPP tasks in the code generation domain and several tasks in the natural language understanding domain, highlighting its potential to enrich user experience across various applications.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/54f326367efea82bf16bab0b436f0f91089cac82.pdf'}, 'TLDR': {'value': 'we proposed a framework that leverages synthetic data, data partitioning, and model adaptation to elicit diverse responses from foundation models.'}, 'supplementary_material': {'value': '/attachment/74efa63cc0126d4585af401fbd484de88642d848.zip'}, '_bibtex': {'value': '@inproceedings{\\nwen2024synthesize,\\ntitle={Synthesize, Partition, then Adapt: Eliciting Diverse Samples from Foundation Models},\\nauthor={Yeming Wen and Swarat Chaudhuri},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sp8wHIsnu9}\\n}'}, 'paperhash': {'value': 'wen|synthesize_partition_then_adapt_eliciting_diverse_samples_from_foundation_models'}},forum = 'sp8wHIsnu9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14454/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14454/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14454/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14454/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'soUXmwL5aK',number = 11945,cdate = 1715713585839,pdate = 1727287989037,odate = 1730873943925,mdate = 1736898935532,tcdate = 1715713585839,tmdate = 1736898935532,ddate = None,content = {'title': {'value': 'Interpretable Generalized Additive Models for Datasets with Missing Values'}, 'authors': {'value': ['Hayden McTavish', 'Jon Donnelly', 'Margo Seltzer', 'Cynthia Rudin']}, 'authorids': {'value': ['~Hayden_McTavish1', '~Jon_Donnelly1', '~Margo_Seltzer1', '~Cynthia_Rudin1']}, 'keywords': {'value': ['Interpretability', 'Missing Data', 'Generalized Additive Models', 'Sparsity']}, 'abstract': {'value': 'Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model’s mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through $\\\\ell_0$ regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naïve inclusion of indicator variables.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce an interpretable GAM approach for missing data which improves accuracy under synthetic missingness while globally improving sparsity, all with no significant cost to real-world accuracy or runtime.'}, 'pdf': {'value': '/pdf/c062c4252109f808254634d38b59116c829caa9c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmctavish2024interpretable,\\ntitle={Interpretable Generalized Additive Models for Datasets with Missing Values},\\nauthor={Hayden McTavish and Jon Donnelly and Margo Seltzer and Cynthia Rudin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=soUXmwL5aK}\\n}'}, 'paperhash': {'value': 'mctavish|interpretable_generalized_additive_models_for_datasets_with_missing_values'}},forum = 'soUXmwL5aK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11945/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11945/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11945/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11945/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'snxWD0Q4EI',number = 18307,cdate = 1715784847654,pdate = 1727288177853,odate = 1730873989129,mdate = 1730873989146,tcdate = 1715784847654,tmdate = 1730873989146,ddate = None,content = {'title': {'value': 'The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information'}, 'authors': {'value': ['Diyuan Wu', 'Ionut-Vlad Modoranu', 'Mher Safaryan', 'Denis Kuznedelev', 'Dan Alistarh']}, 'authorids': {'value': ['~Diyuan_Wu1', '~Ionut-Vlad_Modoranu1', '~Mher_Safaryan1', '~Denis_Kuznedelev1', '~Dan_Alistarh7']}, 'keywords': {'value': ['Optimal Brain Surgeon', 'Sparse Recovery', 'Pruning', 'Second-Order Optimization']}, 'abstract': {'value': 'The rising footprint of machine learning has led to a focus on imposing model sparsity as a means of reducing computational and memory costs. For deep neural networks (DNNs), the state-of-the-art accuracy-vs-sparsity is achieved by heuristics inspired by the classical Optimal Brain Surgeon (OBS) framework [LeCun et al., 1989, Hassibi and Stork, 1992, Hassibi et al., 1993], which leverages loss curvature information to make better pruning decisions. Yet, these results still lack a solid theoretical understanding, and it is unclear whether they can be improved by leveraging connections to the wealth of work on sparse recovery algorithms. In this paper, we draw new connections between these two areas and present new sparse recovery algorithms inspired by the OBS framework that come with theoretical guarantees under reasonable assumptions and have strong practical performance. Specifically, our work starts from the observation that we can leverage curvature information in OBS-like fashion upon the projection step of classic iterative sparse recovery algorithms such as IHT. We show for the first time that this leads both to improved convergence bounds in well-behaved settings and to stronger practical convergence. Furthermore, we present extensions of this approach to training accurate sparse DNNs, and validate it experimentally at scale.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We consider iterative version of OBS framework of finding sparse solutions providing practical evidence and theoretical justifications for convergence.'}, 'pdf': {'value': '/pdf/3b517098477841022b3829daf0dc956e5e6b66a9.pdf'}, 'supplementary_material': {'value': '/attachment/f4069493f220c28e304c85c9b9434d9b12fd1682.zip'}, '_bibtex': {'value': '@inproceedings{\\nwu2024the,\\ntitle={The Iterative Optimal Brain Surgeon: Faster Sparse Recovery by Leveraging Second-Order Information},\\nauthor={Diyuan Wu and Ionut-Vlad Modoranu and Mher Safaryan and Denis Kuznedelev and Dan Alistarh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=snxWD0Q4EI}\\n}'}, 'paperhash': {'value': 'wu|the_iterative_optimal_brain_surgeon_faster_sparse_recovery_by_leveraging_secondorder_information'}},forum = 'snxWD0Q4EI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18307/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18307/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18307/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18307/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sntv8Ac3U2',number = 951,cdate = 1714245453323,pdate = 1727287648393,odate = 1730873844340,mdate = 1730873844354,tcdate = 1714245453323,tmdate = 1730873844354,ddate = None,content = {'title': {'value': 'Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis'}, 'authors': {'value': ['Deepak Sridhar', 'Abhishek Peri', 'Rohith Reddy Rachala', 'Nuno Vasconcelos']}, 'authorids': {'value': ['~Deepak_Sridhar1', '~Abhishek_Peri1', '~Rohith_Reddy_Rachala1', '~Nuno_Vasconcelos1']}, 'keywords': {'value': ['Image Synthesis; Controllable 2D/3D Synthesis; Diffusion']}, 'TLDR': {'value': 'We introduce a new framework for modeling the joint distribution of images and conditioning variables by adapting Stable Diffusion to enhance prompt compliance, controllability and editing of images.'}, 'abstract': {'value': 'Recent advances in generative modeling with diffusion processes (DPs) enabled breakthroughs in image synthesis. Despite impressive image quality, these models have various prompt compliance problems, including low recall in generating multiple objects, difficulty in generating text in images, and meeting constraints like object locations and pose. For fine-grained editing and manipulation, they also require fine-grained semantic or instance maps that are tedious to produce manually. While prompt compliance can be enhanced by addition of loss functions at inference, this is time consuming and does not scale to complex scenes. \\n  To overcome these limitations, this work introduces a new family of  $\\\\textit{Factor Graph Diffusion Models}$ (FG-DMs) that models the joint distribution of images and conditioning variables, such as semantic, sketch, depth or normal maps via a factor graph decomposition. This joint structure has several advantages, including support for efficient sampling based prompt compliance schemes, which produce images of high object recall, semi-automated fine-grained editing, explainability at intermediate levels, ability to produce labeled datasets for the training of downstream models such as segmentation or depth, training with missing data, and continual learning where new conditioning variables can be added with minimal or no modifications to the existing structure. We propose an implementation of FG-DMs by adapting a pre-trained Stable Diffusion (SD) model to implement all FG-DM factors, using only COCO dataset, and show that it is effective in generating images with 15\\\\% higher recall than SD while retaining its generalization ability. We introduce an attention distillation loss that encourages consistency among the attention maps of all factors, improving the fidelity of the generated conditions and image. We also show that training FG-DMs from scratch on MM-CelebA-HQ, Cityscapes, ADE20K, and COCO produce images of high quality (FID) and diversity (LPIPS).'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8f21ac42e948eed5568e8c93b72ba0d79d8038dd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsridhar2024adapting,\\ntitle={Adapting Diffusion Models for Improved Prompt Compliance and Controllable Image Synthesis},\\nauthor={Deepak Sridhar and Abhishek Peri and Rohith Reddy Rachala and Nuno Vasconcelos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sntv8Ac3U2}\\n}'}, 'paperhash': {'value': 'sridhar|adapting_diffusion_models_for_improved_prompt_compliance_and_controllable_image_synthesis'}},forum = 'sntv8Ac3U2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission951/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission951/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission951/-/Revision', 'NeurIPS.cc/2024/Conference/Submission951/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'sn3UrYRItk',number = 9371,cdate = 1715678913230,pdate = 1727287908646,odate = 1730873920245,mdate = 1730873920263,tcdate = 1715678913230,tmdate = 1730873920263,ddate = None,content = {'title': {'value': 'The Impact of Initialization on LoRA Finetuning Dynamics'}, 'authors': {'value': ['Soufiane Hayou', 'Nikhil Ghosh', 'Bin Yu']}, 'authorids': {'value': ['~Soufiane_Hayou1', '~Nikhil_Ghosh1', '~Bin_Yu5']}, 'keywords': {'value': ['Finetuning; LoRA; Large Language Models']}, 'TLDR': {'value': 'Initialization of the adapter weights has crucial impact on LoRA learning dynamics'}, 'abstract': {'value': 'In this paper, we study the role of initialization in Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021). Essentially, to start from the pretrained model, one can either initialize $B$ to zero and $A$ to random, or vice-versa. In both cases, the product $BA$ is equal to zero at initialization, which makes finetuning starts from the pretrained model. These two initialization schemes are seemingly similar. They should in-principle yield the same performance and share the same optimal learning rate. We demonstrate that this is an *incorrect intuition* and that the first scheme (of initializing $B$ to zero and $A$ to random) on average in our experiments yields better performance compared to the other scheme. Our theoretical analysis shows that the reason behind this might be that the first initialization allows the use of larger learning rates (without causing output instability) compared to the second initialization, resulting in more efficient learning of the first scheme. We validate our results with extensive experiments on LLMs.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/73b61afa707a9caef336040d03963125b0ff17ef.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhayou2024the,\\ntitle={The Impact of Initialization on Lo{RA} Finetuning Dynamics},\\nauthor={Soufiane Hayou and Nikhil Ghosh and Bin Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sn3UrYRItk}\\n}'}, 'paperhash': {'value': 'hayou|the_impact_of_initialization_on_lora_finetuning_dynamics'}},forum = 'sn3UrYRItk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9371/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9371/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9371/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9371/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sks7x4I8Bh',number = 13369,cdate = 1715738863833,pdate = 1727288037218,odate = 1730873957512,mdate = 1730873957533,tcdate = 1715738863833,tmdate = 1730873957533,ddate = None,content = {'title': {'value': 'Online Estimation via Offline Estimation: An Information-Theoretic Framework'}, 'authors': {'value': ['Dylan J Foster', 'Yanjun Han', 'Jian Qian', 'Alexander Rakhlin']}, 'authorids': {'value': ['~Dylan_J_Foster1', '~Yanjun_Han1', '~Jian_Qian2', '~Alexander_Rakhlin1']}, 'keywords': {'value': ['online learning', 'interactive decision making', 'oracle-efficient', 'regression', 'classification', 'conditional density estimation']}, 'abstract': {'value': \"The classical theory of statistical estimation aims to estimate a parameter of interest under data generated from a fixed design (''offline estimation''), while the contemporary theory of online learning provides algorithms for estimation under adaptively chosen covariates (''online estimation''). Motivated by connections between estimation and interactive decision making, we ask: is it possible to convert offline estimation algorithms into online estimation algorithms in a black-box fashion? We investigate this question from an information-theoretic perspective by introducing a new framework, Oracle-Efficient Online Estimation (OEOE), where the learner can only interact with the data stream indirectly through a sequence of offline estimators produced by a black-box algorithm operating on the stream. Our main results settle the statistical and computational complexity of online estimation in this framework.\\n\\n   $\\\\bullet$ Statistical complexity. We show that information-theoretically, there exist algorithms that achieve near-optimal online estimation error via black-box offline estimation oracles, and give a nearly-tight characterization for minimax rates in the OEOE framework.\\n\\n  $\\\\bullet$ Computational complexity. We show that the guarantees above cannot be achieved in a computationally efficient fashion in general, but give a refined characterization for the special case of conditional density estimation: computationally efficient online estimation via black-box offline estimation is possible whenever it is possible via unrestricted algorithms.\\n\\nFinally, we apply our results to give offline oracle-efficient algorithms for interactive decision making.\"}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b8cfbc277ea416f34c48378cb8a72149176fc155.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfoster2024online,\\ntitle={Online Estimation via Offline Estimation: An Information-Theoretic Framework},\\nauthor={Dylan J Foster and Yanjun Han and Jian Qian and Alexander Rakhlin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sks7x4I8Bh}\\n}'}, 'paperhash': {'value': 'foster|online_estimation_via_offline_estimation_an_informationtheoretic_framework'}},forum = 'sks7x4I8Bh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13369/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13369/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13369/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13369/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'skeopn3q5Y',number = 6750,cdate = 1715603920748,pdate = 1727287824490,odate = 1730873895854,mdate = 1736605947042,tcdate = 1715603920748,tmdate = 1736605947042,ddate = None,content = {'title': {'value': 'SfPUEL: Shape from Polarization under Unknown Environment Light'}, 'authors': {'value': ['Youwei Lyu', 'Heng Guo', 'Kailong Zhang', 'Si Li', 'Boxin Shi']}, 'authorids': {'value': ['~Youwei_Lyu1', '~Heng_Guo3', '~Kailong_Zhang1', '~Si_Li5', '~Boxin_Shi3']}, 'keywords': {'value': ['shape-from-polarization', 'photometric 3D reconstruction', 'physics-based vision']}, 'TLDR': {'value': 'Single-view shape from polarization by integrating pretrained model knowledge under unknown environment illumination.'}, 'abstract': {'value': 'Shape from polarization (SfP) benefits from advancements like polarization cameras for single-shot normal estimation, but its performance heavily relies on light conditions. This paper proposes SfPUEL, an end-to-end SfP method to jointly estimate surface normal and material under unknown environment light. To handle this challenging light condition, we design a transformer-based framework for enhancing the perception of global context features. We further propose to integrate photometric stereo (PS) priors from pretrained models to enrich extracted features for high-quality normal predictions. As metallic and dielectric materials exhibit different BRDFs, SfPUEL additionally predicts dielectric and metallic material segmentation to further boost performance. Experimental results on synthetic and our collected real-world dataset demonstrate that SfPUEL significantly outperforms existing SfP and single-shot normal estimation methods. The code and dataset is available at https://github.com/YouweiLyu/SfPUEL.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c45f78bb98e028e3ae80abb25f11f945f5cb1994.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlyu2024sfpuel,\\ntitle={Sf{PUEL}: Shape from Polarization under Unknown Environment Light},\\nauthor={Youwei Lyu and Heng Guo and Kailong Zhang and Si Li and Boxin Shi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=skeopn3q5Y}\\n}'}, 'paperhash': {'value': 'lyu|sfpuel_shape_from_polarization_under_unknown_environment_light'}},forum = 'skeopn3q5Y',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6750/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6750/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6750/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6750/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'siPdcro6uD',number = 8873,cdate = 1715670916826,pdate = 1727287893472,odate = 1730873915861,mdate = 1730873915880,tcdate = 1715670916826,tmdate = 1730873915880,ddate = None,content = {'title': {'value': 'OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling'}, 'authors': {'value': ['Linhui Xiao', 'Xiaoshan Yang', 'Fang Peng', 'Yaowei Wang', 'Changsheng Xu']}, 'authorids': {'value': ['~Linhui_Xiao1', '~Xiaoshan_Yang2', '~Fang_Peng1', '~Yaowei_Wang1', '~Changsheng_Xu1']}, 'keywords': {'value': ['Visual Grounding', 'Referring Expression Comprehension', 'Referring Image Segmentation', 'Multimodality', 'Masked Visual Language Modeling']}, 'abstract': {'value': \"Constrained by the separate encoding of vision and language, existing grounding and referring segmentation works heavily rely on bulky Transformer-based fusion en-/decoders and a variety of early-stage interaction technologies. Simultaneously, the current mask visual language modeling (MVLM) fails to capture the nuanced referential relationship between image-text in referring tasks. In this paper, we propose **OneRef**, a minimalist referring framework built on the modality-shared one-tower transformer that unifies the visual and linguistic feature spaces. To modeling the referential relationship, we introduce a novel MVLM paradigm called Mask Referring Modeling (**MRefM**), which encompasses both referring-aware mask image modeling and referring-aware mask language modeling. Both modules not only reconstruct modality-related content but also cross-modal referring content. Within MRefM, we propose a referring-aware dynamic image masking strategy that is aware of the referred region rather than relying on fixed ratios or generic random masking schemes. By leveraging the unified visual language feature space and incorporating MRefM's ability to model the referential relations, our approach enables direct regression of the referring results without resorting to various complex techniques. Our method consistently surpasses existing approaches and achieves SoTA performance on both grounding and segmentation tasks, providing valuable insights for future research. Our code and models are available at https://github.com/linhuixiao/OneRef.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling'}, 'pdf': {'value': '/pdf/1c7025c22a54c45d08ffe811054a011e5b616da7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxiao2024oneref,\\ntitle={OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling},\\nauthor={Linhui Xiao and Xiaoshan Yang and Fang Peng and Yaowei Wang and Changsheng Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=siPdcro6uD}\\n}'}, 'paperhash': {'value': 'xiao|oneref_unified_onetower_expression_grounding_and_segmentation_with_mask_referring_modeling'}},forum = 'siPdcro6uD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8873/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8873/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8873/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8873/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'shYQXpnBLB',number = 9888,cdate = 1715687422898,pdate = 1727287923819,odate = 1730873924320,mdate = 1736731822167,tcdate = 1715687422898,tmdate = 1736731822167,ddate = None,content = {'title': {'value': 'Association of Objects May Engender Stereotypes: Mitigating Association-Engendered Stereotypes in Text-to-Image Generation'}, 'authors': {'value': ['Junlei Zhou', 'Jiashi Gao', 'Xiangyu Zhao', 'Xin Yao', 'Xuetao Wei']}, 'authorids': {'value': ['~Junlei_Zhou4', '~Jiashi_Gao1', '~Xiangyu_Zhao1', '~Xin_Yao1', '~Xuetao_Wei2']}, 'keywords': {'value': ['Stereotypes', 'Diffusion Model', 'Text-to-Image']}, 'TLDR': {'value': 'A novel approach to mitigate association-engendered stereotypes in T2I diffusion models.'}, 'abstract': {'value': \"Text-to-Image (T2I) has witnessed significant advancements, demonstrating superior performance for various generative tasks. However, the presence of stereotypes in T2I introduces harmful biases that require urgent attention as the T2I \\n technology becomes more prominent.\\nPrevious work for stereotype mitigation mainly concentrated on mitigating stereotypes engendered with individual objects within images, which failed to address stereotypes engendered by the association of multiple objects, referred to as *Association-Engendered Stereotypes*. For example, mentioning  ''black people'' and ''houses''  separately in prompts may not exhibit stereotypes. Nevertheless, when these two objects are associated in prompts, the association of ''black people'' with ''poorer houses'' becomes more pronounced. To tackle this issue, we propose a novel framework, MAS, to Mitigate Association-engendered Stereotypes. This framework models the stereotype problem as a probability distribution alignment problem, aiming to align the stereotype probability distribution of the generated image with the stereotype-free distribution. The MAS framework primarily consists of the *Prompt-Image-Stereotype CLIP* (*PIS CLIP*) and *Sensitive Transformer*. The *PIS CLIP* learns the association between prompts, images, and stereotypes, which can establish the mapping of prompts to stereotypes. The *Sensitive Transformer* produces the sensitive constraints, which guide the stereotyped image distribution to align with the stereotype-free probability distribution. Moreover, recognizing that existing metrics are insufficient for accurately evaluating association-engendered stereotypes, we propose a novel metric, *Stereotype-Distribution-Total-Variation*(*SDTV*), to evaluate stereotypes in T2I. Comprehensive experiments demonstrate that our framework effectively mitigates association-engendered stereotypes.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/204050fa8d15e07a423f0a1184f86bf90d8f7cf8.pdf'}, 'supplementary_material': {'value': '/attachment/48d538e77b860668c59f1b79dce6b0d7fbad2d93.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024association,\\ntitle={Association of Objects May Engender Stereotypes: Mitigating Association-Engendered Stereotypes in Text-to-Image Generation},\\nauthor={Junlei Zhou and Jiashi Gao and Xiangyu Zhao and Xin Yao and Xuetao Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=shYQXpnBLB}\\n}'}, 'paperhash': {'value': 'zhou|association_of_objects_may_engender_stereotypes_mitigating_associationengendered_stereotypes_in_texttoimage_generation'}},forum = 'shYQXpnBLB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9888/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9888/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9888/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9888/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sgVOjDqUMT',number = 1613,cdate = 1714719383360,pdate = 1727287666253,odate = 1730873850359,mdate = 1730873850376,tcdate = 1714719383360,tmdate = 1730873850376,ddate = None,content = {'title': {'value': 'MiniCache: KV Cache Compression in Depth Dimension for Large Language Models'}, 'authors': {'value': ['Akide Liu', 'Jing Liu', 'Zizheng Pan', 'Yefei He', 'Gholamreza Haffari', 'Bohan Zhuang']}, 'authorids': {'value': ['~Akide_Liu1', '~Jing_Liu8', '~Zizheng_Pan1', '~Yefei_He1', '~Gholamreza_Haffari1', '~Bohan_Zhuang1']}, 'keywords': {'value': ['KV Cache', 'Large Language Models', 'Efficiency AI']}, 'TLDR': {'value': 'We introduce MiniCache, a novel method to compress the KV cache between adjacent layers from a depth perspective, achieving superior compression ratios, high throughput, and near-lossless performance.'}, 'abstract': {'value': 'A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity.  We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with cross-layer merging achieves a compression ratio of $1.53\\\\times$. Additionally, since MiniCache is orthogonal to existing quantization techniques, it can achieve a compression ratio of up to $5.02\\\\times$ when combined with the 4-bit quantization technique, enhancing inference throughput by approximately $5\\\\times$ and reducing the memory footprint by $41\\\\%$ compared to the FP16 full cache baseline, all while maintaining near-lossless performance. Project is available at https://minicache.vmv.re .'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cf0e786fde9a254832d5b5613361376f8fe0cddb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024minicache,\\ntitle={MiniCache: {KV} Cache Compression in Depth Dimension for Large Language Models},\\nauthor={Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sgVOjDqUMT}\\n}'}, 'paperhash': {'value': 'liu|minicache_kv_cache_compression_in_depth_dimension_for_large_language_models'}},forum = 'sgVOjDqUMT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1613/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1613/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1613/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1613/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sfPxUqzdPI',number = 2829,cdate = 1715168475878,pdate = 1727287701986,odate = 1730873861387,mdate = 1730873861400,tcdate = 1715168475878,tmdate = 1730873861400,ddate = None,content = {'title': {'value': 'Multi-scale Consistency for Robust 3D Registration via Hierarchical Sinkhorn Tree'}, 'authors': {'value': ['Chengwei Ren', 'Yifan Feng', 'Weixiang Zhang', 'Xiao-Ping Zhang', 'Yue Gao']}, 'authorids': {'value': ['~Chengwei_Ren1', '~Yifan_Feng1', '~Weixiang_Zhang1', '~Xiao-Ping_Zhang1', '~Yue_Gao4']}, 'keywords': {'value': ['Point Cloud Registration', 'Coarse-to-fine Mechanism', 'Correspondence Retrieval', 'Multi-scale Consistency']}, 'abstract': {'value': 'We study the problem of retrieving accurate correspondence through multi-scale consistency (MSC) for robust point cloud registration. Existing works in a coarse-to-fine manner either suffer from severe noisy correspondences caused by unreliable coarse matching or struggle to form outlier-free coarse-level correspondence sets. To tackle this, we present Hierarchical Sinkhorn Tree (HST), a pruned tree structure designed to hierarchically measure the local consistency of each coarse correspondence across multiple feature scales, thereby filtering out the local dissimilar ones. In this way, we convert the modeling of MSC for each correspondence into a BFS traversal with pruning of a K-ary tree rooted at the superpoint, with its K nearest neighbors in the feature pyramid serving as child nodes. To achieve efficient pruning and accurate vicinity characterization, we further propose a novel overlap-aware Sinkhorn Distance, which retains only the most likely overlapping points for local measurement and next level exploration. The modeling process essentially involves traversing a pair of HSTs synchronously and aggregating the consistency measures of corresponding tree nodes. Extensive experiments demonstrate HST consistently outperforms the state-of-the-art methods on both indoor and outdoor benchmarks.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present Hierarchical Sinkhorn Tree, a pruned tree structure designed to hierarchically measure the multi-scale consistency of coarse correspondences for point cloud registration.'}, 'pdf': {'value': '/pdf/ee535dc856d1d4bea09ea0ea17affe9d466274a5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nren2024multiscale,\\ntitle={Multi-scale Consistency for Robust 3D Registration via Hierarchical Sinkhorn Tree},\\nauthor={Chengwei Ren and Yifan Feng and Weixiang Zhang and Xiao-Ping Zhang and Yue Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sfPxUqzdPI}\\n}'}, 'paperhash': {'value': 'ren|multiscale_consistency_for_robust_3d_registration_via_hierarchical_sinkhorn_tree'}},forum = 'sfPxUqzdPI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2829/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2829/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2829/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2829/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'seYXqfGT0q',number = 524,cdate = 1713944896286,pdate = 1727287639435,odate = 1730873841487,mdate = 1736173221972,tcdate = 1713944896286,tmdate = 1736173221972,ddate = None,content = {'title': {'value': 'Prototypical Hash Encoding for On-the-Fly Fine-Grained Category Discovery'}, 'authors': {'value': ['Haiyang Zheng', 'Nan Pu', 'Wenjing Li', 'Nicu Sebe', 'Zhun Zhong']}, 'authorids': {'value': ['~Haiyang_Zheng1', '~Nan_Pu1', '~Wenjing_Li4', '~Nicu_Sebe1', '~Zhun_Zhong1']}, 'keywords': {'value': ['Category Discovery', 'On-the-fly', 'Deep Hash']}, 'abstract': {'value': \"In this paper, we study a practical yet challenging task, On-the-fly Category Discovery (OCD), aiming to online discover the newly-coming stream data that belong to both known and unknown classes, by leveraging only known category knowledge contained in labeled data. Previous OCD methods employ the hash-based technique to represent old/new categories by hash codes for instance-wise inference. However, directly mapping features into low-dimensional hash space not only inevitably damages the ability to distinguish classes and but also causes ``high sensitivity'' issue, especially for fine-grained classes, leading to inferior performance. To address these drawbacks, we propose a novel Prototypical Hash Encoding (PHE) framework consisting of Category-aware Prototype Generation (CPG) and Discriminative Category Encoding (DCE) to mitigate the sensitivity of hash code while preserving rich discriminative information contained in high-dimension feature space, in a two-stage projection fashion. CPG enables the model to fully capture the intra-category diversity by representing each category with multiple prototypes. DCE boosts the discrimination ability of hash code with the guidance of the generated category prototypes and the constraint of minimum separation distance. By jointly optimizing CPG and DCE, we demonstrate that these two components are mutually beneficial towards an effective OCD. Extensive experiments show the significant superiority of our PHE over previous methods, e.g. obtaining an improvement of +5.3% in ALL ACC averaged on all datasets. Moreover, due to the nature of the interpretable prototypes, we visually analyze the underlying mechanism of how PHE helps group certain samples into either known or unknown categories. Code is available at https://github.com/HaiyangZheng/PHE.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6c9715b8dcae81abdb9d730649c9a5bf577c074f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024prototypical,\\ntitle={Prototypical Hash Encoding for On-the-Fly Fine-Grained Category Discovery},\\nauthor={Haiyang Zheng and Nan Pu and Wenjing Li and Nicu Sebe and Zhun Zhong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=seYXqfGT0q}\\n}'}, 'paperhash': {'value': 'zheng|prototypical_hash_encoding_for_onthefly_finegrained_category_discovery'}},forum = 'seYXqfGT0q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission524/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission524/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission524/-/Revision', 'NeurIPS.cc/2024/Conference/Submission524/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'seAuMedrm5',number = 13743,cdate = 1715742584643,pdate = 1727288048763,odate = 1730873960080,mdate = 1730873960092,tcdate = 1715742584643,tmdate = 1730873960092,ddate = None,content = {'title': {'value': 'Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers'}, 'authors': {'value': ['Adam Stooke', 'Rohit Prabhavalkar', 'Khe Chai Sim', 'Pedro J Moreno Mengibar']}, 'authorids': {'value': ['~Adam_Stooke3', '~Rohit_Prabhavalkar1', '~Khe_Chai_Sim1', '~Pedro_J_Moreno_Mengibar1']}, 'keywords': {'value': ['ASR', 'Speech', 'Transducers', 'Transformers', 'Alignment']}, 'TLDR': {'value': 'We find that transformer encoders can perform audio-to-text alignment internally during a forward pass, a new phenomenon that allows greatly simplified ASR models.'}, 'abstract': {'value': \"Modern systems for automatic speech recognition, including the RNN-Transducer and Attention-based Encoder-Decoder (AED), are designed so that the encoder is not required to alter the time-position of information from the audio sequence into the embedding; alignment to the final text output is processed during decoding. We discover that the transformer-based encoder adopted in recent years is actually capable of performing the alignment internally during the forward pass, prior to decoding. This new phenomenon enables a simpler and more efficient model, the ''Aligner-Encoder''. To train it, we discard the dynamic programming of RNN-T in favor of the frame-wise cross-entropy loss of AED, while the decoder employs the lighter text-only recurrence of RNN-T without learned cross-attention---it simply scans embedding frames in order from the beginning, producing one token each until predicting the end-of-message. We conduct experiments demonstrating performance remarkably close to the state of the art, including a special inference configuration enabling long-form recognition. In a representative comparison, we measure the total inference time for our model to be 2x faster than RNN-T and 16x faster than AED.  Lastly, we find that the audio-text alignment is clearly visible in the self-attention weights of a certain layer, which could be said to perform ''self-transduction''.\"}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/11159878c4995c45c2677ccd7b2f5bbccb14fb0b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nstooke2024alignerencoders,\\ntitle={Aligner-Encoders: Self-Attention Transformers Can Be Self-Transducers},\\nauthor={Adam Stooke and Rohit Prabhavalkar and Khe Chai Sim and Pedro J Moreno Mengibar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=seAuMedrm5}\\n}'}, 'paperhash': {'value': 'stooke|alignerencoders_selfattention_transformers_can_be_selftransducers'}},forum = 'seAuMedrm5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13743/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13743/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13743/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13743/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'scw6Et4pEr',number = 1842,cdate = 1714840241056,pdate = 1727287673205,odate = 1730873852764,mdate = 1736827796589,tcdate = 1714840241056,tmdate = 1736827796589,ddate = None,content = {'title': {'value': 'DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction'}, 'authors': {'value': ['Qilong Ma', 'Haixu Wu', 'Lanxiang Xing', 'Shangchen Miao', 'Mingsheng Long']}, 'authorids': {'value': ['~Qilong_Ma1', '~Haixu_Wu1', '~Lanxiang_Xing2', '~Shangchen_Miao1', '~Mingsheng_Long5']}, 'keywords': {'value': ['Deep learning', 'Fluid prediction', 'Lagrangian perspective']}, 'TLDR': {'value': 'We propose DeepLag to tackle the intricate fluid dynamics by guiding the Eulerian fluid prediction with learned dynamics of tracked Lagrangian particles.'}, 'abstract': {'value': 'Accurately predicting the future fluid is vital to extensive areas such as meteorology, oceanology, and aerodynamics. However, since the fluid is usually observed from the Eulerian perspective, its moving and intricate dynamics are seriously obscured and confounded in static grids, bringing thorny challenges to the prediction. This paper introduces a new Lagrangian-Eulerian combined paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose DeepLag to discover hidden Lagrangian dynamics within the fluid by tracking the movements of adaptively sampled key particles. Further, DeepLag presents a new paradigm for fluid prediction, where the Lagrangian movement of the tracked particles is inferred from Eulerian observations, and their accumulated Lagrangian dynamics information is incorporated into global Eulerian evolving features to guide future prediction respectively. Tracking key particles not only provides a transparent and interpretable clue for fluid dynamics but also makes our model free from modeling complex correlations among massive grids for better efficiency. Experimentally, DeepLag excels in three challenging fluid prediction tasks covering 2D and 3D, simulated and real-world fluids. Code is available at this repository: https://github.com/thuml/DeepLag.'}, 'pdf': {'value': '/pdf/309d35434496f30fc4a463cf6b6241fd5d45fafb.pdf'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/3456c3e4ec83cf923e34314eebb79a302b01517a.zip'}, '_bibtex': {'value': '@inproceedings{\\nma2024deeplag,\\ntitle={DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid Prediction},\\nauthor={Qilong Ma and Haixu Wu and Lanxiang Xing and Shangchen Miao and Mingsheng Long},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=scw6Et4pEr}\\n}'}, 'paperhash': {'value': 'ma|deeplag_discovering_deep_lagrangian_dynamics_for_intuitive_fluid_prediction'}},forum = 'scw6Et4pEr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1842/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1842/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1842/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1842/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sbsaRj475E',number = 2068,cdate = 1714964425843,pdate = 1727287679904,odate = 1730873854625,mdate = 1730873854638,tcdate = 1714964425843,tmdate = 1730873854638,ddate = None,content = {'title': {'value': 'DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization'}, 'authors': {'value': ['haoweiz', 'Dehua Tang', 'Ji Liu', 'Mingjie Lu', 'Jintu Zheng', 'Jinzhang Peng', 'Dong Li', 'Yu Wang', 'Fan Jiang', 'Lu Tian', 'Spandan Tiwari', 'Ashish Sirasao', 'Jun-Hai Yong', 'Bin Wang', 'Emad Barsoum']}, 'authorids': {'value': ['~haoweiz1', '~Dehua_Tang1', '~Ji_Liu11', '~Mingjie_Lu1', '~Jintu_Zheng1', '~Jinzhang_Peng1', '~Dong_Li13', '~Yu_Wang73', '~Fan_Jiang5', '~Lu_Tian3', '~Spandan_Tiwari1', '~Ashish_Sirasao1', '~Jun-Hai_Yong3', '~Bin_Wang3', '~Emad_Barsoum1']}, 'keywords': {'value': ['Diffusion', 'Pruning', 'Speedup', 'Gradient optimization', 'SuperNet']}, 'TLDR': {'value': 'A diffusion pruner via few-step gradient optimization without retraining the diffusion model.'}, 'abstract': {'value': 'Diffusion models have achieved remarkable progress in the field of image generation due to their outstanding capabilities. However, these models require substantial computing resources because of the multi-step denoising process during inference. While traditional pruning methods have been employed to optimize these models, the retraining process necessitates large-scale training datasets and extensive computational costs to maintain generalization ability, making it neither convenient nor efficient. Recent studies attempt to utilize the similarity of features across adjacent denoising stages to reduce computational costs through simple and static strategies. However, these strategies cannot fully harness the potential of the similar feature patterns across adjacent timesteps. In this work, we propose a novel pruning method that derives an efficient diffusion model via a more intelligent and differentiable pruner. At the core of our approach is casting the model pruning process into a SubNet search process. Specifically, we first introduce a SuperNet based on standard diffusion via adding some backup connections built upon the similar features. We then construct a plugin pruner network and design optimization losses to identify redundant computation. Finally, our method can identify an optimal SubNet through few-step gradient optimization and a simple post-processing procedure. We conduct extensive experiments on various diffusion models including Stable Diffusion series and DiTs. Our DiP-GO approach achieves 4.4 x speedup for SD-1.5 without any loss of accuracy, significantly outperforming the previous state-of-the-art methods.'}, 'pdf': {'value': '/pdf/5a3bc5d77fe2b0c491b53d94bc1da70c0b57b3af.pdf'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nhaoweiz2024dipgo,\\ntitle={DiP-{GO}: A Diffusion Pruner via Few-step Gradient Optimization},\\nauthor={haoweiz and Dehua Tang and Ji Liu and Mingjie Lu and Jintu Zheng and Jinzhang Peng and Dong Li and Yu Wang and Fan Jiang and Lu Tian and Spandan Tiwari and Ashish Sirasao and Jun-Hai Yong and Bin Wang and Emad Barsoum},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sbsaRj475E}\\n}'}, 'paperhash': {'value': 'haoweiz|dipgo_a_diffusion_pruner_via_fewstep_gradient_optimization'}},forum = 'sbsaRj475E',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2068/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2068/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2068/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2068/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'satH8Evs2y',number = 4259,cdate = 1715394373814,pdate = 1727287745641,odate = 1730873874258,mdate = 1734580016470,tcdate = 1715394373814,tmdate = 1734580016470,ddate = None,content = {'title': {'value': 'Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation'}, 'authors': {'value': ['Hangcheng Liu', 'Zhenhu Wu', 'Hao Wang', 'XINGSHUO HAN', 'Shangwei Guo', 'Tao Xiang', 'Tianwei Zhang']}, 'authorids': {'value': ['~Hangcheng_Liu1', '~Zhenhu_Wu1', '~Hao_Wang66', '~XINGSHUO_HAN1', '~Shangwei_Guo1', '~Tao_Xiang2', '~Tianwei_Zhang1']}, 'keywords': {'value': ['monocular depth estimation', 'adversarial patch', 'road dependence']}, 'TLDR': {'value': 'We propose a new road adversarial patch against MDE models based on our groundbreaking findings, which is completely different from previous obstacle patches and better adapts to complex traffic scenarios.'}, 'abstract': {'value': 'Monocular Depth Estimation (MDE) enables the prediction of scene depths from a single RGB image, having been widely integrated into production-grade autonomous driving systems, e.g., Tesla Autopilot. Current adversarial attacks to MDE models focus on attaching an optimized adversarial patch to a designated obstacle. Although effective, this approach presents two inherent limitations: its reliance on specific obstacles and its limited malicious impact. In contrast, we propose a pioneering attack to MDE models that \\\\textit{decouples obstacles from patches physically and deploys optimized patches on roads}, thereby extending the attack scope to arbitrary traffic participants. This approach is inspired by our groundbreaking discovery: \\\\textit{various MDE models with different architectures, trained for autonomous driving, heavily rely on road regions} when predicting depths for different obstacles. Based on this discovery, we design the Adversarial Road Marking (AdvRM) attack, which camouflages patches as ordinary road markings and deploys them on roads, thereby posing a continuous threat within the environment. Experimental results from both dataset simulations and real-world scenarios demonstrate that AdvRM is effective, stealthy, and robust against various MDE models, achieving about 1.507 of Mean Relative Shift Ratio (MRSR) over 8 MDE models. The code is available at \\\\url{https://github.com/a-c-a-c/AdvRM.git}'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a1ebdee7fb14adc46308d7fb5dd26e102ea198cd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024beware,\\ntitle={Beware of Road Markings: A New Adversarial Patch Attack to Monocular Depth Estimation},\\nauthor={Hangcheng Liu and Zhenhu Wu and Hao Wang and XINGSHUO HAN and Shangwei Guo and Tao Xiang and Tianwei Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=satH8Evs2y}\\n}'}, 'paperhash': {'value': 'liu|beware_of_road_markings_a_new_adversarial_patch_attack_to_monocular_depth_estimation'}},forum = 'satH8Evs2y',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4259/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4259/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4259/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4259/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'sZ7jj9kqAy',number = 19659,cdate = 1715792188111,pdate = 1727288214966,odate = 1730873996962,mdate = 1730873996996,tcdate = 1715792188111,tmdate = 1730873996996,ddate = None,content = {'title': {'value': 'SOI: Scaling Down Computational Complexity by Estimating Partial States of the Model'}, 'authors': {'value': ['Grzegorz Stefański', 'Paweł Daniluk', 'Artur Szumaczuk', 'Jakub Tkaczuk']}, 'authorids': {'value': ['~Grzegorz_Stefański1', '~Paweł_Daniluk1', '~Artur_Szumaczuk1', '~Jakub_Tkaczuk1']}, 'keywords': {'value': ['Time series data', 'Computational complexity reduction', 'Latency reduction', 'Real-Time results', 'Inference at the edge', 'Causality']}, 'abstract': {'value': 'Consumer electronics used to follow the miniaturization trend described by Moore’s Law. Despite increased processing power in Microcontroller Units (MCUs), MCUs used in the smallest appliances are still not capable of running even moderately big, state-of-the-art artificial neural networks (ANNs) especially in time-sensitive scenarios. In this work, we present a novel method called Scattered Online Inference (SOI) that aims to reduce the computational complexity of ANNs. SOI leverages the continuity and seasonality of time-series data and model predictions, enabling extrapolation for processing speed improvements, particularly in deeper layers. By applying compression, SOI generates more general inner partial states of ANN, allowing skipping full model recalculation at each inference.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/776e944cdfb41b98a314ae4e2187d64dbc79107c.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': \"@inproceedings{\\nstefa{\\\\'n}ski2024soi,\\ntitle={{SOI}: Scaling Down Computational Complexity by Estimating Partial States of the Model},\\nauthor={Grzegorz Stefa{\\\\'n}ski and Pawe{\\\\l} Daniluk and Artur Szumaczuk and Jakub Tkaczuk},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sZ7jj9kqAy}\\n}\"}, 'paperhash': {'value': 'stefaski|soi_scaling_down_computational_complexity_by_estimating_partial_states_of_the_model'}},forum = 'sZ7jj9kqAy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19659/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19659/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19659/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19659/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'sVZBJoxwk9',number = 19041,cdate = 1715788850580,pdate = 1727288200158,odate = 1730873993407,mdate = 1730873993426,tcdate = 1715788850580,tmdate = 1730873993426,ddate = None,content = {'title': {'value': 'Generalized Eigenvalue Problems with Generative Priors'}, 'authors': {'value': ['Zhaoqiang Liu', 'Wen Li', 'Junren Chen']}, 'authorids': {'value': ['~Zhaoqiang_Liu1', '~Wen_Li2', '~Junren_Chen1']}, 'keywords': {'value': ['Generalized Eigenvalue Problems', 'Generative Priors', 'Optimal Statistical Rate']}, 'abstract': {'value': \"Generalized eigenvalue problems (GEPs) find applications in various fields of science and engineering. For example, principal component analysis, Fisher's discriminant analysis, and canonical correlation analysis are specific instances of GEPs and are widely used in statistical data processing. In this work, we study GEPs under generative priors, assuming that the underlying leading generalized eigenvector lies within the range of a Lipschitz continuous generative model. Under appropriate conditions, we show that any optimal solution to the corresponding optimization problems attains the optimal statistical rate. Moreover, from a computational perspective, we propose an iterative algorithm called the Projected Rayleigh Flow Method (PRFM) to approximate the optimal solution. We theoretically demonstrate that under suitable assumptions, PRFM converges linearly to an estimated vector that achieves the optimal statistical rate. Numerical results are provided to demonstrate the effectiveness of the proposed method.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/38058c562967539b925e50a30dcf0fb692de98ed.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nliu2024generalized,\\ntitle={Generalized Eigenvalue Problems with Generative Priors},\\nauthor={Zhaoqiang Liu and Wen Li and Junren Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sVZBJoxwk9}\\n}'}, 'paperhash': {'value': 'liu|generalized_eigenvalue_problems_with_generative_priors'}},forum = 'sVZBJoxwk9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19041/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19041/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19041/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19041/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sRSjr9SDKR',number = 6060,cdate = 1715582191482,pdate = 1727287803855,odate = 1730873890232,mdate = 1736508913181,tcdate = 1715582191482,tmdate = 1736508913181,ddate = None,content = {'title': {'value': 'Preferential Normalizing Flows'}, 'authors': {'value': ['Petrus Mikkola', 'Luigi Acerbi', 'Arto Klami']}, 'authorids': {'value': ['~Petrus_Mikkola1', '~Luigi_Acerbi1', '~Arto_Klami1']}, 'keywords': {'value': ['normalizing flow', 'elicitation', 'random utility models', 'prior distribution']}, 'TLDR': {'value': \"We show how normalising flows can be fitted for preferential data representing expert's choices between a set of alternatives, as a function-space maximum a posteriori estimate with a novel functional prior.\"}, 'abstract': {'value': \"Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling. We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives. This allows eliciting in principle arbitrarily flexible densities, but flow estimation is susceptible to the challenge of collapsing or diverging probability mass that makes it difficult in practice. We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate. We demonstrate our method by eliciting multivariate belief densities of simulated experts, including the prior belief of a general-purpose large language model over a real-world dataset.\"}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/abd193721754aec5573872648a64077f6c49cd95.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmikkola2024preferential,\\ntitle={Preferential Normalizing Flows},\\nauthor={Petrus Mikkola and Luigi Acerbi and Arto Klami},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sRSjr9SDKR}\\n}'}, 'paperhash': {'value': 'mikkola|preferential_normalizing_flows'}},forum = 'sRSjr9SDKR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6060/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6060/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6060/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6060/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sRILMnkkQd',number = 8872,cdate = 1715670914380,pdate = 1727287893448,odate = 1730873915843,mdate = 1730873915859,tcdate = 1715670914380,tmdate = 1730873915859,ddate = None,content = {'title': {'value': 'UniGAD: Unifying Multi-level Graph Anomaly Detection'}, 'authors': {'value': ['Yiqing Lin', 'Jianheng Tang', 'Chenyi Zi', 'H. Vicky Zhao', 'Yuan Yao', 'Jia Li']}, 'authorids': {'value': ['~Yiqing_Lin3', '~Jianheng_Tang1', '~Chenyi_Zi2', '~H._Vicky_Zhao1', '~Yuan_Yao1', '~Jia_Li4']}, 'keywords': {'value': ['Graph Anomaly Detection', 'Graph Neural Networks']}, 'TLDR': {'value': 'We propose the first unified framework for detecting anomalies at node, edge, and graph levels.'}, 'abstract': {'value': 'Graph Anomaly Detection (GAD) aims to identify uncommon, deviated, or suspicious objects within graph-structured data. Existing methods generally focus on a single graph object type (node, edge, graph, etc.) and often overlook the inherent connections among different object types of graph anomalies. For instance, a money laundering transaction might involve an abnormal account and the broader community it interacts with. To address this, we present UniGAD, the first unified framework for detecting anomalies at node, edge, and graph levels jointly. Specifically, we develop the Maximum Rayleigh Quotient Subgraph Sampler (MRQSampler) that unifies multi-level formats by transferring objects at each level into graph-level tasks on subgraphs. We theoretically prove that MRQSampler maximizes the accumulated spectral energy of subgraphs (i.e., the Rayleigh quotient) to preserve the most significant anomaly information. To further unify multi-level training, we introduce a novel GraphStitch Network to integrate information across different levels, adjust the amount of sharing required at each level, and harmonize conflicting training goals. Comprehensive experiments show that UniGAD outperforms both existing GAD methods specialized for a single task and graph prompt-based approaches for multiple tasks, while also providing robust zero-shot task transferability.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3787248272e67ad0479d37e7cdbee9c24a42fa35.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024unigad,\\ntitle={Uni{GAD}: Unifying Multi-level Graph Anomaly Detection},\\nauthor={Yiqing Lin and Jianheng Tang and Chenyi Zi and H. Vicky Zhao and Yuan Yao and Jia Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sRILMnkkQd}\\n}'}, 'paperhash': {'value': 'lin|unigad_unifying_multilevel_graph_anomaly_detection'}},forum = 'sRILMnkkQd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8872/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8872/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8872/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8872/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'sQApQMBqiP',number = 18668,cdate = 1715786718653,pdate = 1727288190264,odate = 1730873991306,mdate = 1730873991325,tcdate = 1715786718653,tmdate = 1730873991325,ddate = None,content = {'title': {'value': 'Learning Human-like Representations to Enable Learning Human Values'}, 'authors': {'value': ['Andrea Wynn', 'Ilia Sucholutsky', 'Thomas L. Griffiths']}, 'authorids': {'value': ['~Andrea_Wynn1', '~Ilia_Sucholutsky1', '~Thomas_L._Griffiths1']}, 'keywords': {'value': ['value alignment', 'representational alignment', 'kernel methods', 'language models']}, 'abstract': {'value': 'How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9ae0acd7c25e5f1656ba098bea69c635f6f26efd.pdf'}, 'supplementary_material': {'value': '/attachment/2927ddf8ccc260e501e5cf3039918437d349fe4e.zip'}, '_bibtex': {'value': '@inproceedings{\\nwynn2024learning,\\ntitle={Learning Human-like Representations to Enable Learning Human Values},\\nauthor={Andrea Wynn and Ilia Sucholutsky and Thomas L. Griffiths},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sQApQMBqiP}\\n}'}, 'paperhash': {'value': 'wynn|learning_humanlike_representations_to_enable_learning_human_values'}},forum = 'sQApQMBqiP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18668/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18668/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18668/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18668/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sOhFyFFnxT',number = 18526,cdate = 1715785959147,pdate = 1727288186577,odate = 1730873990551,mdate = 1737382300266,tcdate = 1715785959147,tmdate = 1737382300266,ddate = None,content = {'title': {'value': 'Exploring the Precise Dynamics of Single-Layer GAN Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning'}, 'authors': {'value': ['Andrew Bond', 'Zafer Dogan']}, 'authorids': {'value': ['~Andrew_Bond1', '~Zafer_Dogan1']}, 'keywords': {'value': ['Training Dynamics', 'High-Dimensional Analysis', 'Scaling Limit Analysis', 'Subspace Learning']}, 'abstract': {'value': 'Subspace learning is a critical endeavor in contemporary machine learning, particularly given the vast dimensions of modern datasets. In this study, we delve into the training dynamics of a single-layer GAN model from the perspective of subspace learning, framing these GANs as a novel approach to this fundamental task. Through a rigorous scaling limit analysis, we offer insights into the behavior of this model. Extending beyond prior research that primarily focused on sequential feature learning, we investigate the non-sequential scenario, emphasizing the pivotal role of inter-feature interactions in expediting training and enhancing performance, particularly with an uninformed initialization strategy. Our investigation encompasses both synthetic and real-world datasets, such as MNIST and Olivetti Faces, demonstrating the robustness and applicability of our findings to practical scenarios. By bridging our analysis to the realm of subspace learning, we systematically compare the efficacy of GAN-based methods against conventional approaches, both theoretically and empirically. Notably, our results unveil that while all methodologies successfully capture the underlying subspace, GANs exhibit a remarkable capability to acquire a more informative basis, owing to their intrinsic ability to generate new data samples. This elucidates the unique advantage of GAN-based approaches in subspace learning tasks.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c1614b551656053cedd678a3f2a3c41e24876e83.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbond2024exploring,\\ntitle={Exploring the Precise Dynamics of Single-Layer {GAN} Models: Leveraging Multi-Feature Discriminators for High-Dimensional Subspace Learning},\\nauthor={Andrew Bond and Zafer Dogan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sOhFyFFnxT}\\n}'}, 'paperhash': {'value': 'bond|exploring_the_precise_dynamics_of_singlelayer_gan_models_leveraging_multifeature_discriminators_for_highdimensional_subspace_learning'}},forum = 'sOhFyFFnxT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18526/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18526/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18526/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18526/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sNz7tptCH6',number = 10921,cdate = 1715699075484,pdate = 1727287955041,odate = 1730873933358,mdate = 1736999423281,tcdate = 1715699075484,tmdate = 1736999423281,ddate = None,content = {'title': {'value': 'Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning'}, 'authors': {'value': ['Di Ming', 'Peng Ren', 'Yunlong Wang', 'Xin Feng']}, 'authorids': {'value': ['~Di_Ming1', '~Peng_Ren7', '~Yunlong_Wang2', '~Xin_Feng2']}, 'keywords': {'value': ['Adversarial Attack', 'Adversarial Transferability', 'Black-box Attack']}, 'abstract': {'value': 'Vision transformers (ViTs) perform exceptionally well in various computer vision tasks but remain vulnerable to adversarial attacks. Recent studies have shown that the transferability of adversarial examples exists for CNNs, and the same holds true for ViTs. However, existing ViT attacks aggressively regularize the largest token gradients to exact zero within each layer of the surrogate model, overlooking the interactions between layers, which limits their transferability in attacking black-box models. Therefore, in this paper, we focus on boosting the transferability of adversarial attacks on ViTs through adaptive token tuning (ATT). Specifically, we propose three optimization strategies: an adaptive gradient re-scaling strategy to reduce the overall variance of token gradients, a self-paced patch out strategy to enhance the diversity of input tokens, and a hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. We demonstrate that scaling correction of gradient changes using gradient variance across different layers can produce highly transferable adversarial examples. In addition, introducing attentional truncation can mitigate the overfitting over complex interactions between tokens in deep ViT layers to further improve the transferability. On the other hand, using feature importance as a guidance to discard a subset of perturbation patches in each iteration, along with combining self-paced learning and progressively more sampled attacks, significantly enhances the transferability over attacks that use all perturbation patches. Extensive experiments conducted on ViTs, undefended CNNs, and defended CNNs validate the superiority of our proposed ATT attack method. On average, our approach improves the attack performance by 10.1% compared to state-of-the-art transfer-based attacks. Notably, we achieve the best attack performance with an average of 58.3% on three defended CNNs. Code is available at https://github.com/MisterRpeng/ATT.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0eb91b74d74a0ac8616b2c6d1a9f47ca25dff645.pdf'}, 'supplementary_material': {'value': '/attachment/01db82d8e9456e8b8a7b05e0db28c9e122827e91.zip'}, '_bibtex': {'value': '@inproceedings{\\nming2024boosting,\\ntitle={Boosting the Transferability of Adversarial Attack on Vision Transformer with Adaptive Token Tuning},\\nauthor={Di Ming and Peng Ren and Yunlong Wang and Xin Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sNz7tptCH6}\\n}'}, 'paperhash': {'value': 'ming|boosting_the_transferability_of_adversarial_attack_on_vision_transformer_with_adaptive_token_tuning'}},forum = 'sNz7tptCH6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10921/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10921/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10921/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10921/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sKEhebkEdz',number = 4416,cdate = 1715411912690,pdate = 1727287750117,odate = 1730873875699,mdate = 1730873875771,tcdate = 1715411912690,tmdate = 1730873875771,ddate = None,content = {'title': {'value': 'Semi-supervised Knowledge Transfer Across Multi-omic Single-cell Data'}, 'authors': {'value': ['Fan Zhang', 'Tianyu Liu', 'Zihao Chen', 'Xiaojiang Peng', 'Chong Chen', 'Xian-Sheng Hua', 'Xiao Luo', 'Hongyu Zhao']}, 'authorids': {'value': ['~Fan_Zhang27', '~Tianyu_Liu4', '~Zihao_Chen17', '~Xiaojiang_Peng1', '~Chong_Chen2', '~Xian-Sheng_Hua1', '~Xiao_Luo3', '~Hongyu_Zhao1']}, 'keywords': {'value': ['Computional Biology', 'Semi-supervised Learning', 'Transfer Learning']}, 'TLDR': {'value': 'We study the problem of semi-supervised knowledge transfer across multi-omic single-cell data and propose a novel framework named DNACE for the problem.'}, 'abstract': {'value': 'Knowledge transfer between multi-omic single-cell data aims to effectively transfer cell types from scRNA-seq data to unannotated scATAC-seq data. Several approaches aim to reduce the heterogeneity of multi-omic data while maintaining the discriminability of cell types with extensive annotated data. However, in reality, the cost of collecting both a large amount of labeled scRNA-seq data and scATAC-seq data is expensive. Therefore, this paper explores a practical yet underexplored problem of knowledge transfer across multi-omic single-cell data under cell type scarcity. To address this problem, we propose a semi-supervised knowledge transfer framework named Dual label scArcity elimiNation with Cross-omic multi-samplE Mixup (DANCE). To overcome the label scarcity in scRNA-seq data, we generate pseudo-labels based on optimal transport and merge them into the labeled scRNA-seq data. Moreover, we adopt a divide-and-conquer strategy which divides the scATAC-seq data into source-like and target-specific data. For source-like samples, we employ consistency regularization with random perturbations while for target-specific samples, we select a few candidate labels and progressively eliminate incorrect cell types from the label set for additional supervision. Next, we generate virtual scRNA-seq samples with multi-sample Mixup based on the class-wise similarity to reduce cell heterogeneity. Extensive experiments on many benchmark datasets suggest the superiority of our DANCE over a series of state-of-the-art methods.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/de7161289e3fd90f8a1f92ddbf473bb198bfdf40.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024semisupervised,\\ntitle={Semi-supervised Knowledge Transfer Across Multi-omic Single-cell Data},\\nauthor={Fan Zhang and Tianyu Liu and Zihao Chen and Xiaojiang Peng and Chong Chen and Xian-Sheng Hua and Xiao Luo and Hongyu Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sKEhebkEdz}\\n}'}, 'paperhash': {'value': 'zhang|semisupervised_knowledge_transfer_across_multiomic_singlecell_data'}},forum = 'sKEhebkEdz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4416/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4416/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4416/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4416/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sKCKPr8cRL',number = 2108,cdate = 1714977386404,pdate = 1727287681118,odate = 1730873855239,mdate = 1730873855257,tcdate = 1714977386404,tmdate = 1730873855257,ddate = None,content = {'title': {'value': 'Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies'}, 'authors': {'value': ['Chaofan Tao', 'Qian Liu', 'Longxu Dou', 'Niklas Muennighoff', 'Zhongwei Wan', 'Ping Luo', 'Min Lin', 'Ngai Wong']}, 'authorids': {'value': ['~Chaofan_Tao1', '~Qian_Liu2', '~Longxu_Dou1', '~Niklas_Muennighoff1', '~Zhongwei_Wan1', '~Ping_Luo2', '~Min_Lin1', '~Ngai_Wong1']}, 'keywords': {'value': ['Natural Language Processing', 'Scaling Laws', 'Efficient Neural Networks', 'Large Language Models']}, 'TLDR': {'value': 'This paper introduces a novel framework to study models with different vocabularies, substantiating a scaling law that optimizes computational resources with the consideration of vocabulary and other attributes jointly.'}, 'abstract': {'value': 'Research on scaling large language models (LLMs) has primarily focused on model parameters and training data size, overlooking the role of vocabulary size. We investigate how vocabulary size impacts LLM scaling laws by training models ranging from 33M to 3B parameters on up to 500B characters with various vocabulary configurations. We propose three complementary approaches for predicting the compute-optimal vocabulary size: IsoFLOPs analysis, derivative estimation, and parametric fit of the loss function. Our approaches converge on the conclusion that the optimal vocabulary size depends on the compute budget, with larger models requiring larger vocabularies. Most LLMs, however, use insufficient vocabulary sizes. For example, we predict that the optimal vocabulary size of Llama2-70B should have been at least 216K, 7 times larger than its vocabulary of 32K. We validate our predictions empirically by training models with 3B parameters across different FLOPs budgets. Adopting our predicted optimal vocabulary size consistently improves downstream performance over commonly used vocabulary sizes. By increasing the vocabulary size from the conventional 32K to 43K, we improve performance on ARC-Challenge from 29.1 to 32.0 with the same 2.3e21 FLOPs.  Our work highlights the importance of jointly considering tokenization and model scaling for efficient pre-training.  The code and demo are available at https://github.com/sail-sg/scaling-with-vocab and https://hf.co/spaces/sail/scaling-with-vocab-demo.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/134f55cd9235bc28c2d2cba435869c22f9d830f4.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntao2024scaling,\\ntitle={Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies},\\nauthor={Chaofan Tao and Qian Liu and Longxu Dou and Niklas Muennighoff and Zhongwei Wan and Ping Luo and Min Lin and Ngai Wong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sKCKPr8cRL}\\n}'}, 'paperhash': {'value': 'tao|scaling_laws_with_vocabulary_larger_models_deserve_larger_vocabularies'}},forum = 'sKCKPr8cRL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2108/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2108/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2108/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2108/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sIsbOkQmBL',number = 14495,cdate = 1715750617022,pdate = 1727288071227,odate = 1730873966279,mdate = 1730873966311,tcdate = 1715750617022,tmdate = 1730873966311,ddate = None,content = {'title': {'value': 'CultureLLM: Incorporating Cultural Differences into Large Language Models'}, 'authors': {'value': ['CHENG LI', 'Mengzhuo Chen', 'Jindong Wang', 'Sunayana Sitaram', 'Xing Xie']}, 'authorids': {'value': ['~CHENG_LI26', '~Mengzhuo_Chen1', '~Jindong_Wang4', '~Sunayana_Sitaram1', '~Xing_Xie3']}, 'keywords': {'value': ['Culture bias', 'large language models', 'fairness']}, 'TLDR': {'value': 'We propose a cost-effective fine-tuning approach to fine-tune cultural specific LLMs'}, 'abstract': {'value': 'Large language models (LLMs) have been observed to exhibit bias towards certain cultures due to the predominance of training data obtained from English corpora. Considering that multilingual cultural data is often expensive to procure, existing methodologies address this challenge through prompt engineering or culture-specific pre-training. However, these strategies may neglect the knowledge deficiency of low-resource cultures and necessitate substantial computing resources. In this paper, we propose CultureLLM, a cost-effective solution to integrate cultural differences into LLMs. CultureLLM employs the World Value Survey (WVS) as seed data and generates semantically equivalent training data through the proposed semantic data augmentation. Utilizing only $50$ seed samples from WVS with augmented data, we fine-tune culture-specific LLMs as well as a unified model (CultureLLM-One) for $9$ cultures, encompassing both rich and low-resource languages. Extensive experiments conducted on $60$ culture-related datasets reveal that CultureLLM significantly surpasses various counterparts such as GPT-3.5 (by $8.1$\\\\%) and Gemini Pro (by $9.5$\\\\%), demonstrating performance comparable to or exceeding that of GPT-4. Our human study indicates that the generated samples maintain semantic equivalence to the original samples, offering an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.'}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b98b0e66b7f84b55b9c4c0b90ab0a408860590a1.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nli2024culturellm,\\ntitle={Culture{LLM}: Incorporating Cultural Differences into Large Language Models},\\nauthor={CHENG LI and Mengzhuo Chen and Jindong Wang and Sunayana Sitaram and Xing Xie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sIsbOkQmBL}\\n}'}, 'paperhash': {'value': 'li|culturellm_incorporating_cultural_differences_into_large_language_models'}},forum = 'sIsbOkQmBL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14495/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14495/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14495/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14495/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'sGvZyV2iqN',number = 16782,cdate = 1715774338614,pdate = 1727288136329,odate = 1730873981247,mdate = 1730873981261,tcdate = 1715774338614,tmdate = 1730873981261,ddate = None,content = {'title': {'value': 'HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach'}, 'authors': {'value': ['Maxim Nikolaev', 'Mikhail Kuznetsov', 'Dmitry Vetrov', 'Aibek Alanov']}, 'authorids': {'value': ['~Maxim_Nikolaev1', '~Mikhail_Kuznetsov2', '~Dmitry_P._Vetrov1', '~Aibek_Alanov1']}, 'keywords': {'value': ['Generative Model', 'StyleGAN', 'HairSwap']}, 'TLDR': {'value': 'Our paper introduces the HairFast model, which uses a novel architecture in the FS latent space of StyleGAN to achieve high-resolution, near real-time hairstyle transfer with superior results, even when source and target poses differ significantly.'}, 'abstract': {'value': \"Our paper addresses the complex task of transferring a hairstyle from a reference image to an input photo for virtual hair try-on. This task is challenging due to the need to adapt to various photo poses, the sensitivity of hairstyles, and the lack of objective metrics. The current state of the art hairstyle transfer methods use an optimization process for different parts of the approach, making them inexcusably slow. At the same time, faster encoder-based models are of very low quality because they either operate in StyleGAN's W+ space or use other low-dimensional image generators. Additionally, both approaches have a problem with hairstyle transfer when the source pose is very different from the target pose, because they either don't consider the pose at all or deal with it inefficiently. In our paper, we present the HairFast model, which uniquely solves these problems and achieves high resolution, near real-time performance, and superior reconstruction compared to optimization problem-based methods. Our solution includes a new architecture operating in the FS latent space of StyleGAN, an enhanced inpainting approach, and improved encoders for better alignment, color transfer, and a new encoder for post-processing. The effectiveness of our approach is demonstrated on realism metrics after random hairstyle transfer and reconstruction when the original hairstyle is transferred. In the most difficult scenario of transferring both shape and color of a hairstyle from different images, our method performs in less than a second on the Nvidia V100.\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/893be3e12a31ccf9fa15e8524f38b2d07f184d37.pdf'}, 'supplementary_material': {'value': '/attachment/19f6eb10a9d3c30245418bfc244f2d7fb843dfaa.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nnikolaev2024hairfastgan,\\ntitle={HairFast{GAN}: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach},\\nauthor={Maxim Nikolaev and Mikhail Kuznetsov and Dmitry Vetrov and Aibek Alanov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sGvZyV2iqN}\\n}'}, 'paperhash': {'value': 'nikolaev|hairfastgan_realistic_and_robust_hair_transfer_with_a_fast_encoderbased_approach'}},forum = 'sGvZyV2iqN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16782/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16782/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16782/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16782/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'sFaFDcVNbW',number = 5711,cdate = 1715566611454,pdate = 1727287792303,odate = 1730873887435,mdate = 1730873887460,tcdate = 1715566611454,tmdate = 1730873887460,ddate = None,content = {'title': {'value': 'GSGAN: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats'}, 'authors': {'value': ['Sangeek Hyun', 'Jae-Pil Heo']}, 'authorids': {'value': ['~Sangeek_Hyun1', '~Jae-Pil_Heo3']}, 'keywords': {'value': ['3D GANs', 'Generative Adversarial Networks', '3D Gaussian Splatting', '3D Generative Models']}, 'TLDR': {'value': 'For the first time, we leverage 3D Gaussian representation in 3D GANs for efficient rendering with explicit 3D representation.'}, 'abstract': {'value': 'Most advances in 3D Generative Adversarial Networks (3D GANs) largely depend on ray casting-based volume rendering, which incurs demanding rendering costs. One promising alternative is rasterization-based 3D Gaussian Splatting (3D-GS), providing a much faster rendering speed and explicit 3D representation. In this paper, we exploit Gaussian as a 3D representation for 3D GANs by leveraging its efficient and explicit characteristics. However, in an adversarial framework, we observe that a na\\\\\"ive generator architecture suffers from training instability and lacks the capability to adjust the scale of Gaussians. This leads to model divergence and visual artifacts due to the absence of proper guidance for initialized positions of Gaussians and densification to manage their scales adaptively. To address these issues, we introduce GSGAN, a generator architecture with a hierarchical multi-scale Gaussian representation that effectively regularizes the position and scale of generated Gaussians. Specifically, we design a hierarchy of Gaussians where finer-level Gaussians are parameterized by their coarser-level counterparts; the position of finer-level Gaussians would be located near their coarser-level counterparts, and the scale would monotonically decrease as the level becomes finer, modeling both coarse and fine details of the 3D scene. Experimental results demonstrate that ours achieves a significantly faster rendering speed (×100) compared to state-of-the-art 3D consistent GANs with comparable 3D generation capability.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/460ef1b126e0fe69e5f5e9e5794ac258d86790db.pdf'}, 'supplementary_material': {'value': '/attachment/e368067f506167cebdcb6f55919a765c15dde4e7.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nhyun2024gsgan,\\ntitle={{GSGAN}: Adversarial Learning for Hierarchical Generation of 3D Gaussian Splats},\\nauthor={Sangeek Hyun and Jae-Pil Heo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sFaFDcVNbW}\\n}'}, 'paperhash': {'value': 'hyun|gsgan_adversarial_learning_for_hierarchical_generation_of_3d_gaussian_splats'}},forum = 'sFaFDcVNbW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5711/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5711/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5711/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission5711/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'sEpSxteEKJ',number = 15406,cdate = 1715759920993,pdate = 1727288097067,odate = 1730873972724,mdate = 1736935666860,tcdate = 1715759920993,tmdate = 1736935666860,ddate = None,content = {'title': {'value': 'Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction'}, 'authors': {'value': ['Manuel Brenner', 'Christoph Jürgen Hemmer', 'Zahra Monfared', 'Daniel Durstewitz']}, 'authorids': {'value': ['~Manuel_Brenner1', '~Christoph_Jürgen_Hemmer1', '~Zahra_Monfared1', '~Daniel_Durstewitz1']}, 'keywords': {'value': ['recurrent neural networks', 'dynamical systems', 'chaos', 'attractors', 'interpretability']}, 'TLDR': {'value': 'We introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) to derive highly interpretable piecewise-linear models of dynamical systems from time-series data.'}, 'abstract': {'value': 'Dynamical systems theory (DST) is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS combined by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and Rössler systems, AL-RNNs derive, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/313bf596b791fc1b3d19f3d914cb78e8e5886862.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbrenner2024almostlinear,\\ntitle={Almost-Linear {RNN}s Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction},\\nauthor={Manuel Brenner and Christoph J{\\\\\"u}rgen Hemmer and Zahra Monfared and Daniel Durstewitz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sEpSxteEKJ}\\n}'}, 'paperhash': {'value': 'brenner|almostlinear_rnns_yield_highly_interpretable_symbolic_codes_in_dynamical_systems_reconstruction'}},forum = 'sEpSxteEKJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15406/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15406/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15406/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15406/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'sABwo1ZTFi',number = 6333,cdate = 1715590392783,pdate = 1727287811394,odate = 1730873892431,mdate = 1736235182261,tcdate = 1715590392783,tmdate = 1736235182261,ddate = None,content = {'title': {'value': 'Generalizablity of Memorization Neural Network'}, 'authors': {'value': ['Lijia Yu', 'Xiao-Shan Gao', 'Lijun Zhang', 'Yibo Miao']}, 'authorids': {'value': ['~Lijia_Yu2', '~Xiao-Shan_Gao2', '~Lijun_Zhang2', '~Yibo_Miao1']}, 'keywords': {'value': ['generalization', 'memorization', 'sample complexity']}, 'TLDR': {'value': 'generalization of memorization'}, 'abstract': {'value': 'The neural network memorization problem is to study the expressive power of neural networks to interpolate a finite dataset. Although memorization is widely believed to have a close relationship with the strong generalizability of deep learning when using overparameterized models, to the best of our knowledge, there exists no theoretical study on the generalizability of memorization neural networks. In this paper, we give the first theoretical analysis of this topic. Since using i.i.d. training data is a necessary condition for a learning algorithm to be generalizable, memorization and its generalization theory for i.i.d. datasets are developed under mild conditions on the data distribution. First, algorithms are given to construct memorization networks for an i.i.d. dataset, which have the smallest number of parameters and even a constant number of parameters. Second, we show that, in order for the memorization networks to be generalizable, the width of the network must be at least equal to the dimension of the data, which implies that the existing memorization networks with an optimal number of parameters are not generalizable. Third, a lower bound for the sample complexity of general memorization algorithms and the exact sample complexity for memorization algorithms with constant number of parameters are given. As a consequence, it is shown that there exist data distributions such that, to be generalizable for them, the memorization network must have an exponential number of parameters in the data dimension. Finally, an efficient and generalizable memorization algorithm is given when the number of training samples is greater than the efficient memorization sample complexity of the data distribution.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2b83402d8d707813fdb4dd096328e2f58717dda5.pdf'}, 'supplementary_material': {'value': '/attachment/13d8585bea2afd15249f8db90df1bd9cace5dc45.zip'}, '_bibtex': {'value': '@inproceedings{\\nyu2024generalizablity,\\ntitle={Generalizablity of Memorization Neural Network},\\nauthor={Lijia Yu and Xiao-Shan Gao and Lijun Zhang and Yibo Miao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=sABwo1ZTFi}\\n}'}, 'paperhash': {'value': 'yu|generalizablity_of_memorization_neural_network'}},forum = 'sABwo1ZTFi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6333/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6333/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6333/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6333/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's8Pxz7cvHT',number = 4502,cdate = 1715419838249,pdate = 1727287752586,odate = 1730873876591,mdate = 1736961719701,tcdate = 1715419838249,tmdate = 1736961719701,ddate = None,content = {'title': {'value': 'AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks'}, 'authors': {'value': ['Jin Li', 'Ziqiang He', 'Anwei Luo', 'Jian-Fang Hu', 'Z. Jane Wang', 'Xiangui Kang']}, 'authorids': {'value': ['~Jin_Li20', '~Ziqiang_He1', '~Anwei_Luo1', '~Jian-Fang_Hu1', '~Z._Jane_Wang1', '~Xiangui_Kang1']}, 'keywords': {'value': ['Adversarial Attacks', 'Imperceptibility', 'Diffusion Models', 'Deep Neural Networks']}, 'abstract': {'value': 'Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-parametric diffusion process by theoretically exploring basic modeling approach rather than using the denoising or generation abilities of regular diffusion models requiring neural networks. At each step, much subtler yet effective adversarial guidance is crafted using only the attacked model without any additional network, which gradually leads the end of diffusion process from the original image to a desired imperceptible adversarial example. Grounded in a solid theoretical foundation of the proposed non-parametric diffusion process, AdvAD achieves high attack efficacy and imperceptibility with intrinsically lower overall perturbation strength. Additionally, an enhanced version AdvAD-X is proposed to evaluate the extreme of our novel framework under an ideal scenario. Extensive experiments demonstrate the effectiveness of the proposed AdvAD and AdvAD-X. Compared with state-of-the-art imperceptible attacks, AdvAD achieves an average of 99.9% (+17.3%) ASR with 1.34 (-0.97) $l_2$ distance, 49.74 (+4.76) PSNR and 0.9971 (+0.0043) SSIM against four prevalent DNNs with three different architectures on the ImageNet-compatible dataset. Code is available at https://github.com/XianguiKang/AdvAD.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/99d47f7487d0596b71f1288eb93a4d44cd74746d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024advad,\\ntitle={Adv{AD}: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks},\\nauthor={Jin Li and Ziqiang He and Anwei Luo and Jian-Fang Hu and Z. Jane Wang and Xiangui Kang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s8Pxz7cvHT}\\n}'}, 'paperhash': {'value': 'li|advad_exploring_nonparametric_diffusion_for_imperceptible_adversarial_attacks'}},forum = 's8Pxz7cvHT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4502/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4502/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4502/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4502/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's63dtq0mwA',number = 11745,cdate = 1715709779171,pdate = 1727287981725,odate = 1730873942061,mdate = 1730873942082,tcdate = 1715709779171,tmdate = 1730873942082,ddate = None,content = {'title': {'value': 'Understanding Information Storage and Transfer in Multi-Modal Large Language Models'}, 'authors': {'value': ['Samyadeep Basu', 'Martin Grayson', 'Cecily Morrison', 'Besmira Nushi', 'Soheil Feizi', 'Daniela Massiceti']}, 'authorids': {'value': ['~Samyadeep_Basu1', '~Martin_Grayson1', '~Cecily_Morrison1', '~Besmira_Nushi1', '~Soheil_Feizi2', '~Daniela_Massiceti1']}, 'keywords': {'value': ['interpretability', 'multimodal generative models', 'VQA']}, 'TLDR': {'value': 'framework for understanding information storage and flow in multimodal language models'}, 'abstract': {'value': \"Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by \\\\emph{the director in this photo} has won a \\\\emph{Golden Globe}?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) \\\\emph{VQA-Constraints}, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks. We will publicly release our dataset and code.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6c393eac463a81dce11be157a7bde9017cf23675.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbasu2024understanding,\\ntitle={Understanding Information Storage and Transfer in Multi-Modal Large Language Models},\\nauthor={Samyadeep Basu and Martin Grayson and Cecily Morrison and Besmira Nushi and Soheil Feizi and Daniela Massiceti},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s63dtq0mwA}\\n}'}, 'paperhash': {'value': 'basu|understanding_information_storage_and_transfer_in_multimodal_large_language_models'}},forum = 's63dtq0mwA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11745/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11745/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11745/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11745/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's5917zor6V',number = 8527,cdate = 1715663359402,pdate = 1727287882681,odate = 1730873912896,mdate = 1730873912915,tcdate = 1715663359402,tmdate = 1730873912915,ddate = None,content = {'title': {'value': 'On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation'}, 'authors': {'value': ['Yuheng Zhang', 'Nan Jiang']}, 'authorids': {'value': ['~Yuheng_Zhang1', '~Nan_Jiang2']}, 'keywords': {'value': ['Partially Observable Markov Decision Process; Offline Policy Evaluation; Reinforcement Learning Theory']}, 'abstract': {'value': 'We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after  conversion, an exponential object. Recently, Uehara et al. [2022a] proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such as outcome coverage and belief coverage, which enable polynomial bounds on the aforementioned quantities. As a side product, our analyses also lead to the discovery of new algorithms with complementary properties.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ea73856bef4e0b150b71e3a47e27e765b5aba419.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024on,\\ntitle={On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation},\\nauthor={Yuheng Zhang and Nan Jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s5917zor6V}\\n}'}, 'paperhash': {'value': 'zhang|on_the_curses_of_future_and_history_in_futuredependent_value_functions_for_offpolicy_evaluation'}},forum = 's5917zor6V',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8527/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8527/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8527/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8527/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's4Wx2qXhv9',number = 5442,cdate = 1715532576729,pdate = 1727287783981,odate = 1730873884617,mdate = 1736990791129,tcdate = 1715532576729,tmdate = 1736990791129,ddate = None,content = {'title': {'value': 'Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness'}, 'authors': {'value': ['Vaclav Voracek']}, 'authorids': {'value': ['~Vaclav_Voracek1']}, 'keywords': {'value': ['randomized smoothing', 'adversarial robustness', 'confidence interval', 'confidence sequence']}, 'abstract': {'value': 'Randomized smoothing is a popular certified defense against adversarial attacks. In its essence, we need to solve a problem of  statistical estimation which is usually very time-consuming since we need to perform numerous (usually $10^5$) forward passes of the classifier for every point to be certified. In this paper, we review the statistical estimation problems for randomized smoothing to find out if the computational burden is necessary.  In particular, we consider the (standard) task of adversarial robustness where we need to decide if a point is robust at a certain radius or not using as few samples as possible while maintaining statistical guarantees.  We present estimation procedures employing confidence sequences enjoying the same statistical guarantees as the standard methods, with the optimal sample complexities for the estimation task and empirically demonstrate their good performance. Additionally, we provide a randomized version of Clopper-Pearson confidence intervals resulting in strictly stronger certificates.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/63a9b8481d3029115a5ffa3a704f6b2336f79338.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvoracek2024treatment,\\ntitle={Treatment of Statistical Estimation Problems in Randomized Smoothing for Adversarial Robustness},\\nauthor={Vaclav Voracek},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s4Wx2qXhv9}\\n}'}, 'paperhash': {'value': 'voracek|treatment_of_statistical_estimation_problems_in_randomized_smoothing_for_adversarial_robustness'}},forum = 's4Wx2qXhv9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5442/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5442/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5442/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5442/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's3icZC2NLq',number = 14130,cdate = 1715746342792,pdate = 1727288060998,odate = 1730873963530,mdate = 1730873963546,tcdate = 1715746342792,tmdate = 1730873963546,ddate = None,content = {'title': {'value': 'A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation'}, 'authors': {'value': ['Heyang Zhao', 'Jiafan He', 'Quanquan Gu']}, 'authorids': {'value': ['~Heyang_Zhao1', '~Jiafan_He1', '~Quanquan_Gu1']}, 'keywords': {'value': ['Reinforcement learning', 'function approximation']}, 'abstract': {'value': 'The exploration-exploitation dilemma has been a central challenge in reinforcement learning (RL) with complex model classes. In this paper, we propose a new algorithm, Monotonic  Q-Learning with Upper Confidence Bound (MQL-UCB) for RL with general function approximation. Our key algorithmic design includes (1) a general deterministic policy-switching strategy that achieves low switching cost, (2) a monotonic value function structure with carefully controlled function class complexity, and (3) a variance-weighted regression scheme that exploits historical trajectories with high data efficiency. MQL-UCB achieves minimax optimal regret of $\\\\tilde{O}(d\\\\sqrt{HK})$ when $K$ is sufficiently large and near-optimal policy switching cost of $\\\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$ being the planning horizon, and $K$ being the number of episodes. \\n   Our work sheds light on designing provably sample-efficient and deployment-efficient Q-learning with nonlinear function approximation.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b3423ead9010a96399c1d7d679491e9c48a0fd4f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024a,\\ntitle={A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation},\\nauthor={Heyang Zhao and Jiafan He and Quanquan Gu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s3icZC2NLq}\\n}'}, 'paperhash': {'value': 'zhao|a_nearly_optimal_and_lowswitching_algorithm_for_reinforcement_learning_with_general_function_approximation'}},forum = 's3icZC2NLq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14130/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14130/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14130/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14130/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 's2hA6Bz3LE',number = 12808,cdate = 1715728904935,pdate = 1727288018197,odate = 1730873952261,mdate = 1730873952275,tcdate = 1715728904935,tmdate = 1730873952275,ddate = None,content = {'title': {'value': 'Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA'}, 'authors': {'value': ['David Smerkous', 'Qinxun Bai', 'Li Fuxin']}, 'authorids': {'value': ['~David_Smerkous1', '~Qinxun_Bai4', '~Li_Fuxin1']}, 'keywords': {'value': ['bayesian inference', 'variational inference', 'uncertainty quantification', 'deep learning', 'hypernetworks']}, 'TLDR': {'value': 'New kernel to increase feature diversity of ensembles, train hypernetworks, and improve uncertainty estimation of deep ensembles.'}, 'abstract': {'value': 'Particle-based Bayesian deep learning often requires a similarity metric to compare two networks. However, naive similarity metrics lack permutation invariance and are inappropriate for comparing networks. Centered Kernel Alignment (CKA) on feature kernels has been proposed to compare deep networks but has not been used as an optimization objective in Bayesian deep learning. In this paper, we explore the use of CKA in Bayesian deep learning to generate diverse ensembles and hypernetworks that output a network posterior. Noting that CKA projects kernels onto a unit hypersphere and that directly optimizing the CKA objective leads to diminishing gradients when two networks are very similar. We propose adopting the approach of hyperspherical energy (HE) on top of CKA kernels to address this drawback and improve training stability. Additionally, by leveraging CKA-based feature kernels, we derive feature repulsive terms applied to synthetically generated outlier examples. Experiments on both diverse ensembles and hypernetworks show that our approach significantly outperforms baselines in terms of uncertainty quantification in both synthetic and realistic outlier detection tasks.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e8fd6b257ea14297e3fcc15e027f5b978526a38b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsmerkous2024enhancing,\\ntitle={Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of {CKA}},\\nauthor={David Smerkous and Qinxun Bai and Li Fuxin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s2hA6Bz3LE}\\n}'}, 'paperhash': {'value': 'smerkous|enhancing_diversity_in_bayesian_deep_learning_via_hyperspherical_energy_minimization_of_cka'}},forum = 's2hA6Bz3LE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12808/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12808/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12808/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12808/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 's1MoH2pACa',number = 600,cdate = 1713984867767,pdate = 1727287641644,odate = 1730873842297,mdate = 1730873842318,tcdate = 1713984867767,tmdate = 1730873842318,ddate = None,content = {'title': {'value': 'EnsIR: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models'}, 'authors': {'value': ['Shangquan Sun', 'Wenqi Ren', 'Zikun Liu', 'Hyunhee Park', 'Rui Wang', 'Xiaochun Cao']}, 'authorids': {'value': ['~Shangquan_Sun1', '~Wenqi_Ren1', '~Zikun_Liu1', '~Hyunhee_Park1', '~Rui_Wang5', '~Xiaochun_Cao3']}, 'keywords': {'value': ['Model Ensemble', 'Image Restoration', 'Gaussian Mixture Models', 'Expectation Maximization']}, 'TLDR': {'value': 'In this work, a training-free ensemble algorithm is developed to boost the performance of image restoration by combining multiple pre-trained base models at the inference stage.'}, 'abstract': {'value': 'Image restoration has experienced significant advancements due to the development of deep learning. Nevertheless, it encounters challenges related to ill-posed problems, resulting in deviations between single model predictions and ground-truths. Ensemble learning, as a powerful machine learning technique, aims to address these deviations by combining the predictions of multiple base models. Most existing works adopt ensemble learning during the design of restoration models, while only limited research focuses on the inference-stage ensemble of pre-trained restoration models. Regression-based methods fail to enable efficient inference, leading researchers in academia and industry to prefer averaging as their choice for post-training ensemble. To address this, we reformulate the ensemble problem of image restoration into Gaussian mixture models (GMMs) and employ an expectation maximization (EM)-based algorithm to estimate ensemble weights for aggregating prediction candidates. We estimate the range-wise ensemble weights on a reference set and store them in a lookup table (LUT) for efficient ensemble inference on the test set. Our algorithm is model-agnostic and training-free, allowing seamless integration and enhancement of various pre-trained image restoration models. It consistently outperforms regression-based methods and averaging ensemble approaches on 14 benchmarks across 3 image restoration tasks, including super-resolution, deblurring and deraining. The codes and all estimated weights have been released in Github.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/676c72ba568d7c50e683e52999b0ac04f2db0966.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024ensir,\\ntitle={Ens{IR}: An Ensemble Algorithm for Image Restoration via Gaussian Mixture Models},\\nauthor={Shangquan Sun and Wenqi Ren and Zikun Liu and Hyunhee Park and Rui Wang and Xiaochun Cao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=s1MoH2pACa}\\n}'}, 'paperhash': {'value': 'sun|ensir_an_ensemble_algorithm_for_image_restoration_via_gaussian_mixture_models'}},forum = 's1MoH2pACa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission600/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission600/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission600/-/Revision', 'NeurIPS.cc/2024/Conference/Submission600/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rzvVm0LsyK',number = 10205,cdate = 1715691592297,pdate = 1727287932626,odate = 1730873926804,mdate = 1730873926816,tcdate = 1715691592297,tmdate = 1730873926816,ddate = None,content = {'title': {'value': 'ADOPT: Modified Adam Can Converge with Any $\\\\beta_2$ with the Optimal Rate'}, 'authors': {'value': ['Shohei Taniguchi', 'Keno Harada', 'Gouki Minegishi', 'Yuta Oshima', 'Seong Cheol Jeong', 'Go Nagahara', 'Tomoshi Iiyama', 'Masahiro Suzuki', 'Yusuke Iwasawa', 'Yutaka Matsuo']}, 'authorids': {'value': ['~Shohei_Taniguchi1', '~Keno_Harada1', '~Gouki_Minegishi1', '~Yuta_Oshima1', '~Seong_Cheol_Jeong1', '~Go_Nagahara1', '~Tomoshi_Iiyama1', '~Masahiro_Suzuki1', '~Yusuke_Iwasawa1', '~Yutaka_Matsuo1']}, 'keywords': {'value': ['Adam', 'convergence analysis', 'adaptive gradient method']}, 'abstract': {'value': 'Adam is one of the most popular optimization algorithms in deep learning. However, it is known that Adam does not converge in theory unless choosing a hyperparameter, i.e., $\\\\beta_2$, in a problem-dependent manner. There have been many attempts to fix the non-convergence (e.g., AMSGrad), but they require an impractical assumption that the gradient noise is uniformly bounded. In this paper, we propose a new adaptive gradient method named ADOPT, which achieves the optimal convergence rate of $\\\\mathcal{O} ( 1 / \\\\sqrt{T} )$ with any choice of $\\\\beta_2$ without depending on the bounded noise assumption. ADOPT addresses the non-convergence issue of Adam by removing the current gradient from the second moment estimate and changing the order of the momentum update and the normalization by the second moment estimate. We also conduct intensive numerical experiments, and verify that our ADOPT achieves superior results compared to Adam and its variants across a wide range of tasks, including image classification, generative modeling, natural language processing, and deep reinforcement learning. The implementation is available at https://github.com/iShohei220/adopt.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fb3531672b7ca07ba39640b8b32143a1ab71b357.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntaniguchi2024adopt,\\ntitle={{ADOPT}: Modified Adam Can Converge with Any \\\\${\\\\textbackslash}beta\\\\_2\\\\$ with the Optimal Rate},\\nauthor={Shohei Taniguchi and Keno Harada and Gouki Minegishi and Yuta Oshima and Seong Cheol Jeong and Go Nagahara and Tomoshi Iiyama and Masahiro Suzuki and Yusuke Iwasawa and Yutaka Matsuo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rzvVm0LsyK}\\n}'}, 'paperhash': {'value': 'taniguchi|adopt_modified_adam_can_converge_with_any_\\\\beta_2_with_the_optimal_rate'}},forum = 'rzvVm0LsyK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10205/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10205/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10205/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10205/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ry0RXTJwjy',number = 9418,cdate = 1715679469838,pdate = 1727287910190,odate = 1730873920494,mdate = 1734684641746,tcdate = 1715679469838,tmdate = 1734684641746,ddate = None,content = {'title': {'value': 'Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games'}, 'authors': {'value': ['Fanqi Kong', 'Yizhe Huang', 'Song-Chun Zhu', 'Siyuan Qi', 'Xue Feng']}, 'authorids': {'value': ['~Fanqi_Kong1', '~Yizhe_Huang2', '~Song-Chun_Zhu1', '~Siyuan_Qi1', '~Xue_Feng3']}, 'keywords': {'value': ['mixed-motive games', 'multi-agent reinforcement learning', 'cooperation', 'gifting', 'empathy']}, 'abstract': {'value': \"Real-world multi-agent scenarios often involve mixed motives, demanding altruistic agents capable of self-protection against potential exploitation. However, existing approaches often struggle to achieve both objectives. In this paper, based on that empathic responses are modulated by learned social relationships between agents, we propose LASE (**L**earning to balance **A**ltruism and **S**elf-interest based on **E**mpathy), a distributed multi-agent reinforcement learning algorithm that fosters altruistic cooperation through gifting while avoiding exploitation by other agents in mixed-motive games. LASE allocates a portion of its rewards to co-players as gifts, with this allocation adapting dynamically based on the social relationship --- a metric evaluating the friendliness of co-players estimated by counterfactual reasoning. In particular, social relationship measures each co-player by comparing the estimated $Q$-function of current joint action to a counterfactual baseline which marginalizes the co-player's action, with its action distribution inferred by a perspective-taking module. Comprehensive experiments are performed in spatially and temporally extended mixed-motive games, demonstrating LASE's ability to promote group collaboration without compromising fairness and its capacity to adapt policies to various types of interactive co-players.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8b64690213f2681ae3c09e9d3cb33cc9b645d2c5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkong2024learning,\\ntitle={Learning to Balance Altruism and Self-interest Based on Empathy in Mixed-Motive Games},\\nauthor={Fanqi Kong and Yizhe Huang and Song-Chun Zhu and Siyuan Qi and Xue Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ry0RXTJwjy}\\n}'}, 'paperhash': {'value': 'kong|learning_to_balance_altruism_and_selfinterest_based_on_empathy_in_mixedmotive_games'}},forum = 'ry0RXTJwjy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9418/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9418/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9418/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9418/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rvBabL7DUu',number = 4902,cdate = 1715481901160,pdate = 1727287764332,odate = 1730873880112,mdate = 1730873880130,tcdate = 1715481901160,tmdate = 1730873880130,ddate = None,content = {'title': {'value': 'Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation'}, 'authors': {'value': ['Xuehao Cui', 'Guangyang Wu', 'Zhenghao Gan', 'Guangtao Zhai', 'Xiaohong Liu']}, 'authorids': {'value': ['~Xuehao_Cui1', '~Guangyang_Wu1', '~Zhenghao_Gan1', '~Guangtao_Zhai1', '~Xiaohong_Liu2']}, 'keywords': {'value': ['Image Generation', 'QR Code', 'Stable Diffusion', 'Control Network']}, 'abstract': {'value': 'Existing methods to generate aesthetic QR codes, such as image and style transfer techniques, tend to compromise either the visual appeal or the scannability of QR codes when they incorporate human face identity. Addressing these imperfections, we present Face2QR—a novel pipeline specifically designed for generating personalized QR codes that harmoniously blend aesthetics, face identity, and scannability. Our pipeline introduces three innovative components. First, the ID-refined QR integration (IDQR) seamlessly intertwines the background styling with face ID, utilizing a unified SD-based framework with control networks. Second, the ID-aware QR ReShuffle (IDRS) effectively rectifies the conflicts between face IDs and QR patterns, rearranging QR modules to maintain the integrity of facial features without compromising scannability. Lastly, the ID-preserved Scannability Enhancement (IDSE) markedly boosts scanning robustness through latent code optimization, striking a delicate balance between face ID, aesthetic quality and QR functionality. In comprehensive experiments, Face2QR demonstrates remarkable performance, outperforming existing approaches, particularly in preserving facial recognition features within custom QR code designs.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a95aab049e780f04b397e4ea0e14a130da25c7ca.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncui2024faceqr,\\ntitle={Face2{QR}: A Unified Framework for Aesthetic, Face-Preserving, and Scannable {QR} Code Generation},\\nauthor={Xuehao Cui and Guangyang Wu and Zhenghao Gan and Guangtao Zhai and Xiaohong Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rvBabL7DUu}\\n}'}, 'paperhash': {'value': 'cui|face2qr_a_unified_framework_for_aesthetic_facepreserving_and_scannable_qr_code_generation'}},forum = 'rvBabL7DUu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4902/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4902/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4902/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4902/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'rtz4df9IF1',number = 8899,cdate = 1715671509161,pdate = 1727287894849,odate = 1730873916213,mdate = 1736944486041,tcdate = 1715671509161,tmdate = 1736944486041,ddate = None,content = {'title': {'value': 'Optimal Parallelization of Boosting'}, 'authors': {'value': ['Arthur da Cunha', 'Mikael Møller Høgsgaard', 'Kasper Green Larsen']}, 'authorids': {'value': ['~Arthur_da_Cunha1', '~Mikael_Møller_Høgsgaard1', '~Kasper_Green_Larsen1']}, 'keywords': {'value': ['Learning Theory', 'Parallel Boosting', 'PAC-learning', 'Weak to Strong Learning']}, 'abstract': {'value': 'Recent works on the parallel complexity of Boosting have established strong lower bounds on the tradeoff between the number of training rounds $p$ and the total parallel work per round $t$.\\nThese works have also presented highly non-trivial parallel algorithms that shed light on different regions of this tradeoff.\\nDespite these advancements, a significant gap persists between the theoretical lower bounds and the performance of these algorithms across much of the tradeoff space.\\nIn this work, we essentially close this gap by providing both improved lower bounds on the parallel complexity of weak-to-strong learners, and a parallel Boosting algorithm whose performance matches these bounds across the entire $p$ vs. $t$ compromise spectrum, up to logarithmic factors.\\nUltimately, this work settles the parallel complexity of Boosting algorithms that are nearly sample-optimal.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ee88d3d4399c417433f97a457dffc6f174cfe576.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncunha2024optimal,\\ntitle={Optimal Parallelization of Boosting},\\nauthor={Arthur da Cunha and Mikael M{\\\\o}ller H{\\\\o}gsgaard and Kasper Green Larsen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rtz4df9IF1}\\n}'}, 'TLDR': {'value': 'We settle the parallel complexity of Boosting algorithms that are nearly sample-optimal'}, 'paperhash': {'value': 'cunha|optimal_parallelization_of_boosting'}},forum = 'rtz4df9IF1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8899/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8899/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8899/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8899/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rpjh69DUX2',number = 7555,cdate = 1715627861882,pdate = 1727287851195,odate = 1730873903982,mdate = 1730873904002,tcdate = 1715627861882,tmdate = 1730873904002,ddate = None,content = {'title': {'value': 'Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator'}, 'authors': {'value': ['Siyuan Xu', 'Minghui Zhu']}, 'authorids': {'value': ['~Siyuan_Xu4', '~Minghui_Zhu1']}, 'keywords': {'value': ['Meta-learning', 'reinforcement learning']}, 'abstract': {'value': \"Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and generalizability. In this paper, we develop a bilevel optimization framework for meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation, which implements multiple-step policy optimization on one-time data collection. Beyond existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task distribution. This metric measures the distance of the policy adaptation from the learned meta-prior to the task-specific optimum, and quantifies the model's generalizability to the task distribution. We empirically validate the correctness of the derived upper bounds and demonstrate the superior effectiveness of the proposed algorithm over benchmarks.\"}, 'pdf': {'value': '/pdf/151c121043796259ec7bb2338ce9abeda1583f5b.pdf'}, 'supplementary_material': {'value': '/attachment/936bd5c97a81948c88cd463517dd71ba83872c1b.zip'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nxu2024metareinforcement,\\ntitle={Meta-Reinforcement Learning with Universal Policy Adaptation: Provable Near-Optimality under All-task Optimum Comparator},\\nauthor={Siyuan Xu and Minghui Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rpjh69DUX2}\\n}'}, 'paperhash': {'value': 'xu|metareinforcement_learning_with_universal_policy_adaptation_provable_nearoptimality_under_alltask_optimum_comparator'}},forum = 'rpjh69DUX2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7555/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7555/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7555/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7555/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rpZWSDjc4N',number = 5742,cdate = 1715567900697,pdate = 1727287793472,odate = 1730873887732,mdate = 1734660241379,tcdate = 1715567900697,tmdate = 1734660241379,ddate = None,content = {'title': {'value': 'FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors'}, 'authors': {'value': ['Shuai Liu', 'Boyang Li', 'Zhiyu Fang', 'Mingyue Cui', 'Kai Huang']}, 'authorids': {'value': ['~Shuai_Liu7', '~Boyang_Li3', '~Zhiyu_Fang2', '~Mingyue_Cui1', '~Kai_Huang2']}, 'keywords': {'value': ['Explainable artificial intelligence', 'visual explanation', '3D object detection']}, 'abstract': {'value': 'LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and subsequently aggregates these maps to obtain a global visual explanation. To achieve object-specific visual explanations, we refine the global visual explanation using the feature gradient of a target object. Additionally, we introduce a voxel upsampling strategy to align the scale between the activation map and input point cloud. We qualitatively and quantitatively analyze FFAM with multiple detectors on several datasets. Experimental results validate the high-quality visual explanations produced by FFAM. The code is available at \\\\url{https://anonymous.4open.science/r/FFAM-B9AF}.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/08f720727dc29316599fe6ed03219a4f2f88435e.pdf'}, 'supplementary_material': {'value': '/attachment/e8174f1db4e1f011ea626edfaaefc67ffb4c263f.zip'}, '_bibtex': {'value': '@inproceedings{\\nliu2024ffam,\\ntitle={{FFAM}: Feature Factorization Activation Map for Explanation of 3D Detectors},\\nauthor={Shuai Liu and Boyang Li and Zhiyu Fang and Mingyue Cui and Kai Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rpZWSDjc4N}\\n}'}, 'paperhash': {'value': 'liu|ffam_feature_factorization_activation_map_for_explanation_of_3d_detectors'}},forum = 'rpZWSDjc4N',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5742/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5742/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5742/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5742/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'rniiAVjHi5',number = 20951,cdate = 1715799293738,pdate = 1727288245473,odate = 1730874004042,mdate = 1730874004060,tcdate = 1715799293738,tmdate = 1730874004060,ddate = None,content = {'title': {'value': 'Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction'}, 'authors': {'value': ['Anton Rodomanov', 'Xiaowen Jiang', 'Sebastian U Stich']}, 'authorids': {'value': ['~Anton_Rodomanov1', '~Xiaowen_Jiang1', '~Sebastian_U_Stich1']}, 'keywords': {'value': ['convex optimization', 'stochastic optimization', 'adaptive methods', 'universal algorithms', 'acceleration', 'variance reduction', 'AdaGrad', 'SVRG', 'weakly smooth functions', 'Hölder condition', 'inexact oracle', 'complexity estimates']}, 'abstract': {'value': \"We present adaptive gradient methods (both basic and accelerated) for solving\\nconvex composite optimization problems in which the main part is approximately\\nsmooth (a.k.a. $(\\\\delta, L)$-smooth) and can be accessed only via a\\n(potentially biased) stochastic gradient oracle.\\nThis setting covers many interesting examples including Hölder smooth problems\\nand various inexact computations of the stochastic gradient.\\nOur methods use AdaGrad stepsizes and are adaptive in the sense that they do\\nnot require knowing any problem-dependent constants except an estimate of the\\ndiameter of the feasible set but nevertheless achieve the best possible\\nconvergence rates as if they knew the corresponding constants.\\nWe demonstrate that AdaGrad stepsizes work in a variety of situations\\nby proving, in a unified manner, three types of new results.\\nFirst, we establish efficiency guarantees for our methods in the classical\\nsetting where the oracle's variance is uniformly bounded.\\nWe then show that, under more refined assumptions on the variance,\\nthe same methods without any modifications enjoy implicit variance\\nreduction properties allowing us to express their complexity estimates in\\nterms of the variance only at the minimizer.\\nFinally, we show how to incorporate explicit SVRG-type variance reduction into\\nour methods and obtain even faster algorithms.\\nIn all three cases, we present both basic and accelerated algorithms\\nachieving state-of-the-art complexity bounds.\\nAs a direct corollary of our results, we obtain universal stochastic gradient\\nmethods for Hölder smooth problems which can be used in all situations.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/dd912aeb19dbd3162f55e416834322f2e59ce066.pdf'}, '_bibtex': {'value': '@inproceedings{\\nrodomanov2024universality,\\ntitle={Universality of AdaGrad Stepsizes for Stochastic Optimization: Inexact Oracle, Acceleration and Variance Reduction},\\nauthor={Anton Rodomanov and Xiaowen Jiang and Sebastian U Stich},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rniiAVjHi5}\\n}'}, 'paperhash': {'value': 'rodomanov|universality_of_adagrad_stepsizes_for_stochastic_optimization_inexact_oracle_acceleration_and_variance_reduction'}},forum = 'rniiAVjHi5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20951/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20951/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20951/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20951/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rnUEUbRxVu',number = 11191,cdate = 1715702019914,pdate = 1727287963460,odate = 1730873935685,mdate = 1730873935705,tcdate = 1715702019914,tmdate = 1730873935705,ddate = None,content = {'title': {'value': 'DAPE: Data-Adaptive Positional Encoding for Length Extrapolation'}, 'authors': {'value': ['Chuanyang Zheng', 'Yihang Gao', 'Han Shi', 'Minbin Huang', 'Jingyao Li', 'Jing Xiong', 'Xiaozhe Ren', 'Michael Ng', 'Xin Jiang', 'Zhenguo Li', 'Yu Li']}, 'authorids': {'value': ['~Chuanyang_Zheng3', '~Yihang_Gao1', '~Han_Shi1', '~Minbin_Huang1', '~Jingyao_Li2', '~Jing_Xiong4', '~Xiaozhe_Ren1', '~Michael_Ng4', '~Xin_Jiang1', '~Zhenguo_Li1', '~Yu_Li1']}, 'keywords': {'value': ['Transformers', 'context-adaptive positional encoding', 'long context', 'length generalization']}, 'TLDR': {'value': \"We propose a novel context-adaptive positional encoding to improve Transformer's capabilities, especially in long context .\"}, 'abstract': {'value': 'Positional encoding plays a crucial role in transformers, significantly impact- ing model performance and length generalization. Prior research has introduced absolute positional encoding (APE) and relative positional encoding (RPE) to distinguish token positions in given sequences. However, both APE and RPE remain fixed after model training regardless of input data, limiting their adaptability and flexibility. Hence, we expect that the desired positional encoding should be data-adaptive and can be dynamically adjusted with the given attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE) method, which dynamically and semantically adjusts based on input context and learned fixed priors. Experimental validation on real-world datasets (Arxiv, Books3, and CHE) demonstrates that DAPE enhances model performances in terms of trained length and length generalization, where the improvements are statistically significant. The model visualization suggests that our model can keep both local and anti-local information. Finally, we successfully train the model on sequence length 128 and achieve better performance at evaluation sequence length 8192, compared with other static positional encoding methods, revealing the benefit of the adaptive positional encoding method.'}, 'pdf': {'value': '/pdf/49a23aa447043bc41b6e583d0b3a6becd467fcd5.pdf'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024dape,\\ntitle={{DAPE}: Data-Adaptive Positional Encoding for Length Extrapolation},\\nauthor={Chuanyang Zheng and Yihang Gao and Han Shi and Minbin Huang and Jingyao Li and Jing Xiong and Xiaozhe Ren and Michael Ng and Xin Jiang and Zhenguo Li and Yu Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rnUEUbRxVu}\\n}'}, 'supplementary_material': {'value': '/attachment/f2eea472521132e22428b4f574688a8db63733d9.zip'}, 'paperhash': {'value': 'zheng|dape_dataadaptive_positional_encoding_for_length_extrapolation'}},forum = 'rnUEUbRxVu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11191/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11191/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11191/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11191/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rle9X7DQuH',number = 14067,cdate = 1715745622642,pdate = 1727288059341,odate = 1730873963158,mdate = 1736789598596,tcdate = 1715745622642,tmdate = 1736789598596,ddate = None,content = {'title': {'value': 'OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning'}, 'authors': {'value': ['Shengjie Niu', 'Lifan Lin', 'Jian Huang', 'Chao Wang']}, 'authorids': {'value': ['~Shengjie_Niu1', '~Lifan_Lin1', '~Jian_Huang5', '~Chao_Wang13']}, 'keywords': {'value': ['Open-world Semi-Supervised Learning', 'self-labeling', 'consistency loss']}, 'TLDR': {'value': 'Boosting open-world semi-supervised learning with conditional self-labeling and open-world hierarchical thresholding'}, 'abstract': {'value': 'Semi-supervised learning (SSL) offers a robust framework for harnessing the potential of unannotated data. Traditionally, SSL mandates that all classes possess labeled instances. However, the emergence of open-world SSL (OwSSL) introduces a more practical challenge, wherein unlabeled data may encompass samples from unseen classes. This scenario leads to misclassification of unseen classes as known ones, consequently undermining classification accuracy. To overcome this challenge, this study revisits two methodologies from self-supervised and semi-supervised learning, self-labeling and consistency, tailoring them to address the OwSSL problem. Specifically, we propose an effective framework called _OwMatch_, combining conditional self-labeling and open-world hierarchical thresholding.  Theoretically, we analyze the estimation of class distribution on unlabeled data through rigorous statistical analysis, thus demonstrating that OwMatch can ensure the unbiasedness of the label assignment estimator with reliability.  Comprehensive empirical analyses demonstrate that our method yields substantial performance enhancements across both known and unknown classes in comparison to previous studies. Code is available at [https://github.com/niusj03/OwMatch](https://github.com/niusj03/OwMatch).'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3b423ca29732774854a5dcdf83ef4c349192788a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nniu2024owmatch,\\ntitle={OwMatch: Conditional Self-Labeling with Consistency for Open-world Semi-Supervised Learning},\\nauthor={Shengjie Niu and Lifan Lin and Jian Huang and Chao Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rle9X7DQuH}\\n}'}, 'paperhash': {'value': 'niu|owmatch_conditional_selflabeling_with_consistency_for_openworld_semisupervised_learning'}},forum = 'rle9X7DQuH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14067/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14067/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14067/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14067/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'rkuVYosT2c',number = 8333,cdate = 1715657862635,pdate = 1727287876558,odate = 1730873911378,mdate = 1730873911402,tcdate = 1715657862635,tmdate = 1730873911402,ddate = None,content = {'title': {'value': 'Distributed Least Squares in Small Space via Sketching and Bias Reduction'}, 'authors': {'value': ['Sachin Garg', 'Kevin Tan', 'Michal Derezinski']}, 'authorids': {'value': ['~Sachin_Garg2', '~Kevin_Tan3', '~Michal_Derezinski1']}, 'keywords': {'value': ['Matrix Sketching', 'Least squares', 'Randomized Linear Algebra', 'Random Matrix Theory']}, 'TLDR': {'value': 'We give a sparse sketching method running in optimal space and current matrix multiplication time, recovering a nearly-unbiased least squares estimator using two passes over the data.'}, 'abstract': {'value': 'Matrix sketching is a powerful tool for reducing the size of large data matrices. Yet there are fundamental limitations to this size reduction when we want to recover an accurate estimator for a task such as least square regression. We show that these limitations can be circumvented in the distributed setting by designing sketching methods that minimize the bias of the estimator, rather than its error. In particular, we give a sparse sketching method running in optimal space and current matrix multiplication time, which recovers a nearly-unbiased least squares estimator using two passes over the data. This leads to new communication-efficient distributed averaging algorithms for least squares and related tasks, which directly improve on several prior approaches. Our key novelty is a new bias analysis for sketched least squares, giving a sharp characterization of its dependence on the sketch sparsity. The techniques include new higher moment restricted Bai-Silverstein inequalities, which are of independent interest to the non-asymptotic analysis of deterministic equivalents for random matrices that arise from sketching.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2c8cc6686571de555edd96ef6c41649fee4657d7.pdf'}, 'supplementary_material': {'value': '/attachment/7c924336f28f87f8135f7bef791234c4d3c455db.zip'}, '_bibtex': {'value': '@inproceedings{\\ngarg2024distributed,\\ntitle={Distributed Least Squares in Small Space via Sketching and Bias Reduction},\\nauthor={Sachin Garg and Kevin Tan and Michal Derezinski},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rkuVYosT2c}\\n}'}, 'paperhash': {'value': 'garg|distributed_least_squares_in_small_space_via_sketching_and_bias_reduction'}},forum = 'rkuVYosT2c',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8333/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8333/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8333/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8333/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rk2L9YGDi2',number = 2560,cdate = 1715097841735,pdate = 1727287693661,odate = 1730873859002,mdate = 1730873859013,tcdate = 1715097841735,tmdate = 1730873859013,ddate = None,content = {'title': {'value': 'Sequoia: Scalable and Robust Speculative Decoding'}, 'authors': {'value': ['Zhuoming Chen', 'Avner May', 'Ruslan Svirschevski', 'Yu-Hsun Huang', 'Max Ryabinin', 'Zhihao Jia', 'Beidi Chen']}, 'authorids': {'value': ['~Zhuoming_Chen1', '~Avner_May1', '~Ruslan_Svirschevski1', '~Yu-Hsun_Huang1', '~Max_Ryabinin1', '~Zhihao_Jia2', '~Beidi_Chen1']}, 'keywords': {'value': ['LLM inference; Speculative Decoding']}, 'TLDR': {'value': 'Accelerate LLM inference with a scalable and robust tree based speculative decoding algorithm'}, 'abstract': {'value': 'As the usage of large language models (LLMs) grows, it becomes increasingly important to serve them quickly and efficiently. While speculative decoding has recently emerged as a promising direction for accelerating LLM serving, existing methods are limited in their ability to scale to larger speculation budgets and adapt to different hyperparameters. This paper introduces Sequoia, a scalable and robust algorithm for speculative decoding. To improve scalability, Sequoia introduces a dynamic programming algorithm to find an optimal tree structure for the speculated tokens. To achieve robust speculative decoding, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Sequoia improves the decoding speed of Llama2-7B, Llama2-13B, and Vicuna-33B on an A100 GPU by up to $4.04\\\\times$, $3.73\\\\times$, and $2.27 \\\\times$. To serve Llama3-70B-Instruct on a single L40 GPU through offloading, Sequoia reduces the per-token decoding latency to 0.60 s/token, $9.5\\\\times$ faster than DeepSpeed-Zero-Inference.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b37bd645defe5a17e94f0ba7ac5d5cb2c3db1b4f.pdf'}, 'supplementary_material': {'value': '/attachment/768ce5b70a01106626089286159218399d390b88.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024sequoia,\\ntitle={Sequoia: Scalable and Robust Speculative Decoding},\\nauthor={Zhuoming Chen and Avner May and Ruslan Svirschevski and Yu-Hsun Huang and Max Ryabinin and Zhihao Jia and Beidi Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rk2L9YGDi2}\\n}'}, 'paperhash': {'value': 'chen|sequoia_scalable_and_robust_speculative_decoding'}},forum = 'rk2L9YGDi2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2560/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2560/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2560/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2560/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rjSPDVdUaw',number = 16239,cdate = 1715768322312,pdate = 1727288120859,odate = 1730873977948,mdate = 1730873977961,tcdate = 1715768322312,tmdate = 1730873977961,ddate = None,content = {'title': {'value': 'Moving Off-the-Grid: Scene-Grounded Video Representations'}, 'authors': {'value': ['Sjoerd van Steenkiste', 'Daniel Zoran', 'Yi Yang', 'Yulia Rubanova', 'Rishabh Kabra', 'Carl Doersch', 'Dilara Gokay', 'Joseph Heyward', 'Etienne Pot', 'Klaus Greff', 'Drew A. Hudson', 'Thomas Albert Keck', 'Joao Carreira', 'Alexey Dosovitskiy', 'Mehdi S. M. Sajjadi', 'Thomas Kipf']}, 'authorids': {'value': ['~Sjoerd_van_Steenkiste1', '~Daniel_Zoran1', '~Yi_Yang10', '~Yulia_Rubanova2', '~Rishabh_Kabra1', '~Carl_Doersch1', '~Dilara_Gokay1', '~Joseph_Heyward2', '~Etienne_Pot1', '~Klaus_Greff1', '~Drew_Arad_Hudson1', '~Thomas_Albert_Keck1', '~Joao_Carreira1', '~Alexey_Dosovitskiy1', '~Mehdi_S._M._Sajjadi1', '~Thomas_Kipf2']}, 'keywords': {'value': ['Self supervised learning', 'point tracking', 'representation learning']}, 'TLDR': {'value': 'We propose an \"off-the-grid\" representation model which learns from video and binds tokens to scene elements and tracks them over time consistently via self-supervised learning.'}, 'abstract': {'value': 'Current vision models typically maintain a fixed correspondence between their representation structure and image space.\\nEach layer comprises a set of tokens arranged “on-the-grid,” which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present *Moving Off-the-Grid* (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move “off-the-grid” to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective—next frame prediction—trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG’s learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to “on-the-grid” baselines.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c573a5173a879863c518a1ab5e47992488c04e5c.pdf'}, 'supplementary_material': {'value': '/attachment/5417043d61fa1dfc8a68e11ac50a1887c13cc1ec.zip'}, '_bibtex': {'value': '@inproceedings{\\nsteenkiste2024moving,\\ntitle={Moving Off-the-Grid: Scene-Grounded Video Representations},\\nauthor={Sjoerd van Steenkiste and Daniel Zoran and Yi Yang and Yulia Rubanova and Rishabh Kabra and Carl Doersch and Dilara Gokay and Joseph Heyward and Etienne Pot and Klaus Greff and Drew A. Hudson and Thomas Albert Keck and Joao Carreira and Alexey Dosovitskiy and Mehdi S. M. Sajjadi and Thomas Kipf},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rjSPDVdUaw}\\n}'}, 'paperhash': {'value': 'steenkiste|moving_offthegrid_scenegrounded_video_representations'}},forum = 'rjSPDVdUaw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16239/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16239/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16239/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16239/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rhCgizNupi',number = 5179,cdate = 1715510489696,pdate = 1727287776061,odate = 1730873882655,mdate = 1730873882678,tcdate = 1715510489696,tmdate = 1730873882678,ddate = None,content = {'title': {'value': 'Reranking Laws for Language Generation: A Communication-Theoretic Perspective'}, 'authors': {'value': ['António Farinhas', 'Haau-Sing Li', 'Andre Martins']}, 'authorids': {'value': ['~António_Farinhas1', '~Haau-Sing_Li1', '~Andre_Martins1']}, 'keywords': {'value': ['language generation', 'reranking', 'communication theory', 'reliability']}, 'abstract': {'value': 'To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1d824c959b9a0a08e7614ea8420a49d8b2d56b9c.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nfarinhas2024reranking,\\ntitle={Reranking Laws for Language Generation: A Communication-Theoretic Perspective},\\nauthor={Ant{\\\\'o}nio Farinhas and Haau-Sing Li and Andre Martins},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rhCgizNupi}\\n}\"}, 'paperhash': {'value': 'farinhas|reranking_laws_for_language_generation_a_communicationtheoretic_perspective'}},forum = 'rhCgizNupi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5179/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5179/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5179/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5179/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rgwhJ7INtZ',number = 11434,cdate = 1715704857967,pdate = 1727287971029,odate = 1730873938399,mdate = 1736922335434,tcdate = 1715704857967,tmdate = 1736922335434,ddate = None,content = {'title': {'value': 'Super Consistency of Neural Network Landscapes and Learning Rate Transfer'}, 'authors': {'value': ['Lorenzo Noci', 'Alexandru Meterez', 'Thomas Hofmann', 'Antonio Orvieto']}, 'authorids': {'value': ['~Lorenzo_Noci1', '~Alexandru_Meterez1', '~Thomas_Hofmann1', '~Antonio_Orvieto3']}, 'keywords': {'value': ['mup', 'deep learning theory', 'optimization theory', 'edge of stability', 'NTK']}, 'TLDR': {'value': 'We show the some properties of the loss landscape are preserved across different model sizes under the right scaling of the architecture.'}, 'abstract': {'value': \"Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\\\mu$P and its depth extension), then some hyperparameters --- such as the learning rate --- exhibit transfer from small to very large models. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is consistently similar across very different model sizes. In this work, we study the landscape through the lens of the Hessian, with a focus on its largest eigenvalue (i.e. the sharpness), and find that certain spectral properties under $\\\\mu$P are largely independent of the width and depth of the network along the training trajectory. We name this property *super consistency* of the landscape. On the other hand, we show that in the Neural Tangent Kernel (NTK) and other scaling regimes, the sharpness exhibits very different dynamics at different scales. But what causes these differences in the sharpness dynamics? Through a connection between the Hessian's and the NTK's spectrum, we argue that the cause lies in the presence (for $\\\\mu$P) or progressive absence (for the NTK scaling) of feature learning.\\nWe corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4554cac55256e63fef98c0bf476ad4b5b0572873.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnoci2024super,\\ntitle={Super Consistency of Neural Network Landscapes and Learning Rate Transfer},\\nauthor={Lorenzo Noci and Alexandru Meterez and Thomas Hofmann and Antonio Orvieto},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rgwhJ7INtZ}\\n}'}, 'paperhash': {'value': 'noci|super_consistency_of_neural_network_landscapes_and_learning_rate_transfer'}},forum = 'rgwhJ7INtZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11434/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11434/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11434/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11434/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rgtrYVC9n4',number = 66,cdate = 1713825383082,pdate = 1727287629470,odate = 1730873837842,mdate = 1736904947836,tcdate = 1713825383082,tmdate = 1736904947836,ddate = None,content = {'title': {'value': 'Discovering Sparsity Allocation for  Layer-wise Pruning of Large Language Models'}, 'authors': {'value': ['Lujun Li', 'Peijie Dong', 'Zhenheng Tang', 'Xiang Liu', 'Qiang Wang', 'Wenhan Luo', 'Wei Xue', 'Qifeng Liu', 'Xiaowen Chu', 'Yike Guo']}, 'authorids': {'value': ['~Lujun_Li1', '~Peijie_Dong1', '~Zhenheng_Tang2', '~Xiang_Liu10', '~Qiang_Wang14', '~Wenhan_Luo1', '~Wei_Xue5', '~Qifeng_Liu1', '~Xiaowen_Chu2', '~Yike_Guo1']}, 'keywords': {'value': ['network pruning', 'layerwise sparsity allocation', 'large language models', 'model compression.']}, 'abstract': {'value': 'In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models (LLMs).  LLMs have become increasingly powerful, but their large parameter counts make them computationally expensive.  Existing pruning methods for compressing LLMs primarily focus on evaluating redundancies and removing element-wise weights.  However, these methods fail to allocate adaptive layer-wise sparsities, leading to performance degradation in challenging tasks.  We observe that per-layer importance statistics can serve as allocation indications, but their effectiveness depends on the allocation function between layers.  To address this issue, we develop an expression discovery framework to explore potential allocation strategies.  Our allocation functions involve two steps: reducing element-wise metrics to per-layer importance scores, and modelling layer importance to sparsity ratios.  To search for the most effective allocation function, we construct a search space consisting of pre-process, reduction, transform, and post-process operations.  We leverage an evolutionary algorithm to perform crossover and mutation on superior candidates within the population, guided by performance evaluation.  Finally, we seamlessly integrate our discovered functions into various uniform methods, resulting in significant performance improvements.  We conduct extensive experiments on multiple challenging tasks such as arithmetic, knowledge reasoning, and multimodal benchmarks spanning GSM8K, MMLU, SQA, and VQA, demonstrating that our DSA method achieves significant performance gains on the LLaMA-1|2|3, Mistral, and OPT models.   Notably, the LLaMA-1|2|3 model pruned by our DSA reaches 4.73\\\\%|6.18\\\\%|10.65\\\\% gain over the state-of-the-art techniques (e.g., Wanda and SparseGPT).'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'In this paper, we present a new method for optimizing layerwise sparsity allocation in large language models.'}, 'pdf': {'value': '/pdf/aba7c7e8e4ea4b2a16b69717191fff7299facc30.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024discovering,\\ntitle={Discovering Sparsity Allocation for  Layer-wise Pruning of Large Language Models},\\nauthor={Lujun Li and Peijie Dong and Zhenheng Tang and Xiang Liu and Qiang Wang and Wenhan Luo and Wei Xue and Qifeng Liu and Xiaowen Chu and Yike Guo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rgtrYVC9n4}\\n}'}, 'paperhash': {'value': 'li|discovering_sparsity_allocation_for_layerwise_pruning_of_large_language_models'}},forum = 'rgtrYVC9n4',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission66/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission66/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission66/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission66/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'recsheQ7e8',number = 16934,cdate = 1715775615018,pdate = 1727288141155,odate = 1730873982346,mdate = 1730873982364,tcdate = 1715775615018,tmdate = 1730873982364,ddate = None,content = {'title': {'value': 'Aligning to Thousands of Preferences via System Message Generalization'}, 'authors': {'value': ['Seongyun Lee', 'Sue Hyun Park', 'Seungone Kim', 'Minjoon Seo']}, 'authorids': {'value': ['~Seongyun_Lee1', '~Sue_Hyun_Park1', '~Seungone_Kim1', '~Minjoon_Seo1']}, 'keywords': {'value': ['Large language model', 'Preference alignment', 'Pluralistic alignment', 'Multifacetedness', 'System message', 'Instruction tuning']}, 'TLDR': {'value': 'Approach to align LLMs more closely with individual user values by training a model, Janus, using a diverse preference datasets including system prompts.'}, 'abstract': {'value': 'Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public’s preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual’s preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM’s generation behavior to better align with the user’s intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., “You are a helpful assistant”), which limits\\ntheir ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)\\nby adding system messages that reflect unseen user values. JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9cf4d38da0e04837fc03f019034443e46e38e454.pdf'}, 'supplementary_material': {'value': '/attachment/9fa68d543d9376fc0b3f75cf52f1b1191581619b.zip'}, '_bibtex': {'value': '@inproceedings{\\nlee2024aligning,\\ntitle={Aligning to Thousands of Preferences via System Message Generalization},\\nauthor={Seongyun Lee and Sue Hyun Park and Seungone Kim and Minjoon Seo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=recsheQ7e8}\\n}'}, 'paperhash': {'value': 'lee|aligning_to_thousands_of_preferences_via_system_message_generalization'}},forum = 'recsheQ7e8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16934/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16934/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16934/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16934/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 're2jPCnzkA',number = 19738,cdate = 1715792552853,pdate = 1727288216458,odate = 1730873997452,mdate = 1730873997470,tcdate = 1715792552853,tmdate = 1730873997470,ddate = None,content = {'title': {'value': 'MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated Designs'}, 'authors': {'value': ['Quentin Leboutet', 'Nina Wiedemann', 'zhipeng cai', 'Michael Paulitsch', 'Kai Yuan']}, 'authorids': {'value': ['~Quentin_Leboutet1', '~Nina_Wiedemann1', '~zhipeng_cai3', '~Michael_Paulitsch1', '~Kai_Yuan1']}, 'keywords': {'value': ['3D articulated objects', 'diffusion models', 'generative models']}, 'TLDR': {'value': 'We present MIDGArD, a new generative framework for creating 3D articulated objects, separating structure and shape generation.'}, 'abstract': {'value': \"Providing functionality through articulation and interaction with objects is a key objective in 3D generation. We introduce MIDGArD (Modular Interpretable Diffusion over Graphs for Articulated Designs), a novel diffusion-based framework for articulated 3D asset generation. MIDGArD improves over foundational work in the field by enhancing quality, consistency, and controllability in the generation process. This is achieved through MIDGArD's modular approach that separates the problem into two primary components: structure generation and shape generation. The structure generation module of MIDGArD aims at producing coherent articulation features from noisy or incomplete inputs. It acts on the object's structural and kinematic attributes, represented as features of a graph that are being progressively denoised to issue coherent and interpretable articulation solutions. This denoised graph then serves as an advanced conditioning mechanism for the shape generation module, a 3D generative model that populates each link of the articulated structure with consistent 3D meshes. Experiments show the superiority of MIDGArD on the quality, consistency, and interpretability of the generated assets. Importantly, the generated models are fully simulatable, i.e., can be seamlessly integrated into standard physics engines such as MuJoCo, broadening MIDGArD's applicability to fields such as digital content creation, meta realities, and robotics.\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a8c6a5c1a1b447094fcb67f755d8cbf845059d02.pdf'}, 'supplementary_material': {'value': '/attachment/a0e6fcbf36a8a2576351826c2cc667744af2430c.zip'}, '_bibtex': {'value': '@inproceedings{\\nleboutet2024midgard,\\ntitle={{MIDGA}rD: Modular Interpretable Diffusion over Graphs for Articulated Designs},\\nauthor={Quentin Leboutet and Nina Wiedemann and zhipeng cai and Michael Paulitsch and Kai Yuan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=re2jPCnzkA}\\n}'}, 'paperhash': {'value': 'leboutet|midgard_modular_interpretable_diffusion_over_graphs_for_articulated_designs'}},forum = 're2jPCnzkA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19738/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19738/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19738/-/Revision', 'NeurIPS.cc/2024/Conference/-/Desk_Rejected_Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19738/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 're0ly2Ylcu',number = 7332,cdate = 1715618180730,pdate = 1727287843472,odate = 1730873901862,mdate = 1730873901879,tcdate = 1715618180730,tmdate = 1730873901879,ddate = None,content = {'title': {'value': 'Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context'}, 'authors': {'value': ['Jingru Jia', 'Zehua Yuan', 'Junhao Pan', 'Paul E McNamara', 'Deming Chen']}, 'authorids': {'value': ['~Jingru_Jia1', '~Zehua_Yuan1', '~Junhao_Pan1', '~Paul_E_McNamara1', '~Deming_Chen1']}, 'keywords': {'value': ['Large Language Model (LLM)', 'LLM Reasoning', 'Fairness', 'Ethical AI', 'AI Decision-Making', 'LLM Prompting']}, 'TLDR': {'value': 'This paper quantitatively assesses the decision-making behaviors of large language models (LLMs), demonstrating how these models mirror human risk behaviors and exhibit significant variations when embedded with socio-demographic characteristics.'}, 'abstract': {'value': 'When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decision-making tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cfdb54076307baf1e330d87ecd09915bc38f9cff.pdf'}, 'supplementary_material': {'value': '/attachment/d617cc2ce5b176fb5e7343d332d8b224e16f42e1.zip'}, '_bibtex': {'value': '@inproceedings{\\njia2024decisionmaking,\\ntitle={Decision-Making Behavior Evaluation Framework for {LLM}s under Uncertain Context},\\nauthor={Jingru Jia and Zehua Yuan and Junhao Pan and Paul E McNamara and Deming Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=re0ly2Ylcu}\\n}'}, 'paperhash': {'value': 'jia|decisionmaking_behavior_evaluation_framework_for_llms_under_uncertain_context'}},forum = 're0ly2Ylcu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7332/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7332/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7332/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7332/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rbtnRsiXSN',number = 896,cdate = 1714194067372,pdate = 1727287647021,odate = 1730873843898,mdate = 1730873843920,tcdate = 1714194067372,tmdate = 1730873843920,ddate = None,content = {'title': {'value': 'DeMo: Decoupling Motion Forecasting into  Directional Intentions and Dynamic States'}, 'authors': {'value': ['Bozhou Zhang', 'Nan Song', 'Li Zhang']}, 'authorids': {'value': ['~Bozhou_Zhang1', '~Nan_Song4', '~Li_Zhang5']}, 'keywords': {'value': ['Motion Forecasting', 'Autonomous Driving', 'Mamba', 'Attention']}, 'TLDR': {'value': 'A framework that decouples multi-modal trajectory queries into mode queries for directional intentions and state queries for dynamic states, utilizing Attention and Mamba.'}, 'abstract': {'value': \"Accurate motion forecasting for traffic agents is crucial for ensuring the safety and efficiency of autonomous driving systems in dynamically changing environments. Mainstream methods adopt a one-query-one-trajectory paradigm, where each query corresponds to a unique trajectory for predicting multi-modal trajectories. While straightforward and effective, the absence of detailed representation of future trajectories may yield suboptimal outcomes, given that the agent states dynamically evolve over time. To address this problem, we introduce DeMo, a framework that decouples multi-modal trajectory queries into two types: mode queries capturing distinct directional intentions and state queries tracking the agent's dynamic states over time. By leveraging this format, we separately optimize the multi-modality and dynamic evolutionary properties of trajectories. Subsequently, the mode and state queries are integrated to obtain a comprehensive and detailed representation of the trajectories. To achieve these operations, we additionally introduce combined Attention and Mamba techniques for global information aggregation and state sequence modeling, leveraging their respective strengths. Extensive experiments on both the Argoverse 2 and nuScenes benchmarks demonstrate that our DeMo achieves state-of-the-art performance in motion forecasting. In addition, we will make our code and models publicly available.\"}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3d4994978d6ca281a3b560f62d73cecc4d2310f7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024demo,\\ntitle={DeMo: Decoupling Motion Forecasting into  Directional Intentions and Dynamic States},\\nauthor={Bozhou Zhang and Nan Song and Li Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rbtnRsiXSN}\\n}'}, 'paperhash': {'value': 'zhang|demo_decoupling_motion_forecasting_into_directional_intentions_and_dynamic_states'}},forum = 'rbtnRsiXSN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission896/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission896/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission896/-/Revision', 'NeurIPS.cc/2024/Conference/Submission896/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rblaF2euXQ',number = 13671,cdate = 1715741938650,pdate = 1727288046279,odate = 1730873959571,mdate = 1737015137066,tcdate = 1715741938650,tmdate = 1737015137066,ddate = None,content = {'title': {'value': 'Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit'}, 'authors': {'value': ['Seok-Jin Kim', 'Min-hwan Oh']}, 'authorids': {'value': ['~Seok-Jin_Kim1', '~Min-hwan_Oh1']}, 'keywords': {'value': ['contextual bandit', 'greedy algorithm', 'exploration-exploitation', 'sequential decision-making', 'bandit algorithms']}, 'abstract': {'value': \"We study the performance guarantees of exploration-free greedy algorithms for the linear contextual bandit problem. \\nWe introduce a novel condition, named the \\\\textit{Local Anti-Concentration} (LAC) condition, which enables a greedy bandit algorithm to achieve provable efficiency. \\nWe show that the LAC condition is satisfied by a broad class of distributions, including Gaussian, exponential, uniform, Cauchy, and Student's~$t$ distributions, along with other exponential family distributions and their truncated variants. \\nThis significantly expands the class of distributions under which greedy algorithms can perform efficiently. \\nUnder our proposed LAC condition, we prove that the cumulative expected regret of the greedy algorithm for the linear contextual bandit is bounded by $\\\\mathcal{O}(\\\\operatorname{poly} \\\\log T)$. \\nOur results establish the widest range of distributions known to date that allow a sublinear regret bound for greedy algorithms, further achieving a sharp poly-logarithmic regret.\"}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10504514b3fa6d365ede9f7f26954080e223f1ea.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkim2024local,\\ntitle={Local Anti-Concentration Class: Logarithmic Regret for Greedy Linear Contextual Bandit},\\nauthor={Seok-Jin Kim and Min-hwan Oh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rblaF2euXQ}\\n}'}, 'supplementary_material': {'value': '/attachment/e49a91b6d3bd3e543f3d2c1f988904ab77cec978.zip'}, 'paperhash': {'value': 'kim|local_anticoncentration_class_logarithmic_regret_for_greedy_linear_contextual_bandit'}},forum = 'rblaF2euXQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13671/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13671/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13671/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13671/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rajRJ6WKj2',number = 20215,cdate = 1715795133876,pdate = 1727288228072,odate = 1730873999841,mdate = 1736257123446,tcdate = 1715795133876,tmdate = 1736257123446,ddate = None,content = {'title': {'value': 'DeBaRA: Denoising-Based 3D Room Arrangement Generation'}, 'authors': {'value': ['Léopold Maillard', 'Nicolas Sereyjol-Garros', 'Tom Durand', 'Maks Ovsjanikov']}, 'authorids': {'value': ['~Léopold_Maillard1', '~Nicolas_Sereyjol-Garros1', '~Tom_Durand1', '~Maks_Ovsjanikov1']}, 'keywords': {'value': ['Indoor 3D Scene Synthesis', 'Layout Generation', 'Score-based Generative Models', 'Diffusion Models', 'Conditional Generation']}, 'TLDR': {'value': 'We propose DeBaRA, a conditional score-based generative model that performs state-of-the art arrangement generation in bounded indoor scenes and several downstream applications by solely learning 3D object spatial features.'}, 'abstract': {'value': 'Generating realistic and diverse layouts of furnished indoor 3D scenes unlocks multiple interactive applications impacting a wide range of industries. The inherent complexity of object interactions, the limited amount of available data and the requirement to fulfill spatial constraints all make generative modeling for 3D scene synthesis and arrangement challenging. Current methods address these challenges autoregressively or by using off-the-shelf diffusion objectives by simultaneously predicting all attributes without 3D reasoning considerations. In this paper, we introduce DeBaRA, a score-based model specifically tailored for precise, controllable and flexible arrangement generation in a bounded environment. We argue that the most critical component of a scene synthesis system is to accurately establish the size and position of various objects within a restricted area. Based on this insight, we propose a lightweight conditional score-based model designed with 3D spatial awareness at its core. We demonstrate that by focusing on spatial attributes of objects, a single trained DeBaRA model can be leveraged at test time to perform several downstream applications such as scene synthesis, completion and re-arrangement. Further, we introduce a novel Self Score Evaluation procedure so it can be optimally employed alongside external LLM models. We evaluate our approach through extensive experiments and demonstrate significant improvement upon state-of-the-art approaches in a range of scenarios.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8e9f6d8fd7649487d5a585513a50e2992ffc16e7.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nmaillard2024debara,\\ntitle={DeBa{RA}: Denoising-Based 3D Room Arrangement Generation},\\nauthor={L{\\\\'e}opold Maillard and Nicolas Sereyjol-Garros and Tom Durand and Maks Ovsjanikov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rajRJ6WKj2}\\n}\"}, 'paperhash': {'value': 'maillard|debara_denoisingbased_3d_room_arrangement_generation'}},forum = 'rajRJ6WKj2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20215/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20215/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20215/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20215/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'rafVvthuxD',number = 8494,cdate = 1715662367915,pdate = 1727287881594,odate = 1730873912725,mdate = 1730873912744,tcdate = 1715662367915,tmdate = 1730873912744,ddate = None,content = {'title': {'value': 'EM Distillation for One-step Diffusion Models'}, 'authors': {'value': ['Sirui Xie', 'Zhisheng Xiao', 'Diederik P Kingma', 'Tingbo Hou', 'Ying Nian Wu', 'Kevin Patrick Murphy', 'Tim Salimans', 'Ben Poole', 'Ruiqi Gao']}, 'authorids': {'value': ['~Sirui_Xie1', '~Zhisheng_Xiao1', '~Diederik_P_Kingma1', '~Tingbo_Hou2', '~Ying_Nian_Wu1', '~Kevin_Patrick_Murphy1', '~Tim_Salimans1', '~Ben_Poole1', '~Ruiqi_Gao1']}, 'keywords': {'value': ['Generative models']}, 'abstract': {'value': 'While diffusion models  can learn complex distributions, sampling requires a computationally expensive iterative process.  Existing distillation methods enable efficient sampling, but have notable limitations, such as performance degradation with very few sampling steps, reliance on training data access, or mode-seeking optimization that may fail to capture the full distribution. We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality. Our approach is derived through the lens of Expectation-Maximization (EM), where the generator parameters are updated using samples from the joint distribution of the diffusion teacher prior and inferred generator latents. We develop a reparametrized sampling scheme and a noise cancellation technique that together stabilizes the distillation process. We further reveal an interesting connection of our method with existing methods that minimize mode-seeking KL. EMD outperforms existing one-step generative methods in terms of FID scores on ImageNet-64 and ImageNet-128, and compares favorably with prior work on distilling text-to-image diffusion models.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d561a28c837498ba61b189816ed8d05027ee8269.pdf'}, 'TLDR': {'value': 'We propose EM Distillation (EMD), a maximum likelihood-based approach that distills a diffusion model to a one-step generator model with minimal loss of perceptual quality.'}, '_bibtex': {'value': '@inproceedings{\\nxie2024em,\\ntitle={{EM} Distillation for One-step Diffusion Models},\\nauthor={Sirui Xie and Zhisheng Xiao and Diederik P Kingma and Tingbo Hou and Ying Nian Wu and Kevin Patrick Murphy and Tim Salimans and Ben Poole and Ruiqi Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rafVvthuxD}\\n}'}, 'paperhash': {'value': 'xie|em_distillation_for_onestep_diffusion_models'}},forum = 'rafVvthuxD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8494/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8494/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8494/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8494/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'raABeiV71j',number = 14363,cdate = 1715749122177,pdate = 1727288067281,odate = 1730873965170,mdate = 1730873965182,tcdate = 1715749122177,tmdate = 1730873965182,ddate = None,content = {'title': {'value': 'Loki: Low-rank Keys for Efficient Sparse Attention'}, 'authors': {'value': ['Prajwal Singhania', 'Siddharth Singh', 'Shwai He', 'Soheil Feizi', 'Abhinav Bhatele']}, 'authorids': {'value': ['~Prajwal_Singhania1', '~Siddharth_Singh7', '~Shwai_He1', '~Soheil_Feizi2', '~Abhinav_Bhatele1']}, 'keywords': {'value': ['Approximate Attention', 'Low-Dimensional Attention Keys', 'PCA', 'Top-K Attention']}, 'abstract': {'value': 'Inference on large language models (LLMs) can be expensive in terms of the\\ncompute and memory costs involved, especially when long sequence lengths are\\nused. In particular, the self-attention mechanism used in LLM inference contributes\\nsignificantly to these costs, which has sparked an interest in approximating the self-attention \\ncomputation to reduce such costs. In this work, we propose to approximate\\nself-attention by focusing on the dimensionality of key vectors computed in the\\nattention block. Our analysis reveals that key vectors lie in a significantly lower-dimensional \\nspace, consistently across several datasets and models. Exploiting this\\nobservation, we propose Loki, a novel sparse attention method that ranks and selects\\ntokens in the KV-cache based on attention scores computed in low-dimensional\\nspace. Our evaluations show that Loki is able to speed up the attention computation\\ndue to reduced data movement (load/store) and compute costs while maintaining\\nthe efficacy of the models better than other popular approximation methods.'}, 'primary_area': {'value': 'infrastructure'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/939725a4a0f7f31a1b9da256a3877e3c8fbde4bc.pdf'}, 'supplementary_material': {'value': '/attachment/28224bb5ddc1da9e62ed5cf574ca1475b3a5d739.zip'}, '_bibtex': {'value': '@inproceedings{\\nsinghania2024loki,\\ntitle={Loki: Low-rank Keys for Efficient Sparse Attention},\\nauthor={Prajwal Singhania and Siddharth Singh and Shwai He and Soheil Feizi and Abhinav Bhatele},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=raABeiV71j}\\n}'}, 'paperhash': {'value': 'singhania|loki_lowrank_keys_for_efficient_sparse_attention'}},forum = 'raABeiV71j',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14363/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14363/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14363/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14363/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'rYs2Dmn9tD',number = 20983,cdate = 1715799486327,pdate = 1727288246406,odate = 1730874004339,mdate = 1730874004357,tcdate = 1715799486327,tmdate = 1730874004357,ddate = None,content = {'title': {'value': 'Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs'}, 'authors': {'value': ['Ching-An Cheng', 'Allen Nie', 'Adith Swaminathan']}, 'authorids': {'value': ['~Ching-An_Cheng1', '~Allen_Nie1', '~Adith_Swaminathan1']}, 'keywords': {'value': ['Optimization', 'Back-Propagation', 'Automatic Differentiation', 'LLM', 'Language Feedback', 'Execution Trace']}, 'TLDR': {'value': 'Framework for efficient optimization of heterogenous parameters in general computational workflows'}, 'abstract': {'value': 'We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. AutoDiff frameworks, like PyTorch, enable efficient end-to-end optimization of differentiable systems. However, general computational workflows can be non-differentiable and involve rich feedback (e.g. console output or user’s responses), heterogeneous parameters (e.g. prompts, codes), and intricate objectives (beyond maximizing a score). We investigate end-to-end generative optimization – using generative models such as LLMs within the optimizer for automatic updating of general computational workflows. We discover that workflow execution traces are akin to back-propagated gradients in AutoDiff and can provide key information to interpret feedback for efficient optimization. Formally, we frame a new mathematical setup, Optimization with Trace Oracle (OPTO). In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. We provide a Python library, Trace, that efficiently converts a workflow optimization problem into an OPTO instance using PyTorch-like syntax. Using Trace, we develop a general LLM-based generative optimizer called OptoPrime. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We envision Trace as an open research platform for devising novel generative optimizers and developing the next generation of interactive learning agents. Website: https://microsoft.github.io/Trace/.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d03790ef61dce1580c1d113ebc679a0ade344987.pdf'}, 'supplementary_material': {'value': '/attachment/2ff35870bd3cac150509fc276e2f13e6e6256f60.zip'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024trace,\\ntitle={Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and {LLM}s},\\nauthor={Ching-An Cheng and Allen Nie and Adith Swaminathan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rYs2Dmn9tD}\\n}'}, 'paperhash': {'value': 'cheng|trace_is_the_next_autodiff_generative_optimization_with_rich_feedback_execution_traces_and_llms'}},forum = 'rYs2Dmn9tD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20983/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20983/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20983/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20983/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rYjYwuM6yH',number = 19890,cdate = 1715793288803,pdate = 1727288220208,odate = 1730873998190,mdate = 1730873998201,tcdate = 1715793288803,tmdate = 1730873998201,ddate = None,content = {'title': {'value': '3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability'}, 'authors': {'value': ['Baohao Liao', 'Christof Monz']}, 'authorids': {'value': ['~Baohao_Liao1', '~Christof_Monz1']}, 'keywords': {'value': ['parameter-efficient finetuning', 'orthogonal finetuning', 'batching', 'interpretability']}, 'TLDR': {'value': 'Our proposed method, RoAd, is extremely parameter-efficient, batching-efficient and composable.'}, 'abstract': {'value': \"Parameter-efficient finetuning (PEFT) methods effectively adapt large language models (LLMs) to diverse downstream tasks, reducing storage and GPU memory demands. Despite these advantages, several applications pose new challenges to PEFT beyond mere parameter efficiency. One notable challenge involves the efficient deployment of LLMs equipped with multiple task- or user-specific adapters, particularly when different adapters are needed for distinct requests within the same batch. Another challenge is the interpretability of LLMs, which is crucial for understanding how LLMs function. Previous studies introduced various approaches to address different challenges. In this paper, we introduce a novel method, RoAd, which employs a straightforward 2D rotation to adapt LLMs and addresses all the above challenges: (1) RoAd is remarkably parameter-efficient, delivering optimal performance on GLUE, eight commonsense reasoning tasks and four arithmetic reasoning tasks with <0.1% trainable parameters; (2) RoAd facilitates the efficient serving of requests requiring different adapters within a batch, with an overhead comparable to element-wise multiplication instead of batch matrix multiplication; (3) RoAd enhances LLM's interpretability through integration within a framework of distributed interchange intervention, demonstrated via composition experiments.\"}, 'pdf': {'value': '/pdf/5fe855220b9d0e4d60e69355553d93656f841300.pdf'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nliao2024in,\\ntitle={3-in-1: 2D Rotary Adaptation for Efficient Finetuning, Efficient Batching and Composability},\\nauthor={Baohao Liao and Christof Monz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rYjYwuM6yH}\\n}'}, 'paperhash': {'value': 'liao|3in1_2d_rotary_adaptation_for_efficient_finetuning_efficient_batching_and_composability'}},forum = 'rYjYwuM6yH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19890/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19890/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19890/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19890/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rXGxbDJadh',number = 1123,cdate = 1714405507884,pdate = 1727287652796,odate = 1730873845638,mdate = 1730873845655,tcdate = 1714405507884,tmdate = 1730873845655,ddate = None,content = {'title': {'value': 'Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor'}, 'authors': {'value': ['Keji He', 'Kehan Chen', 'Jiawang Bai', 'Yan Huang', 'Qi Wu', 'Shu-Tao Xia', 'Liang Wang']}, 'authorids': {'value': ['~Keji_He1', '~Kehan_Chen3', '~Jiawang_Bai2', '~Yan_Huang2', '~Qi_Wu3', '~Shu-Tao_Xia1', '~Liang_Wang3']}, 'keywords': {'value': ['Vision-and-Language Navigation', 'Multimodal', 'Continous Decision-Making', 'Backdoor Attack']}, 'abstract': {'value': 'Vision-and-Language Navigation (VLN) requires an agent to dynamically explore environments following natural language.\\nThe VLN agent, closely integrated into daily lives, poses a substantial threat to the security of privacy and property upon the occurrence of malicious behavior.\\nHowever, this serious issue has long been overlooked.\\nIn this paper, we pioneer the exploration of an object-aware backdoored VLN, achieved by implanting object-aware backdoors during the training phase. \\nTailored to the unique VLN nature of cross-modality and continuous decision-making, we propose a novel backdoored VLN paradigm: IPR Backdoor. \\nThis enables the agent to act in abnormal behavior once encountering the object triggers during language-guided navigation in unseen environments, thereby executing an attack on the target scene.\\nExperiments demonstrate the effectiveness of our method in both physical and digital spaces across different VLN agents, as well as its robustness to various visual and textual variations. Additionally, our method also well ensures navigation performance in normal scenarios with remarkable stealthiness.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/947b9c499117e1094c174a0a32f3ea8987265e42.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nhe2024everyday,\\ntitle={Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor},\\nauthor={Keji He and Kehan Chen and Jiawang Bai and Yan Huang and Qi Wu and Shu-Tao Xia and Liang Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rXGxbDJadh}\\n}'}, 'paperhash': {'value': 'he|everyday_object_meets_visionandlanguage_navigation_agent_via_backdoor'}},forum = 'rXGxbDJadh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1123/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1123/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1123/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission1123/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rVSc3HIZS4',number = 3878,cdate = 1715332459950,pdate = 1727287733534,odate = 1730873870163,mdate = 1730873870184,tcdate = 1715332459950,tmdate = 1730873870184,ddate = None,content = {'title': {'value': 'Multi-turn Reinforcement Learning with Preference Human Feedback'}, 'authors': {'value': ['Lior Shani', 'Aviv Rosenberg', 'Asaf Cassel', 'Oran Lang', 'Daniele Calandriello', 'Avital Zipori', 'Hila Noga', 'Orgad Keller', 'Bilal Piot', 'Idan Szpektor', 'Avinatan Hassidim', 'Yossi Matias', 'Remi Munos']}, 'authorids': {'value': ['~Lior_Shani2', '~Aviv_Rosenberg1', '~Asaf_Cassel1', '~Oran_Lang1', '~Daniele_Calandriello1', '~Avital_Zipori1', '~Hila_Noga1', '~Orgad_Keller1', '~Bilal_Piot1', '~Idan_Szpektor1', '~Avinatan_Hassidim3', '~Yossi_Matias2', '~Remi_Munos1']}, 'keywords': {'value': ['Reinforcement Learning', 'RLHF', 'Human Feedback', 'Preferences', 'Optimization', 'LLMs', 'Large Language Models', 'Natural Language Processing', 'NLP', 'Learning Theory', 'RL']}, 'abstract': {'value': 'Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks.  Existing methods work by emulating the human preference at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations.  In the tabular setting, we present a novel mirror-descent-based policy optimization algorithm for the general multi-turn preference-based RL problem, and prove its convergence to Nash equilibrium.  To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent  guides  a  student  in  learning  a  random  topic,  and  show  that  a  deep  RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4b797018cf3a0b6670839025a87efe1cbd4ef3e4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshani2024multiturn,\\ntitle={Multi-turn Reinforcement Learning with Preference Human Feedback},\\nauthor={Lior Shani and Aviv Rosenberg and Asaf Cassel and Oran Lang and Daniele Calandriello and Avital Zipori and Hila Noga and Orgad Keller and Bilal Piot and Idan Szpektor and Avinatan Hassidim and Yossi Matias and Remi Munos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rVSc3HIZS4}\\n}'}, 'paperhash': {'value': 'shani|multiturn_reinforcement_learning_with_preference_human_feedback'}},forum = 'rVSc3HIZS4',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3878/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3878/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3878/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3878/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rTxCIWsfsD',number = 16300,cdate = 1715769021547,pdate = 1727288122742,odate = 1730873978399,mdate = 1730873978411,tcdate = 1715769021547,tmdate = 1730873978411,ddate = None,content = {'title': {'value': 'Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions'}, 'authors': {'value': ['Rui Yang', 'Jie Wang', 'Guoping Wu', 'Bin Li']}, 'authorids': {'value': ['~Rui_Yang9', '~Jie_Wang1', '~Guoping_Wu1', '~Bin_Li8']}, 'keywords': {'value': ['Robust Offline Reinforcement Learning', 'Variational Bayesian Inference', 'Diverse Data Corruptions', 'Uncertainty']}, 'abstract': {'value': 'Real-world offline datasets are often subject to data corruptions (such as noise or adversarial attacks) due to sensor failures or malicious attacks. Despite advances in robust offline reinforcement learning (RL), existing methods struggle to learn robust agents under high uncertainty caused by the diverse corrupted data (i.e., corrupted states, actions, rewards, and dynamics), leading to performance degradation in clean environments. To tackle this problem, we propose a novel robust variational Bayesian inference for offline RL (TRACER). It introduces Bayesian inference for the first time to capture the uncertainty via offline data for robustness against all types of data corruptions. Specifically, TRACER first models all corruptions as the uncertainty in the action-value function. Then, to capture such uncertainty, it uses all offline data as the observations to approximate the posterior distribution of the action-value function under a Bayesian inference framework. An appealing feature of TRACER is that it can distinguish corrupted data from clean data using an entropy-based uncertainty measure, since corrupted data often induces higher uncertainty and entropy. Based on the aforementioned measure, TRACER can regulate the loss associated with corrupted data to reduce its influence, thereby enhancing robustness and performance in clean environments. Experiments demonstrate that TRACER significantly outperforms several state-of-the-art approaches across both individual and simultaneous data corruptions.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c5cf320e1eaf394d53aa48fa1e90e081eb9c16eb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024uncertaintybased,\\ntitle={Uncertainty-based Offline Variational Bayesian Reinforcement Learning for Robustness under Diverse Data Corruptions},\\nauthor={Rui Yang and Jie Wang and Guoping Wu and Bin Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rTxCIWsfsD}\\n}'}, 'paperhash': {'value': 'yang|uncertaintybased_offline_variational_bayesian_reinforcement_learning_for_robustness_under_diverse_data_corruptions'}},forum = 'rTxCIWsfsD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16300/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16300/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16300/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16300/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rTONicCCJm',number = 17346,cdate = 1715778974171,pdate = 1727288150845,odate = 1730873983990,mdate = 1730873984008,tcdate = 1715778974171,tmdate = 1730873984008,ddate = None,content = {'title': {'value': 'Learning from Highly Sparse Spatio-temporal Data'}, 'authors': {'value': ['Leyan Deng', 'Chenwang Wu', 'Defu Lian', 'Enhong Chen']}, 'authorids': {'value': ['~Leyan_Deng1', '~Chenwang_Wu1', '~Defu_Lian1', '~Enhong_Chen1']}, 'keywords': {'value': ['spatio-temporal data mining; incomplete data imputation']}, 'abstract': {'value': 'Incomplete spatio-temporal data in real-world has spawned many research.\\nHowever, existing methods often utilize iterative message-passing across temporal and spatial dimensions, resulting in substantial information loss and high computational cost.\\nWe provide a theoretical analysis revealing that such iterative models are not only susceptible to data sparsity but also to graph sparsity, causing unstable performances on different datasets.\\nTo overcome these limitations, we introduce a novel method named One-step Propagation and Confidence-based Refinement (OPCR).\\nIn the first stage, OPCR leverages inherent spatial and temporal relationships by employing sparse attention mechanism.\\nThese modules propagate limited observations directly to the global context through one-step imputation, which are theoretically effected only by data sparsity.\\nFollowing this, we assign confidence levels to the initial imputations by correlating missing data with valid data.\\nThis confidence-based propagation refines the seperate spatial and temporal imputation results through spatio-temporal dependencies.\\nWe evaluate the proposed model across various downstream tasks involving highly sparse spatio-temporal data.\\nEmpirical results indicate that our model outperforms state-of-the-art imputation methods, demonstrating its superior effectiveness and robustness.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6393db9bf39bef496bcbefc862fbb48bc658b42e.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndeng2024learning,\\ntitle={Learning from Highly Sparse Spatio-temporal Data},\\nauthor={Leyan Deng and Chenwang Wu and Defu Lian and Enhong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rTONicCCJm}\\n}'}, 'paperhash': {'value': 'deng|learning_from_highly_sparse_spatiotemporal_data'}},forum = 'rTONicCCJm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17346/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17346/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17346/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17346/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rQYyWGYuzK',number = 16823,cdate = 1715774706672,pdate = 1727288137967,odate = 1730873981601,mdate = 1730873981619,tcdate = 1715774706672,tmdate = 1730873981619,ddate = None,content = {'title': {'value': 'Monomial Matrix Group Equivariant Neural Functional Networks'}, 'authors': {'value': ['Hoang V. Tran', 'Thieu Vo', 'Tho Tran Huu', 'An Nguyen The', 'Tan Minh Nguyen']}, 'authorids': {'value': ['~Hoang_V._Tran1', '~Thieu_Vo1', '~Tho_Tran_Huu1', '~An_Nguyen_The2', '~Tan_Minh_Nguyen1']}, 'keywords': {'value': ['neural functional networks', 'equivariant networks', 'monomial matrices', 'symmetry']}, 'TLDR': {'value': 'We extend the study of the group action on the network weights from the group of permutation matrices to the group of monomial matrices by incorporating scaling symmetries.'}, 'abstract': {'value': \"Neural functional networks (NFNs) have recently gained significant attention due to their diverse applications, ranging from predicting network generalization and network editing to classifying implicit neural representation. Previous NFN designs often depend on permutation symmetries in neural networks' weights, which traditionally arise from the unordered arrangement of neurons in hidden layers. However, these designs do not take into account the weight scaling symmetries of $\\\\operatorname{ReLU}$ networks, and the weight sign flipping symmetries of $\\\\operatorname{sin}$ or $\\\\operatorname{Tanh}$ networks. In this paper, we extend the study of the group action on the network weights from the group of permutation matrices to the group of monomial matrices by incorporating scaling/sign-flipping symmetries. Particularly, we encode these scaling/sign-flipping symmetries by designing our corresponding equivariant and invariant layers. We name our new family of NFNs the Monomial Matrix Group Equivariant Neural Functional Networks (Monomial-NFN). Because of the expansion of the symmetries, Monomial-NFN has much fewer independent trainable parameters compared to the baseline NFNs in the literature, thus enhancing the model's efficiency. Moreover, for fully connected and convolutional neural networks, we theoretically prove that all groups that leave these networks invariant while acting on their weight spaces are some subgroups of the monomial matrix group. We provide empirical evidences to demonstrate the advantages of our model over existing baselines, achieving competitive performance and efficiency. The code is publicly available at https://github.com/MathematicalAI-NUS/Monomial-NFN.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/6ed88010e6cf6cc8e9dc09993366a200e281e01f.zip'}, 'pdf': {'value': '/pdf/2935396fb886cad4fc4100231f4219f1851a2f78.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntran2024monomial,\\ntitle={Monomial Matrix Group Equivariant Neural Functional Networks},\\nauthor={Hoang V. Tran and Thieu Vo and Tho Tran Huu and An Nguyen The and Tan Minh Nguyen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rQYyWGYuzK}\\n}'}, 'paperhash': {'value': 'tran|monomial_matrix_group_equivariant_neural_functional_networks'}},forum = 'rQYyWGYuzK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16823/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16823/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16823/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16823/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rPgc5brxmT',number = 17434,cdate = 1715779476506,pdate = 1727288153424,odate = 1730873984444,mdate = 1730873984480,tcdate = 1715779476506,tmdate = 1730873984480,ddate = None,content = {'title': {'value': 'Interaction-Force Transport Gradient Flows'}, 'authors': {'value': ['Egor Gladin', 'Pavel Dvurechensky', 'Alexander Mielke', 'Jia-Jie Zhu']}, 'authorids': {'value': ['~Egor_Gladin1', '~Pavel_Dvurechensky1', '~Alexander_Mielke1', '~Jia-Jie_Zhu1']}, 'keywords': {'value': ['kernel methods', 'gradient flow', 'optimal transport', 'Wasserstein', 'Fisher-Rao', 'Hellinger', 'unbalanced optimal transport', 'partial differential equation', 'optimization', 'calculus of variations', 'variational inference', 'MCMC', 'MMD']}, 'abstract': {'value': 'This paper presents a new gradient flow dissipation geometry over non-negative and probability measures.\\nThis is motivated by a principled construction that combines the unbalanced optimal transport and interaction forces modeled by reproducing kernels. Using a precise connection between the Hellinger geometry and the maximum mean discrepancy (MMD), we propose the interaction-force transport (IFT) gradient flows and its spherical variant via an infimal convolution of the Wasserstein and spherical MMD tensors. We then develop a particle-based optimization algorithm based on the JKO-splitting scheme of the mass-preserving spherical IFT gradient flows. Finally, we provide both theoretical global exponential convergence guarantees and improved empirical simulation results for applying the IFT gradient flows to the sampling task of MMD-minimization. Furthermore, we prove that the spherical IFT  gradient flow enjoys the best of both worlds by providing the global exponential convergence guarantee for both the MMD and KL energy.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3bfdb95270fe666d69c52987a01a3f5350c03d86.pdf'}, 'TLDR': {'value': 'We propose the spherical interaction-force transport (IFT) gradient flows with the global exponential convergence analysis for both the MMD and the KL-energy gradient flows. Numerical experiments show stable inference performance.'}, 'supplementary_material': {'value': '/attachment/8db33b69455e12ccaa01f7ce23564c12cc0b382c.zip'}, '_bibtex': {'value': '@inproceedings{\\ngladin2024interactionforce,\\ntitle={Interaction-Force Transport Gradient Flows},\\nauthor={Egor Gladin and Pavel Dvurechensky and Alexander Mielke and Jia-Jie Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rPgc5brxmT}\\n}'}, 'paperhash': {'value': 'gladin|interactionforce_transport_gradient_flows'}},forum = 'rPgc5brxmT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17434/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17434/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17434/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17434/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rPKCrzdqJx',number = 1973,cdate = 1714914634022,pdate = 1727287677138,odate = 1730873853875,mdate = 1730873853888,tcdate = 1714914634022,tmdate = 1730873853888,ddate = None,content = {'title': {'value': 'Regret Minimization in Stackelberg Games with Side Information'}, 'authors': {'value': ['Keegan Harris', 'Steven Wu', 'Maria Florina Balcan']}, 'authorids': {'value': ['~Keegan_Harris1', '~Steven_Wu1', '~Maria_Florina_Balcan1']}, 'keywords': {'value': ['Stackelberg game', 'context', 'online learning']}, 'TLDR': {'value': \"We introduce the problem of learning in Stackelberg games where each players' utility depends on an external context.\"}, 'abstract': {'value': \"Algorithms for playing in Stackelberg games have been deployed in real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader commits to a (context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on the online setting in which a sequence of followers arrive over time, and the context may change from round-to-round. In sharp contrast to the non-contextual version, we show that it is impossible for the leader to achieve good performance (measured by regret) in the full adversarial setting.  Motivated by our impossibility result, we show that no-regret learning is possible in two natural relaxations: the setting in which the sequence of followers is chosen stochastically and the sequence of contexts is adversarial, and the setting in which the sequence of contexts is stochastic and the sequence of followers is chosen by an adversary.\"}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e13ba07995ed5c87b819586a30e5c574bc86beb6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nharris2024regret,\\ntitle={Regret Minimization in Stackelberg Games with Side Information},\\nauthor={Keegan Harris and Steven Wu and Maria Florina Balcan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rPKCrzdqJx}\\n}'}, 'paperhash': {'value': 'harris|regret_minimization_in_stackelberg_games_with_side_information'}},forum = 'rPKCrzdqJx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1973/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1973/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1973/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1973/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rM3FFH1mqk',number = 13062,cdate = 1715734285249,pdate = 1727288027459,odate = 1730873954751,mdate = 1730873954771,tcdate = 1715734285249,tmdate = 1730873954771,ddate = None,content = {'title': {'value': 'Semidefinite Relaxations of the Gromov-Wasserstein Distance'}, 'authors': {'value': ['Junyu Chen', 'Binh Nguyen', 'Shang Hui Koh', 'Yong Sheng Soh']}, 'authorids': {'value': ['~Junyu_Chen5', '~Binh_Nguyen2', '~Shang_Hui_Koh1', '~Yong_Sheng_Soh1']}, 'keywords': {'value': ['optimal transport', 'gromov-wasserstein', 'semidefinite programming', 'optimization']}, 'TLDR': {'value': 'We propose a semi-definite programming (SDP) relaxation of the GW distance.'}, 'abstract': {'value': 'The Gromov-Wasserstein (GW) distance is an extension of the optimal transport problem that allows one to match objects between incomparable spaces.  At its core, the GW distance is specified as the solution of a non-convex quadratic program and is not known to be tractable to solve.  In particular, existing solvers for the GW distance are only able to find locally optimal solutions.  In this work, we propose a semi-definite programming (SDP) relaxation of the GW distance. The relaxation can be viewed as the Lagrangian dual of the GW distance augmented with constraints that relate to the linear and quadratic terms of transportation plans. In particular, our relaxation provides a tractable (polynomial-time) algorithm to compute globally optimal transportation plans (in some instances) together with an accompanying proof of global optimality.  Our numerical experiments suggest that the proposed relaxation is strong in that it frequently computes the globally optimal solution.  Our Python implementation is available at https://github.com/tbng/gwsdp.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/27fc72a84eb27e72c7b20792e2bcf5a42dfcc79a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024semidefinite,\\ntitle={Semidefinite Relaxations of the Gromov-Wasserstein Distance},\\nauthor={Junyu Chen and Binh Nguyen and Shang Hui Koh and Yong Sheng Soh},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rM3FFH1mqk}\\n}'}, 'paperhash': {'value': 'chen|semidefinite_relaxations_of_the_gromovwasserstein_distance'}},forum = 'rM3FFH1mqk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13062/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13062/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13062/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13062/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rM24UUgZg8',number = 5914,cdate = 1715576330836,pdate = 1727287799086,odate = 1730873889247,mdate = 1736512455257,tcdate = 1715576330836,tmdate = 1736512455257,ddate = None,content = {'title': {'value': 'Activating Self-Attention for Multi-Scene Absolute Pose Regression'}, 'authors': {'value': ['Miso Lee', 'Jihwan Kim', 'Jae-Pil Heo']}, 'authorids': {'value': ['~Miso_Lee1', '~Jihwan_Kim1', '~Jae-Pil_Heo3']}, 'keywords': {'value': ['Multi-Scene Absolute Pose Regression', 'Transformer', 'Attention Collapse']}, 'TLDR': {'value': 'Unleashing the potential of self-attention by rectifying the distortion of query-key embedding space'}, 'abstract': {'value': 'Multi-scene absolute pose regression addresses the demand for fast and memory-efficient camera pose estimation across various real-world environments. Nowadays, transformer-based model has been devised to regress the camera pose directly in multi-scenes. Despite its potential, transformer encoders are underutilized due to the collapsed self-attention map, having low representation capacity. This work highlights the problem and investigates it from a new perspective: distortion of query-key embedding space. Based on the statistical analysis, we reveal that queries and keys are mapped in completely different spaces while only a few keys are blended into the query region. This leads to the collapse of the self-attention map as all queries are considered similar to those few keys. Therefore, we propose simple but effective solutions to activate self-attention. Concretely, we present an auxiliary loss that aligns queries and keys, preventing the distortion of query-key space and encouraging the model to find global relations by self-attention. In addition, the fixed sinusoidal positional encoding is adopted instead of undertrained learnable one to reflect appropriate positional clues into the inputs of self-attention. As a result, our approach resolves the aforementioned problem effectively, thus outperforming existing methods in both outdoor and indoor scenes.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/19b70328de6443308da1720b1d795eaac9afdd78.pdf'}, 'supplementary_material': {'value': '/attachment/4d1b452d98fbb473862b63c0f95f9034b0d160ff.zip'}, '_bibtex': {'value': '@inproceedings{\\nlee2024activating,\\ntitle={Activating Self-Attention for Multi-Scene Absolute Pose Regression},\\nauthor={Miso Lee and Jihwan Kim and Jae-Pil Heo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rM24UUgZg8}\\n}'}, 'paperhash': {'value': 'lee|activating_selfattention_for_multiscene_absolute_pose_regression'}},forum = 'rM24UUgZg8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5914/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5914/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5914/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5914/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rLJisJmMKw',number = 14669,cdate = 1715752463282,pdate = 1727288076185,odate = 1730873967694,mdate = 1736996074567,tcdate = 1715752463282,tmdate = 1736996074567,ddate = None,content = {'title': {'value': 'GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping'}, 'authors': {'value': ['Junyoung Seo', 'Kazumi Fukuda', 'Takashi Shibuya', 'Takuya Narihira', 'Naoki Murata', 'Shoukang Hu', 'Chieh-Hsin Lai', 'Seungryong Kim', 'Yuki Mitsufuji']}, 'authorids': {'value': ['~Junyoung_Seo1', '~Kazumi_Fukuda1', '~Takashi_Shibuya1', '~Takuya_Narihira2', '~Naoki_Murata1', '~Shoukang_Hu1', '~Chieh-Hsin_Lai2', '~Seungryong_Kim1', '~Yuki_Mitsufuji1']}, 'keywords': {'value': ['Single-shot Novel View Generation', 'Generative Models', 'Novel View Generation', 'Diffusion Models']}, 'abstract': {'value': 'Generating novel views from a single image remains a challenging task due to the complexity of 3D scenes and the limited diversity in the existing multi-view datasets to train a model on. Recent research combining large-scale text-to-image (T2I) models with monocular depth estimation (MDE) has shown promise in handling in-the-wild images. In these methods, an input view is geometrically warped to novel views with estimated depth maps, then the warped image is inpainted by T2I models. However, they struggle with noisy depth maps and loss of semantic details when warping an input view to novel viewpoints. In this paper, we propose a novel approach for single-shot novel view synthesis, a semantic-preserving generative warping framework that enables T2I generative models to learn where to warp and where to generate, through augmenting cross-view attention with self-attention. Our approach addresses the limitations of existing methods by conditioning the generative model on source view images and incorporating geometric warping signals. Qualitative and quantitative evaluations demonstrate that our model outperforms existing methods in both in-domain and out-of-domain scenarios. Project page is available at https://GenWarp-NVS.github.io.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f33fe003e425f37d2ee8df9265466cf4a4998ed1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nseo2024genwarp,\\ntitle={GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping},\\nauthor={Junyoung Seo and Kazumi Fukuda and Takashi Shibuya and Takuya Narihira and Naoki Murata and Shoukang Hu and Chieh-Hsin Lai and Seungryong Kim and Yuki Mitsufuji},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rLJisJmMKw}\\n}'}, 'paperhash': {'value': 'seo|genwarp_single_image_to_novel_views_with_semanticpreserving_generative_warping'}},forum = 'rLJisJmMKw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14669/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14669/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14669/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14669/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rL7OtNsD9a',number = 1172,cdate = 1714455374605,pdate = 1727287654063,odate = 1730873846179,mdate = 1730873846201,tcdate = 1714455374605,tmdate = 1730873846201,ddate = None,content = {'title': {'value': 'Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning'}, 'authors': {'value': ['Dongsu Lee', 'Minhae Kwon']}, 'authorids': {'value': ['~Dongsu_Lee1', '~Minhae_Kwon1']}, 'keywords': {'value': ['Reinforcement learning', 'Episodic future thinking', 'multi-agent reinforcement learning']}, 'abstract': {'value': 'Understanding cognitive processes in multi-agent interactions is a primary goal in cognitive science. It can guide the direction of artificial intelligence (AI) research toward social decision-making in multi-agent systems, which includes uncertainty from character heterogeneity. In this paper, we introduce *episodic future thinking (EFT) mechanism* for a reinforcement learning (RL) agent, inspired by the cognitive processes observed in animals. To enable future thinking functionality, we first develop a *multi-character policy* that captures diverse characters with an ensemble of heterogeneous policies. The *character* of an agent is defined as a different weight combination on reward components, representing distinct behavioral preferences. The future thinking agent collects observation-action trajectories of the target agents and leverages the pre-trained multi-character policy to infer their characters. Once the character is inferred, the agent predicts the upcoming actions of target agents and simulates the potential future scenario. This capability allows the agent to adaptively select the optimal action, considering the predicted future scenario in multi-agent scenarios. To evaluate the proposed mechanism, we consider the multi-agent autonomous driving scenario in which autonomous vehicles with different driving traits are on the road. Simulation results demonstrate that the EFT mechanism with accurate character inference leads to a higher reward than existing multi-agent solutions. We also confirm that the effect of reward improvement remains valid across societies with different levels of character diversity.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/60e8c7e9988c5c4ebbf3be916b263f3f55247926.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024episodic,\\ntitle={Episodic Future Thinking Mechanism for Multi-agent Reinforcement Learning},\\nauthor={Dongsu Lee and Minhae Kwon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rL7OtNsD9a}\\n}'}, 'paperhash': {'value': 'lee|episodic_future_thinking_mechanism_for_multiagent_reinforcement_learning'}},forum = 'rL7OtNsD9a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1172/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1172/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1172/-/Revision', 'NeurIPS.cc/2024/Conference/-/Desk_Rejected_Submission', 'NeurIPS.cc/2024/Conference/Submission1172/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rIOl7KbSkv',number = 12729,cdate = 1715727260626,pdate = 1727288015947,odate = 1730873951292,mdate = 1730873951309,tcdate = 1715727260626,tmdate = 1730873951309,ddate = None,content = {'title': {'value': 'No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices'}, 'authors': {'value': ['Qi Pang', 'Shengyuan Hu', 'Wenting Zheng', 'Virginia Smith']}, 'authorids': {'value': ['~Qi_Pang1', '~Shengyuan_Hu2', '~Wenting_Zheng1', '~Virginia_Smith1']}, 'keywords': {'value': ['watermarking', 'large language models', 'security', 'privacy']}, 'TLDR': {'value': 'We reveal and evaluate new attack vectors that exploit the common design choices of LLM watermarks.'}, 'abstract': {'value': 'Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems  surprisingly susceptible to attack---leading to fundamental trade-offs in robustness, utility, and usability. \\nTo navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose  guidelines and defenses for LLM watermarking in practice.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bb004f77c167bc7493180ec14d476519fd86acc7.pdf'}, '_bibtex': {'value': '@inproceedings{\\npang2024no,\\ntitle={No Free Lunch in {LLM} Watermarking: Trade-offs in Watermarking Design Choices},\\nauthor={Qi Pang and Shengyuan Hu and Wenting Zheng and Virginia Smith},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rIOl7KbSkv}\\n}'}, 'paperhash': {'value': 'pang|no_free_lunch_in_llm_watermarking_tradeoffs_in_watermarking_design_choices'}},forum = 'rIOl7KbSkv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12729/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12729/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12729/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12729/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rIOTceoNc8',number = 20564,cdate = 1715797125618,pdate = 1727288236667,odate = 1730874002006,mdate = 1730874002018,tcdate = 1715797125618,tmdate = 1730874002018,ddate = None,content = {'title': {'value': 'Graph Coarsening with Message-Passing Guarantees'}, 'authors': {'value': ['Antonin Joly', 'Nicolas Keriven']}, 'authorids': {'value': ['~Antonin_Joly1', '~Nicolas_Keriven1']}, 'keywords': {'value': ['graph coarsening', 'message passing', 'graph neural network']}, 'TLDR': {'value': 'We propose a new message-passing paradigm specific to coarsened graphs, with theoretical guarantees from the original graph.'}, 'abstract': {'value': 'Graph coarsening aims to reduce the size of a large graph while preserving some of its key properties, which has been used in many applications to reduce computational load and memory footprint. For instance, in graph machine learning, training Graph Neural Networks (GNNs) on coarsened graphs leads to drastic savings in time and memory. However, GNNs rely on the Message-Passing (MP) paradigm, and classical spectral preservation guarantees for graph coarsening do not directly lead to theoretical guarantees when performing naive message-passing on the coarsened graph.\\n\\nIn this work, we propose a new message-passing operation specific to coarsened graphs, which exhibit theoretical guarantees on the preservation of the propagated signal. Interestingly, and in a sharp departure from previous proposals, this operation on coarsened graphs is oriented, even when the original graph is undirected. We conduct node classification tasks on synthetic and real data and observe improved results compared to performing naive message-passing on the coarsened graph.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/18c2962b68647987ac2041fa833e47449b2d7b51.pdf'}, '_bibtex': {'value': '@inproceedings{\\njoly2024graph,\\ntitle={Graph Coarsening with Message-Passing Guarantees},\\nauthor={Antonin Joly and Nicolas Keriven},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rIOTceoNc8}\\n}'}, 'paperhash': {'value': 'joly|graph_coarsening_with_messagepassing_guarantees'}},forum = 'rIOTceoNc8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20564/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20564/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20564/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20564/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rI80PHlnFm',number = 7815,cdate = 1715640017123,pdate = 1727287859569,odate = 1730873906815,mdate = 1730873906832,tcdate = 1715640017123,tmdate = 1730873906832,ddate = None,content = {'title': {'value': 'Model Based Inference of Synaptic Plasticity Rules'}, 'authors': {'value': ['Yash Mehta', 'Danil Tyulmankov', 'Adithya E. Rajagopalan', 'Glenn C Turner', 'James E Fitzgerald', 'Jan Funke']}, 'authorids': {'value': ['~Yash_Mehta1', '~Danil_Tyulmankov1', '~Adithya_E._Rajagopalan1', '~Glenn_C_Turner1', '~James_E_Fitzgerald1', '~Jan_Funke3']}, 'keywords': {'value': ['computational neuroscience', 'plasticity rules', 'synaptic plasticity', 'biologically plausible learning']}, 'abstract': {'value': \"Inferring the synaptic plasticity rules that govern learning in the brain is a key challenge in neuroscience. We present a novel computational method to infer these rules from experimental data, applicable to both neural and behavioral data. Our approach approximates plasticity rules using a parameterized function, employing either truncated Taylor series for theoretical interpretability or multilayer perceptrons. These plasticity parameters are optimized via gradient descent over entire trajectories to align closely with observed neural activity or behavioral learning dynamics. This method can uncover complex rules that induce long nonlinear time dependencies, particularly involving factors like postsynaptic activity and current synaptic weights. We validate our approach through simulations, successfully recovering established rules such as Oja's, as well as more intricate plasticity rules with reward-modulated terms. We assess the robustness of our technique to noise and apply it to behavioral data from \\\\textit{Drosophila} in a probabilistic reward-learning experiment. Notably, our findings reveal an active forgetting component in reward learning in flies, improving predictive accuracy over previous models. This modeling framework offers a promising new avenue for elucidating the computational principles of synaptic plasticity and learning in the brain.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We developed a computational method to infer complex synaptic plasticity rules from neural and behavioral data, revealing new insights like active forgetting in reward learning in flies.'}, 'pdf': {'value': '/pdf/a21c61f9cd6d8ed27bf3d681bd43b7ddb7a63c58.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmehta2024model,\\ntitle={Model Based Inference of Synaptic Plasticity Rules},\\nauthor={Yash Mehta and Danil Tyulmankov and Adithya E. Rajagopalan and Glenn C Turner and James E Fitzgerald and Jan Funke},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rI80PHlnFm}\\n}'}, 'paperhash': {'value': 'mehta|model_based_inference_of_synaptic_plasticity_rules'}},forum = 'rI80PHlnFm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7815/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7815/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7815/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7815/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rI7oZj1WMc',number = 8041,cdate = 1715651077347,pdate = 1727287866249,odate = 1730873908729,mdate = 1730873908749,tcdate = 1715651077347,tmdate = 1730873908749,ddate = None,content = {'title': {'value': 'Learning Successor Features the Simple Way'}, 'authors': {'value': ['Raymond Chua', 'Arna Ghosh', 'Christos Kaplanis', 'Blake Aaron Richards', 'Doina Precup']}, 'authorids': {'value': ['~Raymond_Chua1', '~Arna_Ghosh1', '~Christos_Kaplanis2', '~Blake_Aaron_Richards1', '~Doina_Precup1']}, 'keywords': {'value': ['deep reinforcement learning', 'representation learning', 'continual reinforcement learning', 'successor feature', 'successor representation']}, 'TLDR': {'value': 'A simple approach for learning Successor Features from pixel-level observations for Continual Reinforcement Learning'}, 'abstract': {'value': 'In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporal-difference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs.  We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid) and 3D (Miniworld) mazes, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/284ead1f3023f7d437837821d112a4abd2d00a1f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchua2024learning,\\ntitle={Learning Successor Features the Simple Way},\\nauthor={Raymond Chua and Arna Ghosh and Christos Kaplanis and Blake Aaron Richards and Doina Precup},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rI7oZj1WMc}\\n}'}, 'paperhash': {'value': 'chua|learning_successor_features_the_simple_way'}},forum = 'rI7oZj1WMc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8041/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8041/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8041/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8041/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rGEDFS3emy',number = 8075,cdate = 1715651911727,pdate = 1727287867429,odate = 1730873909085,mdate = 1730873909097,tcdate = 1715651911727,tmdate = 1730873909097,ddate = None,content = {'title': {'value': 'F-OAL: Forward-only Online Analytic Learning with Fast Training and Low Memory Footprint in Class Incremental Learning'}, 'authors': {'value': ['Huiping Zhuang', 'Yuchen Liu', 'Run He', 'Kai Tong', 'Ziqian Zeng', 'Cen Chen', 'Yi Wang', 'Lap-Pui Chau']}, 'authorids': {'value': ['~Huiping_Zhuang2', '~Yuchen_Liu16', '~Run_He1', '~Kai_Tong1', '~Ziqian_Zeng1', '~Cen_Chen4', '~Yi_Wang48', '~Lap-Pui_Chau3']}, 'keywords': {'value': ['Class incremental learning', 'closed-form solution', 'exemplar-free', 'continual learning', 'online continual learning']}, 'abstract': {'value': 'Online Class Incremental Learning (OCIL) aims to train models incrementally, where data arrive in mini-batches, and previous data are not accessible. A major challenge in OCIL is Catastrophic Forgetting, i.e., the loss of previously learned knowledge. Among existing baselines, replay-based methods show competitive results but requires extra memory for storing exemplars, while exemplar-free (i.e., data need not be stored for replay in production) methods are resource friendly but often lack accuracy. In this paper, we propose an exemplar-free approach—Forward-only Online Analytic Learning (F-OAL). Unlike traditional methods, F-OAL does not rely on back-propagation and is forward-only, significantly reducing memory usage and computational time. Cooperating with a pre-trained frozen encoder with Feature Fusion, F-OAL only needs to update a linear classifier by recursive least square. This approach simultaneously achieves high accuracy and low resource consumption. Extensive experiments on bench mark datasets demonstrate F-OAL’s robust performance in OCIL scenarios. Code is available at: https://github.com/liuyuchen-cz/F-OAL'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e226708f2029c076b49ce0f8780b0b25c1a15cb8.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhuang2024foal,\\ntitle={F-{OAL}: Forward-only Online Analytic Learning with Fast Training and Low Memory Footprint in Class Incremental Learning},\\nauthor={Huiping Zhuang and Yuchen Liu and Run He and Kai Tong and Ziqian Zeng and Cen Chen and Yi Wang and Lap-Pui Chau},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rGEDFS3emy}\\n}'}, 'paperhash': {'value': 'zhuang|foal_forwardonly_online_analytic_learning_with_fast_training_and_low_memory_footprint_in_class_incremental_learning'}},forum = 'rGEDFS3emy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8075/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8075/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8075/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8075/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rF1YRtZfoJ',number = 9843,cdate = 1715686530504,pdate = 1727287922530,odate = 1730873924122,mdate = 1730873924138,tcdate = 1715686530504,tmdate = 1730873924138,ddate = None,content = {'title': {'value': 'CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models'}, 'authors': {'value': ['Saurav Jha', 'Dong Gong', 'Lina Yao']}, 'authorids': {'value': ['~Saurav_Jha1', '~Dong_Gong1', '~Lina_Yao2']}, 'keywords': {'value': ['Continual Learning', 'Vision-language models', 'Finetuning']}, 'abstract': {'value': 'Continual learning (CL) aims to help deep neural networks to learn new knowledge while retaining what has been learned. Owing to their powerful generalizability,  pre-trained vision-language models such as Contrastive Language-Image Pre-training (CLIP)  have lately gained traction as practical CL candidates. However, the domain mismatch between the pre-training and the downstream CL tasks calls for finetuning of the CLIP on the latter. The deterministic nature of the existing finetuning methods makes them overlook the many possible interactions across the modalities and deems them unsafe for high-risk tasks requiring reliable uncertainty estimation. To address these, our work proposes **C**ontinual **L**e**A**rning with **P**robabilistic finetuning (CLAP) - a probabilistic modeling framework over visual-guided text features per task, thus providing more calibrated CL finetuning. Unlike recent data-hungry anti-forgetting CL techniques, CLAP alleviates forgetting by exploiting the rich pre-trained knowledge of CLIP for weight initialization and distribution regularization of task-specific parameters. Cooperating with the diverse range of existing prompting methods, CLAP can surpass the predominant deterministic finetuning approaches for CL with CLIP. We conclude with out-of-the-box applications of superior uncertainty estimation abilities of CLAP including novel data detection and exemplar selection within the existing CL setups. Our code is available at https://github.com/srvCodes/clap4clip.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/649fc2bc1d6ab7ff1bb07d921e2180c36c2ccf3b.pdf'}, 'supplementary_material': {'value': '/attachment/e5e8c43764c96a86f693d79ed29b24bbb5140da7.zip'}, 'TLDR': {'value': 'We propose a probabilistic finetuning method for continual learning with pre-trained CLIP model to enable better in-domain knowledge acquisition, generalization, and model calibration.'}, '_bibtex': {'value': '@inproceedings{\\njha2024clapclip,\\ntitle={{CLAP}4{CLIP}: Continual Learning with Probabilistic Finetuning for Vision-Language Models},\\nauthor={Saurav Jha and Dong Gong and Lina Yao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rF1YRtZfoJ}\\n}'}, 'paperhash': {'value': 'jha|clap4clip_continual_learning_with_probabilistic_finetuning_for_visionlanguage_models'}},forum = 'rF1YRtZfoJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9843/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9843/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9843/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9843/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rDoPMODpki',number = 7614,cdate = 1715630359452,pdate = 1727287853131,odate = 1730873904627,mdate = 1730873904646,tcdate = 1715630359452,tmdate = 1730873904646,ddate = None,content = {'title': {'value': 'KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge'}, 'authors': {'value': ['Pengcheng Jiang', 'Lang Cao', 'Cao Xiao', 'Parminder Bhatia', 'Jimeng Sun', 'Jiawei Han']}, 'authorids': {'value': ['~Pengcheng_Jiang2', '~Lang_Cao2', '~Cao_Xiao2', '~Parminder_Bhatia1', '~Jimeng_Sun3', '~Jiawei_Han1']}, 'keywords': {'value': ['Knowledge Graphs', 'Large Language Models', 'Link Prediction']}, 'abstract': {'value': 'Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4\\\\%, 13.5\\\\%, and 11.9\\\\% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6\\\\%, 6.7\\\\%, and 17.7\\\\% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8e57c825b4a1824ff155a994d0dd93f8025cf334.pdf'}, 'supplementary_material': {'value': '/attachment/284ffd635e93b4cf1ff793f443798e258de12b05.zip'}, '_bibtex': {'value': '@inproceedings{\\njiang2024kgfit,\\ntitle={{KG}-{FIT}: Knowledge Graph Fine-Tuning Upon Open-World Knowledge},\\nauthor={Pengcheng Jiang and Lang Cao and Cao Xiao and Parminder Bhatia and Jimeng Sun and Jiawei Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rDoPMODpki}\\n}'}, 'paperhash': {'value': 'jiang|kgfit_knowledge_graph_finetuning_upon_openworld_knowledge'}},forum = 'rDoPMODpki',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7614/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7614/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7614/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7614/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rCnZrFikX6',number = 19386,cdate = 1715790633953,pdate = 1727288208620,odate = 1730873995273,mdate = 1730873995291,tcdate = 1715790633953,tmdate = 1730873995291,ddate = None,content = {'title': {'value': 'Neural Persistence Dynamics'}, 'authors': {'value': ['Sebastian Zeng', 'Florian Graf', 'Martin Uray', 'Stefan Huber', 'Roland Kwitt']}, 'authorids': {'value': ['~Sebastian_Zeng1', '~Florian_Graf2', '~Martin_Uray1', '~Stefan_Huber2', '~Roland_Kwitt1']}, 'keywords': {'value': ['Persistent Homology', 'Multi-Time Attention', 'Latent ODE', 'Collective Behavior', 'Physical Sciences']}, 'TLDR': {'value': 'We consider the problem of learning the dynamics in the topology of time-evolving point clouds, observed in systems that exhibit collective behavior such as swarms of birds, insects, or fish.'}, 'abstract': {'value': 'We consider the problem of learning the dynamics in the topology of time-evolving point clouds, the prevalent spatiotemporal model for systems exhibiting collective behavior, such as swarms of insects and birds or particles in physics. In such systems, patterns emerge from (local) interactions among self-propelled entities. While several well-understood governing equations for motion and interaction exist, they are notoriously difficult to fit to data, as most prior work requires knowledge about individual motion trajectories, i.e., a requirement that is challenging to satisfy with an increasing number of entities. To evade such confounding factors, we investigate collective behavior from a _topological perspective_, but instead of summarizing entire observation sequences (as done previously), we propose learning a latent dynamical model from topological features _per time point_. The latter is then used to formulate a downstream regression task to predict the parametrization of some a priori specified governing equation. We implement this idea based on a latent ODE learned from vectorized (static) persistence diagrams and show that a combination of recent stability results for persistent homology justifies this modeling choice. Various (ablation) experiments not only demonstrate the relevance of each model component but provide compelling empirical evidence that our proposed model -- _Neural Persistence Dynamics_ -- substantially outperforms the state-of-the-art across a diverse set of parameter regression tasks.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/725852d7da210f82d39124a2e28439575e405bd7.pdf'}, 'supplementary_material': {'value': '/attachment/f828bf4be1cf823089b8da5879060b076f6f7bd6.zip'}, '_bibtex': {'value': '@inproceedings{\\nzeng2024neural,\\ntitle={Neural Persistence Dynamics},\\nauthor={Sebastian Zeng and Florian Graf and Martin Uray and Stefan Huber and Roland Kwitt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rCnZrFikX6}\\n}'}, 'paperhash': {'value': 'zeng|neural_persistence_dynamics'}},forum = 'rCnZrFikX6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19386/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19386/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19386/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19386/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'rCXTkIhkbF',number = 3974,cdate = 1715346075377,pdate = 1727287736282,odate = 1730873871031,mdate = 1730873871057,tcdate = 1715346075377,tmdate = 1730873871057,ddate = None,content = {'title': {'value': 'Improving Deep Learning Optimization through Constrained Parameter Regularization'}, 'authors': {'value': ['Jörg K.H. Franke', 'Michael Hefenbrock', 'Gregor Koehler', 'Frank Hutter']}, 'authorids': {'value': ['~Jörg_K.H._Franke1', '~Michael_Hefenbrock1', '~Gregor_Koehler1', '~Frank_Hutter1']}, 'keywords': {'value': ['Deep Learning', 'Regularization', 'Augmented Lagrangian']}, 'abstract': {'value': \"Regularization is a critical component in deep learning. The most commonly used approach, weight decay, applies a constant penalty coefficient uniformly across all parameters. This may be overly restrictive for some parameters, while insufficient for others. To address this, we present Constrained Parameter Regularization (CPR) as an alternative to traditional weight decay. Unlike the uniform application of a single penalty, CPR enforces an upper bound on a statistical measure, such as the L$_2$-norm, of individual parameter matrices. Consequently, learning becomes a constraint optimization problem, which we tackle using an adaptation of the augmented Lagrangian method. CPR introduces only a minor runtime overhead and only requires setting an upper bound. We propose simple yet efficient mechanisms for initializing this bound, making CPR rely on no hyperparameter or one, akin to weight decay. Our empirical studies on computer vision and language modeling tasks demonstrate CPR's effectiveness. The results show that CPR can outperform traditional weight decay and increase performance in pre-training and fine-tuning.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/550c636f174d72a8593d652148fdb97d6944ae3e.pdf'}, 'supplementary_material': {'value': '/attachment/75db04187cacdcbf868816c6448356c494636797.zip'}, '_bibtex': {'value': '@inproceedings{\\nfranke2024improving,\\ntitle={Improving Deep Learning Optimization through Constrained Parameter Regularization},\\nauthor={J{\\\\\"o}rg K.H. Franke and Michael Hefenbrock and Gregor Koehler and Frank Hutter},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=rCXTkIhkbF}\\n}'}, 'paperhash': {'value': 'franke|improving_deep_learning_optimization_through_constrained_parameter_regularization'}},forum = 'rCXTkIhkbF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3974/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3974/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3974/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3974/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r8YntmAd0g',number = 8468,cdate = 1715661385611,pdate = 1727287880495,odate = 1730873912531,mdate = 1730873912549,tcdate = 1715661385611,tmdate = 1730873912549,ddate = None,content = {'title': {'value': 'DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction'}, 'authors': {'value': ['Xinwei Zhang', 'Zhiqi Bu', 'Mingyi Hong', 'Meisam Razaviyayn']}, 'authorids': {'value': ['~Xinwei_Zhang1', '~Zhiqi_Bu1', '~Mingyi_Hong1', '~Meisam_Razaviyayn1']}, 'keywords': {'value': ['differential privacy', 'optimization', 'low-pass filter', 'signal processing']}, 'abstract': {'value': \"Privacy is a growing concern in modern deep-learning systems and applications. Differentially private (DP) training prevents the leakage of sensitive information in the collected training data from the trained machine learning models. DP optimizers, including DP stochastic gradient descent (DPSGD) and its variants, privatize the training procedure by gradient clipping and *DP noise* injection. However, in practice, DP models trained using DPSGD and its variants often suffer from significant model performance degradation. Such degradation prevents the application of DP optimization in many key tasks, such as foundation model pretraining. In this paper, we provide a novel *signal processing perspective* to the design and analysis of DP optimizers. We show that a ''frequency domain'' operation called *low-pass filtering* can be used to effectively reduce the impact of DP noise.  More specifically, by defining the ''frequency domain'' for both the gradient and differential privacy (DP) noise, we have developed a new component, called DOPPLER. This component is designed for DP algorithms and works by effectively amplifying the gradient while suppressing DP noise within this frequency domain. As a result, it maintains privacy guarantees and enhances the quality of the DP-protected model. Our experiments show that the proposed DP optimizers with a low-pass filter outperform their counterparts without the filter on various models and datasets. Both theoretical and practical evidence suggest that the  DOPPLER is effective in closing the gap between DP and non-DP training.\"}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f55eefb1a27c15148c2b579611582a133f6f2a1f.pdf'}, 'supplementary_material': {'value': '/attachment/5f180602df6994b271c40afca495805423da0bad.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024doppler,\\ntitle={{DOPPLER}: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction},\\nauthor={Xinwei Zhang and Zhiqi Bu and Mingyi Hong and Meisam Razaviyayn},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r8YntmAd0g}\\n}'}, 'paperhash': {'value': 'zhang|doppler_differentially_private_optimizers_with_lowpass_filter_for_privacy_noise_reduction'}},forum = 'r8YntmAd0g',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8468/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8468/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8468/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8468/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r8M9SfYMDi',number = 7911,cdate = 1715646068541,pdate = 1727287862822,odate = 1730873907535,mdate = 1730873907547,tcdate = 1715646068541,tmdate = 1730873907547,ddate = None,content = {'title': {'value': 'Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum'}, 'authors': {'value': ['Hadi Pouransari', 'Chun-Liang Li', 'Jen-Hao Rick Chang', 'Pavan Kumar Anasosalu Vasu', 'Cem Koc', 'Vaishaal Shankar', 'Oncel Tuzel']}, 'authorids': {'value': ['~Hadi_Pouransari1', '~Chun-Liang_Li1', '~Jen-Hao_Rick_Chang1', '~Pavan_Kumar_Anasosalu_Vasu1', '~Cem_Koc1', '~Vaishaal_Shankar1', '~Oncel_Tuzel2']}, 'keywords': {'value': ['Large Language Models', 'Efficient Training', 'Learning Efficiency', 'Long Context']}, 'TLDR': {'value': 'Introducing dataset decomposition to optimize training of large language models using mixture of sequence lengths, significantly improving learning efficiency (up to 3 times faster to reach a certain accuracy).'}, 'abstract': {'value': 'Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/da7a0f3f0f653a5adc873d2bc203533e37998c1f.pdf'}, '_bibtex': {'value': '@inproceedings{\\npouransari2024dataset,\\ntitle={Dataset Decomposition: Faster {LLM} Training with Variable Sequence Length Curriculum},\\nauthor={Hadi Pouransari and Chun-Liang Li and Jen-Hao Rick Chang and Pavan Kumar Anasosalu Vasu and Cem Koc and Vaishaal Shankar and Oncel Tuzel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r8M9SfYMDi}\\n}'}, 'paperhash': {'value': 'pouransari|dataset_decomposition_faster_llm_training_with_variable_sequence_length_curriculum'}},forum = 'r8M9SfYMDi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7911/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7911/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7911/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7911/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r70jUOpDCM',number = 3968,cdate = 1715345447331,pdate = 1727287736000,odate = 1730873870914,mdate = 1734572674654,tcdate = 1715345447331,tmdate = 1734572674654,ddate = None,content = {'title': {'value': 'Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model'}, 'authors': {'value': ['Yuheng Shi', 'Minjing Dong', 'Chang Xu']}, 'authorids': {'value': ['~Yuheng_Shi1', '~Minjing_Dong1', '~Chang_Xu4']}, 'keywords': {'value': ['Image Classification', 'Generic vision model', 'State Space Model']}, 'abstract': {'value': 'Despite the significant achievements of Vision Transformers (ViTs) in various vision tasks, they are constrained by the quadratic complexity. Recently, State Space Models (SSMs) have garnered widespread attention due to their global receptive field and linear complexity with respect to the input length, demonstrating substantial potential across fields including natural language processing and computer vision. To improve the performance of SSMs in vision tasks, a multi-scan strategy is widely adopted, which leads to significant redundancy of SSMs. For a better trade-off between efficiency and performance, we analyze the underlying reasons behind the success of the multi-scan strategy, where long-range dependency plays an important role. Based on the analysis, we introduce Multi-Scale Vision Mamba (MSVMamba) to preserve the superiority of SSMs in vision tasks with limited parameters. It employs a multi-scale 2D scanning technique on both original and downsampled feature maps, which not only benefits long-range dependency learning but also reduces computational costs. Additionally, we integrate a Convolutional Feed-Forward Network (ConvFFN) to address the lack of channel mixing. Our experiments demonstrate that MSVMamba is highly competitive, with the MSVMamba-Tiny model achieving 83.0% top-1 accuracy on ImageNet, 46.9% box mAP, and 42.5% instance mAP with the Mask R-CNN framework, 1x training schedule on COCO, and 47.9% mIoU with single-scale testing on ADE20K.  Code is available at https://github.com/YuHengsss/MSVMamba.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/38df95e2f7b9e4e65c57f789574f602bf2f62a6a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshi2024multiscale,\\ntitle={Multi-Scale {VM}amba: Hierarchy in Hierarchy Visual State Space Model},\\nauthor={Yuheng Shi and Minjing Dong and Chang Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r70jUOpDCM}\\n}'}, 'paperhash': {'value': 'shi|multiscale_vmamba_hierarchy_in_hierarchy_visual_state_space_model'}},forum = 'r70jUOpDCM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3968/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3968/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3968/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3968/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r6tnDXIkNS',number = 2134,cdate = 1714982480010,pdate = 1727287681872,odate = 1730873855578,mdate = 1730873855602,tcdate = 1714982480010,tmdate = 1730873855602,ddate = None,content = {'title': {'value': 'Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set'}, 'authors': {'value': ['Wenyuan Zhang', 'Yu-Shen Liu', 'Zhizhong Han']}, 'authorids': {'value': ['~Wenyuan_Zhang1', '~Yu-Shen_Liu1', '~Zhizhong_Han2']}, 'keywords': {'value': ['Gaussian Splatting', 'Signed distance function', '3D reconstruction']}, 'abstract': {'value': 'It is vital to infer a signed distance function (SDF) for multi-view based surface reconstruction. 3D Gaussian splatting (3DGS) provides a novel perspective for volume rendering, and shows advantages in rendering efficiency and quality. Although 3DGS provides a promising neural rendering option, it is still hard to infer SDFs for surface reconstruction with 3DGS due to the discreteness, the sparseness, and the off-surface drift of 3D Gaussians. To resolve these issues, we propose a method that seamlessly merge 3DGS with the learning of neural SDFs. Our key idea is to more effectively constrain the SDF inference with the multi-view consistency. To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF, and then render the aligned 3D Gaussians through the differentiable rasterization. Meanwhile, we update the neural SDF by pulling neighboring space to the pulled 3D Gaussians, which progressively refine the signed distance field near the surface. With both differentiable pulling and splatting, we jointly optimize 3D Gaussians and the neural SDF with both RGB and geometry constraints, which recovers more accurate, smooth, and complete surfaces with more geometry details. Our numerical and visual comparisons show our superiority over the state-of-the-art results on the widely used benchmarks.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/840219aff71fd45610db672eca004b4ce1d0ff91.pdf'}, 'supplementary_material': {'value': '/attachment/e297a77afb12be75ef8059819041eb65df612b44.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024neural,\\ntitle={Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set},\\nauthor={Wenyuan Zhang and Yu-Shen Liu and Zhizhong Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r6tnDXIkNS}\\n}'}, 'paperhash': {'value': 'zhang|neural_signed_distance_function_inference_through_splatting_3d_gaussians_pulled_on_zerolevel_set'}},forum = 'r6tnDXIkNS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2134/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2134/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2134/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2134/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r6V7EjANUK',number = 3955,cdate = 1715344253189,pdate = 1727287735613,odate = 1730873870758,mdate = 1730873870776,tcdate = 1715344253189,tmdate = 1730873870776,ddate = None,content = {'title': {'value': 'GSDF: 3DGS Meets SDF for Improved Neural Rendering and Reconstruction'}, 'authors': {'value': ['Mulin Yu', 'Tao Lu', 'Linning Xu', 'Lihan Jiang', 'Yuanbo Xiangli', 'Bo Dai']}, 'authorids': {'value': ['~Mulin_Yu2', '~Tao_Lu4', '~Linning_Xu2', '~Lihan_Jiang2', '~Yuanbo_Xiangli1', '~Bo_Dai2']}, 'keywords': {'value': ['Neural Rendering; 3D Reconstruction;3D Gaussian Splatting; Signed Distance Field']}, 'TLDR': {'value': 'We propose GSDF, a dual-branch system that enhances rendering and reconstruction at the same time, leveraging the mutual geometry regularization and guidance between Gaussain primitives and neural surface.'}, 'abstract': {'value': 'Representing 3D scenes from multiview images remains a core challenge in computer vision and graphics, requiring both reliable rendering and reconstruction, which often conflicts due to the mismatched prioritization of image quality over precise underlying scene geometry. Although both neural implicit surfaces and explicit Gaussian primitives have advanced with neural rendering techniques, current methods impose strict constraints on density fields or primitive shapes, which enhances the affinity for geometric reconstruction at the sacrifice of rendering quality. To address this dilemma, we introduce GSDF, a dual-branch architecture combining 3D Gaussian Splatting (3DGS) and neural Signed Distance Fields (SDF). Our approach leverages mutual guidance and joint supervision during the training process to mutually enhance reconstruction and rendering. Specifically, our method guides the Gaussian primitives to locate near potential surfaces and accelerates the SDF convergence. This implicit mutual guidance ensures robustness and accuracy in both synthetic and real-world scenarios. Experimental results demonstrate that our method boosts the SDF optimization process to reconstruct more detailed geometry, while reducing floaters and blurry edge artifacts in rendering by aligning Gaussian primitives with the underlying geometry.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6249ed4cb62d4456ad444614a177383b1b92faf9.pdf'}, 'supplementary_material': {'value': '/attachment/81b40dc443e1ab9b1c59a8c0db71c1f4e1f7a79c.zip'}, '_bibtex': {'value': '@inproceedings{\\nyu2024gsdf,\\ntitle={{GSDF}: 3{DGS} Meets {SDF} for Improved Neural Rendering and Reconstruction},\\nauthor={Mulin Yu and Tao Lu and Linning Xu and Lihan Jiang and Yuanbo Xiangli and Bo Dai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r6V7EjANUK}\\n}'}, 'paperhash': {'value': 'yu|gsdf_3dgs_meets_sdf_for_improved_neural_rendering_and_reconstruction'}},forum = 'r6V7EjANUK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3955/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3955/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3955/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3955/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'r5spnrY6H3',number = 9950,cdate = 1715688379815,pdate = 1727287925616,odate = 1730873924925,mdate = 1734864408168,tcdate = 1715688379815,tmdate = 1734864408168,ddate = None,content = {'title': {'value': 'RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation'}, 'authors': {'value': ['Changli Wu', 'Qi Chen', 'Jiayi Ji', 'Haowei Wang', 'Yiwei Ma', 'You Huang', 'Gen Luo', 'Hao Fei', 'Xiaoshuai Sun', 'Rongrong Ji']}, 'authorids': {'value': ['~Changli_Wu1', '~Qi_Chen17', '~Jiayi_Ji1', '~Haowei_Wang1', '~Yiwei_Ma1', '~You_Huang1', '~Gen_Luo1', '~Hao_Fei1', '~Xiaoshuai_Sun3', '~Rongrong_Ji5']}, 'keywords': {'value': ['3D Referring Expression Segmentation', 'Spatial Awareness Modeling', 'Rule-guided Supervision']}, 'abstract': {'value': '3D Referring Expression Segmentation (3D-RES) aims to segment 3D objects by correlating referring expressions with point clouds. However, traditional approaches frequently encounter issues like over-segmentation or mis-segmentation, due to insufficient emphasis on spatial information of instances. In this paper, we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizing solely the spatial information of the target instance for supervision. This approach enables the network to accurately depict the spatial relationships among all entities described in the text, thus enhancing the reasoning capabilities. The RG-SAN consists of the Text-driven Localization Module (TLM) and the Rule-guided Weak Supervision (RWS) strategy. The TLM initially locates all mentioned instances and iteratively refines their positional information. The RWS strategy, acknowledging that only target objects have supervised positional information, employs dependency tree rules to precisely guide the core instance’s positioning. Extensive testing on the ScanRefer benchmark has shown that RG-SAN not only establishes new performance benchmarks, with an mIoU increase of 5.1 points, but also exhibits significant improvements in robustness when processing descriptions with spatial ambiguity. All codes are available at https://github.com/sosppxo/RG-SAN.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/074c8caaa0b5feabaad18b25db6c0ee86ed09863.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024rgsan,\\ntitle={{RG}-{SAN}: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation},\\nauthor={Changli Wu and Qi Chen and Jiayi Ji and Haowei Wang and Yiwei Ma and You Huang and Gen Luo and Hao Fei and Xiaoshuai Sun and Rongrong Ji},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r5spnrY6H3}\\n}'}, 'paperhash': {'value': 'wu|rgsan_ruleguided_spatial_awareness_network_for_endtoend_3d_referring_expression_segmentation'}},forum = 'r5spnrY6H3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9950/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9950/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9950/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9950/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r5nev2SHtJ',number = 11686,cdate = 1715708732018,pdate = 1727287979439,odate = 1730873941110,mdate = 1736801511327,tcdate = 1715708732018,tmdate = 1736801511327,ddate = None,content = {'title': {'value': 'From Causal to Concept-Based Representation Learning'}, 'authors': {'value': ['Goutham Rajendran', 'Simon Buchholz', 'Bryon Aragam', 'Bernhard Schölkopf', 'Pradeep Kumar Ravikumar']}, 'authorids': {'value': ['~Goutham_Rajendran1', '~Simon_Buchholz1', '~Bryon_Aragam1', '~Bernhard_Schölkopf1', '~Pradeep_Kumar_Ravikumar1']}, 'keywords': {'value': ['concept learning', 'causal representation learning', 'interpretable representation learning']}, 'TLDR': {'value': 'We formally study how to extract concepts from data, by utilizing ideas from the causal representation learning and interpretability literatures.'}, 'abstract': {'value': 'To build intelligent machine learning systems, modern representation learning attempts to recover latent generative factors from data, such as in causal representation learning. A key question in this growing field is to provide rigorous conditions under which latent factors can be identified and thus, potentially learned. Motivated by extensive empirical literature on linear representations and concept learning, we propose to relax causal notions with a geometric notion of concepts. We formally define a notion of concepts and show rigorously that they can be provably recovered from diverse data. Instead of imposing assumptions on the \"true\" generative latent space, we assume that concepts can be represented linearly in this latent space. The tradeoff is that instead of identifying the \"true\" generative factors, we identify a subset of desired human-interpretable concepts that are relevant for a given application. Experiments on synthetic data, multimodal CLIP models and large language models supplement our results and show the utility of our approach. In this way, we provide a foundation for moving from causal representations to interpretable, concept-based representations by bringing together ideas from these two neighboring disciplines.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/abb49fa44fa001a4ca6a32e9b2c5040ac4ed23be.pdf'}, 'supplementary_material': {'value': '/attachment/29a6685daae442bbd86861b65b772ff3ce925ea3.zip'}, '_bibtex': {'value': '@inproceedings{\\nrajendran2024from,\\ntitle={From Causal to Concept-Based Representation Learning},\\nauthor={Goutham Rajendran and Simon Buchholz and Bryon Aragam and Bernhard Sch{\\\\\"o}lkopf and Pradeep Kumar Ravikumar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r5nev2SHtJ}\\n}'}, 'paperhash': {'value': 'rajendran|from_causal_to_conceptbased_representation_learning'}},forum = 'r5nev2SHtJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11686/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11686/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11686/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11686/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r3c0WGCXgt',number = 922,cdate = 1714214160598,pdate = 1727287647668,odate = 1730873844093,mdate = 1730873844111,tcdate = 1714214160598,tmdate = 1730873844111,ddate = None,content = {'title': {'value': 'How Control Information Influences Multilingual Text Image Generation and Editing?'}, 'authors': {'value': ['Boqiang Zhang', 'Zuan Gao', 'Yadong Qu', 'Hongtao Xie']}, 'authorids': {'value': ['~Boqiang_Zhang2', '~Zuan_Gao1', '~Yadong_Qu1', '~Hongtao_Xie2']}, 'keywords': {'value': ['Scene Text Generation', 'Scene Text Editing', 'Diffusion']}, 'abstract': {'value': 'Visual text generation has significantly advanced through diffusion models aimed at producing images with readable and realistic text. Recent works primarily use a ControlNet-based framework, employing standard font text images to control diffusion models. Recognizing the critical role of control information in generating high-quality text, we investigate its influence from three perspectives: input encoding, role at different stages, and output features. Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps. 2) Control information plays distinct roles at different stages of the denoising process. 3) Output control features significantly differ from the base and skip features of the U-Net decoder in the frequency domain. Based on these insights, we propose TextGen, a novel framework designed to enhance generation quality by optimizing control information. We improve input and output features using Fourier analysis to emphasize relevant information and reduce noise. Additionally, we employ a two-stage generation framework to align the different roles of control information at different stages. Furthermore, we introduce an effective and lightweight dataset for training. Our method achieves state-of-the-art performance in both Chinese and English text generation. The code and dataset are available at https://github.com/CyrilSterling/TextGen.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7a553d8fd3667ac8a79ed09b379e81b341dcb53d.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzhang2024how,\\ntitle={How Control Information Influences Multilingual Text Image Generation and Editing?},\\nauthor={Boqiang Zhang and Zuan Gao and Yadong Qu and Hongtao Xie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r3c0WGCXgt}\\n}'}, 'paperhash': {'value': 'zhang|how_control_information_influences_multilingual_text_image_generation_and_editing'}},forum = 'r3c0WGCXgt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission922/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission922/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission922/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission922/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'r0eSCJ6qsL',number = 11588,cdate = 1715707016702,pdate = 1727287975767,odate = 1730873940094,mdate = 1730873940114,tcdate = 1715707016702,tmdate = 1730873940114,ddate = None,content = {'title': {'value': 'AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation'}, 'authors': {'value': ['Anil Kag', 'Huseyin Coskun', 'Jierun Chen', 'Junli Cao', 'Willi Menapace', 'Aliaksandr Siarohin', 'Sergey Tulyakov', 'Jian Ren']}, 'authorids': {'value': ['~Anil_Kag1', '~Huseyin_Coskun1', '~Jierun_Chen1', '~Junli_Cao2', '~Willi_Menapace1', '~Aliaksandr_Siarohin1', '~Sergey_Tulyakov1', '~Jian_Ren2']}, 'keywords': {'value': ['Text-to-Image Generation', 'Hybrid Architectures']}, 'abstract': {'value': 'Neural network architecture design requires making many crucial decisions. The common desiderata is that similar decisions, with little modifications, can be reused in a variety of tasks and applications. To satisfy that, architectures must provide promising latency and performance trade-offs, support a variety of tasks, scale efficiently with respect to the amounts of data and compute, leverage available data from other tasks, and efficiently support various hardware. To this end, we introduce AsCAN---a hybrid architecture, combining both convolutional and transformer blocks. We revisit the key design principles of hybrid architectures and propose a simple and effective \\\\emph{asymmetric} architecture, where the distribution of convolutional and transformer blocks is \\\\emph{asymmetric}, containing more convolutional blocks in the earlier stages, followed by more transformer blocks in later stages. AsCAN supports a variety of tasks: recognition, segmentation, class-conditional image generation, and features a superior trade-off between performance and latency. We then scale the same architecture to solve a large-scale text-to-image task and show state-of-the-art performance compared to the most recent public and commercial models. Notably, without performing any optimization of inference time our model shows faster execution, even when compared to works that do such optimization, highlighting the advantages and the value of our approach.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose a hybrid architecture with asymmetric distribution of convolution and attention blocks in different network stages to achieve superior latency-vs-performance trade-off in image recognition and generation tasks.'}, 'pdf': {'value': '/pdf/4136dd4ddffaf95b4c4b7e86a402302343ad550b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkag2024ascan,\\ntitle={As{CAN}: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation},\\nauthor={Anil Kag and Huseyin Coskun and Jierun Chen and Junli Cao and Willi Menapace and Aliaksandr Siarohin and Sergey Tulyakov and Jian Ren},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=r0eSCJ6qsL}\\n}'}, 'paperhash': {'value': 'kag|ascan_asymmetric_convolutionattention_networks_for_efficient_recognition_and_generation'}},forum = 'r0eSCJ6qsL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11588/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11588/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11588/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11588/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qzwAG8qxI1',number = 2269,cdate = 1715011602921,pdate = 1727287685547,odate = 1730873857008,mdate = 1730873857026,tcdate = 1715011602921,tmdate = 1730873857026,ddate = None,content = {'title': {'value': 'Bridging OOD Detection and Generalization: A Graph-Theoretic View'}, 'authors': {'value': ['Han Wang', 'Yixuan Li']}, 'authorids': {'value': ['~Han_Wang19', '~Yixuan_Li1']}, 'keywords': {'value': ['graph spectral', 'distribution shift']}, 'abstract': {'value': \"In the context of modern machine learning, models deployed in real-world scenarios often encounter diverse data shifts like covariate and semantic shifts, leading to challenges in both out-of-distribution (OOD) generalization and detection. Despite considerable attention to these issues separately, a unified framework for theoretical understanding and practical usage is lacking. To bridge the gap, we introduce a graph-theoretic framework to jointly tackle both OOD generalization and detection problems. By leveraging the graph formulation, data representations are obtained through the factorization of the graph's adjacency matrix, enabling us to derive provable error quantifying OOD generalization and detection performance. Empirical results showcase competitive performance in comparison to existing methods, thereby validating our theoretical underpinnings.\"}, 'pdf': {'value': '/pdf/c9c294f6dd6249359b43866f023196ca44bf1f4b.pdf'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nwang2024bridging,\\ntitle={Bridging {OOD} Detection and Generalization: A Graph-Theoretic View},\\nauthor={Han Wang and Yixuan Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qzwAG8qxI1}\\n}'}, 'paperhash': {'value': 'wang|bridging_ood_detection_and_generalization_a_graphtheoretic_view'}},forum = 'qzwAG8qxI1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2269/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2269/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2269/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2269/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qyaz3XP0FN',number = 12034,cdate = 1715715056951,pdate = 1727287992342,odate = 1730873944742,mdate = 1730873944757,tcdate = 1715715056951,tmdate = 1730873944757,ddate = None,content = {'title': {'value': 'Parametric model reduction of mean-field and stochastic systems via higher-order action matching'}, 'authors': {'value': ['Jules Berman', 'Tobias Blickhan', 'Benjamin Peherstorfer']}, 'authorids': {'value': ['~Jules_Berman1', '~Tobias_Blickhan1', '~Benjamin_Peherstorfer2']}, 'keywords': {'value': ['partial differential equations', 'reduced modeling', 'model reduction', 'stochastic dynamical systems', 'generative models']}, 'TLDR': {'value': 'Learning surrogate models to efficiently predict behavior of systems with stochastic and mean-field dynamics across varying physics parameters.'}, 'abstract': {'value': 'The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters. The learned models can act as surrogates of classical numerical models to efficiently predict the system behavior over the physics parameters. Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameter- and time-dependent gradient fields that represent approximations of the population dynamics. The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters. We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process. We demonstrate on Vlasov-Poisson instabilities as well as on high-dimensional particle and chaotic systems that our approach accurately predicts population dynamics over a wide range of parameters and outperforms state-of-the-art diffusion-based and flow-based modeling that simply condition on time and physics parameters.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4f95db67d2e94e6fdc17fe1ba3eacae2b3028669.pdf'}, '_bibtex': {'value': '@inproceedings{\\nberman2024parametric,\\ntitle={Parametric model reduction of mean-field and stochastic systems via higher-order action matching},\\nauthor={Jules Berman and Tobias Blickhan and Benjamin Peherstorfer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qyaz3XP0FN}\\n}'}, 'paperhash': {'value': 'berman|parametric_model_reduction_of_meanfield_and_stochastic_systems_via_higherorder_action_matching'}},forum = 'qyaz3XP0FN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12034/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12034/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12034/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12034/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qxS4IvtLdD',number = 15293,cdate = 1715758847509,pdate = 1727288093714,odate = 1730873971837,mdate = 1735148970302,tcdate = 1715758847509,tmdate = 1735148970302,ddate = None,content = {'title': {'value': 'Fast samplers for Inverse Problems in Iterative Refinement models'}, 'authors': {'value': ['Kushagra Pandey', 'Ruihan Yang', 'Stephan Mandt']}, 'authorids': {'value': ['~Kushagra_Pandey1', '~Ruihan_Yang1', '~Stephan_Mandt1']}, 'keywords': {'value': ['Inverse Problems', 'Diffusion models', 'Fast sampling']}, 'TLDR': {'value': 'We present fast samplers for solving inverse problems in diffusion and flow models'}, 'abstract': {'value': \"Constructing fast samplers for unconditional diffusion and flow-matching models has received much attention recently; however, existing methods for solving *inverse problems*, such as super-resolution, inpainting, or deblurring, still require hundreds to thousands of iterative steps to obtain high-quality results. We propose a plug-and-play framework for constructing efficient samplers for inverse problems, requiring only *pre-trained* diffusion or flow-matching models. We present *Conditional Conjugate Integrators*, which leverage the specific form of the inverse problem to project the respective conditional diffusion/flow dynamics into a more amenable space for sampling. Our method complements popular posterior approximation methods for solving inverse problems using diffusion/flow models. We evaluate the proposed method's performance on various linear image restoration tasks across multiple datasets, employing diffusion and flow-matching models. Notably, on challenging inverse problems like 4x super-resolution on the ImageNet dataset, our method can generate high-quality samples in as few as *5* conditional sampling steps and outperforms competing baselines requiring 20-1000 steps. Our code will be publicly available at https://github.com/mandt-lab/c-pigdm.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8db789ae698f114981956818acb6bf46e7cd37c7.pdf'}, '_bibtex': {'value': '@inproceedings{\\npandey2024fast,\\ntitle={Fast samplers for Inverse Problems in Iterative Refinement models},\\nauthor={Kushagra Pandey and Ruihan Yang and Stephan Mandt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qxS4IvtLdD}\\n}'}, 'paperhash': {'value': 'pandey|fast_samplers_for_inverse_problems_in_iterative_refinement_models'}},forum = 'qxS4IvtLdD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15293/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15293/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15293/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15293/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qwl3EiDi9r',number = 15155,cdate = 1715757680611,pdate = 1727288089699,odate = 1730873970920,mdate = 1736822117375,tcdate = 1715757680611,tmdate = 1736822117375,ddate = None,content = {'title': {'value': 'Integrating GNN and Neural ODEs for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion'}, 'authors': {'value': ['Masahito Uwamichi', 'Simon K. Schnyder', 'Tetsuya J. Kobayashi', 'Satoshi Sawai']}, 'authorids': {'value': ['~Masahito_Uwamichi1', '~Simon_K._Schnyder1', '~Tetsuya_J._Kobayashi1', '~Satoshi_Sawai1']}, 'keywords': {'value': ['Deep Learning', 'Neural Differential Equations', 'Graph Neural Networks', 'System Identification', 'Active Matter', 'Collective Motion', 'Non-Reciprocal']}, 'abstract': {'value': 'Analyzing the motion of multiple biological agents, be it cells or individual animals, is pivotal for the understanding of complex collective behaviors. \\nWith the advent of advanced microscopy, detailed images of complex tissue formations involving multiple cell types have become more accessible in recent years. However, deciphering the underlying rules that govern cell movements is far from trivial. Here, we present a novel deep learning framework for estimating the underlying equations of motion from observed trajectories, a pivotal step in decoding such complex dynamics. \\nOur framework integrates graph neural networks with neural differential equations, enabling effective prediction of two-body interactions based on the states of the interacting entities. We demonstrate the efficacy of our approach through two numerical experiments. First, we used simulated data from a toy model to tune the hyperparameters. Based on the obtained hyperparameters, we then applied this approach to a more complex model with non-reciprocal forces that mimic the collective dynamics of the cells of slime molds. Our results show that the proposed method can accurately estimate the functional forms of two-body interactions -- even when they are nonreciprocal -- thereby precisely replicating both individual and collective behaviors within these systems.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c76fbeb22b921e1ce786a87c928ee88301ded938.pdf'}, 'supplementary_material': {'value': '/attachment/afbc048d5b7c6331922874fd73b31f7470892a32.zip'}, 'TLDR': {'value': 'We present a framework to estimate non-reciprocal two-body interactions from trajectories in mixed-species collective motion. Our method accurately replicates interactions and collective behaviors, demonstrated through numerical experiments.'}, '_bibtex': {'value': '@inproceedings{\\nuwamichi2024integrating,\\ntitle={Integrating {GNN} and Neural {ODE}s for Estimating Non-Reciprocal Two-Body Interactions in Mixed-Species Collective Motion},\\nauthor={Masahito Uwamichi and Simon K. Schnyder and Tetsuya J. Kobayashi and Satoshi Sawai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qwl3EiDi9r}\\n}'}, 'paperhash': {'value': 'uwamichi|integrating_gnn_and_neural_odes_for_estimating_nonreciprocal_twobody_interactions_in_mixedspecies_collective_motion'}},forum = 'qwl3EiDi9r',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15155/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15155/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15155/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15155/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'qwgfh2fTtN',number = 9040,cdate = 1715673563140,pdate = 1727287899132,odate = 1730873917032,mdate = 1730873917066,tcdate = 1715673563140,tmdate = 1730873917066,ddate = None,content = {'title': {'value': 'Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision'}, 'authors': {'value': ['Zhiqing Sun', 'Longhui Yu', 'Yikang Shen', 'Weiyang Liu', 'Yiming Yang', 'Sean Welleck', 'Chuang Gan']}, 'authorids': {'value': ['~Zhiqing_Sun1', '~Longhui_Yu1', '~Yikang_Shen1', '~Weiyang_Liu1', '~Yiming_Yang1', '~Sean_Welleck1', '~Chuang_Gan1']}, 'keywords': {'value': ['easy-to-hard generalization', 'scalable oversight', 'AI alignment']}, 'abstract': {'value': 'Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as easy-to-hard generalization. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the (process-supervised) reward models on easy problems (e.g., level 1-3), and then uses them to evaluate the performance of policy models on hard problems. We show that such easy-to-hard generalization from evaluators can enable easy-to-hard generalizations in generators either through re-ranking or reinforcement learning (RL). Notably, our process-supervised 7b RL model and 34b model (reranking@1024) achieves an accuracy of 34.0% and 52.5% on MATH500, respectively, despite only using human supervision on easy problems. Our approach suggests a promising path toward AI systems that advance beyond the frontier of human supervision.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7ac27b372b7a8932bf5f686c56f5f50af5154e87.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024easytohard,\\ntitle={Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision},\\nauthor={Zhiqing Sun and Longhui Yu and Yikang Shen and Weiyang Liu and Yiming Yang and Sean Welleck and Chuang Gan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qwgfh2fTtN}\\n}'}, 'paperhash': {'value': 'sun|easytohard_generalization_scalable_alignment_beyond_human_supervision'}},forum = 'qwgfh2fTtN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9040/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9040/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9040/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9040/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qvdc0oCX2n',number = 12591,cdate = 1715724682911,pdate = 1727288011334,odate = 1730873950268,mdate = 1730873950290,tcdate = 1715724682911,tmdate = 1730873950290,ddate = None,content = {'title': {'value': 'CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning'}, 'authors': {'value': ['Yiping Wang', 'Yifang Chen', 'Wendan Yan', 'Alex Fang', 'Wenjing Zhou', 'Kevin Jamieson', 'Simon Shaolei Du']}, 'authorids': {'value': ['~Yiping_Wang2', '~Yifang_Chen1', '~Wendan_Yan1', '~Alex_Fang1', '~Wenjing_Zhou2', '~Kevin_Jamieson1', '~Simon_Shaolei_Du1']}, 'keywords': {'value': ['contrastive learning', 'visual-language pretraining', 'data selection', 'CLIP']}, 'TLDR': {'value': 'we design universal data selection methods for CLIP pretraining and achieve near SOTA results with less than 10% of preprocessing resources. Combining our methods with the current best one, we can achieve a new state-of-the-art.'}, 'abstract': {'value': \"Data selection has emerged as a core issue for large-scale visual-language model pretaining (e.g., CLIP), particularly with noisy web-curated datasets. Three main data selection approaches are: (1) leveraging external non-CLIP models to aid data selection, (2) training new CLIP-style embedding models that are more effective at selecting high-quality data than the original OpenAI CLIP model, and (3) designing better metrics or strategies universally applicable to any CLIP embedding without requiring specific model properties (e.g., CLIPScore is one popular metric).  While the first two approaches have been extensively studied, the third remains under-explored. In this paper, we advance the third approach by proposing two new methods. Firstly, instead of classical CLIP scores that only consider the alignment between two modalities from a single sample, we introduce $\\\\textbf{negCLIPLoss}$,  a method inspired by CLIP training loss that adds the alignment between one sample and its contrastive pairs as an extra normalization term to CLIPScore for better quality measurement. Secondly, when downstream tasks are known, we propose a new norm-based metric, $\\\\textbf{NormSim}$, to measure the similarity between pretraining data and target data. We test our methods on the data selection benchmark, DataComp [Gadre et al., 2023]. Compared to the best baseline using only OpenAI's CLIP-L/14, our methods achieve a 5.3\\\\% improvement on ImageNet-1k and a 2.8\\\\% improvement on 38 downstream evaluation tasks. Moreover, both $\\\\textbf{negCLIPLoss}$ and $\\\\textbf{NormSim}$ are compatible with existing techniques. By combining our methods with the current best methods DFN [Fang et al., 2023] and HYPE [Kim et al., 2024], we can boost average performance on downstream tasks by 0.9\\\\%, achieving a new state-of-the-art on the DataComp-medium benchmark.\"}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5235d1d349bb8a0f3f2aa81d39b90ef150f6e36e.pdf'}, 'supplementary_material': {'value': '/attachment/bed22c85dcb601b44ba769a2b988a8e4aca65387.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024cliploss,\\ntitle={{CLIPL}oss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning},\\nauthor={Yiping Wang and Yifang Chen and Wendan Yan and Alex Fang and Wenjing Zhou and Kevin Jamieson and Simon Shaolei Du},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qvdc0oCX2n}\\n}'}, 'paperhash': {'value': 'wang|cliploss_and_normbased_data_selection_methods_for_multimodal_contrastive_learning'}},forum = 'qvdc0oCX2n',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12591/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12591/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12591/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12591/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qu5NTwZtxA',number = 10940,cdate = 1715699280455,pdate = 1727287955830,odate = 1730873933547,mdate = 1735265111155,tcdate = 1715699280455,tmdate = 1735265111155,ddate = None,content = {'title': {'value': 'Towards Editing Time Series'}, 'authors': {'value': ['Baoyu Jing', 'Shuqi Gu', 'Tianyu Chen', 'Zhiyu Yang', 'Dongsheng Li', 'Jingrui He', 'Kan Ren']}, 'authorids': {'value': ['~Baoyu_Jing1', '~Shuqi_Gu1', '~Tianyu_Chen8', '~Zhiyu_Yang6', '~Dongsheng_Li2', '~Jingrui_He1', '~Kan_Ren1']}, 'keywords': {'value': ['Time Series', 'Editing', 'Diffusion Model']}, 'abstract': {'value': 'Synthesizing time series data is pivotal in modern society, aiding effective decision making and ensuring privacy preservation in various scenarios. Time series are associated with various attributes, including trends, seasonality, and external information such as location. Recent research has predominantly focused on random unconditional synthesis or conditional synthesis. Nonetheless, these paradigms generate time series from scratch and are incapable of manipulating existing time series samples. This paper introduces a novel task, called Time Series Editing (TSE), to synthesize time series by manipulating existing time series. The objective is to modify the given time series according to the specified attributes while preserving other properties unchanged. This task is not trivial due to the inadequacy of data coverage and the intricate relationships between time series and their attributes. To address these issues, we introduce a novel diffusion model, called TEdit. The proposed TEdit is trained using a novel bootstrap learning algorithm that effectively enhances the coverage of the original data. It is also equipped with an innovative multi-resolution modeling and generation paradigm to capture the complex relationships between time series and their attributes. Experimental results demonstrate the efficacy of TEdit for editing specified attributes upon the existing time series data. The project page is at https://seqml.github.io/tse.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/aa6814d8038dae41d99378ac4404411bfc04b7c0.pdf'}, '_bibtex': {'value': '@inproceedings{\\njing2024towards,\\ntitle={Towards Editing Time Series},\\nauthor={Baoyu Jing and Shuqi Gu and Tianyu Chen and Zhiyu Yang and Dongsheng Li and Jingrui He and Kan Ren},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qu5NTwZtxA}\\n}'}, 'paperhash': {'value': 'jing|towards_editing_time_series'}},forum = 'qu5NTwZtxA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10940/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10940/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10940/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10940/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qrlguvKu7a',number = 16420,cdate = 1715770215067,pdate = 1727288126285,odate = 1730873979190,mdate = 1730873979204,tcdate = 1715770215067,tmdate = 1730873979204,ddate = None,content = {'title': {'value': 'PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator'}, 'authors': {'value': ['Hanshu Yan', 'Xingchao Liu', 'Jiachun Pan', 'Jun Hao Liew', 'qiang liu', 'Jiashi Feng']}, 'authorids': {'value': ['~Hanshu_Yan1', '~Xingchao_Liu1', '~Jiachun_Pan1', '~Jun_Hao_Liew1', '~qiang_liu4', '~Jiashi_Feng1']}, 'keywords': {'value': ['Flow Model', 'Diffusion Model', 'Generative Model', 'Image Generation']}, 'abstract': {'value': 'We present Piecewise Rectified Flow (PeRFlow), a flow-based method for accelerating diffusion models. PeRFlow divides the sampling process of generative flows into several time windows and straightens the trajectories in each interval via the reflow operation, thereby approaching piecewise linear flows. PeRFlow achieves superior performance in a few-step generation. Moreover, through dedicated parameterizations, the PeRFlow models inherit knowledge from the pretrained diffusion models. Thus, the training converges fast and the obtained models show advantageous transfer ability, serving as universal plug-and-play accelerators that are compatible with various workflows based on the pre-trained diffusion models.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6d54224720154ecaea2664620b9ff3691864fde5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyan2024perflow,\\ntitle={Pe{RF}low: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator},\\nauthor={Hanshu Yan and Xingchao Liu and Jiachun Pan and Jun Hao Liew and qiang liu and Jiashi Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qrlguvKu7a}\\n}'}, 'paperhash': {'value': 'yan|perflow_piecewise_rectified_flow_as_universal_plugandplay_accelerator'}},forum = 'qrlguvKu7a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16420/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16420/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16420/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16420/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qrfp4eeZ47',number = 21624,cdate = 1715802774887,pdate = 1727288259035,odate = 1730874006442,mdate = 1730874006466,tcdate = 1715802774887,tmdate = 1730874006466,ddate = None,content = {'title': {'value': 'FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing'}, 'authors': {'value': ['Jitesh Joshi', 'Sos Agaian', 'Youngjun Cho']}, 'authorids': {'value': ['~Jitesh_Joshi1', '~Sos_Agaian1', '~Youngjun_Cho1']}, 'keywords': {'value': ['Time-series estimation', 'remote photo-plethysmography', 'spatial-temporal attention', 'non-negative matrix factorization']}, 'TLDR': {'value': 'This work introduces the Factorized Self-Attention Module, which computes multidimensional attention through nonnegative matrix factorization, and integrate it into FactorizePhys, a proposed 3D-CNN model for robust rPPG signal extraction.'}, 'abstract': {'value': \"Remote photoplethysmography (rPPG) enables non-invasive extraction of blood volume pulse signals through imaging, transforming spatial-temporal data into time series signals. Advances in end-to-end rPPG approaches have focused on this transformation where attention mechanisms are crucial for feature extraction. However, existing methods compute attention disjointly across spatial, temporal, and channel dimensions. Here, we propose the Factorized Self-Attention Module (FSAM), which jointly computes multidimensional attention from voxel embeddings using nonnegative matrix factorization. To demonstrate FSAM's effectiveness, we developed FactorizePhys, an end-to-end 3D-CNN architecture for estimating blood volume pulse signals from raw video frames. Our approach adeptly factorizes voxel embeddings to achieve comprehensive spatial, temporal, and channel attention, enhancing performance of generic signal extraction tasks. Furthermore, we deploy FSAM within an existing 2D-CNN-based rPPG architecture to illustrate its versatility. FSAM and FactorizePhys are thoroughly evaluated against state-of-the-art rPPG methods, each representing different types of architecture and attention mechanism. We perform ablation studies to investigate the architectural decisions and hyperparameters of FSAM. Experiments on four publicly available datasets and intuitive visualization of learned spatial-temporal features substantiate the effectiveness of FSAM and enhanced cross-dataset generalization in estimating rPPG signals, suggesting its broader potential as a multidimensional attention mechanism. The code is accessible at https://github.com/PhysiologicAILab/FactorizePhys.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9abd14b56c34451b8fc9ee44ba4395c988c4d34d.pdf'}, '_bibtex': {'value': '@inproceedings{\\njoshi2024factorizephys,\\ntitle={FactorizePhys: Matrix Factorization for Multidimensional Attention in Remote Physiological Sensing},\\nauthor={Jitesh Joshi and Sos Agaian and Youngjun Cho},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qrfp4eeZ47}\\n}'}, 'supplementary_material': {'value': '/attachment/08900221396b55d861a2f70c0e4bf77ef2a8b210.zip'}, 'paperhash': {'value': 'joshi|factorizephys_matrix_factorization_for_multidimensional_attention_in_remote_physiological_sensing'}},forum = 'qrfp4eeZ47',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21624/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21624/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21624/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21624/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qqQFOcUEqM',number = 5027,cdate = 1715497241348,pdate = 1727287770086,odate = 1730873881328,mdate = 1730873881346,tcdate = 1715497241348,tmdate = 1730873881346,ddate = None,content = {'title': {'value': 'Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models'}, 'authors': {'value': ['Mengyuan Chen', 'Junyu Gao', 'Changsheng Xu']}, 'authorids': {'value': ['~Mengyuan_Chen1', '~Junyu_Gao1', '~Changsheng_Xu1']}, 'keywords': {'value': ['out-of-distribution detection', 'zero-shot OOD detection', 'pre-trained vision-language models']}, 'abstract': {'value': 'A straightforward pipeline for zero-shot out-of-distribution (OOD) detection involves selecting potential OOD labels from an extensive semantic pool and then leveraging a pre-trained vision-language model to perform classification on both in-distribution (ID) and OOD labels. In this paper, we theorize that enhancing performance requires expanding the semantic pool, while increasing the expected probability of selected OOD labels being activated by OOD samples, and ensuring low mutual dependence among the activations of these OOD labels. A natural expansion manner is to adopt a larger lexicon; however, the inevitable introduction of numerous synonyms and uncommon words fails to meet the above requirements, indicating that viable expansion manners move beyond merely selecting words from a lexicon. Since OOD detection aims to correctly classify input images into ID/OOD class groups, we can \"make up\" OOD label candidates which are not standard class names but beneficial for the process. Observing that the original semantic pool is comprised of unmodified specific class names, we correspondingly construct a conjugated semantic pool (CSP) consisting of modified superclass names, each serving as a cluster center for samples sharing similar properties across different categories. Consistent with our established theory, expanding OOD label candidates with the CSP satisfies the requirements and outperforms existing works by 7.89% in FPR95. Codes are available in https://github.com/MengyuanChen21/NeurIPS2024-CSP.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We show theoretical conditions for improving zero-shot OOD detection and propose a feasible method.'}, 'pdf': {'value': '/pdf/3c7240aebfc66bb661b3ec74c805bcb284715406.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024conjugated,\\ntitle={Conjugated Semantic Pool Improves {OOD} Detection with Pre-trained Vision-Language Models},\\nauthor={Mengyuan Chen and Junyu Gao and Changsheng Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qqQFOcUEqM}\\n}'}, 'paperhash': {'value': 'chen|conjugated_semantic_pool_improves_ood_detection_with_pretrained_visionlanguage_models'}},forum = 'qqQFOcUEqM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5027/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5027/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5027/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5027/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qpeAtfUWOQ',number = 15780,cdate = 1715763542860,pdate = 1727288106855,odate = 1730873974575,mdate = 1736984217021,tcdate = 1715763542860,tmdate = 1736984217021,ddate = None,content = {'title': {'value': 'Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting'}, 'authors': {'value': ['Ruiqi Li', 'Yiu-ming Cheung']}, 'authorids': {'value': ['~Ruiqi_Li7', '~Yiu-ming_Cheung1']}, 'keywords': {'value': ['Neural Rendering', 'Uncertainty Quantification']}, 'TLDR': {'value': 'We quantify the uncertainty in 3D Gaussian Splatting by deviating Gaussians to construction model space samples and learn with variational inference. .'}, 'abstract': {'value': 'Recently, 3D Gaussian Splatting (3DGS) has become popular in reconstructing dense 3D representations of appearance and geometry. However, the learning pipeline in 3DGS inherently lacks the ability to quantify uncertainty, which is an important factor in applications like robotics mapping and navigation. In this paper, we propose an uncertainty estimation method built upon the Bayesian inference framework. Specifically, we propose a method to build variational multi-scale 3D Gaussians, where we leverage explicit scale information in 3DGS parameters to construct diversified parameter space samples. We develop an offset table technique to draw local multi-scale samples efficiently by offsetting selected attributes and sharing other base attributes. Then, the offset table is learned by variational inference with multi-scale prior. The learned offset posterior can quantify the uncertainty of each individual Gaussian component, and be used in the forward pass to infer the predictive uncertainty. Extensive experimental results on various benchmark datasets show that the proposed method provides well-aligned calibration performance on estimated uncertainty and better rendering quality compared with the previous methods that enable uncertainty quantification with view synthesis. Besides, by leveraging the model parameter uncertainty estimated by our method, we can remove noisy Gaussians automatically, thereby obtaining a high-fidelity part of the reconstructed scene, which is of great help in improving the visual quality.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8f70d2aaf3fde28609d247eee1e0166da808d459.pdf'}, 'supplementary_material': {'value': '/attachment/0510d95a78e5b2f2187c06e66cb604c5fe9c7164.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024variational,\\ntitle={Variational Multi-scale Representation for Estimating Uncertainty in 3D Gaussian Splatting},\\nauthor={Ruiqi Li and Yiu-ming Cheung},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qpeAtfUWOQ}\\n}'}, 'paperhash': {'value': 'li|variational_multiscale_representation_for_estimating_uncertainty_in_3d_gaussian_splatting'}},forum = 'qpeAtfUWOQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15780/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15780/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15780/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15780/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qp5VbGTaM0',number = 17015,cdate = 1715776361173,pdate = 1727288142851,odate = 1730873982789,mdate = 1730873982802,tcdate = 1715776361173,tmdate = 1730873982802,ddate = None,content = {'title': {'value': 'On Softmax Direct Preference Optimization for Recommendation'}, 'authors': {'value': ['Yuxin Chen', 'Junfei Tan', 'An Zhang', 'Zhengyi Yang', 'Leheng Sheng', 'Enzhi Zhang', 'Xiang Wang', 'Tat-Seng Chua']}, 'authorids': {'value': ['~Yuxin_Chen9', '~Junfei_Tan1', '~An_Zhang2', '~Zhengyi_Yang1', '~Leheng_Sheng2', '~Enzhi_Zhang1', '~Xiang_Wang6', '~Tat-Seng_Chua2']}, 'keywords': {'value': ['Sequential Recommendation', 'Large Language Models']}, 'abstract': {'value': 'Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (\\\\textbf{S-DPO}) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, which is extended from the traditional full-ranking Plackett-Luce (PL) model to partial rankings and connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has an inherent benefit of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while providing better rewards for preferred items. Our codes are available at https://github.com/chenyuxin1999/S-DPO.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/022ace71d895bdc3f62937c9f3b596dabc27162b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024on,\\ntitle={On Softmax Direct Preference Optimization for Recommendation},\\nauthor={Yuxin Chen and Junfei Tan and An Zhang and Zhengyi Yang and Leheng Sheng and Enzhi Zhang and Xiang Wang and Tat-Seng Chua},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qp5VbGTaM0}\\n}'}, 'paperhash': {'value': 'chen|on_softmax_direct_preference_optimization_for_recommendation'}},forum = 'qp5VbGTaM0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17015/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17015/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17015/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17015/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qo7NtGMr2u',number = 21287,cdate = 1715801107471,pdate = 1727288251739,odate = 1730874005240,mdate = 1730874005259,tcdate = 1715801107471,tmdate = 1730874005259,ddate = None,content = {'title': {'value': 'Symmetry Discovery Beyond Affine Transformations'}, 'authors': {'value': ['Ben Shaw', 'Abram Magner', 'Kevin R. Moon']}, 'authorids': {'value': ['~Ben_Shaw1', '~Abram_Magner1', '~Kevin_R._Moon1']}, 'keywords': {'value': ['symmetry detection', 'isometries', 'infinitesimal generators', 'Killing vectors', 'Riemannian metric', 'transformation groups', 'manifold learning']}, 'abstract': {'value': 'Symmetry detection has been shown to improve various machine learning tasks. In the context of continuous symmetry detection, current state of the art experiments are limited to the detection of affine transformations. Under the manifold assumption, we outline a framework for discovering continuous symmetry in data beyond the affine transformation group. We also provide a similar framework for discovering discrete symmetry. We experimentally compare our method to an existing method known as LieGAN and show that our method is competitive at detecting affine symmetries for large sample sizes and superior than LieGAN for small sample sizes. We also show our method is able to detect continuous symmetries beyond the affine group and is  generally more computationally efficient than LieGAN.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bf49ca3bbb3f5fb5fde6b792c1f40ac882b3599a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshaw2024symmetry,\\ntitle={Symmetry Discovery Beyond Affine Transformations},\\nauthor={Ben Shaw and Abram Magner and Kevin R. Moon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qo7NtGMr2u}\\n}'}, 'paperhash': {'value': 'shaw|symmetry_discovery_beyond_affine_transformations'}},forum = 'qo7NtGMr2u',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21287/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21287/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21287/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21287/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qmoVQbwmCY',number = 17476,cdate = 1715779779191,pdate = 1727288154688,odate = 1730873984718,mdate = 1730873984737,tcdate = 1715779779191,tmdate = 1730873984737,ddate = None,content = {'title': {'value': 'Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention'}, 'authors': {'value': ['Avinash Kori', 'Francesco Locatello', 'Ainkaran Santhirasekaram', 'Francesca Toni', 'Ben Glocker', 'Fabio De Sousa Ribeiro']}, 'authorids': {'value': ['~Avinash_Kori1', '~Francesco_Locatello1', '~Ainkaran_Santhirasekaram1', '~Francesca_Toni1', '~Ben_Glocker1', '~Fabio_De_Sousa_Ribeiro1']}, 'keywords': {'value': ['Object-centric learning', 'Probabilistic slot-attention', 'Identifiability', 'latent mixture models']}, 'TLDR': {'value': 'We propose a method to learn identifiable object-centric representation up to a proposed equivalence relation.'}, 'abstract': {'value': 'Learning modular object-centric representations is said to be crucial for systematic generalization. Existing methods show promising object-binding capabilities empirically, but theoretical identifiability guarantees remain relatively underdeveloped. Understanding when object-centric representations can theoretically be identified is important for scaling slot-based methods to high-dimensional images with correctness guarantees. To that end, we propose a probabilistic slot-attention algorithm that imposes an *aggregate* mixture prior over object-centric slot representations, thereby providing slot identifiability guarantees without supervision, up to an equivalence relation. We provide empirical verification of our theoretical identifiability result using both simple 2-dimensional data and high-resolution imaging datasets.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2cbe4a06cb62c34ccdb43f09403b5dd7fd435fa1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkori2024identifiable,\\ntitle={Identifiable Object-Centric Representation Learning via Probabilistic Slot Attention},\\nauthor={Avinash Kori and Francesco Locatello and Ainkaran Santhirasekaram and Francesca Toni and Ben Glocker and Fabio De Sousa Ribeiro},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qmoVQbwmCY}\\n}'}, 'paperhash': {'value': 'kori|identifiable_objectcentric_representation_learning_via_probabilistic_slot_attention'}},forum = 'qmoVQbwmCY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17476/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17476/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17476/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17476/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qlH21Ig1IC',number = 17770,cdate = 1715781772478,pdate = 1727288161367,odate = 1730873986178,mdate = 1730873986197,tcdate = 1715781772478,tmdate = 1730873986197,ddate = None,content = {'title': {'value': 'Adaptive Proximal Gradient Method for Convex Optimization'}, 'authors': {'value': ['Yura Malitsky', 'Konstantin Mishchenko']}, 'authorids': {'value': ['~Yura_Malitsky1', '~Konstantin_Mishchenko1']}, 'keywords': {'value': ['adaptive methods', 'gradient descent', 'proximal gradient method']}, 'TLDR': {'value': 'Adaptive versions of GD and ProxGD with large steps.'}, 'abstract': {'value': 'In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d2a0a683bcb17c37d11b5e070cf45776aa8507c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmalitsky2024adaptive,\\ntitle={Adaptive Proximal Gradient Method for Convex Optimization},\\nauthor={Yura Malitsky and Konstantin Mishchenko},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qlH21Ig1IC}\\n}'}, 'paperhash': {'value': 'malitsky|adaptive_proximal_gradient_method_for_convex_optimization'}},forum = 'qlH21Ig1IC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17770/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17770/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17770/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17770/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qkoZgJhxsA',number = 4310,cdate = 1715400133252,pdate = 1727287747278,odate = 1730873875008,mdate = 1730873875026,tcdate = 1715400133252,tmdate = 1730873875026,ddate = None,content = {'title': {'value': 'SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models'}, 'authors': {'value': ['Jiayu Liu', 'Zhenya Huang', 'Tong Xiao', 'Jing Sha', 'Jinze Wu', 'Qi Liu', 'Shijin Wang', 'Enhong Chen']}, 'authorids': {'value': ['~Jiayu_Liu2', '~Zhenya_Huang2', '~Tong_Xiao7', '~Jing_Sha1', '~Jinze_Wu1', '~Qi_Liu3', '~Shijin_Wang1', '~Enhong_Chen1']}, 'keywords': {'value': ['Socratic Teaching', 'Large Language Models']}, 'abstract': {'value': 'Large language models (LLMs) are considered a crucial technology for advancing intelligent education since they exhibit the potential for an in-depth understanding of teaching scenarios and providing students with personalized guidance. Nonetheless, current LLM-based application in personalized teaching predominantly follows a \"Question-Answering\" paradigm, where students are passively provided with answers and explanations. In this paper, we propose SocraticLM, which achieves a Socratic \"Thought-Provoking\" teaching paradigm that fulfills the role of a real classroom teacher in actively engaging students in the thought process required for genuine problem-solving mastery. To build SocraticLM, we first propose a novel \"Dean-Teacher-Student\" multi-agent pipeline to construct a new dataset, SocraTeach, which contains $35$K meticulously crafted Socratic-style multi-round (equivalent to $208$K single-round) teaching dialogues grounded in fundamental mathematical problems. Our dataset simulates authentic teaching scenarios, interacting with six representative types of simulated students with different cognitive states, and strengthening four crucial teaching abilities. SocraticLM is then fine-tuned on SocraTeach with three strategies balancing its teaching and reasoning abilities. Moreover, we contribute a comprehensive evaluation system encompassing five pedagogical dimensions for assessing the teaching quality of LLMs. Extensive experiments verify that SocraticLM achieves significant improvements in the teaching performance, outperforming GPT4 by more than 12\\\\%. Our dataset and code is available at https://github.com/Ljyustc/SocraticLM.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0ffd4457ec8b4fc554d8b483ae93e3e188155407.pdf'}, 'supplementary_material': {'value': '/attachment/81bc6359aad617a25345cb5efe6dabba70223da9.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nliu2024socraticlm,\\ntitle={Socratic{LM}: Exploring Socratic Personalized Teaching with Large Language Models},\\nauthor={Jiayu Liu and Zhenya Huang and Tong Xiao and Jing Sha and Jinze Wu and Qi Liu and Shijin Wang and Enhong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qkoZgJhxsA}\\n}'}, 'paperhash': {'value': 'liu|socraticlm_exploring_socratic_personalized_teaching_with_large_language_models'}},forum = 'qkoZgJhxsA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4310/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4310/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4310/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission4310/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qfCQ54ZTX1',number = 9136,cdate = 1715675142250,pdate = 1727287901744,odate = 1730873917858,mdate = 1730873917879,tcdate = 1715675142250,tmdate = 1730873917879,ddate = None,content = {'title': {'value': 'Entity Alignment with Noisy Annotations from Large Language Models'}, 'authors': {'value': ['Shengyuan Chen', 'Qinggang Zhang', 'Junnan Dong', 'Wen Hua', 'Qing Li', 'Xiao Huang']}, 'authorids': {'value': ['~Shengyuan_Chen3', '~Qinggang_Zhang2', '~Junnan_Dong1', '~Wen_Hua1', '~Qing_Li5', '~Xiao_Huang1']}, 'keywords': {'value': ['Entity alignment', 'Large language  models', 'probabilistic reasoning', 'active learning']}, 'TLDR': {'value': 'LLM4EA is a novel framework for entity alignment with noisy annotations from large language models, by effectively learn from noisy annotations and optimizing query budget utility..'}, 'abstract': {'value': 'Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5452a4d776ca7849e8b6bafef8a42a3bf6b72a37.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024entity,\\ntitle={Entity Alignment with Noisy Annotations from Large Language Models},\\nauthor={Shengyuan Chen and Qinggang Zhang and Junnan Dong and Wen Hua and Qing Li and Xiao Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qfCQ54ZTX1}\\n}'}, 'paperhash': {'value': 'chen|entity_alignment_with_noisy_annotations_from_large_language_models'}},forum = 'qfCQ54ZTX1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9136/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9136/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9136/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9136/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qf2uZAdy1N',number = 16721,cdate = 1715773704689,pdate = 1727288134152,odate = 1730873980811,mdate = 1730873980831,tcdate = 1715773704689,tmdate = 1730873980831,ddate = None,content = {'title': {'value': 'Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity'}, 'authors': {'value': ['Philip Amortila', 'Dylan J Foster', 'Nan Jiang', 'Akshay Krishnamurthy', 'Zakaria Mhammedi']}, 'authorids': {'value': ['~Philip_Amortila1', '~Dylan_J_Foster1', '~Nan_Jiang2', '~Akshay_Krishnamurthy1', '~Zakaria_Mhammedi1']}, 'keywords': {'value': ['Reinforcement Learning', 'Representation Learning', 'Latent Dynamics', 'Function Approximation']}, 'abstract': {'value': \"Real-world applications of reinforcement learning often involve  environments where agents operate on complex, high-dimensional observations, but the underlying (``latent'')  dynamics are comparatively simple. However, beyond restrictive settings\\n  such as tabular latent dynamics,  the fundamental statistical requirements and algorithmic principles for *reinforcement learning under latent dynamics* are poorly\\n  understood.\\n\\n  This paper addresses the question of reinforcement learning under *general latent dynamics* from a\\n  statistical and algorithmic perspective.  On the statistical side, our main negative\\nresult shows that *most* well-studied settings for reinforcement learning with function approximation become intractable when composed with rich observations; we complement this with a positive result, identifying *latent pushforward coverability* as a\\ngeneral condition that enables statistical tractability. Algorithmically, we develop provably efficient *observable-to-latent* reductions ---that is, reductions that transform an arbitrary algorithm for the\\n  latent MDP into an algorithm that can operate on rich observations--- in two settings: one where the agent has access to hindsight\\nobservations of the latent dynamics (Lee et al., 2023) and one\\nwhere the agent can estimate *self-predictive* latent models (Schwarzer et al., 2020). Together, our results serve as a\\n  first step toward a unified statistical and algorithmic theory for\\nreinforcement learning under latent dynamics.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/17710a946394531d22cd1cf32e0a7fd7bac1e6ac.pdf'}, 'TLDR': {'value': 'We study the statistical requirements and algorithmic principles for reinforcement learning under general latent dynamics'}, '_bibtex': {'value': '@inproceedings{\\namortila2024reinforcement,\\ntitle={Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity},\\nauthor={Philip Amortila and Dylan J Foster and Nan Jiang and Akshay Krishnamurthy and Zakaria Mhammedi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qf2uZAdy1N}\\n}'}, 'paperhash': {'value': 'amortila|reinforcement_learning_under_latent_dynamics_toward_statistical_and_algorithmic_modularity'}},forum = 'qf2uZAdy1N',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16721/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16721/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16721/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16721/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qf1ncViBr5',number = 3306,cdate = 1715258158253,pdate = 1727287715939,odate = 1730873864978,mdate = 1730873864989,tcdate = 1715258158253,tmdate = 1730873864989,ddate = None,content = {'title': {'value': 'einspace: Searching for Neural Architectures from Fundamental Operations'}, 'authors': {'value': ['Linus Ericsson', 'Miguel Espinosa', 'Chenhongyi Yang', 'Antreas Antoniou', 'Amos Storkey', 'Shay B Cohen', 'Steven McDonagh', 'Elliot J. Crowley']}, 'authorids': {'value': ['~Linus_Ericsson1', '~Miguel_Espinosa1', '~Chenhongyi_Yang3', '~Antreas_Antoniou3', '~Amos_Storkey1', '~Shay_B_Cohen1', '~Steven_McDonagh1', '~Elliot_J._Crowley1']}, 'keywords': {'value': ['neural architecture search', 'nas', 'deep learning architectures', 'context-free grammars', 'cfg', 'pcfg', 'neural networks', 'search space']}, 'abstract': {'value': 'Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren’t diverse enough to include such transformations *a priori*. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce `einspace`, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce an expressive NAS search space, containing diverse SOTA architectures. When searching in this space we find new SOTA and improvements on existing architectures.'}, 'pdf': {'value': '/pdf/efec03c6fc6f3cdf740e493587209da213f4a1f2.pdf'}, 'supplementary_material': {'value': '/attachment/9eb285992f675cdf9ec119c8dffc3b8c722eeed7.zip'}, '_bibtex': {'value': '@inproceedings{\\nericsson2024einspace,\\ntitle={einspace: Searching for Neural Architectures from Fundamental Operations},\\nauthor={Linus Ericsson and Miguel Espinosa and Chenhongyi Yang and Antreas Antoniou and Amos Storkey and Shay B Cohen and Steven McDonagh and Elliot J. Crowley},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qf1ncViBr5}\\n}'}, 'paperhash': {'value': 'ericsson|einspace_searching_for_neural_architectures_from_fundamental_operations'}},forum = 'qf1ncViBr5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3306/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3306/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3306/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3306/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qevq3FZ63J',number = 13994,cdate = 1715744977550,pdate = 1727288057015,odate = 1730873962780,mdate = 1730873962798,tcdate = 1715744977550,tmdate = 1730873962798,ddate = None,content = {'title': {'value': 'MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution'}, 'authors': {'value': ['Wei Tao', 'Yucheng Zhou', 'Yanlin Wang', 'Wenqiang Zhang', 'Hongyu Zhang', 'Yu Cheng']}, 'authorids': {'value': ['~Wei_Tao4', '~Yucheng_Zhou1', '~Yanlin_Wang1', '~Wenqiang_Zhang1', '~Hongyu_Zhang1', '~Yu_Cheng1']}, 'keywords': {'value': ['Code Change', 'LLM Agent', 'Software Evolution']}, 'abstract': {'value': 'In software development, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code.\\nLarge Language Models (LLMs) have shown promise in code generation but face difficulties in resolving Github issues, particularly at the repository level. \\nTo overcome this challenge, we empirically study the reason why LLMs fail to resolve GitHub issues and analyze the major factors. \\nMotivated by the empirical findings, we propose a novel LLM-based **M**ulti-**A**gent framework for **G**itHub **I**ssue re**S**olution, **MAGIS**, consisting of four agents customized for software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. \\nThis framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. \\nIn experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. \\nMAGIS can resolve **13.94%** GitHub issues, significantly outperforming the baselines.\\nSpecifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the advanced LLM.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/160f5e4c2c7ce5f4555901cb61fa6bd97dbfbd5c.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntao2024magis,\\ntitle={{MAGIS}: {LLM}-Based Multi-Agent Framework for GitHub Issue Resolution},\\nauthor={Wei Tao and Yucheng Zhou and Yanlin Wang and Wenqiang Zhang and Hongyu Zhang and Yu Cheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qevq3FZ63J}\\n}'}, 'paperhash': {'value': 'tao|magis_llmbased_multiagent_framework_for_github_issue_resolution'}},forum = 'qevq3FZ63J',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13994/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13994/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13994/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13994/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qdV1vp1AtL',number = 16318,cdate = 1715769233591,pdate = 1727288123407,odate = 1730873978558,mdate = 1735933383996,tcdate = 1715769233591,tmdate = 1735933383996,ddate = None,content = {'title': {'value': 'Universal Sample Coding'}, 'authors': {'value': ['Szymon Kobus', 'Tze-Yang Tung', 'Deniz Gunduz']}, 'authorids': {'value': ['~Szymon_Kobus1', '~Tze-Yang_Tung1', '~Deniz_Gunduz1']}, 'keywords': {'value': ['source coding', 'compression', 'sampling', 'channel simulation', 'federated learning', 'generative AI']}, 'TLDR': {'value': 'Communication of multiple samples from an unknown probability distribution and its applications to federated learning and generative inference.'}, 'abstract': {'value': 'In this work, we study the problem of communicating multiple samples from an unknown probability distribution using as few bits as possible. This is a generalization of the channel simulation problem, which has recently found applications and achieved state of the art results in realistic image compression, neural network compression, and communication-efficient federated learning. In this problem, the transmitter wants the receiver to generate multiple independent and identically distributed (i.i.d.) samples from a target distribution $P$, while the transmitter and the receiver have access to independent samples from a reference distribution $Q$. The core idea is to employ channel simulation in multiple rounds while updating the reference distribution $Q$ after each round in order to reduce the KL-divergence between $P$ and $Q$, thereby reducing the communication cost in subsequent rounds. We derive a lower bound on the expected communication cost and construct a practical algorithm that achieves the lower bound up to a multiplicative constant. We then employ this algorithm in communication-efficient federated learning, in which model updates correspond to samples from a distribution, and achieve a 37% reduction in the communication load. To further highlight the potential of sample communication for generative models, we show that the number of bits needed to communicate samples from a large language model can be reduced by up to 16 times, compared to entropy-based data compression.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4e0b407650f94bab40dee287555bc359ee6201d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkobus2024universal,\\ntitle={Universal Sample Coding},\\nauthor={Szymon Kobus and Tze-Yang Tung and Deniz Gunduz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qdV1vp1AtL}\\n}'}, 'paperhash': {'value': 'kobus|universal_sample_coding'}},forum = 'qdV1vp1AtL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16318/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16318/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16318/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16318/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qd8blc0o0F',number = 6618,cdate = 1715600128618,pdate = 1727287820748,odate = 1730873894829,mdate = 1736922974062,tcdate = 1715600128618,tmdate = 1736922974062,ddate = None,content = {'title': {'value': 'GRANOLA: Adaptive Normalization for Graph Neural Networks'}, 'authors': {'value': ['Moshe Eliasof', 'Beatrice Bevilacqua', 'Carola-Bibiane Schönlieb', 'Haggai Maron']}, 'authorids': {'value': ['~Moshe_Eliasof1', '~Beatrice_Bevilacqua1', '~Carola-Bibiane_Schönlieb1', '~Haggai_Maron1']}, 'keywords': {'value': ['Graph Neural Networks', 'Normalization Layer in GNNs']}, 'abstract': {'value': 'Despite the widespread adoption of Graph Neural Networks (GNNs), these models often incorporate off-the-shelf normalization layers like BatchNorm or InstanceNorm, which were not originally designed for GNNs. Consequently, these normalization layers may not effectively capture the unique characteristics of graph-structured data, potentially even weakening the expressive power of the overall architecture. \\nWhile existing graph-specific normalization layers have been proposed, they often struggle to offer substantial and consistent benefits. In this paper, we propose GRANOLA, a novel graph-adaptive normalization layer. Unlike existing normalization layers, GRANOLA normalizes node features by adapting to the specific characteristics of the graph, particularly by generating expressive representations of its nodes, obtained by leveraging the propagation of Random Node Features (RNF) in the graph. We provide theoretical results that support our design choices as well as an extensive empirical evaluation demonstrating the superior performance of GRANOLA over existing normalization techniques. Furthermore, GRANOLA emerges as the top-performing method among all baselines in the same time complexity class of Message Passing Neural Networks (MPNNs).'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We study the effect of normalization layers in GNNs and propose a novel layer, called GRANOLA, that is expressive and graph adaptive.'}, 'pdf': {'value': '/pdf/71a7486065f5d9b76c6b6737c020d7aad13ebd3e.pdf'}, '_bibtex': {'value': '@inproceedings{\\neliasof2024granola,\\ntitle={{GRANOLA}: Adaptive Normalization for Graph Neural Networks},\\nauthor={Moshe Eliasof and Beatrice Bevilacqua and Carola-Bibiane Sch{\\\\\"o}nlieb and Haggai Maron},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qd8blc0o0F}\\n}'}, 'paperhash': {'value': 'eliasof|granola_adaptive_normalization_for_graph_neural_networks'}},forum = 'qd8blc0o0F',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6618/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6618/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6618/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6618/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qcPlGtzwW9',number = 11480,cdate = 1715705432873,pdate = 1727287972546,odate = 1730873939046,mdate = 1730873939067,tcdate = 1715705432873,tmdate = 1730873939067,ddate = None,content = {'title': {'value': 'Tighter Convergence Bounds for Shuffled SGD via Primal-Dual Perspective'}, 'authors': {'value': ['Xufeng Cai', 'Cheuk Yin Lin', 'Jelena Diakonikolas']}, 'authorids': {'value': ['~Xufeng_Cai1', '~Cheuk_Yin_Lin1', '~Jelena_Diakonikolas2']}, 'keywords': {'value': ['primal-dual analysis', 'cyclic methods', 'shuffled sgd']}, 'TLDR': {'value': 'We provide fine-grained primal-dual analysis and tighter bounds for shuffled SGD by interpreting it as a primal-dual method with cyclic updates on the dual side. Obtained improvements are up to order-$O(\\\\sqrt{n})$ both theoretically and empirically.'}, 'abstract': {'value': 'Stochastic gradient descent (SGD) is perhaps the most prevalent optimization method in modern machine learning. Contrary to the empirical practice of sampling from the datasets \\\\emph{without replacement} and with (possible) reshuffling at each epoch, the theoretical counterpart of SGD usually relies on the assumption of \\\\emph{sampling with replacement}. It is only very recently that SGD using sampling without replacement -- shuffled SGD -- has been analyzed with matching upper and lower bounds. However, we observe that those bounds are too pessimistic to explain often superior empirical performance of data permutations (sampling without replacement) over vanilla counterparts (sampling with replacement) on machine learning problems. Through fine-grained analysis in the lens of primal-dual cyclic coordinate methods and the introduction of novel smoothness parameters, we present several results for shuffled SGD on smooth and non-smooth convex losses, where our novel analysis framework provides tighter convergence bounds over all popular shuffling schemes (IG, SO, and RR). Notably, our new bounds predict faster convergence than existing bounds in the literature -- by up to a factor of $O(\\\\sqrt{n})$, mirroring benefits from tighter convergence bounds using component smoothness parameters in randomized coordinate methods. Lastly, we numerically demonstrate on common machine learning datasets that our bounds are indeed much tighter, thus offering a bridge between theory and practice.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/01bfa90837d76a8a6b8a6d5ec563940e2c01557b.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ncai2024tighter,\\ntitle={Tighter Convergence Bounds for Shuffled {SGD} via Primal-Dual Perspective},\\nauthor={Xufeng Cai and Cheuk Yin Lin and Jelena Diakonikolas},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qcPlGtzwW9}\\n}'}, 'paperhash': {'value': 'cai|tighter_convergence_bounds_for_shuffled_sgd_via_primaldual_perspective'}},forum = 'qcPlGtzwW9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11480/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11480/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11480/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11480/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qbvt3ocQxB',number = 4658,cdate = 1715437108375,pdate = 1727287756959,odate = 1730873877944,mdate = 1730873877965,tcdate = 1715437108375,tmdate = 1730873877965,ddate = None,content = {'title': {'value': 'IODA: Instance-Guided One-shot Domain Adaptation for Super-Resolution'}, 'authors': {'value': ['Zaizuo Tang', 'Yu-Bin Yang']}, 'authorids': {'value': ['~Zaizuo_Tang1', '~Yu-Bin_Yang3']}, 'keywords': {'value': ['one-shot domain adaptation', 'super resolution', 'domain adaptation']}, 'TLDR': {'value': 'We propose an Instance-guided One-shot Domain Adaptation for Super-Resolution (IODA) to enable efficient domain adaptation with only a single unlabeled target domain LR image.'}, 'abstract': {'value': 'The domain adaptation method effectively mitigates the negative impact of domain gaps on the performance of super-resolution (SR) networks through the guidance of numerous target domain low-resolution (LR) images. However, in real-world scenarios, the availability of target domain LR images is often limited, sometimes even to just one, which inevitably impairs the domain adaptation performance of SR networks. We propose Instance-guided One-shot Domain Adaptation for Super-Resolution (IODA) to enable efficient domain adaptation with only a single unlabeled target domain LR image. To address the limited diversity of the target domain distribution caused by a single target domain LR image, we propose an instance-guided target domain distribution expansion strategy. This strategy effectively expands the diversity of the target domain distribution by generating instance-specific features focused on different instances within the image. For SR tasks emphasizing texture details, we propose an image-guided domain adaptation method. Compared to existing methods that use text representation for domain difference, this method utilizes pixel-level representation with higher granularity, enabling efficient domain adaptation guidance for SR networks. Finally, we validate the effectiveness of IODA on multiple datasets and various network architectures, achieving satisfactory one-shot domain adaptation for SR networks. Our code is available at https://github.com/ZaizuoTang/IODA.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8e30ba7218af6df482043e5a2bc47e57f1b975d2.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntang2024ioda,\\ntitle={{IODA}: Instance-Guided One-shot Domain Adaptation for Super-Resolution},\\nauthor={Zaizuo Tang and Yu-Bin Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qbvt3ocQxB}\\n}'}, 'paperhash': {'value': 'tang|ioda_instanceguided_oneshot_domain_adaptation_for_superresolution'}},forum = 'qbvt3ocQxB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4658/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4658/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4658/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4658/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qamfjyhPeg',number = 5181,cdate = 1715510678180,pdate = 1727287776131,odate = 1730873882722,mdate = 1736447291155,tcdate = 1715510678180,tmdate = 1736447291155,ddate = None,content = {'title': {'value': 'Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach'}, 'authors': {'value': ['Yarin Bar', 'Shalev Shaer', 'Yaniv Romano']}, 'authorids': {'value': ['~Yarin_Bar1', '~Shalev_Shaer2', '~Yaniv_Romano1']}, 'keywords': {'value': ['Test Time Domain Adaptation', 'Online Learning', 'Testing by Betting', 'Martingale', 'Distribution Shift Detection']}, 'abstract': {'value': \"We present a novel approach for test-time adaptation via online self-training, consisting of two components. First, we introduce a statistical framework that detects distribution shifts in the classifier's entropy values obtained on a stream of unlabeled samples. Second, we devise an online adaptation mechanism that utilizes the evidence of distribution shifts captured by the detection tool to dynamically update the classifier's parameters. The resulting adaptation process drives the distribution of test entropy values obtained from the self-trained classifier to match those of the source domain, building invariance to distribution shifts. This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy. Our approach combines concepts in betting martingales and online learning to form a detection tool capable of quickly reacting to distribution shifts. We then reveal a tight relation between our adaptation scheme and optimal transport, which forms the basis of our novel self-supervised loss. Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/820bc9cc609a406d8c2735bf931ca4b9d522ab2e.pdf'}, 'supplementary_material': {'value': '/attachment/583f707678efb06ce3bd550bc2f6967ba99d1ce6.zip'}, 'TLDR': {'value': \"A novel self-training approach for adapting ML models to test-time distribution shifts by monitoring the model's output and aligning it with the source domain's statistics.\"}, '_bibtex': {'value': '@inproceedings{\\nbar2024protected,\\ntitle={Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach},\\nauthor={Yarin Bar and Shalev Shaer and Yaniv Romano},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qamfjyhPeg}\\n}'}, 'paperhash': {'value': 'bar|protected_testtime_adaptation_via_online_entropy_matching_a_betting_approach'}},forum = 'qamfjyhPeg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5181/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5181/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5181/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5181/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qaRT6QTIqJ',number = 11541,cdate = 1715706278025,pdate = 1727287974467,odate = 1730873939515,mdate = 1737021131064,tcdate = 1715706278025,tmdate = 1737021131064,ddate = None,content = {'title': {'value': 'The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains'}, 'authors': {'value': ['Ezra Edelman', 'Nikolaos Tsilivis', 'Benjamin L. Edelman', 'eran malach', 'Surbhi Goel']}, 'authorids': {'value': ['~Ezra_Edelman1', '~Nikolaos_Tsilivis1', '~Benjamin_L._Edelman1', '~eran_malach1', '~Surbhi_Goel1']}, 'keywords': {'value': ['Deep Learning Theory', 'Transformers', 'Induction Heads', 'Phase Transitions', 'In-context learning', 'Markov Chains']}, 'TLDR': {'value': 'We characterize the emergence of induction heads in transformer models using the synthetic task of learning markov chains in-context.'}, 'abstract': {'value': \"Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \\\\emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between the transformer's layers, and uncovering evidence that the presence of the simpler unigram solution may delay formation of the final bigram solution. We examine how learning is affected by varying the prior distribution over Markov chains, and consider the generalization of our in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n > 2$.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ba8fdbd5805a42659e3fa6242f7b586e1734fb87.pdf'}, '_bibtex': {'value': '@inproceedings{\\nedelman2024the,\\ntitle={The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},\\nauthor={Ezra Edelman and Nikolaos Tsilivis and Benjamin L. Edelman and eran malach and Surbhi Goel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qaRT6QTIqJ}\\n}'}, 'paperhash': {'value': 'edelman|the_evolution_of_statistical_induction_heads_incontext_learning_markov_chains'}},forum = 'qaRT6QTIqJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11541/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11541/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11541/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11541/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qaC4sSztlF',number = 15620,cdate = 1715762135577,pdate = 1727288102512,odate = 1730873973844,mdate = 1730873973861,tcdate = 1715762135577,tmdate = 1730873973861,ddate = None,content = {'title': {'value': 'Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing'}, 'authors': {'value': ['Peiran Dong', 'Bingjie WANG', 'Song Guo', 'Junxiao Wang', 'Jie ZHANG', 'Zicong Hong']}, 'authorids': {'value': ['~Peiran_Dong1', '~Bingjie_WANG1', '~Song_Guo5', '~Junxiao_Wang1', '~Jie_ZHANG18', '~Zicong_Hong1']}, 'keywords': {'value': ['Diffusion based models', 'Safe generation', 'Concept Transfer', 'Representation editing']}, 'abstract': {'value': \"Recent advancements in vision-language-to-image (VL2I) diffusion generation have made significant progress. While generating images from broad vision-language inputs holds promise, it also raises concerns about potential misuse, such as copying artistic styles without permission, which could have legal and social consequences. Therefore, it's crucial to establish governance frameworks to ensure ethical and copyright integrity, especially with widely used diffusion models. To address these issues, researchers have explored various approaches, such as dataset filtering, adversarial perturbations, machine unlearning, and inference-time refusals. However, these methods often lack either scalability or effectiveness. In response, we propose a new framework called causal representation editing (CRE), which extends representation editing from large language models (LLMs) to diffusion-based models. CRE enhances the efficiency and flexibility of safe content generation by intervening at diffusion timesteps causally linked to unsafe concepts. This allows for precise removal of harmful content while preserving acceptable content quality, demonstrating superior effectiveness, precision and scalability compared to existing methods. CRE can handle complex scenarios, including incomplete or blurred representations of unsafe concepts, offering a promising solution to challenges in managing harmful content generation in diffusion-based models.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/42c9be9da32a3cbbb45c06be9f939e4d9578ed08.pdf'}, 'supplementary_material': {'value': '/attachment/ce29af17a50f338c016ea85e35de6c8dd185e168.zip'}, '_bibtex': {'value': '@inproceedings{\\ndong2024towards,\\ntitle={Towards Safe Concept Transfer of Multi-Modal Diffusion via Causal Representation Editing},\\nauthor={Peiran Dong and Bingjie WANG and Song Guo and Junxiao Wang and Jie ZHANG and Zicong Hong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qaC4sSztlF}\\n}'}, 'paperhash': {'value': 'dong|towards_safe_concept_transfer_of_multimodal_diffusion_via_causal_representation_editing'}},forum = 'qaC4sSztlF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15620/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15620/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15620/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission15620/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qZSwlcLMCS',number = 13779,cdate = 1715742961749,pdate = 1727288049864,odate = 1730873960418,mdate = 1736900378748,tcdate = 1715742961749,tmdate = 1736900378748,ddate = None,content = {'title': {'value': 'Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling'}, 'authors': {'value': ['Jiatao Gu', 'Ying Shen', 'Shuangfei Zhai', 'Yizhe Zhang', 'Navdeep Jaitly', 'Joshua M. Susskind']}, 'authorids': {'value': ['~Jiatao_Gu1', '~Ying_Shen4', '~Shuangfei_Zhai3', '~Yizhe_Zhang2', '~Navdeep_Jaitly1', '~Joshua_M._Susskind1']}, 'keywords': {'value': ['diffusion models', 'autoregressive models', 'latent variable models', 'text-to-image']}, 'TLDR': {'value': 'We combine diffusion model with autoregressive priors for improved output diversity, interpretability, and controllability'}, 'abstract': {'value': 'Diffusion models have emerged as a powerful tool for generating high-quality images from textual descriptions. Despite their successes, these models often exhibit limited diversity in the sampled images, particularly when sampling with a high classifier-free guidance weight. To address this issue, we present Kaleido, a novel approach that enhances the diversity of samples by incorporating autoregressive latent priors. Kaleido integrates an autoregressive language model that encodes the original caption and generates latent variables, serving as abstract and intermediary representations for guiding and facilitating the image generation process.\\nIn this paper, we explore a variety of discrete latent representations, including textual descriptions, detection bounding boxes, object blobs, and visual tokens. These representations diversify and enrich the input conditions to the diffusion models, enabling more diverse outputs.\\nOur experimental results demonstrate that Kaleido effectively broadens the diversity of the generated image samples from a given textual description while maintaining high image quality. Furthermore, we show that Kaleido adheres closely to the guidance provided by the generated latent variables, demonstrating its capability to effectively control and direct the image generation process.'}, 'pdf': {'value': '/pdf/968a467d6aff11757ad5c6fc59b14a59382f6b13.pdf'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\ngu2024kaleido,\\ntitle={Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling},\\nauthor={Jiatao Gu and Ying Shen and Shuangfei Zhai and Yizhe Zhang and Navdeep Jaitly and Joshua M. Susskind},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qZSwlcLMCS}\\n}'}, 'paperhash': {'value': 'gu|kaleido_diffusion_improving_conditional_diffusion_models_with_autoregressive_latent_modeling'}},forum = 'qZSwlcLMCS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13779/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13779/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13779/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13779/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qZFshkbWDo',number = 8876,cdate = 1715670928269,pdate = 1727287893765,odate = 1730873915974,mdate = 1730873915986,tcdate = 1715670928269,tmdate = 1730873915986,ddate = None,content = {'title': {'value': 'Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense'}, 'authors': {'value': ['Rui Min', 'Zeyu Qin', 'Nevin L. Zhang', 'Li Shen', 'Minhao Cheng']}, 'authorids': {'value': ['~Rui_Min1', '~Zeyu_Qin1', '~Nevin_L._Zhang1', '~Li_Shen1', '~Minhao_Cheng1']}, 'keywords': {'value': ['Backdoor Safety', 'Safety Tuning', 'Superficial Safety']}, 'abstract': {'value': \"Backdoor attacks pose a significant threat to Deep Neural Networks (DNNs) as they allow attackers to manipulate model predictions with backdoor triggers. To address these security vulnerabilities, various backdoor purification methods have been proposed to purify compromised models. Typically, these purified models exhibit low Attack Success Rates (ASR), rendering them resistant to backdoored inputs. However, \\\\textit{Does achieving a low ASR through current safety purification methods truly eliminate learned backdoor features from the pretraining phase?} In this paper, we provide an affirmative answer to this question by thoroughly investigating the \\\\textit{Post-Purification Robustness} of current backdoor purification methods. We find that current safety purification methods are vulnerable to the rapid re-learning of backdoor behavior, even when further fine-tuning of purified models is performed using a very small number of poisoned samples. Based on this, we further propose the practical Query-based Reactivation Attack (QRA) which could effectively reactivate the backdoor by merely querying purified models. We find the failure to achieve satisfactory post-purification robustness stems from the insufficient deviation of purified models from the backdoored model along the backdoor-connected path. To improve the post-purification robustness, we propose a straightforward tuning defense, Path-Aware Minimization (PAM), which promotes deviation along backdoor-connected paths with extra model updates. Extensive experiments demonstrate that PAM significantly improves post-purification robustness while maintaining a good clean accuracy and low ASR. Our work provides a new perspective on understanding the effectiveness of backdoor safety tuning and highlights the importance of faithfully assessing the model's safety.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/14f9b5d9e306ef6145cc9fda36c6da97dfecb26a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmin2024uncovering,\\ntitle={Uncovering, Explaining, and Mitigating the Superficial Safety of Backdoor Defense},\\nauthor={Rui Min and Zeyu Qin and Nevin L. Zhang and Li Shen and Minhao Cheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qZFshkbWDo}\\n}'}, 'paperhash': {'value': 'min|uncovering_explaining_and_mitigating_the_superficial_safety_of_backdoor_defense'}},forum = 'qZFshkbWDo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8876/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8876/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8876/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8876/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qXidsICaja',number = 215,cdate = 1713850656437,pdate = 1727287632813,odate = 1730873839226,mdate = 1730873839243,tcdate = 1713850656437,tmdate = 1730873839243,ddate = None,content = {'title': {'value': 'Expert-level protocol translation for self-driving labs'}, 'authors': {'value': ['Yu-Zhe Shi', 'Fanxu Meng', 'Haofei Hou', 'Zhangqian Bi', 'Qiao Xu', 'Lecheng Ruan', 'Qining Wang']}, 'authorids': {'value': ['~Yu-Zhe_Shi1', '~Fanxu_Meng3', '~Haofei_Hou2', '~Zhangqian_Bi2', '~Qiao_Xu2', '~Lecheng_Ruan1', '~Qining_Wang1']}, 'keywords': {'value': ['Self-Driving Laboratories', 'Domain-Specific Language', 'Structural Representation', 'Knowledge Externalization']}, 'abstract': {'value': 'Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/b5e13d6050c51fb7e508c02b7b87db8cae62a632.zip'}, 'pdf': {'value': '/pdf/32c1b38c663bb9937862966e4eb3988a23de5cf9.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nshi2024expertlevel,\\ntitle={Expert-level protocol translation for self-driving labs},\\nauthor={Yu-Zhe Shi and Fanxu Meng and Haofei Hou and Zhangqian Bi and Qiao Xu and Lecheng Ruan and Qining Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qXidsICaja}\\n}'}, 'paperhash': {'value': 'shi|expertlevel_protocol_translation_for_selfdriving_labs'}},forum = 'qXidsICaja',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission215/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission215/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission215/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission215/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'qXZVSy9LFR',number = 5684,cdate = 1715565192410,pdate = 1727287791816,odate = 1730873887283,mdate = 1736945141162,tcdate = 1715565192410,tmdate = 1736945141162,ddate = None,content = {'title': {'value': 'Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning'}, 'authors': {'value': ['Zebang Cheng', 'Zhi-Qi Cheng', 'Jun-Yan He', 'Kai Wang', 'Yuxiang Lin', 'Zheng Lian', 'Xiaojiang Peng', 'Alexander G Hauptmann']}, 'authorids': {'value': ['~Zebang_Cheng1', '~Zhi-Qi_Cheng1', '~Jun-Yan_He2', '~Kai_Wang8', '~Yuxiang_Lin1', '~Zheng_Lian3', '~Xiaojiang_Peng1', '~Alexander_G_Hauptmann1']}, 'keywords': {'value': ['Instruction Tuning', 'Multi-modal Emotion Recognition', 'Multi-modal Emotion Reasoning']}, 'abstract': {'value': 'Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling.\\nHowever, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/69037d9dc1226a2b7b2be7d9130add1da034141a.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ncheng2024emotionllama,\\ntitle={Emotion-{LL}a{MA}: Multimodal Emotion Recognition and Reasoning with Instruction Tuning},\\nauthor={Zebang Cheng and Zhi-Qi Cheng and Jun-Yan He and Kai Wang and Yuxiang Lin and Zheng Lian and Xiaojiang Peng and Alexander G Hauptmann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qXZVSy9LFR}\\n}'}, 'paperhash': {'value': 'cheng|emotionllama_multimodal_emotion_recognition_and_reasoning_with_instruction_tuning'}},forum = 'qXZVSy9LFR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5684/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5684/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5684/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission5684/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qWi6ESgBjB',number = 17537,cdate = 1715780199487,pdate = 1727288155907,odate = 1730873984984,mdate = 1730873985003,tcdate = 1715780199487,tmdate = 1730873985003,ddate = None,content = {'title': {'value': 'Prune and Repaint: Content-Aware Image Retargeting for any Ratio'}, 'authors': {'value': ['Feihong Shen', 'Chao Li', 'Yifeng Geng', 'Yongjian Deng', 'Hao Chen']}, 'authorids': {'value': ['~Feihong_Shen1', '~Chao_Li17', '~Yifeng_Geng2', '~Yongjian_Deng1', '~Hao_Chen39']}, 'keywords': {'value': ['Image Retargeting', 'content-aware seam-carving', 'adaptive repainting']}, 'abstract': {'value': 'Image retargeting is the task of adjusting the aspect ratio of images to suit different display devices or presentation environments. However, existing retargeting methods often struggle to balance the preservation of key semantics and image quality, resulting in either deformation or loss of important objects, or the introduction of local artifacts such as discontinuous pixels and inconsistent regenerated content. To address these issues, we propose a content-aware retargeting method called PruneRepaint. It incorporates semantic importance for each pixel to guide the identification of regions that need to be pruned or preserved in order to maintain key semantics. Additionally, we introduce an adaptive repainting module that selects image regions for repainting based on the distribution of pruned pixels and the proportion between foreground size and target aspect ratio, thus achieving local smoothness after pruning. By focusing on the content and structure of the foreground, our PruneRepaint approach adaptively avoids key content loss and deformation, while effectively mitigating artifacts with local repainting. We conduct experiments on the public RetargetMe benchmark and demonstrate through objective experimental results and subjective user studies that our method outperforms previous approaches in terms of preserving semantics and aesthetics, as well as better generalization across diverse aspect ratios. Codes will be available at\\n https://github.com/fhshen2022/PruneRepaint.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d6d82c4a6bf1d235a81e7372c40454169ddead0a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshen2024prune,\\ntitle={Prune and Repaint: Content-Aware Image Retargeting for any Ratio},\\nauthor={Feihong Shen and Chao Li and Yifeng Geng and Yongjian Deng and Hao Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qWi6ESgBjB}\\n}'}, 'paperhash': {'value': 'shen|prune_and_repaint_contentaware_image_retargeting_for_any_ratio'}},forum = 'qWi6ESgBjB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17537/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17537/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17537/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17537/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'qWi33pPecC',number = 4378,cdate = 1715409382044,pdate = 1727287748877,odate = 1730873875429,mdate = 1735106348520,tcdate = 1715409382044,tmdate = 1735106348520,ddate = None,content = {'title': {'value': 'Most Influential Subset Selection: Challenges, Promises, and Beyond'}, 'authors': {'value': ['Yuzheng Hu', 'Pingbang Hu', 'Han Zhao', 'Jiaqi Ma']}, 'authorids': {'value': ['~Yuzheng_Hu1', '~Pingbang_Hu1', '~Han_Zhao1', '~Jiaqi_Ma1']}, 'keywords': {'value': ['influential subset', 'influence function', 'data attribution', 'interpretability']}, 'TLDR': {'value': 'We provide a comprehensive study of the common practices in the Most Influential Subset Selection (MISS) problem.'}, 'abstract': {'value': 'How can we attribute the behaviors of machine learning models to their training data? While the classic influence function sheds light on the impact of individual samples, it often fails to capture the more complex and pronounced collective influence of a set of samples. To tackle this challenge, we study the Most Influential Subset Selection (MISS) problem, which aims to identify a subset of training samples with the greatest collective influence. We conduct a comprehensive analysis of the prevailing approaches in MISS, elucidating their strengths and weaknesses. Our findings reveal that influence-based greedy heuristics, a dominant class of algorithms in MISS, can provably fail even in linear regression. We delineate the failure modes, including the errors of influence function and the non-additive structure of the collective influence. Conversely, we demonstrate that an adaptive version of these heuristics which applies them iteratively, can effectively capture the interactions among samples and thus partially address the issues. Experiments on real-world datasets corroborate these theoretical findings, and further demonstrate that the merit of adaptivity can extend to more complex scenarios such as classification tasks and non-linear neural networks. We conclude our analysis by emphasizing the inherent trade-off between performance and computational efficiency, questioning the use of additive metrics such as the linear datamodeling score, and offering a range of discussions.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/51cd9620327c3e23453b59a431e678bf4d94adff.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024most,\\ntitle={Most Influential Subset Selection: Challenges, Promises, and Beyond},\\nauthor={Yuzheng Hu and Pingbang Hu and Han Zhao and Jiaqi Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qWi33pPecC}\\n}'}, 'paperhash': {'value': 'hu|most_influential_subset_selection_challenges_promises_and_beyond'}},forum = 'qWi33pPecC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4378/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4378/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4378/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4378/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qTypwXvNJa',number = 16621,cdate = 1715772615309,pdate = 1727288131638,odate = 1730873980220,mdate = 1730873980237,tcdate = 1715772615309,tmdate = 1730873980237,ddate = None,content = {'title': {'value': 'Geodesic Optimization for Predictive Shift Adaptation on EEG data'}, 'authors': {'value': ['Apolline Mellot', 'Antoine Collas', 'Sylvain Chevallier', 'Alexandre Gramfort', 'Denis Alexander Engemann']}, 'authorids': {'value': ['~Apolline_Mellot1', '~Antoine_Collas1', '~Sylvain_Chevallier1', '~Alexandre_Gramfort1', '~Denis_Alexander_Engemann1']}, 'keywords': {'value': ['EEG', 'brain age', 'Neurosciences', 'Riemannian geometry', 'Domain Adaptation', 'Mixed-effects models']}, 'TLDR': {'value': 'This paper proposes Geodesic Optimization for Predictive Shift Adaptation to address multi-source domain adaptation where source domains have distinct y distributions in the context of brain age prediction from EEG covariance matrices.'}, 'abstract': {'value': \"Electroencephalography (EEG) data is often collected from diverse contexts involving different populations and EEG devices. This variability can induce distribution shifts in the data $X$ and in the biomedical variables of interest $y$, thus limiting the application of supervised machine learning (ML) algorithms. While domain adaptation (DA) methods have been developed to mitigate the impact of these shifts, such methods struggle when distribution shifts occur simultaneously in $X$ and $y$. As state-of-the-art ML models for EEG represent the data by spatial covariance matrices, which lie on the Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is appealing to study DA techniques operating on the SPD manifold. This paper proposes a novel method termed Geodesic Optimization for Predictive Shift Adaptation (GOPSA) to address test-time multi-source DA for situations in which source domains have distinct $y$ distributions. GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model. We performed empirical benchmarks on the cross-site generalization of age-prediction models with resting-state EEG data from a large multi-national dataset (HarMNqEEG), which included $14$ recording sites and more than $1500$ human participants. Compared to state-of-the-art methods, our results showed that GOPSA achieved significantly higher performance on three regression metrics ($R^2$, MAE, and Spearman's $\\\\rho$) for several source-target site combinations, highlighting its effectiveness in tackling multi-source DA with predictive shifts in EEG data analysis. Our method has the potential to combine the advantages of mixed-effects modeling with machine learning for biomedical applications of EEG, such as multicenter clinical trials.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cf7f84f31fb960f2d970680577da62f7b2f7ee5d.pdf'}, 'supplementary_material': {'value': '/attachment/6f86484449baa97d11c0db12d8eaeb11e6996514.zip'}, '_bibtex': {'value': '@inproceedings{\\nmellot2024geodesic,\\ntitle={Geodesic Optimization for Predictive Shift Adaptation on {EEG} data},\\nauthor={Apolline Mellot and Antoine Collas and Sylvain Chevallier and Alexandre Gramfort and Denis Alexander Engemann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qTypwXvNJa}\\n}'}, 'paperhash': {'value': 'mellot|geodesic_optimization_for_predictive_shift_adaptation_on_eeg_data'}},forum = 'qTypwXvNJa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16621/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16621/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16621/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16621/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qRnmLJQHgx',number = 20332,cdate = 1715795825147,pdate = 1727288231631,odate = 1730874000854,mdate = 1730874000876,tcdate = 1715795825147,tmdate = 1730874000876,ddate = None,content = {'title': {'value': '4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities'}, 'authors': {'value': ['Roman Bachmann', 'Oğuzhan Fatih Kar', 'David Mizrahi', 'Ali Garjani', 'Mingfei Gao', 'David Griffiths', 'Jiaming Hu', 'Afshin Dehghan', 'Amir Zamir']}, 'authorids': {'value': ['~Roman_Bachmann1', '~Oğuzhan_Fatih_Kar1', '~David_Mizrahi1', '~Ali_Garjani1', '~Mingfei_Gao1', '~David_Griffiths1', '~Jiaming_Hu2', '~Afshin_Dehghan5', '~Amir_Zamir1']}, 'keywords': {'value': ['multimodal learning', 'multitask learning', 'representation learning', 'transfer learning', 'foundation models', 'generative models', 'computer vision']}, 'abstract': {'value': 'Current multimodal and multitask foundation models, like 4M or UnifiedIO, show promising results. However, their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually small) number of modalities and tasks they are trained on. In this paper, we develop a single any-to-any model trained on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on images and text along with several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example, image metadata or color palettes.\\n\\nA crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text.\\n    \\nThrough this, we show the possibility of training one model to solve at least 3x more tasks/modalities than existing models and doing so without a loss in performance. In addition, this enables more fine-grained and controllable multimodal generation capabilities and allows studying the distillation of models trained on diverse data and objectives into one unified model.\\nWe scale the training to a three billion parameter and different datasets. The multimodal models and training code are open sourced at https://4m.epfl.ch/.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/727e1b1b115255a1630016a67956056a2b237be2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbachmann2024m,\\ntitle={4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities},\\nauthor={Roman Bachmann and O{\\\\u{g}}uzhan Fatih Kar and David Mizrahi and Ali Garjani and Mingfei Gao and David Griffiths and Jiaming Hu and Afshin Dehghan and Amir Zamir},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qRnmLJQHgx}\\n}'}, 'paperhash': {'value': 'bachmann|4m21_an_anytoany_vision_model_for_tens_of_tasks_and_modalities'}},forum = 'qRnmLJQHgx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20332/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20332/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20332/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20332/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qQlmONeI5k',number = 7354,cdate = 1715618882889,pdate = 1727287844336,odate = 1730873902282,mdate = 1735370949577,tcdate = 1715618882889,tmdate = 1735370949577,ddate = None,content = {'title': {'value': 'Empowering Visible-Infrared Person Re-Identification with Large Foundation Models'}, 'authors': {'value': ['Zhangyi Hu', 'Bin Yang', 'Mang Ye']}, 'authorids': {'value': ['~Zhangyi_Hu1', '~Bin_Yang7', '~Mang_Ye1']}, 'keywords': {'value': ['multi-modal', 'large foundation model', 'Visible-Infrared Person Re-Identification']}, 'abstract': {'value': 'Visible-Infrared Person Re-identification (VI-ReID) is a challenging cross-modal retrieval task due to significant modality differences, primarily resulting from the absence of color information in the infrared modality. The development of large foundation models like Large Language Models (LLMs) and Vision Language Models (VLMs) motivates us to explore a feasible solution to empower VI-ReID with off-the-shelf large foundation models. To this end, we propose a novel Text-enhanced VI-ReID framework driven by Large Foundation Models (TVI-LFM). The core idea is to enrich the representation of the infrared modality with textual descriptions automatically generated by VLMs. Specifically, we incorporate a pre-trained VLM to extract textual features from texts generated by VLM and augmented by LLM, and incrementally fine-tune the text encoder to minimize the domain gap between generated texts and original visual modalities. Meanwhile, to enhance the infrared modality with extracted textual representations, we leverage modality alignment capabilities of VLMs and VLM-generated feature-level filters. This enables the text model to learn complementary features from the infrared modality, ensuring the semantic structural consistency between the fusion modality and the visible modality. Furthermore, we introduce modality joint learning to align features across all modalities, ensuring that textual features maintain stable semantic representation of overall pedestrian appearance during complementary information learning. Additionally, a modality ensemble retrieval strategy is proposed to leverage complementary strengths of each query modality to improve retrieval effectiveness and robustness. Extensive experiments on three expanded VI-ReID datasets demonstrate that our method significantly improves the retrieval performance, paving the way for the utilization of large foundation models in downstream multi-modal retrieval tasks.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9eb1ad9b9b53277ab4f1691f9973e3689dd054a4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024empowering,\\ntitle={Empowering Visible-Infrared Person Re-Identification with Large Foundation Models},\\nauthor={Zhangyi Hu and Bin Yang and Mang Ye},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qQlmONeI5k}\\n}'}, 'paperhash': {'value': 'hu|empowering_visibleinfrared_person_reidentification_with_large_foundation_models'}},forum = 'qQlmONeI5k',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7354/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7354/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7354/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7354/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'qPpVDzPhSL',number = 4277,cdate = 1715396128173,pdate = 1727287746236,odate = 1730873874484,mdate = 1730873874501,tcdate = 1715396128173,tmdate = 1730873874501,ddate = None,content = {'title': {'value': 'Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases'}, 'authors': {'value': ['Zian Su', 'Xiangzhe Xu', 'Ziyang Huang', 'Kaiyuan Zhang', 'Xiangyu Zhang']}, 'authorids': {'value': ['~Zian_Su1', '~Xiangzhe_Xu1', '~Ziyang_Huang4', '~Kaiyuan_Zhang1', '~Xiangyu_Zhang3']}, 'keywords': {'value': ['Code Language Models', 'Human-Understandable Binary Recovery', 'Cross-Modal Prompting']}, 'abstract': {'value': 'Human-Oriented Binary Reverse Engineering (HOBRE) lies at the intersection of binary and source code, aiming to lift binary code to human-readable content relevant to source code, thereby bridging the binary-source semantic gap. Recent advancements in uni-modal code model pre-training, particularly in generative Source Code Foundation Models (SCFMs) and binary understanding models, have laid the groundwork for transfer learning applicable to HOBRE. However, existing approaches for HOBRE rely heavily on uni-modal models like SCFMs for supervised fine-tuning or general LLMs for prompting, resulting in sub-optimal performance. Inspired by recent progress in large multi-modal models, we propose that it is possible to harness the strengths of uni-modal code models from both sides to bridge the semantic gap effectively. In this paper, we introduce a novel probe-and-recover framework that incorporates a binary-source encoder-decoder model and black-box LLMs for binary analysis. Our approach leverages the pre-trained knowledge within SCFMs to synthesize relevant, symbol-rich code fragments as context. This additional context enables black-box LLMs to enhance recovery accuracy. We demonstrate significant improvements in zero-shot binary summarization and binary function name recovery, with a 10.3% relative gain in CHRF and a 16.7% relative gain in a GPT4-based metric for summarization, as well as a 6.7% and 7.4% absolute increase in token-level precision and recall for name recovery, respectively. These results highlight the effectiveness of our approach in automating and improving binary code analysis.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5cbff746a2a2d748695aeed7679dc157b97f5e6d.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nsu2024source,\\ntitle={Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases},\\nauthor={Zian Su and Xiangzhe Xu and Ziyang Huang and Kaiyuan Zhang and Xiangyu Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qPpVDzPhSL}\\n}'}, 'paperhash': {'value': 'su|source_code_foundation_models_are_transferable_binary_analysis_knowledge_bases'}},forum = 'qPpVDzPhSL',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4277/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4277/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4277/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission4277/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qOSFiJdVkZ',number = 21363,cdate = 1715801422195,pdate = 1727288253901,odate = 1730874005569,mdate = 1730874005581,tcdate = 1715801422195,tmdate = 1730874005581,ddate = None,content = {'title': {'value': 'Continual learning with the neural tangent ensemble'}, 'authors': {'value': ['Ari S Benjamin', 'Christian-Gernot Pehle', 'Kyle Daruwalla']}, 'authorids': {'value': ['~Ari_S_Benjamin1', '~Christian-Gernot_Pehle1', '~Kyle_Daruwalla1']}, 'keywords': {'value': ['continual learning', 'catastrophic forgetting', 'Bayesian ensembles', 'Boosting and Ensemble Methods', 'mixture of experts']}, 'TLDR': {'value': 'All network classifiers are ensembles; each edge provides a classifier. If you weigh them by their posterior probability you (almost) get SGD.'}, 'abstract': {'value': 'A natural strategy for continual learning is to weigh a Bayesian ensemble of fixed functions. This suggests that if a (single) neural network could be interpreted as an ensemble, one could design effective algorithms that learn without forgetting. To realize this possibility, we observe that a neural network classifier with N parameters can be interpreted as a weighted ensemble of N classifiers, and that in the lazy regime limit these classifiers are fixed throughout learning. We call these classifiers the *neural tangent experts* and show they output valid probability distributions over the labels. We then derive the likelihood and posterior probability of each expert given past data. Surprisingly,  the posterior updates for these experts are equivalent to a scaled and projected form of stochastic gradient descent (SGD) over the network weights. Away from the lazy regime, networks can be seen as ensembles of adaptive experts which improve over time. These results offer a new interpretation of neural networks as Bayesian ensembles of experts, providing a principled framework for understanding and mitigating catastrophic forgetting in continual learning settings.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7ca99de5c6a3ad1c6a158db1bba6a3eb0841e7bc.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbenjamin2024continual,\\ntitle={Continual learning with the neural tangent ensemble},\\nauthor={Ari S Benjamin and Christian-Gernot Pehle and Kyle Daruwalla},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qOSFiJdVkZ}\\n}'}, 'paperhash': {'value': 'benjamin|continual_learning_with_the_neural_tangent_ensemble'}},forum = 'qOSFiJdVkZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21363/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21363/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21363/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21363/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qNXRXUC90b',number = 5483,cdate = 1715538651842,pdate = 1727287785110,odate = 1730873885069,mdate = 1730873885086,tcdate = 1715538651842,tmdate = 1730873885086,ddate = None,content = {'title': {'value': 'Uncertainty-aware Fine-tuning of Segmentation Foundation Models'}, 'authors': {'value': ['Kangning Liu', 'Brian L. Price', 'Jason Kuen', 'Yifei Fan', 'Zijun Wei', 'Luis Figueroa', 'Krzysztof J. Geras', 'Carlos Fernandez-Granda']}, 'authorids': {'value': ['~Kangning_Liu1', '~Brian_L._Price2', '~Jason_Kuen1', '~Yifei_Fan1', '~Zijun_Wei2', '~Luis_Figueroa1', '~Krzysztof_J._Geras1', '~Carlos_Fernandez-Granda1']}, 'keywords': {'value': ['Segmentation foundation model']}, 'abstract': {'value': 'The Segment Anything Model (SAM) is a large-scale foundation model that has revolutionized segmentation methodology. Despite its impressive generalization ability, the segmentation accuracy of SAM on images with intricate structures is often unsatisfactory. Recent works have proposed lightweight fine-tuning using high-quality annotated data to improve accuracy on such images. However, here we provide extensive empirical evidence that this strategy leads to forgetting how to \"segment anything\": these models lose the original generalization abilities of SAM, in the sense that they perform worse for segmentation tasks not represented in the annotated fine-tuning set. To improve performance without forgetting, we introduce a novel framework that combines high-quality annotated data with a large unlabeled dataset. The framework relies on two methodological innovations. First, we quantify the uncertainty in the SAM pseudo labels associated with the unlabeled data and leverage it to perform uncertainty-aware fine-tuning. Second, we encode the type of segmentation task associated with each training example using a $\\\\textit{task prompt}$ to reduce ambiguity. We evaluated the proposed Segmentation with Uncertainty Model (SUM) on a diverse test set consisting of 14 public benchmarks, where it achieves state-of-the-art results. Notably, our method consistently surpasses SAM by 3-6 points in mean IoU and 4-7 in mean boundary IoU across point-prompt interactive segmentation rounds. Code is available at https://github.com/Kangningthu/SUM'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0fc25afd9425b35ac875b47913c45db57670edef.pdf'}, 'TLDR': {'value': 'We introduce the Segmentation with Uncertainty Model (SUM), which enhances the accuracy of segmentation foundation models by incorporating an uncertainty-aware training loss and prompt sampling based on the estimated uncertainty of pseudo-labels.'}, '_bibtex': {'value': '@inproceedings{\\nliu2024uncertaintyaware,\\ntitle={Uncertainty-aware Fine-tuning of Segmentation Foundation Models},\\nauthor={Kangning Liu and Brian L. Price and Jason Kuen and Yifei Fan and Zijun Wei and Luis Figueroa and Krzysztof J. Geras and Carlos Fernandez-Granda},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qNXRXUC90b}\\n}'}, 'paperhash': {'value': 'liu|uncertaintyaware_finetuning_of_segmentation_foundation_models'}},forum = 'qNXRXUC90b',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5483/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5483/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5483/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5483/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'qLnXPVvwLx',number = 8561,cdate = 1715664336492,pdate = 1727287883808,odate = 1730873913000,mdate = 1730873913071,tcdate = 1715664336492,tmdate = 1730873913071,ddate = None,content = {'title': {'value': 'Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs'}, 'authors': {'value': ['Yuxuan Qiao', 'Haodong Duan', 'Xinyu Fang', 'Junming Yang', 'Lin Chen', 'Songyang Zhang', 'Jiaqi Wang', 'Dahua Lin', 'Kai Chen']}, 'authorids': {'value': ['~Yuxuan_Qiao1', '~Haodong_Duan1', '~Xinyu_Fang1', '~Junming_Yang1', '~Lin_Chen18', '~Songyang_Zhang1', '~Jiaqi_Wang1', '~Dahua_Lin1', '~Kai_Chen4']}, 'keywords': {'value': ['Multi-modality', 'VLM', 'evaluation']}, 'TLDR': {'value': 'This paper presents Prism, a framework that can be used for: 1) analyzing the perception and reasoning capabilities of VLMs; 2) solving general visual questions efficiently'}, 'abstract': {'value': \"Vision Language Models (VLMs) demonstrate remarkable proficiency in addressing a wide array of visual questions, which requires strong perception and reasoning faculties. Assessing these two competencies independently is crucial for model refinement, despite the inherent difficulty due to the intertwined nature of seeing and reasoning in existing VLMs. To tackle this issue, we present Prism, an innovative framework designed to disentangle the perception and reasoning processes involved in visual question solving. Prism comprises two distinct stages: a perception stage that utilizes a VLM to extract and articulate visual information in textual form, and a reasoning stage that formulates responses based on the extracted visual information using a Large Language Model (LLM). This modular design enables the systematic comparison and assessment of both proprietary and open-source VLM for their perception and reasoning strengths. Our analytical framework provides several valuable insights, underscoring Prism's potential as a cost-effective solution for vision-language tasks.\\nBy combining a streamlined VLM focused on perception with a powerful LLM tailored for reasoning, Prism achieves superior results in general vision-language tasks while substantially cutting down on training and operational expenses. Quantitative evaluations show that Prism, when configured with a vanilla 2B LLaVA and freely accessible GPT-3.5, delivers performance on par with VLMs $10 \\\\times$ larger on the rigorous multimodal benchmark MMStar.\"}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a26416cef18f1dfea2d59d021c9813a3f39f36da.pdf'}, '_bibtex': {'value': '@inproceedings{\\nqiao2024prism,\\ntitle={Prism: A Framework for Decoupling and Assessing the Capabilities of {VLM}s},\\nauthor={Yuxuan Qiao and Haodong Duan and Xinyu Fang and Junming Yang and Lin Chen and Songyang Zhang and Jiaqi Wang and Dahua Lin and Kai Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qLnXPVvwLx}\\n}'}, 'paperhash': {'value': 'qiao|prism_a_framework_for_decoupling_and_assessing_the_capabilities_of_vlms'}},forum = 'qLnXPVvwLx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8561/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8561/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8561/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8561/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qKfiWNHp6k',number = 13154,cdate = 1715735866727,pdate = 1727288030256,odate = 1730873955408,mdate = 1730873955422,tcdate = 1715735866727,tmdate = 1730873955422,ddate = None,content = {'title': {'value': 'Recognize Any Regions'}, 'authors': {'value': ['Haosen Yang', 'Chuofan Ma', 'Bin Wen', 'Yi Jiang', 'Zehuan Yuan', 'Xiatian Zhu']}, 'authorids': {'value': ['~Haosen_Yang1', '~Chuofan_Ma1', '~Bin_Wen1', '~Yi_Jiang2', '~Zehuan_Yuan1', '~Xiatian_Zhu3']}, 'keywords': {'value': ['Open Vocabulary Object Recognition;  Zero-shot; Representation Learning']}, 'abstract': {'value': 'Understanding the semantics of individual regions or patches of unconstrained images, such as open-world object detection, remains a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, \\nand deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient architecture, named RegionSpot, designed to integrate position-aware localization knowledge from a localization foundation model (e.g., SAM) with semantic information from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge while minimizing training overhead, we keep both foundation models frozen, focusing optimization efforts solely on a lightweight attention-based knowledge integration module.\\nExtensive experiments in open-world object recognition show that our  RegionSpot achieves significant performance gain over prior alternatives, along with substantial computational savings (e.g., training our model with 3 million data in a single day using 8 V100 GPUs). \\nRegionSpot outperforms GLIP-L by 2.9 in mAP on LVIS val set,  with an even larger margin of 13.1 AP for more challenging and rare categories, and a 2.5 AP increase on ODinW. Furthermore, it exceeds GroundingDINO-L by 11.0 AP for rare categories on the LVIS minival set.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e158974032bb53bfca7244c8a0e2c67406654e37.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024recognize,\\ntitle={Recognize Any Regions},\\nauthor={Haosen Yang and Chuofan Ma and Bin Wen and Yi Jiang and Zehuan Yuan and Xiatian Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qKfiWNHp6k}\\n}'}, 'paperhash': {'value': 'yang|recognize_any_regions'}},forum = 'qKfiWNHp6k',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13154/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13154/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13154/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13154/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qK4iS49KDm',number = 12755,cdate = 1715727731333,pdate = 1727288016492,odate = 1730873951544,mdate = 1734861877356,tcdate = 1715727731333,tmdate = 1734861877356,ddate = None,content = {'title': {'value': 'Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit'}, 'authors': {'value': ['Jason D. Lee', 'Kazusato Oko', 'Taiji Suzuki', 'Denny Wu']}, 'authorids': {'value': ['~Jason_D._Lee1', '~Kazusato_Oko1', '~Taiji_Suzuki1', '~Denny_Wu2']}, 'keywords': {'value': ['single-index model', 'feature learning', 'statistical query', 'SGD']}, 'abstract': {'value': 'We study the problem of gradient descent learning of a single-index target function $f_*(\\\\boldsymbol{x}) = \\\\textstyle\\\\sigma_*\\\\left(\\\\langle\\\\boldsymbol{x},\\\\boldsymbol{\\\\theta}\\\\rangle\\\\right)$ under isotropic Gaussian data in $\\\\mathbb{R}^d$, \\nwhere the unknown link function $\\\\sigma_*:\\\\mathbb{R}\\\\to\\\\mathbb{R}$ has information exponent $p$ (defined as the lowest degree in the Hermite expansion). Prior works showed that gradient-based training of neural networks can learn this target with $n\\\\gtrsim d^{\\\\Theta(p)}$ samples, and such complexity is predicted to be necessary by the correlational statistical query lower bound. \\nSurprisingly, we prove that a two-layer neural network optimized by an SGD-based algorithm (on the squared loss) learns $f_*$ with a complexity that is not governed by the information exponent. Specifically, for arbitrary polynomial single-index models, we establish a sample and runtime complexity of $n \\\\simeq T = \\\\Theta(d\\\\cdot\\\\mathrm{polylog} d)$, where $\\\\Theta(\\\\cdot)$ hides a constant only depending on the degree of $\\\\sigma_*$; this dimension dependence matches the information theoretic limit up to polylogarithmic factors. More generally, we show that $n\\\\gtrsim d^{(p_*-1)\\\\vee 1}$ samples are sufficient to achieve low generalization error, where $p_* \\\\le p$ is the \\\\textit{generative exponent} of the link function. Core to our analysis is the reuse of minibatch in the gradient computation, which gives rise to higher-order information beyond correlational queries.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5c351e805429bc780ae5fab35b4eaecf013991eb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024neural,\\ntitle={Neural network learns low-dimensional polynomials with {SGD} near the information-theoretic limit},\\nauthor={Jason D. Lee and Kazusato Oko and Taiji Suzuki and Denny Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qK4iS49KDm}\\n}'}, 'paperhash': {'value': 'lee|neural_network_learns_lowdimensional_polynomials_with_sgd_near_the_informationtheoretic_limit'}},forum = 'qK4iS49KDm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12755/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12755/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12755/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12755/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'qInb7EUmxz',number = 11357,cdate = 1715703878412,pdate = 1727287968669,odate = 1730873937837,mdate = 1736848181281,tcdate = 1715703878412,tmdate = 1736848181281,ddate = None,content = {'title': {'value': 'Persistence Homology Distillation for Semi-supervised Continual Learning'}, 'authors': {'value': ['YanFan', 'Yu Wang', 'Pengfei Zhu', 'Dongyue Chen', 'Qinghua Hu']}, 'authorids': {'value': ['~YanFan1', '~Yu_Wang33', '~Pengfei_Zhu1', '~Dongyue_Chen3', '~Qinghua_Hu1']}, 'keywords': {'value': ['Continual learning; Knowledge Distillation; Semi-supervised learning; Topological data anaylsis; Persistence Homology;']}, 'abstract': {'value': 'Semi-supervised continual learning (SSCL) has attracted significant attention for addressing catastrophic forgetting in semi-supervised data. Knowledge distillation, which leverages data representation and pair-wise similarity, has shown significant potential in preserving information in SSCL. However, traditional distillation strategies often fail in unlabeled data with inaccurate or noisy information, limiting their efficiency in feature spaces undergoing substantial changes during continual learning. To address these limitations, we propose Persistence Homology Distillation (PsHD) to preserve intrinsic structural information that is insensitive to noise in semi-supervised continual learning. First, we capture the structural features using persistence homology by homological evolution across different scales in vision data, where the multi-scale characteristic established its stability under noise interference. Next, we propose a persistence homology distillation loss in SSCL and design an acceleration algorithm to reduce the computational cost of persistence homology in our module. Furthermore, we demonstrate the superior stability of PsHD compared to sample representation and pair-wise similarity distillation methods theoretically and experimentally. Finally, experimental results on three widely used datasets validate that the new PsHD outperforms state-of-the-art with 3.9% improvements on average, and also achieves 1.5%  improvements while reducing 60% memory buffer size, highlighting the potential of utilizing unlabeled data in SSCL. Our code is available: https://github.com/fanyan0411/PsHD.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/76590e62cff7d532d98bbc02ac2483fecfc73c9a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyanfan2024persistence,\\ntitle={Persistence Homology Distillation for Semi-supervised Continual Learning},\\nauthor={YanFan and Yu Wang and Pengfei Zhu and Dongyue Chen and Qinghua Hu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qInb7EUmxz}\\n}'}, 'paperhash': {'value': 'yanfan|persistence_homology_distillation_for_semisupervised_continual_learning'}},forum = 'qInb7EUmxz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11357/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11357/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11357/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11357/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qIkYlfDZaI',number = 2659,cdate = 1715139907535,pdate = 1727287696983,odate = 1730873859908,mdate = 1734919302252,tcdate = 1715139907535,tmdate = 1734919302252,ddate = None,content = {'title': {'value': 'A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning'}, 'authors': {'value': ['Yixiong Zou', 'Shuai Yi', 'Yuhua Li', 'Ruixuan Li']}, 'authorids': {'value': ['~Yixiong_Zou1', '~Shuai_Yi5', '~Yuhua_Li2', '~Ruixuan_Li1']}, 'keywords': {'value': ['Cross-Domain Few-Shot Learning', 'CLS Token', 'Vision Transformer']}, 'abstract': {'value': 'Vision Transformer (ViT) has shown great power in learning from large-scale datasets. However, collecting sufficient data for expert knowledge is always difficult. To handle this problem, Cross-Domain Few-Shot Learning (CDFSL) has been proposed to transfer the source-domain knowledge learned from sufficient data to target domains where only scarce data is available. In this paper, we find an intriguing phenomenon neglected by previous works for the CDFSL task based on ViT: leaving the CLS token to random initialization, instead of loading source-domain trained parameters, could consistently improve target-domain performance. We then delve into this phenomenon for an interpretation. We find **the CLS token naturally absorbs domain information** due to the inherent structure of the ViT, which is represented as the low-frequency component in the Fourier frequency space of images. Based on this phenomenon and interpretation, we further propose a method for the CDFSL task to decouple the domain information in the CLS token during the source-domain training, and adapt the CLS token on the target domain for efficient few-shot learning. Extensive experiments on four benchmarks validate our rationale and state-of-the-art performance. Our codes are available at https://github.com/Zoilsen/CLS_Token_CDFSL.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We find the CLS token naturally absorbs domain information, and propose to decouple domain information from the CLS token and adapt it for cross-domain few-shot learning.'}, 'pdf': {'value': '/pdf/094a7e1a785ccc56a91ff11df3f3815a786a77a0.pdf'}, 'supplementary_material': {'value': '/attachment/0edb608e5ba1deff13035359839547d01c55065b.zip'}, '_bibtex': {'value': '@inproceedings{\\nzou2024a,\\ntitle={A Closer Look at the {CLS} Token for Cross-Domain Few-Shot Learning},\\nauthor={Yixiong Zou and Shuai Yi and Yuhua Li and Ruixuan Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qIkYlfDZaI}\\n}'}, 'paperhash': {'value': 'zou|a_closer_look_at_the_cls_token_for_crossdomain_fewshot_learning'}},forum = 'qIkYlfDZaI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2659/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2659/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2659/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2659/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qGiZQb1Khm',number = 10211,cdate = 1715691682809,pdate = 1727287932794,odate = 1730873926907,mdate = 1730873926927,tcdate = 1715691682809,tmdate = 1730873926927,ddate = None,content = {'title': {'value': 'Watermarking Makes Language Models Radioactive'}, 'authors': {'value': ['Tom Sander', 'Pierre Fernandez', 'Alain Oliviero Durmus', 'Matthijs Douze', 'Teddy Furon']}, 'authorids': {'value': ['~Tom_Sander1', '~Pierre_Fernandez1', '~Alain_Oliviero_Durmus1', '~Matthijs_Douze1', '~Teddy_Furon1']}, 'keywords': {'value': ['Watermarking', 'Large Language Models', 'Membership Inference']}, 'abstract': {'value': 'We investigate the radioactivity of text generated by large language models (LLM), \\\\ie whether it is possible to detect that such synthetic input was used to train a subsequent LLM.\\nCurrent methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees.\\nWe discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.\\nOur new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM.\\nWe link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process.\\nFor instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\\\\%$ of training text is watermarked.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'LLM watermarking, intended for generated text detection, has the secondary effect of revealing when synthetic data are used to fine-tune another model.'}, 'pdf': {'value': '/pdf/c64b470e2896272a99180a7d3ad4df270ed3e516.pdf'}, 'supplementary_material': {'value': '/attachment/1f8e528f4717242ed8e45f4bf6e102e262e849cc.zip'}, '_bibtex': {'value': '@inproceedings{\\nsander2024watermarking,\\ntitle={Watermarking Makes Language Models Radioactive},\\nauthor={Tom Sander and Pierre Fernandez and Alain Oliviero Durmus and Matthijs Douze and Teddy Furon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qGiZQb1Khm}\\n}'}, 'paperhash': {'value': 'sander|watermarking_makes_language_models_radioactive'}},forum = 'qGiZQb1Khm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10211/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10211/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10211/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10211/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qEpi8uWX3N',number = 1402,cdate = 1714623614477,pdate = 1727287659999,odate = 1730873848487,mdate = 1730873848504,tcdate = 1714623614477,tmdate = 1730873848504,ddate = None,content = {'title': {'value': 'HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning'}, 'authors': {'value': ['Chunlin Tian', 'Zhan Shi', 'Zhijiang Guo', 'Li Li', 'Cheng-zhong Xu']}, 'authorids': {'value': ['~Chunlin_Tian1', '~Zhan_Shi3', '~Zhijiang_Guo2', '~Li_Li10', '~Cheng-zhong_Xu1']}, 'keywords': {'value': ['Large Language Models', 'Efficient Fine-Tuning', 'Asymmetric Structure']}, 'abstract': {'value': 'Adapting Large Language Models (LLMs) to new tasks through fine-tuning has been made more efficient by the introduction of Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA. However, these methods often underperform compared to full fine-tuning, particularly in scenarios involving complex datasets. This issue becomes even more pronounced in complex domains, highlighting the need for improved PEFT approaches that can achieve better performance. Through a series of experiments, we have uncovered two critical insights that shed light on the training and parameter inefficiency of LoRA. Building on these insights, we have developed HydraLoRA, a LoRA framework with an asymmetric structure that eliminates the need for domain expertise. Our experiments demonstrate that HydraLoRA outperforms other PEFT approaches, even those that rely on domain knowledge during the training and inference phases. Our anonymous codes are submitted with the paper and will be publicly available. Code is available: https://github.com/Clin0212/HydraLoRA.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/60e4bb51758f975380df1586e785d29a101c7f4a.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntian2024hydralora,\\ntitle={HydraLo{RA}: An Asymmetric Lo{RA} Architecture for Efficient Fine-Tuning},\\nauthor={Chunlin Tian and Zhan Shi and Zhijiang Guo and Li Li and Cheng-zhong Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qEpi8uWX3N}\\n}'}, 'paperhash': {'value': 'tian|hydralora_an_asymmetric_lora_architecture_for_efficient_finetuning'}},forum = 'qEpi8uWX3N',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1402/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1402/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1402/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1402/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'qDuqp1nZZ6',number = 10848,cdate = 1715698314135,pdate = 1727287952942,odate = 1730873932814,mdate = 1730873932826,tcdate = 1715698314135,tmdate = 1730873932826,ddate = None,content = {'title': {'value': 'Differentially Private Equivalence Testing for Continuous Distributions and Applications'}, 'authors': {'value': ['Or Sheffet', 'Daniel Omer']}, 'authorids': {'value': ['~Or_Sheffet1', '~Daniel_Omer1']}, 'keywords': {'value': ['Differential Privacy', 'Equivalence Tester', 'Continuous Distributions']}, 'TLDR': {'value': 'The first paper to give a DP equivalence tester for continuous distrbiutions'}, 'abstract': {'value': \"We present the first algorithm for testing equivalence  between two continuous distributions using differential privacy (DP). Our algorithm is a private version of the algorithm of Diakonikolas et al. \\nThe algorithm of Diakonikolas et al uses the data itself to repeatedly discretize the real line so that --- when the two distributions are far apart in ${\\\\cal A}_k$-norm --- one of the discretized distributions exhibits large $L_2$-norm difference; and upon repeated sampling such large gap would be detected. Designing its private analogue poses two difficulties. First, our DP algorithm can not resample new datapoints as a change to a single datapoint may lead to a very large change in the descretization of the real line. In contrast, the (sorted) index of the discretization point changes only by $1$ between neighboring instances, and so we use a novel algorithm that set the discretization points using random Bernoulli noise, resulting in only a few buckets being affected under the right coupling. Second, our algorithm, which doesn't resample data, requires we also revisit the utility analysis of the original algorithm and prove its correctness w.r.t. the original sorted data; a problem we tackle using sampling a subset of Poisson-drawn size from each discretized bin. Lastly, since any distribution can be reduced to a continuous distribution, our algorithm is successfully carried to multiple other families of distributions and thus has numerous applications.\"}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f16088ff7a96775260e2b6bf5b1061b7e41ec0ad.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsheffet2024differentially,\\ntitle={Differentially Private Equivalence Testing for Continuous Distributions and Applications},\\nauthor={Or Sheffet and Daniel Omer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qDuqp1nZZ6}\\n}'}, 'paperhash': {'value': 'sheffet|differentially_private_equivalence_testing_for_continuous_distributions_and_applications'}},forum = 'qDuqp1nZZ6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10848/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10848/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10848/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10848/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qDfPSWXSLt',number = 2445,cdate = 1715072340035,pdate = 1727287690349,odate = 1730873858047,mdate = 1730873858065,tcdate = 1715072340035,tmdate = 1730873858065,ddate = None,content = {'title': {'value': 'Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting'}, 'authors': {'value': ['Ziyi Yang', 'Xinyu Gao', 'Yang-Tian Sun', 'Yi-Hua Huang', 'Xiaoyang Lyu', 'Wen Zhou', 'Shaohui Jiao', 'XIAOJUAN QI', 'Xiaogang Jin']}, 'authorids': {'value': ['~Ziyi_Yang4', '~Xinyu_Gao1', '~Yang-Tian_Sun1', '~Yi-Hua_Huang1', '~Xiaoyang_Lyu1', '~Wen_Zhou10', '~Shaohui_Jiao1', '~XIAOJUAN_QI2', '~Xiaogang_Jin1']}, 'keywords': {'value': ['Gaussian Splatting', 'specular highlights modeling', 'real-time rendering']}, 'abstract': {'value': 'The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS  frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/318102c1ccdb1f36d39b709bb4becf3633743e6f.pdf'}, 'TLDR': {'value': 'We employ ASG appearance field to enhance the 3D-GS for modeling scenes with specular highlights.'}, 'supplementary_material': {'value': '/attachment/d8ff4bc7ce08fcc450e5e5c50974d1f9421a1be8.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024specgaussian,\\ntitle={Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting},\\nauthor={Ziyi Yang and Xinyu Gao and Yang-Tian Sun and Yi-Hua Huang and Xiaoyang Lyu and Wen Zhou and Shaohui Jiao and XIAOJUAN QI and Xiaogang Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qDfPSWXSLt}\\n}'}, 'paperhash': {'value': 'yang|specgaussian_anisotropic_viewdependent_appearance_for_3d_gaussian_splatting'}},forum = 'qDfPSWXSLt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2445/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2445/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2445/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2445/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qCpCy0EQAJ',number = 15877,cdate = 1715764481318,pdate = 1727288109920,odate = 1730873975380,mdate = 1730873975401,tcdate = 1715764481318,tmdate = 1730873975401,ddate = None,content = {'title': {'value': 'Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets'}, 'authors': {'value': ['Vijaya Raghavan T Ramkumar', 'Elahe Arani', 'Bahram Zonooz']}, 'authorids': {'value': ['~Vijaya_Raghavan_T_Ramkumar1', '~Elahe_Arani1', '~Bahram_Zonooz1']}, 'keywords': {'value': ['Small datasets Generalization', 'Overfitting', 'Iterative training', 'Neurogenesis']}, 'TLDR': {'value': \"A novel iterative learning paradigm with data-aware dynamic masking removes redundant connections, increases DNNs' capacity for learning, and improves generalization on small datasets.\"}, 'abstract': {'value': \"The efficacy of deep learning techniques is contingent upon access to large volumes of data (labeled or unlabeled). However, in practical domains such as medical applications, data availability is often limited. This presents a significant challenge: How can we effectively train deep neural networks on relatively small datasets while improving generalization? Recent works have explored evolutionary or iterative training paradigms, which reinitialize a subset of parameters to enhance generalization performance for small datasets. However, these methods typically rely on randomly selected parameter subsets and maintain fixed masks throughout training, potentially leading to suboptimal outcomes. Inspired by neurogenesis in the brain, we propose a novel iterative training framework, Dynamic Neural Regeneration (DNR), that employs a data-aware dynamic masking scheme to eliminate redundant connections by estimating their significance. This approach increases the model's capacity for further learning through random weight reinitialization. Experimental results demonstrate that our approach outperforms existing methods in accuracy and robustness, highlighting its potential for real-world applications where data collection is challenging.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a7d867f3aeb3174151ac0c7855cc7e821c712892.pdf'}, '_bibtex': {'value': '@inproceedings{\\nramkumar2024dynamic,\\ntitle={Dynamic Neural Regeneration: Enhancing Deep Learning Generalization on Small Datasets},\\nauthor={Vijaya Raghavan T Ramkumar and Elahe Arani and Bahram Zonooz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qCpCy0EQAJ}\\n}'}, 'paperhash': {'value': 'ramkumar|dynamic_neural_regeneration_enhancing_deep_learning_generalization_on_small_datasets'}},forum = 'qCpCy0EQAJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15877/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15877/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15877/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15877/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'qCJ1dq5M7N',number = 1137,cdate = 1714420809649,pdate = 1727287653026,odate = 1730873845715,mdate = 1730873845738,tcdate = 1714420809649,tmdate = 1730873845738,ddate = None,content = {'title': {'value': 'FouRA: Fourier Low-Rank Adaptation'}, 'authors': {'value': ['Shubhankar Borse', 'Shreya Kadambi', 'Nilesh Prasad Pandey', 'Kartikeya Bhardwaj', 'Viswanath Ganapathy', 'Sweta Priyadarshi', 'Risheek Garrepalli', 'Rafael Esteves', 'Munawar Hayat', 'Fatih Porikli']}, 'authorids': {'value': ['~Shubhankar_Borse1', '~Shreya_Kadambi1', '~Nilesh_Prasad_Pandey1', '~Kartikeya_Bhardwaj3', '~Viswanath_Ganapathy1', '~Sweta_Priyadarshi1', '~Risheek_Garrepalli1', '~Rafael_Esteves1', '~Munawar_Hayat2', '~Fatih_Porikli2']}, 'keywords': {'value': ['Low Rank Adapters', 'Fourier Transform', 'Generative Models']}, 'abstract': {'value': 'While Low-Rank Adaptation (LoRA) has proven beneficial for efficiently fine-tuning large models, LoRA fine-tuned text-to-image diffusion models lack diversity in the generated images, as the model tends to copy data from the observed training samples. This effect becomes more pronounced at higher values of adapter strength and for adapters with higher ranks which are fine-tuned on smaller datasets. To address these challenges, we present FouRA, a novel low-rank method that learns projections in the Fourier domain along with learning a flexible input-dependent adapter rank selection strategy. Through extensive experiments and analysis, we show that FouRA successfully solves the problems related to data copying and distribution collapse while significantly improving the generated image quality. We demonstrate that FouRA enhances the generalization of fine-tuned models thanks to its adaptive rank selection. We further show that the learned projections in the frequency domain are decorrelated and prove effective when merging multiple adapters. While FouRA is motivated for vision tasks, we also demonstrate its merits for language tasks on commonsense reasoning and GLUE benchmarks.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2fc90e37f14bb97913686628fb9da31aa3b72204.pdf'}, '_bibtex': {'value': '@inproceedings{\\nborse2024foura,\\ntitle={Fou{RA}: Fourier Low-Rank Adaptation},\\nauthor={Shubhankar Borse and Shreya Kadambi and Nilesh Prasad Pandey and Kartikeya Bhardwaj and Viswanath Ganapathy and Sweta Priyadarshi and Risheek Garrepalli and Rafael Esteves and Munawar Hayat and Fatih Porikli},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qCJ1dq5M7N}\\n}'}, 'paperhash': {'value': 'borse|foura_fourier_lowrank_adaptation'}},forum = 'qCJ1dq5M7N',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1137/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1137/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1137/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1137/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'qAP6RyYIJc',number = 19646,cdate = 1715792145764,pdate = 1727288214599,odate = 1730873996828,mdate = 1730873996850,tcdate = 1715792145764,tmdate = 1730873996850,ddate = None,content = {'title': {'value': 'Stealth edits to large language models'}, 'authors': {'value': ['Oliver Sutton', 'Qinghua Zhou', 'Wei Wang', 'Desmond Higham', 'Alexander N. Gorban', 'Alexander Bastounis', 'Ivan Y Tyukin']}, 'authorids': {'value': ['~Oliver_Sutton1', '~Qinghua_Zhou1', '~Wei_Wang113', '~Desmond_Higham1', '~Alexander_N._Gorban2', '~Alexander_Bastounis1', '~Ivan_Y_Tyukin1']}, 'keywords': {'value': ['large language models', 'stealth attacks', 'memory editing']}, 'TLDR': {'value': 'We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining.'}, 'abstract': {'value': \"We reveal the theoretical foundations of techniques for editing large language models, and present new methods which can do so without requiring retraining. Our theoretical insights show that a single metric (a measure of the intrinsic dimension of the model's features) can be used to assess a model's editability and reveals its previously unrecognised susceptibility to malicious *stealth attacks*. This metric is fundamental to predicting the success of a variety of editing approaches, and reveals new bridges between disparate families of editing methods. We collectively refer to these as *stealth editing* methods, because they directly update a model's weights to specify its response to specific known hallucinating prompts without affecting other model behaviour. By carefully applying our theoretical insights, we are able to introduce a new *jet-pack* network block which is optimised for highly selective model editing, uses only standard network operations, and can be inserted into existing networks. We also reveal the vulnerability of language models to stealth attacks: a small change to a model's weights which fixes its response to a single attacker-chosen prompt. Stealth attacks are computationally simple, do not require access to or knowledge of the model's training data, and therefore represent a potent yet previously unrecognised threat to redistributed foundation models. Extensive experimental results illustrate and support our methods and their theoretical underpinnings. Demos and source code are available at https://github.com/qinghua-zhou/stealth-edits.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/78f0aabe3822aa144ee58c3dbd985671f0e398aa.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsutton2024stealth,\\ntitle={Stealth edits to large language models},\\nauthor={Oliver Sutton and Qinghua Zhou and Wei Wang and Desmond Higham and Alexander N. Gorban and Alexander Bastounis and Ivan Y Tyukin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=qAP6RyYIJc}\\n}'}, 'paperhash': {'value': 'sutton|stealth_edits_to_large_language_models'}},forum = 'qAP6RyYIJc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19646/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19646/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19646/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19646/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'q9dKv1AK6l',number = 19456,cdate = 1715791036198,pdate = 1727288210175,odate = 1730873995733,mdate = 1737023438884,tcdate = 1715791036198,tmdate = 1737023438884,ddate = None,content = {'title': {'value': 'Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates'}, 'authors': {'value': ['Jincheng Mei', 'Bo Dai', 'Alekh Agarwal', 'Sharan Vaswani', 'Anant Raj', 'Csaba Szepesvari', 'Dale Schuurmans']}, 'authorids': {'value': ['~Jincheng_Mei1', '~Bo_Dai1', '~Alekh_Agarwal2', '~Sharan_Vaswani1', '~Anant_Raj2', '~Csaba_Szepesvari1', '~Dale_Schuurmans1']}, 'keywords': {'value': ['stochastic gradient bandit', 'arbitrary stepsize', 'global convergence']}, 'TLDR': {'value': 'stochastic gradient bandit algorithm converges to a globally optimal policy almost surely using any constant learning rate'}, 'abstract': {'value': 'We provide a new understanding of the stochastic gradient bandit algorithm by showing that it converges to a globally optimal policy almost surely using \\\\emph{any} constant learning rate. This result demonstrates that the stochastic gradient algorithm continues to balance exploration and exploitation appropriately even in scenarios where standard smoothness and noise control assumptions break down. The proofs are based on novel findings about action sampling rates and the relationship between cumulative progress and noise, and extend the current understanding of how simple stochastic gradient methods behave in bandit settings.'}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0bbd1798a4031e02af33d9abe0df4e2209ac83a5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmei2024small,\\ntitle={Small steps no more: Global convergence of stochastic gradient bandits for arbitrary learning rates},\\nauthor={Jincheng Mei and Bo Dai and Alekh Agarwal and Sharan Vaswani and Anant Raj and Csaba Szepesvari and Dale Schuurmans},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=q9dKv1AK6l}\\n}'}, 'paperhash': {'value': 'mei|small_steps_no_more_global_convergence_of_stochastic_gradient_bandits_for_arbitrary_learning_rates'}},forum = 'q9dKv1AK6l',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19456/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19456/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19456/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19456/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'q9RLsvYOB3',number = 8140,cdate = 1715654176922,pdate = 1727287870124,odate = 1730873909833,mdate = 1736737584705,tcdate = 1715654176922,tmdate = 1736737584705,ddate = None,content = {'title': {'value': 'FlexPlanner: Flexible 3D Floorplanning via Deep Reinforcement Learning in Hybrid Action Space with Multi-Modality Representation'}, 'authors': {'value': ['Ruizhe Zhong', 'Xingbo Du', 'Shixiong Kai', 'Zhentao Tang', 'Siyuan Xu', 'Jianye HAO', 'Mingxuan Yuan', 'Junchi Yan']}, 'authorids': {'value': ['~Ruizhe_Zhong1', '~Xingbo_Du1', '~Shixiong_Kai1', '~Zhentao_Tang1', '~Siyuan_Xu5', '~Jianye_HAO1', '~Mingxuan_Yuan1', '~Junchi_Yan2']}, 'keywords': {'value': ['Floorplanning', '3D Floorplanning', 'EDA', 'Reinforcement Learning']}, 'abstract': {'value': 'In the Integrated Circuit (IC) design flow, floorplanning (FP) determines the position and shape of each block. Serving as a prototype for downstream tasks, it is critical and establishes the upper bound of the final PPA (Power, Performance, Area). However, with the emergence of 3D IC with stacked layers, existing methods are not flexible enough to handle the versatile constraints. Besides, they typically face difficulties in aligning the cross-die modules in 3D ICs due to their heuristic representations, which could potentially result in severe data transfer failures. To address these issues, we propose FlexPlanner, a flexible learning-based method in hybrid action space with multi-modality representation to simultaneously handle position, aspect ratio, and alignment of blocks. To our best knowledge, FlexPlanner is the first learning-based approach to discard heuristic-based search in the 3D FP task. Thus, the solution space is not limited by the heuristic floorplanning representation, allowing for significant improvements in both wirelength and alignment scores. Specifically, FlexPlanner models 3D FP based on multi-modalities, including vision, graph, and sequence. To address the non-trivial heuristic-dependent issue, we design a sophisticated policy network with hybrid action space and asynchronous layer decision mechanism that allow for determining the versatile properties of each block. Experiments on public benchmarks MCNC and GSRC show the effectiveness. We significantly improve the alignment score from 0.474 to 0.940 and achieve an average reduction of 16% in wirelength. Moreover, our method also demonstrates zero-shot transferability on unseen circuits.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c0e4fa9a645d6802617ca69252eb2fb05f9faddd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhong2024flexplanner,\\ntitle={FlexPlanner: Flexible 3D Floorplanning via Deep Reinforcement Learning in Hybrid Action Space with Multi-Modality Representation},\\nauthor={Ruizhe Zhong and Xingbo Du and Shixiong Kai and Zhentao Tang and Siyuan Xu and Jianye HAO and Mingxuan Yuan and Junchi Yan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=q9RLsvYOB3}\\n}'}, 'paperhash': {'value': 'zhong|flexplanner_flexible_3d_floorplanning_via_deep_reinforcement_learning_in_hybrid_action_space_with_multimodality_representation'}},forum = 'q9RLsvYOB3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8140/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8140/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8140/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8140/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'q7TxGUWlhD',number = 13850,cdate = 1715743544965,pdate = 1727288052408,odate = 1730873961111,mdate = 1730873961131,tcdate = 1715743544965,tmdate = 1730873961131,ddate = None,content = {'title': {'value': 'N-agent Ad Hoc Teamwork'}, 'authors': {'value': ['Caroline Wang', 'Arrasy Rahman', 'Ishan Durugkar', 'Elad Liebman', 'Peter Stone']}, 'authorids': {'value': ['~Caroline_Wang1', '~Arrasy_Rahman1', '~Ishan_Durugkar1', '~Elad_Liebman1', '~Peter_Stone1']}, 'keywords': {'value': ['ad hoc teamwork', 'reinforcement learning', 'multi-agent systems', 'multi-agent reinforcement learning']}, 'TLDR': {'value': 'Proposes a generalization of ad hoc teamwork to the N-agent setting.'}, 'abstract': {'value': 'Current approaches to learning cooperative multi-agent behaviors assume relatively restrictive settings. In standard fully cooperative multi-agent reinforcement learning, the learning algorithm controls *all* agents in the scenario, while in ad hoc teamwork, the learning algorithm usually assumes control over only a *single* agent in the scenario. However, many cooperative settings in the real world are much less restrictive. For example, in an autonomous driving scenario,  a company might train its cars with the same learning algorithm, yet once on the road, these cars must cooperate with cars from another company. Towards expanding the class of scenarios that cooperative learning methods may optimally address, we introduce $N$*-agent ad hoc teamwork* (NAHT), where a set of autonomous agents must interact and cooperate with dynamically varying numbers and types of teammates. This paper formalizes the problem, and proposes the *Policy Optimization with Agent Modelling* (POAM) algorithm. POAM is a policy gradient, multi-agent reinforcement learning approach to the NAHT problem, that enables adaptation to diverse teammate behaviors by learning representations of teammate behaviors. Empirical evaluation on tasks from the multi-agent particle environment and StarCraft II shows that POAM improves cooperative task returns compared to baseline approaches, and enables out-of-distribution generalization to unseen teammates.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b2a493d4f38a4116108b0ba02a974d3b686c5421.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024nagent,\\ntitle={N-agent Ad Hoc Teamwork},\\nauthor={Caroline Wang and Arrasy Rahman and Ishan Durugkar and Elad Liebman and Peter Stone},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=q7TxGUWlhD}\\n}'}, 'paperhash': {'value': 'wang|nagent_ad_hoc_teamwork'}},forum = 'q7TxGUWlhD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13850/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13850/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13850/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13850/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'q5CkneUn6K',number = 4290,cdate = 1715398123570,pdate = 1727287746649,odate = 1730873874653,mdate = 1737130903088,tcdate = 1715398123570,tmdate = 1737130903088,ddate = None,content = {'title': {'value': 'Enhancing LLM’s Cognition via Structurization'}, 'authors': {'value': ['Kai Liu', 'Zhihang Fu', 'Chao Chen', 'Wei Zhang', 'Rongxin Jiang', 'Fan Zhou', 'Yaowu Chen', 'Yue Wu', 'Jieping Ye']}, 'authorids': {'value': ['~Kai_Liu8', '~Zhihang_Fu1', '~Chao_Chen19', '~Wei_Zhang103', '~Rongxin_Jiang1', '~Fan_Zhou13', '~Yaowu_Chen2', '~Yue_Wu18', '~Jieping_Ye4']}, 'keywords': {'value': ['Large Language Models', 'Structurization', 'Augmentation']}, 'abstract': {'value': 'When reading long-form text, human cognition is complex and structurized. While large language models (LLMs) process input contexts through a causal and sequential perspective, this approach can potentially limit their ability to handle intricate and complex inputs effectively. To enhance LLM’s cognition capability, this paper presents a novel concept of context structurization. Specifically, we transform the plain, unordered contextual sentences into well-ordered and hierarchically structurized elements. By doing so, LLMs can better grasp intricate and extended contexts through precise attention and information-seeking along the organized structures. Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval). Empirical results show consistent and significant performance gains afforded by a single-round structurization. In particular, we boost the open-sourced LLaMA2-70B model to achieve comparable performance against GPT-3.5-Turbo as the halluci- nation evaluator. Besides, we show the feasibility of distilling advanced LLMs’ language processing abilities to a smaller yet effective StruXGPT-7B to execute structurization, addressing the practicality of our approach. Code is available at https://github.com/alibaba/struxgpt.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c68bdb95ee2d720998ae9511792e68fcfb9d7185.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nliu2024enhancing,\\ntitle={Enhancing {LLM}{\\\\textquoteright}s Cognition via Structurization},\\nauthor={Kai Liu and Zhihang Fu and Chao Chen and Wei Zhang and Rongxin Jiang and Fan Zhou and Yaowu Chen and Yue Wu and Jieping Ye},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=q5CkneUn6K}\\n}'}, 'paperhash': {'value': 'liu|enhancing_llms_cognition_via_structurization'}},forum = 'q5CkneUn6K',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4290/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4290/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4290/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission4290/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'q3XavKPorV',number = 14469,cdate = 1715750323917,pdate = 1727288070401,odate = 1730873966080,mdate = 1730873966104,tcdate = 1715750323917,tmdate = 1730873966104,ddate = None,content = {'title': {'value': 'Self-Play Fine-tuning of Diffusion Models for Text-to-image Generation'}, 'authors': {'value': ['Huizhuo Yuan', 'Zixiang Chen', 'Kaixuan Ji', 'Quanquan Gu']}, 'authorids': {'value': ['~Huizhuo_Yuan1', '~Zixiang_Chen1', '~Kaixuan_Ji2', '~Quanquan_Gu1']}, 'keywords': {'value': ['Diffusion models; Stable Diffusion; RLHF; Self-play']}, 'abstract': {'value': \"Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images (``winner'' and ``loser'' images) for each text prompt.\\nIn this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data. Codes are available at \\\\url{https://github.com/uclaml/SPIN-Diffusion/}.\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9d5d2794e81f70938ba8c3030bf9444cec8d00a9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyuan2024selfplay,\\ntitle={Self-Play Fine-tuning of Diffusion Models for Text-to-image Generation},\\nauthor={Huizhuo Yuan and Zixiang Chen and Kaixuan Ji and Quanquan Gu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=q3XavKPorV}\\n}'}, 'paperhash': {'value': 'yuan|selfplay_finetuning_of_diffusion_models_for_texttoimage_generation'}},forum = 'q3XavKPorV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14469/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14469/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14469/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14469/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'pzJjlnMvk5',number = 3211,cdate = 1715243145367,pdate = 1727287713108,odate = 1730873864262,mdate = 1730873864278,tcdate = 1715243145367,tmdate = 1730873864278,ddate = None,content = {'title': {'value': 'NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory'}, 'authors': {'value': ['Navami Kairanda', 'Marc Habermann', 'Christian Theobalt', 'Vladislav Golyanik']}, 'authorids': {'value': ['~Navami_Kairanda1', '~Marc_Habermann1', '~Christian_Theobalt2', '~Vladislav_Golyanik1']}, 'keywords': {'value': ['Neural field', 'implicit representation', 'cloth simulation', 'consistent simulation', 'multi-resolution', 'Kirchhoff-Love theory']}, 'abstract': {'value': 'Despite existing 3D cloth simulators producing realistic results, they predominantly operate on discrete surface representations (e.g. points and meshes) with a fixed spatial resolution, which often leads to large memory consumption and resolution-dependent simulations. Moreover, back-propagating gradients through the existing solvers is difficult and they hence cannot be easily integrated into modern neural architectures. In response, this paper re-thinks physically plausible cloth simulation: We propose NeuralClothSim, i.e., a new quasistatic cloth simulator using thin shells, in which surface deformation is encoded in neural network weights in form of a neural field. Our memory-efficient solver operates on a new continuous coordinate-based surface representation called neural deformation fields (NDFs); it supervises NDF equilibria with the laws of the non-linear Kirchhoff-Love shell theory with a non-linear anisotropic material model. NDFs are adaptive: They 1) allocate their capacity to the deformation details and 2) allow surface state queries at arbitrary spatial resolutions without re-training. We show how to train NeuralClothSim while imposing hard boundary conditions and demonstrate multiple applications, such as material interpolation and simulation editing. The experimental results highlight the effectiveness of our continuous neural formulation.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3dec444f18c27dd63e0072ceec22ae0f3f5068a9.pdf'}, 'supplementary_material': {'value': '/attachment/3d03d8213f50c0ea2648865edb9d7001562ea4bd.zip'}, 'TLDR': {'value': 'We propose a new quasistatic cloth simulator using thin shells, in which surface deformation is encoded in neural network weights as a neural field.'}, '_bibtex': {'value': '@inproceedings{\\nkairanda2024neuralclothsim,\\ntitle={NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory},\\nauthor={Navami Kairanda and Marc Habermann and Christian Theobalt and Vladislav Golyanik},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pzJjlnMvk5}\\n}'}, 'paperhash': {'value': 'kairanda|neuralclothsim_neural_deformation_fields_meet_the_thin_shell_theory'}},forum = 'pzJjlnMvk5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3211/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3211/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3211/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3211/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pyqPUf36D2',number = 7116,cdate = 1715612935574,pdate = 1727287836042,odate = 1730873899556,mdate = 1730873899570,tcdate = 1715612935574,tmdate = 1730873899570,ddate = None,content = {'title': {'value': 'Pseudo-Private Data Guided Model Inversion Attacks'}, 'authors': {'value': ['Xiong Peng', 'Bo Han', 'Feng Liu', 'Tongliang Liu', 'Mingyuan Zhou']}, 'authorids': {'value': ['~Xiong_Peng1', '~Bo_Han1', '~Feng_Liu2', '~Tongliang_Liu1', '~Mingyuan_Zhou1']}, 'keywords': {'value': ['Model Inversion Attacks']}, 'abstract': {'value': 'In model inversion attacks (MIAs), adversaries attempt to recover private training data by exploiting access to a well-trained target model. Recent advancements have improved MIA performance using a two-stage generative framework. This approach first employs a generative adversarial network to learn a fixed distributional prior, which is then used to guide the inversion process during the attack. However, in this paper, we observed a phenomenon that such a fixed prior would lead to a low probability of sampling actual private data during the inversion process due to the inherent distribution gap between the prior distribution and the private data distribution, thereby constraining attack performance. To address this limitation, we propose increasing the density around high-quality pseudo-private data—recovered samples through model inversion that exhibit characteristics of the private training data—by slightly tuning the generator. This strategy effectively increases the probability of sampling actual private data that is close to these pseudo-private data during the inversion process. After integrating our method, the generative model inversion pipeline is strengthened, leading to improvements over state-of-the-art MIAs. This paves the way for new research directions in generative MIAs.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/ab952f8b298019580810ea5dc7040dc50c241775.zip'}, 'pdf': {'value': '/pdf/d4eb8bbc60ca2fb9ed32715d6b6b8c101d79cb91.pdf'}, '_bibtex': {'value': '@inproceedings{\\npeng2024pseudoprivate,\\ntitle={Pseudo-Private Data Guided Model Inversion Attacks},\\nauthor={Xiong Peng and Bo Han and Feng Liu and Tongliang Liu and Mingyuan Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pyqPUf36D2}\\n}'}, 'paperhash': {'value': 'peng|pseudoprivate_data_guided_model_inversion_attacks'}},forum = 'pyqPUf36D2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7116/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7116/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7116/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7116/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pwRVGRWtGg',number = 13766,cdate = 1715742841060,pdate = 1727288049652,odate = 1730873960287,mdate = 1730873960306,tcdate = 1715742841060,tmdate = 1730873960306,ddate = None,content = {'title': {'value': \"Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans\"}, 'authors': {'value': ['Jen-tse Huang', 'Man Ho LAM', 'Eric John Li', 'Shujie Ren', 'Wenxuan Wang', 'Wenxiang Jiao', 'Zhaopeng Tu', 'Michael Lyu']}, 'authorids': {'value': ['~Jen-tse_Huang1', '~Man_Ho_LAM1', '~Eric_John_Li1', '~Shujie_Ren1', '~Wenxuan_Wang2', '~Wenxiang_Jiao1', '~Zhaopeng_Tu1', '~Michael_Lyu1']}, 'keywords': {'value': ['LLM', 'Evaluation', 'Emotions']}, 'TLDR': {'value': \"We introduce EmotionBench (which includes eight negative emotions) to evaluate LLMs' emotional alignments with human norms collected from >1200 human responses.\"}, 'abstract': {'value': 'Evaluating Large Language Models’ (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4d6e71e0ca7fffae0c70fd69763ea99167e3d197.pdf'}, 'supplementary_material': {'value': '/attachment/b1009fd2f333180134e04f3f7deb2e53c12aeb2a.zip'}, '_bibtex': {'value': \"@inproceedings{\\nhuang2024apathetic,\\ntitle={Apathetic or Empathetic? Evaluating {LLM}s' Emotional Alignments with Humans},\\nauthor={Jen-tse Huang and Man Ho LAM and Eric John Li and Shujie Ren and Wenxuan Wang and Wenxiang Jiao and Zhaopeng Tu and Michael Lyu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pwRVGRWtGg}\\n}\"}, 'paperhash': {'value': 'huang|apathetic_or_empathetic_evaluating_llms_emotional_alignments_with_humans'}},forum = 'pwRVGRWtGg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13766/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13766/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13766/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13766/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pwLdvYIMrF',number = 14970,cdate = 1715755694612,pdate = 1727288084125,odate = 1730873969598,mdate = 1737783769595,tcdate = 1715755694612,tmdate = 1737783769595,ddate = None,content = {'title': {'value': 'Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning'}, 'authors': {'value': ['Yeongbin Seo', 'Dongha Lee', 'Jinyoung Yeo']}, 'authorids': {'value': ['~Yeongbin_Seo1', '~Dongha_Lee1', '~Jinyoung_Yeo1']}, 'keywords': {'value': ['continual learning', 'continual knowledge learning', 'large language models', 'meta-learning', 'train-attention', 'token weight']}, 'abstract': {'value': 'Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. The code and the dataset are available online.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2d2fc4beb4ba2418dd2a4c680959b5708e85b13e.pdf'}, 'supplementary_material': {'value': '/attachment/ee3f367218a95940bdb9e8012b9a8b9870276c78.zip'}, 'TLDR': {'value': 'enhancing continual knowledge learning performance through meta-learning based token weighted learning method'}, '_bibtex': {'value': '@inproceedings{\\nyeongbin2024trainattention,\\ntitle={Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning},\\nauthor={Yeongbin Seo and Dongha Lee and Jinyoung Yeo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pwLdvYIMrF}\\n}'}, 'paperhash': {'value': 'seo|trainattention_metalearning_where_to_focus_in_continual_knowledge_learning'}},forum = 'pwLdvYIMrF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14970/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14970/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14970/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14970/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pwKkNSuuEs',number = 14672,cdate = 1715752514775,pdate = 1727288076338,odate = 1730873967751,mdate = 1736100176968,tcdate = 1715752514775,tmdate = 1736100176968,ddate = None,content = {'title': {'value': 'Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification'}, 'authors': {'value': ['Yunshi Wen', 'Tengfei Ma', 'Tsui-Wei Weng', 'Lam M. Nguyen', 'Anak Agung Julius']}, 'authorids': {'value': ['~Yunshi_Wen1', '~Tengfei_Ma1', '~Tsui-Wei_Weng1', '~Lam_M._Nguyen1', '~Anak_Agung_Julius1']}, 'keywords': {'value': ['Time-series', 'Interpretability', 'Self-supervised Learning', 'Pre-trained Model']}, 'TLDR': {'value': 'A generalizable, interpretable, pre-trained model for time-series modeling and classification.'}, 'abstract': {'value': 'In time-series analysis, many recent works seek to provide a unified view and representation for time-series across multiple domains, leading to the development of foundation models for time-series data. Despite diverse modeling techniques, existing models are black boxes and fail to provide insights and explanations about their representations. In this paper, we present VQShape, a pre-trained, generalizable, and interpretable model for time-series representation learning and classification. By introducing a novel representation for time-series data, we forge a connection between the latent space of VQShape and shape-level features. Using vector quantization, we show that time-series from different domains can be described using a unified set of low-dimensional codes, where each code can be represented as an abstracted shape in the time domain. On classification tasks, we show that the representations of VQShape can be utilized to build interpretable classifiers, achieving comparable performance to specialist models. Additionally, in zero-shot learning, VQShape and its codebook can generalize to previously unseen datasets and domains that are not included in the pre-training process. The code and pre-trained weights are available at https://github.com/YunshiWen/VQShape.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7dcbe405896b35c34e94f65d2ab90d9a8ee10a82.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwen2024abstracted,\\ntitle={Abstracted Shapes as Tokens - A Generalizable and Interpretable Model for Time-series Classification},\\nauthor={Yunshi Wen and Tengfei Ma and Tsui-Wei Weng and Lam M. Nguyen and Anak Agung Julius},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pwKkNSuuEs}\\n}'}, 'paperhash': {'value': 'wen|abstracted_shapes_as_tokens_a_generalizable_and_interpretable_model_for_timeseries_classification'}},forum = 'pwKkNSuuEs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14672/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14672/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14672/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14672/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'psG4LXlDNs',number = 13555,cdate = 1715740728953,pdate = 1727288042897,odate = 1730873958692,mdate = 1730873958713,tcdate = 1715740728953,tmdate = 1730873958713,ddate = None,content = {'title': {'value': 'Achieving $\\\\tilde{O}(1/\\\\epsilon)$ Sample Complexity for Constrained Markov Decision Process'}, 'authors': {'value': ['Jiashuo Jiang', 'Yinyu Ye']}, 'authorids': {'value': ['~Jiashuo_Jiang1', '~Yinyu_Ye1']}, 'keywords': {'value': ['constrained MDP', 'reinforcement learning', 'online linear programming']}, 'abstract': {'value': 'We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\\\\frac{1}{\\\\Delta\\\\cdot\\\\epsilon}\\\\cdot\\\\log^2(1/\\\\epsilon))$ sample complexity bound, with $\\\\Delta$ being a problem-dependent parameter, yet independent of $\\\\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\\\\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms of the dependency on $\\\\epsilon$. To achieve this advance, we develop a new framework for analyzing CMDP problems. To be specific, our algorithm operates in the primal space and we resolve the primal LP for the CMDP problem at each period in an online manner, with \\\\textit{adaptive} remaining resource capacities. The key elements of our algorithm are: i) a characterization of the instance hardness via LP basis, ii) an eliminating procedure that identifies one optimal basis of the primal LP, and; iii) a resolving procedure that is adaptive to the remaining resources and sticks to the characterized optimal basis.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d266944c7c83f38bc65d9643812af49872f309c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiang2024achieving,\\ntitle={Achieving \\\\${\\\\textbackslash}tilde\\\\{O\\\\}(1/{\\\\textbackslash}epsilon)\\\\$ Sample Complexity for Constrained Markov Decision Process},\\nauthor={Jiashuo Jiang and Yinyu Ye},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=psG4LXlDNs}\\n}'}, 'paperhash': {'value': 'jiang|achieving_\\\\tildeo1\\\\epsilon_sample_complexity_for_constrained_markov_decision_process'}},forum = 'psG4LXlDNs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13555/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13555/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13555/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13555/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'psDrko9v1D',number = 342,cdate = 1713870734841,pdate = 1727287635417,odate = 1730873840050,mdate = 1734591987191,tcdate = 1713870734841,tmdate = 1734591987191,ddate = None,content = {'title': {'value': 'Efficient Combinatorial Optimization via Heat Diffusion'}, 'authors': {'value': ['Hengyuan Ma', 'Wenlian Lu', 'Jianfeng Feng']}, 'authorids': {'value': ['~Hengyuan_Ma1', '~Wenlian_Lu1', '~Jianfeng_Feng2']}, 'keywords': {'value': ['Combinatorial optimization', 'Heat equation']}, 'abstract': {'value': \"Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature. The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/61bd38e29a750a8f1021582b9a9180767853efdf.pdf'}, 'TLDR': {'value': 'We propose a new framework for combinatorial optimization based the insight that heat diffusion can propagate information from distant area to the solver for efficient navigation.'}, '_bibtex': {'value': '@inproceedings{\\nma2024efficient,\\ntitle={Efficient Combinatorial Optimization via Heat Diffusion},\\nauthor={Hengyuan Ma and Wenlian Lu and Jianfeng Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=psDrko9v1D}\\n}'}, 'supplementary_material': {'value': '/attachment/6ebe041c2a6204dbbe820c64633ed1e5855c2364.zip'}, 'paperhash': {'value': 'ma|efficient_combinatorial_optimization_via_heat_diffusion'}},forum = 'psDrko9v1D',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission342/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission342/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission342/-/Revision', 'NeurIPS.cc/2024/Conference/Submission342/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'prgxz9fYbf',number = 9566,cdate = 1715681856687,pdate = 1727287914889,odate = 1730873921872,mdate = 1730873921892,tcdate = 1715681856687,tmdate = 1730873921892,ddate = None,content = {'title': {'value': 'Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines'}, 'authors': {'value': ['Edward Milsom', 'Ben Anson', 'Laurence Aitchison']}, 'authorids': {'value': ['~Edward_Milsom1', '~Ben_Anson1', '~Laurence_Aitchison1']}, 'keywords': {'value': ['gaussian process', 'deep gaussian process', 'kernel methods', 'representation learning']}, 'TLDR': {'value': 'We improve an existing kernel method to achieve 94.5% test accuracy on CIFAR-10, a significant increase over the current SOTA for kernel methods.'}, 'abstract': {'value': 'Recent work developed convolutional deep kernel machines, achieving 92.7% test accuracy on CIFAR-10 using a ResNet-inspired architecture, which is SOTA for kernel methods. However, this still lags behind neural networks, which easily achieve over 94% test accuracy with similar architectures. In this work we introduce several modifications to improve the convolutional deep kernel machine’s generalisation, including stochastic kernel regularisation, which adds noise to the learned Gram matrices during training. The resulting model achieves 94.5% test accuracy on CIFAR-10. This finding has important theoretical and practical implications, as it demonstrates that the ability to perform well on complex tasks like image classification is not unique to neural networks. Instead, other approaches including deep kernel methods can achieve excellent performance on such tasks, as long as they have the capacity to learn representations from data.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ba510678d2e5eebc0c16d764b5c28f2674005597.pdf'}, 'supplementary_material': {'value': '/attachment/43f32caf210fb3c8264ea08f31531fae2a5b18a2.zip'}, '_bibtex': {'value': '@inproceedings{\\nmilsom2024stochastic,\\ntitle={Stochastic Kernel Regularisation Improves Generalisation in Deep Kernel Machines},\\nauthor={Edward Milsom and Ben Anson and Laurence Aitchison},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=prgxz9fYbf}\\n}'}, 'paperhash': {'value': 'milsom|stochastic_kernel_regularisation_improves_generalisation_in_deep_kernel_machines'}},forum = 'prgxz9fYbf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9566/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9566/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9566/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9566/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'preo49P1VY',number = 3122,cdate = 1715231891291,pdate = 1727287710977,odate = 1730873863710,mdate = 1730873863728,tcdate = 1715231891291,tmdate = 1730873863728,ddate = None,content = {'title': {'value': 'Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers'}, 'authors': {'value': ['Sukjun Hwang', 'Aakash Lahoti', 'Ratish Puduppully', 'Tri Dao', 'Albert Gu']}, 'authorids': {'value': ['~Sukjun_Hwang1', '~Aakash_Lahoti1', '~Ratish_Puduppully1', '~Tri_Dao1', '~Albert_Gu1']}, 'keywords': {'value': ['Deep learning', 'sequence models', 'state space models', 'mamba']}, 'abstract': {'value': 'A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers.  This paper studies a unifying *matrix mixer* view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed *sequence alignment*, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (**Hydra**), parameterized as a *quasiseparable matrix mixer*, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, \\\\name outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/975e174ed724f910ffb63bad51c2578e145b4c50.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhwang2024hydra,\\ntitle={Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers},\\nauthor={Sukjun Hwang and Aakash Lahoti and Ratish Puduppully and Tri Dao and Albert Gu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=preo49P1VY}\\n}'}, 'paperhash': {'value': 'hwang|hydra_bidirectional_state_space_models_through_generalized_matrix_mixers'}},forum = 'preo49P1VY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3122/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3122/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3122/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3122/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'prXfM5X2Db',number = 10607,cdate = 1715695783392,pdate = 1727287946412,odate = 1730873930994,mdate = 1736013643755,tcdate = 1715695783392,tmdate = 1736013643755,ddate = None,content = {'title': {'value': 'Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching'}, 'authors': {'value': ['Yongqi Wang', 'Wenxiang Guo', 'Rongjie Huang', 'Jiawei Huang', 'Zehan Wang', 'Fuming You', 'Ruiqi Li', 'Zhou Zhao']}, 'authorids': {'value': ['~Yongqi_Wang1', '~Wenxiang_Guo1', '~Rongjie_Huang1', '~Jiawei_Huang5', '~Zehan_Wang2', '~Fuming_You3', '~Ruiqi_Li2', '~Zhou_Zhao3']}, 'keywords': {'value': ['video-to-audio generation', 'rectified flow model', 'efficient generation']}, 'TLDR': {'value': 'We design a video-to-audio generation model with higher quality and fewer sampling steps.'}, 'abstract': {'value': 'Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. \\nWe propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22\\\\%, and 6.2\\\\% improvement in inception score over the strong diffusion-based baseline. Audio samples and code are available at http://frieren-v2a.github.io.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c239fa113e1666d0b63e267c415f2d45354382c0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024frieren,\\ntitle={Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching},\\nauthor={Yongqi Wang and Wenxiang Guo and Rongjie Huang and Jiawei Huang and Zehan Wang and Fuming You and Ruiqi Li and Zhou Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=prXfM5X2Db}\\n}'}, 'paperhash': {'value': 'wang|frieren_efficient_videotoaudio_generation_network_with_rectified_flow_matching'}},forum = 'prXfM5X2Db',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10607/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10607/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10607/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10607/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pqi4vqBYXW',number = 4654,cdate = 1715436831741,pdate = 1727287756835,odate = 1730873877860,mdate = 1730873877870,tcdate = 1715436831741,tmdate = 1730873877870,ddate = None,content = {'title': {'value': 'Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation'}, 'authors': {'value': ['Hongbo Wang', 'Jie Cao', 'Jin Liu', 'Xiaoqiang Zhou', 'Huaibo Huang', 'Ran He']}, 'authorids': {'value': ['~Hongbo_Wang3', '~Jie_Cao2', '~Jin_Liu10', '~Xiaoqiang_Zhou2', '~Huaibo_Huang1', '~Ran_He1']}, 'keywords': {'value': ['3D Generation', 'Hallucination Detection and Mitigation', 'Multi-modal Inference', 'View Alignment']}, 'TLDR': {'value': \"We address the 'Janus Problem' in 3D content generation with a novel method that leverages multimodal models to enhance multi-view consistency without requiring 3D training data.\"}, 'abstract': {'value': 'Recent advancements in 3D content generation have been significant, primarily due to the visual priors provided by pretrained diffusion models. However, large 2D visual models exhibit spatial perception hallucinations, leading to multi-view inconsistency in 3D content generated through Score Distillation Sampling (SDS). This phenomenon, characterized by overfitting to specific views, is referred to as the \"Janus Problem\". In this work, we investigate the hallucination issues of pretrained models and find that large multimodal models without geometric constraints possess the capability to infer geometric structures, which can be utilized to mitigate multi-view inconsistency. Building on this, we propose a novel tuning-free method. We represent the multimodal inconsistency query information to detect specific hallucinations in 3D content, using this as an enhanced prompt to re-consist the 2D renderings of 3D and jointly optimize the structure and appearance across different views. Our approach does not require 3D training data and can be implemented plug-and-play within existing frameworks. Extensive experiments demonstrate that our method significantly improves the consistency of 3D content generation and specifically mitigates hallucinations caused by pretrained large models, achieving state-of-the-art performance compared to other optimization methods.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fb79eff5c9130f4e3bb3bf8229453ab45ce5a1f3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024hallod,\\ntitle={Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation},\\nauthor={Hongbo Wang and Jie Cao and Jin Liu and Xiaoqiang Zhou and Huaibo Huang and Ran He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pqi4vqBYXW}\\n}'}, 'paperhash': {'value': 'wang|hallo3d_multimodal_hallucination_detection_and_mitigation_for_consistent_3d_content_generation'}},forum = 'pqi4vqBYXW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4654/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4654/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4654/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4654/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pqYceEa87j',number = 5292,cdate = 1715521296874,pdate = 1727287779322,odate = 1730873883672,mdate = 1735147130534,tcdate = 1715521296874,tmdate = 1735147130534,ddate = None,content = {'title': {'value': 'Spectral Editing of Activations for Large Language Model Alignment'}, 'authors': {'value': ['Yifu QIU', 'Zheng Zhao', 'Yftah Ziser', 'Anna Korhonen', 'Edoardo Ponti', 'Shay B Cohen']}, 'authorids': {'value': ['~Yifu_QIU1', '~Zheng_Zhao2', '~Yftah_Ziser1', '~Anna_Korhonen1', '~Edoardo_Ponti1', '~Shay_B_Cohen1']}, 'keywords': {'value': ['Large Language Model', 'Alignment', 'Spectral Decomposition', 'Representation Engineering', 'Model Editing']}, 'abstract': {'value': 'Large language models (LLMs) often exhibit undesirable behaviours, such as generating untruthful or biased content. Editing their internal representations has been shown to be effective in mitigating such behaviours on top of the existing alignment methods. We propose a novel inference-time editing method, namely spectral editing of activations (SEA), to project the input representations into directions with maximal covariance with the positive demonstrations (e.g., truthful) while minimising covariance with the negative demonstrations (e.g., hallucinated). We also extend our method to non-linear editing using feature functions. We run extensive experiments on benchmarks concerning truthfulness and bias with six open-source LLMs of different sizes and model families. The results demonstrate the superiority of SEA in effectiveness, generalisation to similar tasks, as well as computation and data efficiency. We also show that SEA editing only has a limited negative impact on other model capabilities.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': \"We propose a novel inference-time editing method for LLM's activations, namely spectral editing of activations (SEA), to align the LLMs with the objectives in truthfulness and bias.\"}, 'pdf': {'value': '/pdf/309a57afcf8bfef9e232743dc0f09597f4f7b601.pdf'}, 'supplementary_material': {'value': '/attachment/635b5790922c9248dd48bf2c276381cbb0b7a78d.zip'}, '_bibtex': {'value': '@inproceedings{\\nqiu2024spectral,\\ntitle={Spectral Editing of Activations for Large Language Model Alignment},\\nauthor={Yifu QIU and Zheng Zhao and Yftah Ziser and Anna Korhonen and Edoardo Ponti and Shay B Cohen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pqYceEa87j}\\n}'}, 'paperhash': {'value': 'qiu|spectral_editing_of_activations_for_large_language_model_alignment'}},forum = 'pqYceEa87j',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5292/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5292/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5292/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5292/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'pqD7ckR8AF',number = 10803,cdate = 1715697865873,pdate = 1727287951757,odate = 1730873932297,mdate = 1734566029530,tcdate = 1715697865873,tmdate = 1734566029530,ddate = None,content = {'title': {'value': 'SuperDeepFool: a new fast and accurate minimal adversarial attack'}, 'authors': {'value': ['Alireza Abdolahpourrostam', 'Mahed Abroshan', 'Seyed-Mohsen Moosavi-Dezfooli']}, 'authorids': {'value': ['~Alireza_Abdolahpourrostam1', '~Mahed_Abroshan1', '~Seyed-Mohsen_Moosavi-Dezfooli1']}, 'keywords': {'value': ['Deep Learning', 'Adversarial Attacks', 'Robustness', 'Interpretable AI', 'ML Security']}, 'TLDR': {'value': 'We have introduced a family of parameter-free, fast, and parallelizable algorithms for crafting optimal adversarial perturbations.'}, 'abstract': {'value': 'Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal $\\\\ell_{2}$ adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal $\\\\ell_{2}$ adversarial perturbations.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5c55feebf4e4e8379a8f6d39df9ec3dbd17d77b4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nabdolahpourrostam2024superdeepfool,\\ntitle={SuperDeepFool: a new fast and accurate minimal adversarial attack},\\nauthor={Alireza Abdolahpourrostam and Mahed Abroshan and Seyed-Mohsen Moosavi-Dezfooli},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pqD7ckR8AF}\\n}'}, 'paperhash': {'value': 'abdolahpourrostam|superdeepfool_a_new_fast_and_accurate_minimal_adversarial_attack'}},forum = 'pqD7ckR8AF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10803/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10803/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10803/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10803/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'poE54GOq2l',number = 18804,cdate = 1715787489400,pdate = 1727288193868,odate = 1730873991885,mdate = 1730873991906,tcdate = 1715787489400,tmdate = 1730873991906,ddate = None,content = {'title': {'value': 'SnapKV: LLM Knows What You are Looking for Before Generation'}, 'authors': {'value': ['Yuhong Li', 'Yingbing Huang', 'Bowen Yang', 'Bharat Venkitesh', 'Acyr Locatelli', 'Hanchen Ye', 'Tianle Cai', 'Patrick Lewis', 'Deming Chen']}, 'authorids': {'value': ['~Yuhong_Li2', '~Yingbing_Huang1', '~Bowen_Yang1', '~Bharat_Venkitesh1', '~Acyr_Locatelli1', '~Hanchen_Ye1', '~Tianle_Cai1', '~Patrick_Lewis2', '~Deming_Chen1']}, 'keywords': {'value': ['Large Language Model', 'Key-Value Cache Compression', 'Natural Language Processing', 'Inference', 'Machine Learning']}, 'abstract': {'value': \"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.\\n\\nWe discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/552afd2aab769168da2c7e0bc8b7d95d6f98725d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024snapkv,\\ntitle={Snap{KV}: {LLM} Knows What You are Looking for Before Generation},\\nauthor={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=poE54GOq2l}\\n}'}, 'paperhash': {'value': 'li|snapkv_llm_knows_what_you_are_looking_for_before_generation'}},forum = 'poE54GOq2l',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18804/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18804/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18804/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18804/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pnmUiVAGnv',number = 2241,cdate = 1715004055883,pdate = 1727287684627,odate = 1730873856677,mdate = 1730873856698,tcdate = 1715004055883,tmdate = 1730873856698,ddate = None,content = {'title': {'value': 'CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation'}, 'authors': {'value': ['Zhongzhen Huang', 'Yankai Jiang', 'Rongzhao Zhang', 'Shaoting Zhang', 'Xiaofan Zhang']}, 'authorids': {'value': ['~Zhongzhen_Huang1', '~Yankai_Jiang1', '~Rongzhao_Zhang1', '~Shaoting_Zhang4', '~Xiaofan_Zhang2']}, 'keywords': {'value': ['Promptable model', 'Visual-Textual prompt', 'Multi-organ and tumor segmentation']}, 'abstract': {'value': 'Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors. Specifically, we introduce $\\\\textbf{\\\\textit{CAT}}$, an innovative model that $\\\\textbf{C}$oordinates $\\\\textbf{A}$natomical prompts derived from 3D cropped images with $\\\\textbf{T}$extual prompts enriched by medical domain knowledge. The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction. To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts. Trained on a consortium of 10 public CT datasets, $\\\\textbf{\\\\textit{CAT}}$ demonstrates superior performance in multiple segmentation tasks. Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages. This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We intro a novel automatic model that coordinates anatomical-textual prompts for multi-organ and tumor segmentation.'}, 'pdf': {'value': '/pdf/0dcf67da2efd207182eda04b8b667b49326c2479.pdf'}, 'supplementary_material': {'value': '/attachment/f349e7c7b2cd60cddaeda8b41f4825e127649975.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024cat,\\ntitle={{CAT}: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation},\\nauthor={Zhongzhen Huang and Yankai Jiang and Rongzhao Zhang and Shaoting Zhang and Xiaofan Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pnmUiVAGnv}\\n}'}, 'paperhash': {'value': 'huang|cat_coordinating_anatomicaltextual_prompts_for_multiorgan_and_tumor_segmentation'}},forum = 'pnmUiVAGnv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2241/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2241/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2241/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2241/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'plH8gW7tPQ',number = 8580,cdate = 1715664751899,pdate = 1727287884258,odate = 1730873913165,mdate = 1730873913183,tcdate = 1715664751899,tmdate = 1730873913183,ddate = None,content = {'title': {'value': 'Algorithmic Capabilities of Random Transformers'}, 'authors': {'value': ['Ziqian Zhong', 'Jacob Andreas']}, 'authorids': {'value': ['~Ziqian_Zhong1', '~Jacob_Andreas1']}, 'keywords': {'value': ['transformer', 'deep learning', 'interpretability', 'capability', 'emergence', 'randomness', 'language models']}, 'TLDR': {'value': 'Randomly initialized transformers may be more powerful than you think!'}, 'abstract': {'value': 'Trained transformer models have been found to implement interpretable procedures for tasks like arithmetic and associative recall, but little is understood about how the circuits that implement these procedures originate during training. To what extent do they depend on the supervisory signal provided to models, and to what extent are they attributable to behavior already present in models at the beginning of training? To investigate these questions, we investigate what functions can be learned by randomly initialized transformers in which only the embedding layers are optimized, so that the only input--output mappings learnable from data are those already implemented (up to a choice of encoding scheme) by the randomly initialized model. We find that these random transformers can perform a wide range of meaningful algorithmic tasks, including modular arithmetic, in-weights and in-context associative recall, decimal addition, parenthesis balancing, and even some aspects of natural language text generation. Our results indicate that some algorithmic capabilities are present in transformers (and accessible via appropriately structured inputs) even before these models are trained.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/edb6b6c967e15383353752d484f3eeea16da6215.pdf'}, 'supplementary_material': {'value': '/attachment/d226e75cd84d1a4dbc5ca1a8e02159ca8a793ea3.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhong2024algorithmic,\\ntitle={Algorithmic Capabilities of Random Transformers},\\nauthor={Ziqian Zhong and Jacob Andreas},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=plH8gW7tPQ}\\n}'}, 'paperhash': {'value': 'zhong|algorithmic_capabilities_of_random_transformers'}},forum = 'plH8gW7tPQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8580/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8580/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8580/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8580/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pje1Y71jad',number = 13626,cdate = 1715741539028,pdate = 1727288045217,odate = 1730873959377,mdate = 1730873959395,tcdate = 1715741539028,tmdate = 1730873959395,ddate = None,content = {'title': {'value': 'Cost-efficient Knowledge-based Question Answering with Large Language Models'}, 'authors': {'value': ['Junnan Dong', 'Qinggang Zhang', 'Chuang Zhou', 'Hao Chen', 'Daochen Zha', 'Xiao Huang']}, 'authorids': {'value': ['~Junnan_Dong1', '~Qinggang_Zhang2', '~Chuang_Zhou1', '~Hao_Chen18', '~Daochen_Zha1', '~Xiao_Huang1']}, 'keywords': {'value': ['Large Language Models', 'Knowledge-based Question Answering', 'Efficient Machine Learning']}, 'abstract': {'value': 'Knowledge-based question answering (KBQA) is widely used in many scenarios that necessitate domain knowledge. Large language models (LLMs) bring opportunities to KBQA, while their costs are significantly higher and absence of domain-specific knowledge during pre-training. We are motivated to combine LLMs and prior small models on knowledge graphs (KGMs) for both inferential accuracy and cost saving. However, it remains challenging since accuracy and cost are not readily combined in the optimization as two distinct metrics. It is also laborious for model selection since different models excel in diverse knowledge. To this end, we propose Coke, a novel cost-efficient strategy for KBQA with LLMs, modeled as a tailored multi-armed bandit problem to minimize calls to LLMs within limited budgets. We first formulate the accuracy expectation with a cluster-level Thompson Sampling for either KGMs or LLMs. A context-aware policy is optimized to further distinguish the expert model subject to the question semantics. The overall decision is bounded by the cost regret according to historical expenditure on failures. Extensive experiments showcase the superior performance of Coke, which moves the Pareto frontier with up to 20.89% saving of GPT-4 fees while achieving a 2.74% higher accuracy on the benchmark datasets.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper moves the Pareto frontier of both inferential accuracy and cost saving for knowledge-based question answering with large langue models.'}, 'pdf': {'value': '/pdf/0c4dc789433d497c2f2c0f0da165be3b5c9f715b.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024costefficient,\\ntitle={Cost-efficient Knowledge-based Question Answering with Large Language Models},\\nauthor={Junnan Dong and Qinggang Zhang and Chuang Zhou and Hao Chen and Daochen Zha and Xiao Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pje1Y71jad}\\n}'}, 'paperhash': {'value': 'dong|costefficient_knowledgebased_question_answering_with_large_language_models'}},forum = 'pje1Y71jad',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13626/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13626/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13626/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13626/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pjD08dtAh0',number = 299,cdate = 1713861801095,pdate = 1727287634776,odate = 1730873839840,mdate = 1730873839858,tcdate = 1713861801095,tmdate = 1730873839858,ddate = None,content = {'title': {'value': 'HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid'}, 'authors': {'value': ['Xinyu Xu', 'Yizheng Zhang', 'Yong-Lu Li', 'Lei Han', 'Cewu Lu']}, 'authorids': {'value': ['~Xinyu_Xu2', '~Yizheng_Zhang1', '~Yong-Lu_Li1', '~Lei_Han1', '~Cewu_Lu3']}, 'keywords': {'value': ['Human-Scene Interaction; Object Rearrangement; Vision-Language-Action Model; Physical Humanoid']}, 'abstract': {'value': 'Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications. \\n    However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications.\\n    To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language. \\n    A teacher-student framework is utilized to develop HumanVLA.\\n    A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior.\\n    Then, it is distilled into a vision-language-action model via behavior cloning.\\n    We propose several key insights to facilitate the large-scale learning process.\\n    To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks.\\n    Through extensive experiments and analysis, we demonstrate the effectiveness of our approach.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce HumanVLA, which performs a varity of object rearrangement tasks directed by vision and language by physical humanoid.'}, 'pdf': {'value': '/pdf/ad32b2e9331156429744c3a06a443f8e2b0be44a.pdf'}, 'supplementary_material': {'value': '/attachment/46fa8c802381832248958f02765a7f740a0f3ea2.zip'}, '_bibtex': {'value': '@inproceedings{\\nxu2024humanvla,\\ntitle={Human{VLA}: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid},\\nauthor={Xinyu Xu and Yizheng Zhang and Yong-Lu Li and Lei Han and Cewu Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pjD08dtAh0}\\n}'}, 'paperhash': {'value': 'xu|humanvla_towards_visionlanguage_directed_object_rearrangement_by_physical_humanoid'}},forum = 'pjD08dtAh0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission299/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission299/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission299/-/Revision', 'NeurIPS.cc/2024/Conference/Submission299/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'piOzFx9whU',number = 9708,cdate = 1715684410635,pdate = 1727287918579,odate = 1730873922698,mdate = 1730873922724,tcdate = 1715684410635,tmdate = 1730873922724,ddate = None,content = {'title': {'value': 'Wasserstein Distributionally Robust Optimization through the Lens of Structural Causal Models and Individual Fairness'}, 'authors': {'value': ['Ahmad Reza Ehyaei', 'Golnoosh Farnadi', 'Samira Samadi']}, 'authorids': {'value': ['~Ahmad_Reza_Ehyaei1', '~Golnoosh_Farnadi1', '~Samira_Samadi1']}, 'keywords': {'value': ['Wasserstein Distributionally Robust Optimization', 'Individual Fairness', 'Structural Causal Model', 'Regularized Optimization']}, 'abstract': {'value': 'In recent years, Wasserstein Distributionally Robust Optimization (DRO) has garnered substantial interest for its efficacy in data-driven decision-making under distributional uncertainty. However, limited research has explored the application of DRO to address individual fairness concerns, particularly when considering causal structures and discrete sensitive attributes in learning problems.\\nTo address this gap, we first formulate the DRO problem from the perspectives of causality and individual fairness. We then present the DRO dual formulation as an efficient tool to convert the main problem into a more tractable and computationally efficient form. Next, we characterize the closed form of the approximate worst-case loss quantity as a regularizer, eliminating the max-step in the Min-Max DRO problem. We further estimate the regularizer in more general cases and explore the relationship between DRO and classical robust optimization. Finally, by removing the assumption of a known structural causal model, we provide finite sample error bounds when designing DRO with empirical distributions and estimated causal structures to ensure efficiency and robust learning.'}, 'primary_area': {'value': 'fairness'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8b6d32560285d89309a13bf12325235ba025cc2c.pdf'}, 'supplementary_material': {'value': '/attachment/36b96fbb84d779888fed7a50cb401eb86d3ce4e7.zip'}, '_bibtex': {'value': '@inproceedings{\\nehyaei2024wasserstein,\\ntitle={Wasserstein Distributionally Robust Optimization through the Lens of Structural Causal Models and Individual Fairness},\\nauthor={Ahmad Reza Ehyaei and Golnoosh Farnadi and Samira Samadi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=piOzFx9whU}\\n}'}, 'paperhash': {'value': 'ehyaei|wasserstein_distributionally_robust_optimization_through_the_lens_of_structural_causal_models_and_individual_fairness'}},forum = 'piOzFx9whU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9708/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9708/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9708/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9708/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pgUQFIJ6BE',number = 3977,cdate = 1715346559267,pdate = 1727287736374,odate = 1730873871167,mdate = 1734707520517,tcdate = 1715346559267,tmdate = 1734707520517,ddate = None,content = {'title': {'value': 'Near-Optimal Distributed Minimax Optimization under the Second-Order Similarity'}, 'authors': {'value': ['Qihao Zhou', 'Haishan Ye', 'Luo Luo']}, 'authorids': {'value': ['~Qihao_Zhou1', '~Haishan_Ye2', '~Luo_Luo1']}, 'keywords': {'value': ['distributed optimization', 'minimax optmization', 'second-order similarity']}, 'abstract': {'value': 'This paper considers the distributed convex-concave minimax optimization under the second-order similarity.\\nWe propose stochastic variance-reduced optimistic gradient sliding (SVOGS) method, which takes the advantage of the finite-sum structure in the objective by involving the mini-batch client sampling and variance reduction.\\nWe prove SVOGS can achieve the $\\\\varepsilon$-duality gap within communication rounds of \\n${\\\\mathcal O}(\\\\delta D^2/\\\\varepsilon)$, \\ncommunication complexity of ${\\\\mathcal O}(n+\\\\sqrt{n}\\\\delta D^2/\\\\varepsilon)$,\\nand local gradient calls of \\n$\\\\tilde{\\\\mathcal O}(n+(\\\\sqrt{n}\\\\delta+L)D^2/\\\\varepsilon\\\\log(1/\\\\varepsilon))$, \\nwhere $n$ is the number of nodes, $\\\\delta$ is the degree of the second-order similarity, $L$ is the smoothness parameter and $D$ is the diameter of the constraint set.\\nWe can verify that all of above complexity (nearly) matches the corresponding lower bounds.\\nFor the specific $\\\\mu$-strongly-convex-$\\\\mu$-strongly-convex case, \\nour algorithm has the upper bounds on communication rounds,  communication complexity, and local gradient calls of $\\\\mathcal O(\\\\delta/\\\\mu\\\\log(1/\\\\varepsilon))$, ${\\\\mathcal O}((n+\\\\sqrt{n}\\\\delta/\\\\mu)\\\\log(1/\\\\varepsilon))$, and $\\\\tilde{\\\\mathcal O}(n+(\\\\sqrt{n}\\\\delta+L)/\\\\mu)\\\\log(1/\\\\varepsilon))$ respectively, which are also nearly tight.\\nFurthermore, we conduct the numerical experiments to show the empirical advantages of proposed method.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b064e9a30204be0e7d128cbb02db397e56aba3b8.pdf'}, 'supplementary_material': {'value': '/attachment/75e8ea30251fc488f76336345898287bb1fc8544.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024nearoptimal,\\ntitle={Near-Optimal Distributed Minimax Optimization under the Second-Order Similarity},\\nauthor={Qihao Zhou and Haishan Ye and Luo Luo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pgUQFIJ6BE}\\n}'}, 'paperhash': {'value': 'zhou|nearoptimal_distributed_minimax_optimization_under_the_secondorder_similarity'}},forum = 'pgUQFIJ6BE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3977/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3977/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3977/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3977/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pgQCsyKdpN',number = 14497,cdate = 1715750628595,pdate = 1727288071338,odate = 1730873966398,mdate = 1736951793755,tcdate = 1715750628595,tmdate = 1736951793755,ddate = None,content = {'title': {'value': 'AdaptiveISP: Learning an Adaptive Image Signal Processor for Object Detection'}, 'authors': {'value': ['Yujin Wang', 'Xu Tian yi', 'Zhang Fan', 'Tianfan Xue', 'Jinwei Gu']}, 'authorids': {'value': ['~Yujin_Wang2', '~Xu_Tian_yi1', '~Zhang_Fan1', '~Tianfan_Xue2', '~Jinwei_Gu1']}, 'keywords': {'value': ['Image Signal Processor', 'Image Processing', 'Reinforcement Learning']}, 'abstract': {'value': 'Image Signal Processors (ISPs) convert raw sensor signals into digital images, which significantly influence the image quality and the performance of downstream computer vision tasks. \\n    Designing ISP pipeline and tuning ISP parameters are two key steps for building an imaging and vision system.\\n    To find optimal ISP configurations, recent works use deep neural networks as a proxy to search for ISP parameters or ISP pipelines. However, these methods are primarily designed to maximize the image quality, which are sub-optimal in the performance of high-level computer vision tasks such as detection, recognition, and tracking. Moreover, after training, the learned ISP pipelines are mostly fixed at the inference time, whose performance degrades in dynamic scenes. \\n    To jointly optimize ISP structures and parameters, we propose AdaptiveISP, a task-driven and scene-adaptive ISP. \\n    One key observation is that for the majority of input images, only a few processing modules are needed to improve the performance of downstream recognition tasks, and only a few inputs require more processing.\\n    Based on this, AdaptiveISP utilizes deep reinforcement learning to automatically generate an optimal ISP pipeline and the associated ISP parameters to maximize the detection performance. Experimental results show that AdaptiveISP not only surpasses the prior state-of-the-art methods for object detection but also dynamically manages the trade-off between detection performance and computational cost, especially suitable for scenes with large dynamic range variations.\\nProject website: https://openimaginglab.github.io/AdaptiveISP/.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6921203f4240132b3df9ea0162c0fe9a74c4e59a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024adaptiveisp,\\ntitle={Adaptive{ISP}: Learning an Adaptive Image Signal Processor for Object Detection},\\nauthor={Yujin Wang and Xu Tian yi and Zhang Fan and Tianfan Xue and Jinwei Gu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pgQCsyKdpN}\\n}'}, 'paperhash': {'value': 'wang|adaptiveisp_learning_an_adaptive_image_signal_processor_for_object_detection'}},forum = 'pgQCsyKdpN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14497/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14497/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14497/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14497/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pfvcsgFrJ6',number = 9142,cdate = 1715675318324,pdate = 1727287902001,odate = 1730873918002,mdate = 1734566022998,tcdate = 1715675318324,tmdate = 1734566022998,ddate = None,content = {'title': {'value': 'On Causal Discovery in the Presence of Deterministic Relations'}, 'authors': {'value': ['Loka Li', 'Haoyue Dai', 'Hanin Al Ghothani', 'Biwei Huang', 'Jiji Zhang', 'Shahar Harel', 'Isaac Bentwich', 'Guangyi Chen', 'Kun Zhang']}, 'authorids': {'value': ['~Loka_Li1', '~Haoyue_Dai1', '~Hanin_Al_Ghothani1', '~Biwei_Huang1', '~Jiji_Zhang1', '~Shahar_Harel2', '~Isaac_Bentwich1', '~Guangyi_Chen1', '~Kun_Zhang1']}, 'keywords': {'value': ['causal discovery', 'deterministic relations']}, 'abstract': {'value': 'Many causal discovery methods typically rely on the assumption of independent noise, yet real-life situations often involve deterministic relationships. In these cases, observed variables are represented as deterministic functions of their parental variables without noise.\\nWhen determinism is present, constraint-based methods encounter challenges due to the violation of the faithfulness assumption. In this paper, we find, supported by both theoretical analysis and empirical evidence, that score-based methods with exact search can naturally address the issues of deterministic relations under rather mild assumptions. Nonetheless, exact score-based methods can be computationally expensive. To enhance the efficiency and scalability, we develop a novel framework for causal discovery that can detect and handle deterministic relations, called Determinism-aware Greedy Equivalent Search (DGES). DGES comprises three phases: (1) identify minimal deterministic clusters (i.e., a minimal set of variables with deterministic relationships), (2) run modified Greedy Equivalent Search (GES) to obtain an initial graph, and (3) perform exact search exclusively on the deterministic cluster and its neighbors. The proposed DGES accommodates both linear and nonlinear causal relationships, as well as both continuous and discrete data types. Furthermore, we investigate the identifiability conditions of DGES. We conducted extensive experiments on both simulated and real-world datasets to show the efficacy of our proposed method.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper proposes a novel method called DGES to conduct causal discovery with deterministic relations.'}, 'pdf': {'value': '/pdf/ce7a7f7e4d625dc499209811e78bd27b12c56ca4.pdf'}, 'supplementary_material': {'value': '/attachment/e564eb57d44d2c413ed7b99b7e0af0e2f09d1c17.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024on,\\ntitle={On Causal Discovery in the Presence of Deterministic Relations},\\nauthor={Loka Li and Haoyue Dai and Hanin Al Ghothani and Biwei Huang and Jiji Zhang and Shahar Harel and Isaac Bentwich and Guangyi Chen and Kun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pfvcsgFrJ6}\\n}'}, 'paperhash': {'value': 'li|on_causal_discovery_in_the_presence_of_deterministic_relations'}},forum = 'pfvcsgFrJ6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9142/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9142/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9142/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9142/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pf4OuJyn4Q',number = 12175,cdate = 1715717696154,pdate = 1727287997319,odate = 1730873946274,mdate = 1730873946314,tcdate = 1715717696154,tmdate = 1730873946314,ddate = None,content = {'title': {'value': 'Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms'}, 'authors': {'value': ['Rafael Rafailov', 'Yaswanth Chittepu', 'Ryan Park', 'Harshit Sikchi', 'Joey Hejna', 'W. Bradley Knox', 'Chelsea Finn', 'Scott Niekum']}, 'authorids': {'value': ['~Rafael_Rafailov1', '~Yaswanth_Chittepu1', '~Ryan_Park1', '~Harshit_Sikchi1', '~Joey_Hejna1', '~W._Bradley_Knox2', '~Chelsea_Finn1', '~Scott_Niekum1']}, 'keywords': {'value': ['Reinforcement Learning From Human Feedback', 'Direct Preference Optimization', 'Reward Hacking']}, 'TLDR': {'value': 'We define and explore the reward over-optimization phenomenon in direct alignment algorithms, such as DPO'}, 'abstract': {'value': 'Reinforcement Learning from Human Feedback (RLHF)has been crucial to the recent success of Large Language Models (LLMs), however it is often a complex and brittle process. In the classical RLHF framework, a reward model is first trained to represent human preferences, which is in turn used by an online reinforcement learning (RL) algorithm to optimized the LLM. A prominent issue with such methods is reward over-optimization or reward hacking, where the performance as measured by the learned proxy reward model increases, but the true model quality plateaus or even deteriorates. Direct Alignment Algorithms (DDAs), such as Direct Preference Optimization (DPO) have emerged as alternatives to the classical RLHF pipeline. However, despite not training a separate proxy reward model or using RL, they still commonly deteriorate from over-optimization. While the so-called reward hacking phenomenon is not well-defined for DAAs, we still uncover similar trends: at higher KL-budgets, DAA algorithms exhibit similar degradation patters to their classic RLHF counterparts. In particular, we find that DAA methods deteriorate not only across a wide range of KL-budgets, but also often before even a single epoch of the dataset is completed. Through extensive empirical experimentation this work formulates the reward over-optimization or hacking problem for DAAs and explores its consequences across objectives, training regimes, and model scales.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e28365fb2dd2a9b94f8225bee790989091ef456f.pdf'}, 'supplementary_material': {'value': '/attachment/a328dc0d62a6e5c792f1e0aca02f183839ead34d.zip'}, '_bibtex': {'value': '@inproceedings{\\nrafailov2024scaling,\\ntitle={Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms},\\nauthor={Rafael Rafailov and Yaswanth Chittepu and Ryan Park and Harshit Sikchi and Joey Hejna and W. Bradley Knox and Chelsea Finn and Scott Niekum},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pf4OuJyn4Q}\\n}'}, 'paperhash': {'value': 'rafailov|scaling_laws_for_reward_model_overoptimization_in_direct_alignment_algorithms'}},forum = 'pf4OuJyn4Q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12175/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12175/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12175/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12175/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pebP89l4v6',number = 1452,cdate = 1714639011381,pdate = 1727287661526,odate = 1730873848965,mdate = 1736242613066,tcdate = 1714639011381,tmdate = 1736242613066,ddate = None,content = {'title': {'value': 'Sharing Key Semantics in Transformer Makes Efficient Image Restoration'}, 'authors': {'value': ['Bin Ren', 'Yawei Li', 'Jingyun Liang', 'Rakesh Ranjan', 'Mengyuan Liu', 'Rita Cucchiara', 'Luc Van Gool', 'Ming-Hsuan Yang', 'Nicu Sebe']}, 'authorids': {'value': ['~Bin_Ren2', '~Yawei_Li1', '~Jingyun_Liang1', '~Rakesh_Ranjan2', '~Mengyuan_Liu2', '~Rita_Cucchiara1', '~Luc_Van_Gool1', '~Ming-Hsuan_Yang1', '~Nicu_Sebe1']}, 'keywords': {'value': ['Low-level Vision', 'Image Restoration', 'Vision Transformer']}, 'abstract': {'value': \"Image Restoration (IR), a classic low-level vision task, has witnessed significant advancements through deep models that effectively model global information. Notably, the emergence of Vision Transformers (ViTs) has further propelled these advancements. When computing, the self-attention mechanism, a cornerstone of ViTs, tends to encompass all global cues, even those from semantically unrelated objects or regions. This inclusivity introduces computational inefficiencies, particularly noticeable with high input resolution, as it requires processing irrelevant information, thereby impeding efficiency. Additionally, for IR, it is commonly noted that small segments of a degraded image, particularly those closely aligned semantically, provide particularly relevant information to aid in the restoration process, as they contribute essential contextual cues crucial for accurate reconstruction. To address these challenges, we propose boosting IR's performance by sharing the key semantics via Transformer for IR (i.e., SemanIR) in this paper. Specifically, SemanIR initially constructs a sparse yet comprehensive key-semantic dictionary within each transformer stage by establishing essential semantic connections for every degraded patch. Subsequently, this dictionary is shared across all subsequent transformer blocks within the same stage. This strategy optimizes attention calculation within each block by focusing exclusively on semantically related components stored in the key-semantic dictionary. As a result, attention calculation achieves linear computational complexity within each window. Extensive experiments across 6 IR tasks confirm the proposed SemanIR's state-of-the-art performance, quantitatively and qualitatively showcasing advancements. The visual results, code, and trained models are available at: https://github.com/Amazingren/SemanIR.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a48850ffc0e7efafed75334a90640999a96ee448.pdf'}, 'supplementary_material': {'value': '/attachment/951d033e5c576600fe755692c7c9878192d3d59a.zip'}, '_bibtex': {'value': '@inproceedings{\\nren2024sharing,\\ntitle={Sharing Key Semantics in Transformer Makes Efficient Image Restoration},\\nauthor={Bin Ren and Yawei Li and Jingyun Liang and Rakesh Ranjan and Mengyuan Liu and Rita Cucchiara and Luc Van Gool and Ming-Hsuan Yang and Nicu Sebe},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pebP89l4v6}\\n}'}, 'TLDR': {'value': 'We propose SemanIR, a Transformer-based approach for image restoration that optimizes attention computation by focusing on semantically relevant regions, achieving linear complexity and state-of-the-art results across multiple tasks.'}, 'paperhash': {'value': 'ren|sharing_key_semantics_in_transformer_makes_efficient_image_restoration'}},forum = 'pebP89l4v6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1452/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1452/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1452/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1452/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pc4GSBi1Hx',number = 6770,cdate = 1715604524404,pdate = 1727287825107,odate = 1730873896026,mdate = 1736834559324,tcdate = 1715604524404,tmdate = 1736834559324,ddate = None,content = {'title': {'value': 'LoTLIP: Improving Language-Image Pre-training for Long Text Understanding'}, 'authors': {'value': ['Wei Wu', 'Kecheng Zheng', 'Shuailei Ma', 'Fan Lu', 'Yuxin Guo', 'Yifei Zhang', 'Wei Chen', 'Qingpei Guo', 'Yujun Shen', 'Zheng-Jun Zha']}, 'authorids': {'value': ['~Wei_Wu20', '~Kecheng_Zheng2', '~Shuailei_Ma1', '~Fan_Lu4', '~Yuxin_Guo2', '~Yifei_Zhang4', '~Wei_Chen34', '~Qingpei_Guo1', '~Yujun_Shen1', '~Zheng-Jun_Zha2']}, 'keywords': {'value': ['Language-Image Pre-Training']}, 'abstract': {'value': 'In this work, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. Towards this problem, our initial attempt is to relabel the data with long captions, however, directly learning with which may lead to performance degradation in understanding short text (e.g., in the image classification task). Then, after incorporating corner tokens to aggregate diverse textual information, we manage to help the model catch up to its original level of short text understanding yet greatly enhance its capability of long text understanding. We further look into whether the model can continuously benefit from longer captions and notice a clear trade-off between the performance and the efficiency. Finally, we validate the effectiveness of our approach using a self-constructed large-scale dataset, which consists of 100M long caption oriented text-image pairs. Our method achieves superior performance in long-text-image retrieval tasks. The project page is available at https://wuw2019.github.io/lot-lip.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/518a23baa775e8d55d373e5cac99a56ac9171bb1.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024lotlip,\\ntitle={Lo{TLIP}: Improving Language-Image Pre-training for Long Text Understanding},\\nauthor={Wei Wu and Kecheng Zheng and Shuailei Ma and Fan Lu and Yuxin Guo and Yifei Zhang and Wei Chen and Qingpei Guo and Yujun Shen and Zheng-Jun Zha},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pc4GSBi1Hx}\\n}'}, 'paperhash': {'value': 'wu|lotlip_improving_languageimage_pretraining_for_long_text_understanding'}},forum = 'pc4GSBi1Hx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6770/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6770/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6770/-/Revision', 'NeurIPS.cc/2024/Conference/-/Desk_Rejected_Submission', 'NeurIPS.cc/2024/Conference/Submission6770/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'paobkszgIA',number = 4600,cdate = 1715431904518,pdate = 1727287755269,odate = 1730873877374,mdate = 1730873877394,tcdate = 1715431904518,tmdate = 1730873877394,ddate = None,content = {'title': {'value': 'End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning'}, 'authors': {'value': ['Xin Yang', 'YAN WENDING', 'Michael Bi Mi', 'Yuan Yuan', 'Robby T. Tan']}, 'authorids': {'value': ['~Xin_Yang13', '~YAN_WENDING1', '~Michael_Bi_Mi1', '~Yuan_Yuan12', '~Robby_T._Tan1']}, 'keywords': {'value': ['optical-flow free', 'video semantic segmentation', 'adverse conditions']}, 'abstract': {'value': 'Adverse weather conditions can significantly degrade the video frames, causing existing video semantic segmentation methods to produce erroneous predictions. In this work, we target adverse weather conditions and introduce an end-to-end domain adaptation strategy that leverages a fusion block, temporal-spatial teacher-student learning, and a temporal weather degradation augmentation approach. The fusion block integrates temporal information from adjacent frames at the feature level, trained end-to-end, eliminating the need for pretrained optical flow, distinguishing our method from existing approaches. Our teacher-student approach involves two teachers: one focuses on exploring temporal information from adjacent frames, and the other harnesses spatial information from the current frame. Finally, we apply temporal weather degradation augmentation to consecutive frames to more accurately represent adverse weather degradations. Our method achieves a performance of 25.4 and 33.0 mIoU on the adaptation from VIPER and Synthia to MVSS, respectively, representing an improvement of 4.3 and 5.8 mIoU over the existing state-of-the-art method.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/61806e8257932882d9ce48b17c3f6ff437775eb6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024endtoend,\\ntitle={End-to-End Video Semantic Segmentation in Adverse Weather using Fusion Blocks and Temporal-Spatial Teacher-Student Learning},\\nauthor={Xin Yang and YAN WENDING and Michael Bi Mi and Yuan Yuan and Robby T. Tan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=paobkszgIA}\\n}'}, 'paperhash': {'value': 'yang|endtoend_video_semantic_segmentation_in_adverse_weather_using_fusion_blocks_and_temporalspatial_teacherstudent_learning'}},forum = 'paobkszgIA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4600/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4600/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4600/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4600/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'paYwtPBpyZ',number = 7861,cdate = 1715643286422,pdate = 1727287861223,odate = 1730873907188,mdate = 1730873907206,tcdate = 1715643286422,tmdate = 1730873907206,ddate = None,content = {'title': {'value': 'Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Generation'}, 'authors': {'value': ['Guillaume Huguet', 'James Vuckovic', 'Kilian FATRAS', 'Eric Thibodeau-Laufer', 'Pablo Lemos', 'Riashat Islam', 'Cheng-Hao Liu', 'Jarrid Rector-Brooks', 'Tara Akhound-Sadegh', 'Michael M. Bronstein', 'Alexander Tong', 'Joey Bose']}, 'authorids': {'value': ['~Guillaume_Huguet1', '~James_Vuckovic1', '~Kilian_FATRAS1', '~Eric_Thibodeau-Laufer1', '~Pablo_Lemos1', '~Riashat_Islam1', '~Cheng-Hao_Liu1', '~Jarrid_Rector-Brooks2', '~Tara_Akhound-Sadegh1', '~Michael_M._Bronstein1', '~Alexander_Tong1', '~Joey_Bose1']}, 'keywords': {'value': ['Proteins', 'Flow Matching', 'Generative Models']}, 'TLDR': {'value': 'We propose FoldFlow++ a new sequence conditioned protein structure generative model using flow-matching which can be finetuned for Motif Scaffolding.'}, 'abstract': {'value': 'Proteins are essential for almost all biological processes and derive their diverse functions from complex $3 \\\\rm D$ structures, which are in turn determined by their amino acid sequences. \\nIn this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow++, a novel sequence-conditioned $\\\\text{SE}(3)$-equivariant flow matching model for protein structure generation. FoldFlow++ presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase \\ndiversity and novelty of generated samples -- crucial for de-novo drug design -- we\\ntrain FoldFlow++ at scale on a new dataset \\nthat is an order of magnitude \\nlarger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow++ to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow++ outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow++ makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/503e86547852b43509aa82eecef8210d45232c5b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuguet2024sequenceaugmented,\\ntitle={Sequence-Augmented {SE}(3)-Flow Matching For Conditional Protein Generation},\\nauthor={Guillaume Huguet and James Vuckovic and Kilian FATRAS and Eric Thibodeau-Laufer and Pablo Lemos and Riashat Islam and Cheng-Hao Liu and Jarrid Rector-Brooks and Tara Akhound-Sadegh and Michael M. Bronstein and Alexander Tong and Joey Bose},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=paYwtPBpyZ}\\n}'}, 'paperhash': {'value': 'huguet|sequenceaugmented_se3flow_matching_for_conditional_protein_generation'}},forum = 'paYwtPBpyZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7861/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7861/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7861/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7861/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pXFiHHySEw',number = 6062,cdate = 1715582239620,pdate = 1727287803909,odate = 1730873890275,mdate = 1730873890292,tcdate = 1715582239620,tmdate = 1730873890292,ddate = None,content = {'title': {'value': 'Multi-Stage Predict+Optimize for (Mixed Integer) Linear Programs'}, 'authors': {'value': ['Xinyi HU', 'Jasper C.H. Lee', 'Jimmy H.M. Lee', 'Peter J. Stuckey']}, 'authorids': {'value': ['~Xinyi_HU2', '~Jasper_C.H._Lee1', '~Jimmy_H.M._Lee1', '~Peter_J._Stuckey1']}, 'keywords': {'value': ['Constraint Optimization', 'Machine Learning', 'Predict+Optimize']}, 'abstract': {'value': 'The recently-proposed framework of Predict+Optimize tackles optimization problems with parameters that are unknown at solving time, in a supervised learning setting. Prior frameworks consider only the scenario where all unknown parameters are (eventually) revealed simultaneously. In this work, we propose Multi-Stage Predict+Optimize, a novel extension catering to applications where unknown parameters are revealed in sequential stages, with optimization decisions made in between. We further develop three training algorithms for neural networks (NNs) for our framework as proof of concept, both of which handle all mixed integer linear programs. The first baseline algorithm is a natural extension of prior work, training a single NN which makes a single prediction of unknown parameters. The second and third algorithms instead leverage the possibility of updating parameter predictions between stages, and trains one NN per stage. To handle the interdependency between the neural networks, we adopt sequential and parallelized versions of coordinate descent for training. Experimentation on three benchmarks demonstrates the superior learning performance of our methods over classical approaches.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f3791048483811412cf9912413a9ba8a587bf482.pdf'}, 'supplementary_material': {'value': '/attachment/fe403c74aef3898516e178f45898f1b78014d78b.zip'}, '_bibtex': {'value': '@inproceedings{\\nhu2024multistage,\\ntitle={Multi-Stage Predict+Optimize for (Mixed Integer) Linear Programs},\\nauthor={Xinyi HU and Jasper C.H. Lee and Jimmy H.M. Lee and Peter J. Stuckey},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pXFiHHySEw}\\n}'}, 'paperhash': {'value': 'hu|multistage_predictoptimize_for_mixed_integer_linear_programs'}},forum = 'pXFiHHySEw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6062/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6062/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6062/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6062/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pX71TM2MLh',number = 13211,cdate = 1715736700306,pdate = 1727288031754,odate = 1730873955839,mdate = 1730873955876,tcdate = 1715736700306,tmdate = 1730873955876,ddate = None,content = {'title': {'value': 'Data Free Backdoor Attacks'}, 'authors': {'value': ['Bochuan Cao', 'Jinyuan Jia', 'Chuxuan Hu', 'Wenbo Guo', 'Zhen Xiang', 'Jinghui Chen', 'Bo Li', 'Dawn Song']}, 'authorids': {'value': ['~Bochuan_Cao1', '~Jinyuan_Jia2', '~Chuxuan_Hu1', '~Wenbo_Guo1', '~Zhen_Xiang1', '~Jinghui_Chen1', '~Bo_Li19', '~Dawn_Song1']}, 'keywords': {'value': ['Data free backdoor attacks']}, 'TLDR': {'value': 'We propose a data-free backdoor attack without changing the architecture of a classifier.'}, 'abstract': {'value': \"Backdoor attacks aim to inject a backdoor into a classifier such that it predicts any input with an attacker-chosen backdoor trigger as an attacker-chosen target class. \\nExisting backdoor attacks require either retraining the classifier with some clean data or modifying the model's architecture.\\nAs a result, they are 1) not applicable when clean data is unavailable, 2) less efficient when the model is large, and 3) less stealthy due to architecture changes. \\nIn this work, we propose DFBA, a novel retraining-free and data-free backdoor attack without changing the model architecture. \\nTechnically, our proposed method modifies a few parameters of a classifier to inject a backdoor. \\nThrough theoretical analysis, we verify that our injected backdoor is provably undetectable and unremovable by various state-of-the-art defenses under mild assumptions. \\nOur evaluation on multiple datasets further demonstrates that our injected backdoor: 1) incurs negligible classification loss, 2) achieves 100\\\\% attack success rates, and 3) bypasses six existing state-of-the-art defenses. \\nMoreover, our comparison with a state-of-the-art non-data-free backdoor attack shows our attack is more stealthy and effective against various defenses while achieving less classification accuracy loss.\\nWe will release our code upon paper acceptance.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/626a446188f0b3dac28d22823764a7655735a226.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncao2024data,\\ntitle={Data Free Backdoor Attacks},\\nauthor={Bochuan Cao and Jinyuan Jia and Chuxuan Hu and Wenbo Guo and Zhen Xiang and Jinghui Chen and Bo Li and Dawn Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pX71TM2MLh}\\n}'}, 'paperhash': {'value': 'cao|data_free_backdoor_attacks'}},forum = 'pX71TM2MLh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13211/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13211/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13211/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13211/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'pWowK7jqok',number = 4486,cdate = 1715418176684,pdate = 1727287751994,odate = 1730873876391,mdate = 1736732179967,tcdate = 1715418176684,tmdate = 1736732179967,ddate = None,content = {'title': {'value': 'E-Motion: Future Motion Simulation via Event Sequence Diffusion'}, 'authors': {'value': ['Song Wu', 'Zhiyu Zhu', 'Junhui Hou', 'Guangming Shi', 'Jinjian Wu']}, 'authorids': {'value': ['~Song_Wu7', '~Zhiyu_Zhu1', '~Junhui_Hou2', '~Guangming_Shi1', '~Jinjian_Wu1']}, 'keywords': {'value': ['Event-based vision', 'video diffusion model']}, 'abstract': {'value': \"Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems. The source code is\\npublicly available at https://github.com/p4r4mount/E-Motion.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion prediction framework.'}, 'pdf': {'value': '/pdf/4468e8e5ba691cc274470c891f728ce7aa135bb1.pdf'}, 'supplementary_material': {'value': '/attachment/b7cd23bba2a4e949afb6a1b5dde875bca9559798.zip'}, '_bibtex': {'value': '@inproceedings{\\nwu2024emotion,\\ntitle={E-Motion: Future Motion Simulation via Event Sequence Diffusion},\\nauthor={Song Wu and Zhiyu Zhu and Junhui Hou and Guangming Shi and Jinjian Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pWowK7jqok}\\n}'}, 'paperhash': {'value': 'wu|emotion_future_motion_simulation_via_event_sequence_diffusion'}},forum = 'pWowK7jqok',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4486/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4486/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4486/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4486/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pW9Jwim918',number = 15463,cdate = 1715760532912,pdate = 1727288098687,odate = 1730873973145,mdate = 1730873973170,tcdate = 1715760532912,tmdate = 1730873973170,ddate = None,content = {'title': {'value': \"ReMoDetect: Reward Models Recognize Aligned LLM's Generations\"}, 'authors': {'value': ['Hyunseok Lee', 'Jihoon Tack', 'Jinwoo Shin']}, 'authorids': {'value': ['~Hyunseok_Lee1', '~Jihoon_Tack1', '~Jinwoo_Shin1']}, 'keywords': {'value': ['Large Language Model', 'LLM Generated Text Detection', 'Reward Model']}, 'abstract': {'value': 'The remarkable capabilities and easy accessibility of large language models (LLMs) have significantly increased societal risks (e.g., fake news generation), necessitating the development of LLM-generated text (LGT) detection methods for safe usage. However, detecting LGTs is challenging due to the vast number of LLMs, making it impractical to account for each LLM individually; hence, it is crucial to identify the common characteristics shared by these models. In this paper, we draw attention to a common feature of recent powerful LLMs, namely the alignment training, i.e., training LLMs to generate human-preferable texts. Our key finding is that as these aligned LLMs are trained to maximize the human preferences, they generate texts with higher estimated preferences even than human-written texts; thus, such texts are easily detected by using the reward model (i.e., an LLM trained to model human preference distribution). Based on this finding, we propose two training schemes to further improve the detection ability of the reward model, namely (i) continual preference fine-tuning to make reward model prefer aligned LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a rephrased texts from human-written texts using aligned LLMs), which serves as a median preference text corpus between LGTs and human-written texts to learn the decision boundary better. We provide an extensive evaluation by considering six text domains across twelve aligned LLMs, where our method demonstrates state-of-the-art results.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/22c6bad5b2f8fdc92745b87003ce003f7567ec18.pdf'}, 'supplementary_material': {'value': '/attachment/09566f45889e628ec12dab055fec86d2deeda0fd.zip'}, 'TLDR': {'value': 'We propose an effective aligned LLM-generated text detection method using reward model.'}, '_bibtex': {'value': \"@inproceedings{\\nlee2024remodetect,\\ntitle={ReMoDetect: Reward Models Recognize Aligned {LLM}'s Generations},\\nauthor={Hyunseok Lee and Jihoon Tack and Jinwoo Shin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pW9Jwim918}\\n}\"}, 'paperhash': {'value': 'lee|remodetect_reward_models_recognize_aligned_llms_generations'}},forum = 'pW9Jwim918',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15463/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15463/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15463/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15463/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pVPyCgXv57',number = 10258,cdate = 1715692365649,pdate = 1727287934145,odate = 1730873927441,mdate = 1730873927454,tcdate = 1715692365649,tmdate = 1730873927454,ddate = None,content = {'title': {'value': 'Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers'}, 'authors': {'value': ['Dong Hoon Lee', 'Seunghoon Hong']}, 'authorids': {'value': ['~Dong_Hoon_Lee1', '~Seunghoon_Hong2']}, 'keywords': {'value': ['token merging', 'token reduction method', 'efficient inference', 'vision transformer']}, 'abstract': {'value': 'Recent token reduction methods for Vision Transformers (ViTs) incorporate token merging, which measures the similarities between token embeddings and combines the most similar pairs.\\nHowever, their merging policies are directly dependent on intermediate features in ViTs, which prevents exploiting features tailored for merging and requires end-to-end training to improve token merging.\\nIn this paper, we propose Decoupled Token Embedding for Merging (DTEM) that enhances token merging through a decoupled embedding learned via a continuously relaxed token merging process.\\nOur method introduces a lightweight embedding module decoupled from the ViT forward pass to extract dedicated features for token merging, thereby addressing the restriction from using intermediate features.\\nThe continuously relaxed token merging, applied during training, enables us to learn the decoupled embeddings in a differentiable manner.\\nThanks to the decoupled structure, our method can be seamlessly integrated into existing ViT backbones and trained either modularly by learning only the decoupled embeddings or end-to-end by fine-tuning. \\nWe demonstrate the applicability of DTEM on various tasks, including classification, captioning, and segmentation, with consistent improvement in token merging.\\nEspecially in the ImageNet-1k classification, DTEM achieves a 37.2\\\\% reduction in FLOPs while maintaining a top-1 accuracy of 79.85\\\\% with DeiT-small.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c098dfd7c7d7b63200ff456647b3ba86799aa874.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024learning,\\ntitle={Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers},\\nauthor={Dong Hoon Lee and Seunghoon Hong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pVPyCgXv57}\\n}'}, 'paperhash': {'value': 'lee|learning_to_merge_tokens_via_decoupled_embedding_for_efficient_vision_transformers'}},forum = 'pVPyCgXv57',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10258/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10258/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10258/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10258/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pU0z2sNM1M',number = 6582,cdate = 1715599149580,pdate = 1727287819276,odate = 1730873894559,mdate = 1730873894577,tcdate = 1715599149580,tmdate = 1730873894577,ddate = None,content = {'title': {'value': 'Causal Dependence Plots'}, 'authors': {'value': ['Joshua R. Loftus', 'Lucius E.J. Bynum', 'Sakina Hansen']}, 'authorids': {'value': ['~Joshua_R._Loftus1', '~Lucius_E.J._Bynum1', '~Sakina_Hansen1']}, 'keywords': {'value': ['Interpretable machine learning', 'interpretability', 'explainable AI', 'explainability', 'causality', 'partial dependence plots', 'total dependence plots', 'model agnostic explanations']}, 'TLDR': {'value': 'We introduce a framework for creating model explanation plots based explicitly on causal relationships and illustrate several types including the popular existing method of partial dependence plots as a special case'}, 'abstract': {'value': \"To use artificial intelligence and machine learning models wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how a model's predicted outcome depends on changes in a given predictor *along with consequent causal changes in other predictor variables*. Crucially, this differs from standard methods based on independence or holding other predictors constant, such as regression coefficients or Partial Dependence Plots (PDPs). Our explanatory framework generalizes PDPs, including them as a special case, as well as a variety of other interpretive plots that show, for example, the total, direct, and indirect effects of causal mediation. We demonstrate with simulations and real data experiments how CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9e16578e540aacca76cc98f43eae7737b7a37f31.pdf'}, 'supplementary_material': {'value': '/attachment/45f40fd01b23181ce54a7c57fb1c88e88cdc2669.zip'}, '_bibtex': {'value': '@inproceedings{\\nloftus2024causal,\\ntitle={Causal Dependence Plots},\\nauthor={Joshua R. Loftus and Lucius E.J. Bynum and Sakina Hansen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pU0z2sNM1M}\\n}'}, 'paperhash': {'value': 'loftus|causal_dependence_plots'}},forum = 'pU0z2sNM1M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6582/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6582/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6582/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6582/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pRSgf5VdD0',number = 18458,cdate = 1715785651915,pdate = 1727288184378,odate = 1730873989896,mdate = 1736801893346,tcdate = 1715785651915,tmdate = 1736801893346,ddate = None,content = {'title': {'value': 'Learning Partitions from Context'}, 'authors': {'value': ['Simon Buchholz']}, 'authorids': {'value': ['~Simon_Buchholz1']}, 'keywords': {'value': ['clustering', 'gradient-flow', 'token embeddings', 'partitions', 'complexity', 'sample complexity']}, 'abstract': {'value': 'In this paper, we study the problem of learning the structure of a discrete set of $N$ tokens based on their interactions with other tokens. We focus on a setting where the tokens can be partitioned into a small number of classes, and there exists a real-valued function $f$ defined on certain sets of tokens. This function, which captures the interactions between tokens, depends only on the class memberships of its arguments. The goal is to recover the class memberships of all tokens from a finite number of samples of $f$. We begin by analyzing this problem from both complexity-theoretic and information-theoretic viewpoints. We prove that it is NP-complete in general, and for random instances, we show that on the order of $N\\\\ln(N)$ samples, implying very sparse interactions, suffice to identify the partition. We then investigate the conditions under which gradient flow dynamics of token embeddings can reveal the class structure, finding that this is achievable in certain settings when given on the order of $N^2\\\\ln^2(N)$ samples.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4fffbfdafd0f7750a4f096113cda71ac93d0b5ba.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbuchholz2024learning,\\ntitle={Learning Partitions from Context},\\nauthor={Simon Buchholz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pRSgf5VdD0}\\n}'}, 'paperhash': {'value': 'buchholz|learning_partitions_from_context'}},forum = 'pRSgf5VdD0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18458/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18458/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18458/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18458/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pRQmRaonxf',number = 13525,cdate = 1715740382324,pdate = 1727288041883,odate = 1730873958408,mdate = 1730873958428,tcdate = 1715740382324,tmdate = 1730873958428,ddate = None,content = {'title': {'value': 'Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models'}, 'authors': {'value': ['Chengshuai Shi', 'Kun Yang', 'Jing Yang', 'Cong Shen']}, 'authorids': {'value': ['~Chengshuai_Shi1', '~Kun_Yang7', '~Jing_Yang3', '~Cong_Shen1']}, 'keywords': {'value': ['In-context Learning; Multi-agent Competitive Games; Transformers; Decision-making']}, 'TLDR': {'value': 'This work provides a theoretical understanding of the in-context game-playing capabilities of pre-trained transformers, broadening the research scope of in-context RL from the single-agent scenario to the multi-agent competitive games.'}, 'abstract': {'value': 'The in-context learning (ICL) capability of pre-trained models based on the transformer architecture has received growing interest in recent years. While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting. This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., in-context game-playing (ICGP). Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings. As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a739b11c92fd5cfc39cc60917a0bafb7c9f5b8cf.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshi2024transformers,\\ntitle={Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models},\\nauthor={Chengshuai Shi and Kun Yang and Jing Yang and Cong Shen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pRQmRaonxf}\\n}'}, 'paperhash': {'value': 'shi|transformers_as_game_players_provable_incontext_gameplaying_capabilities_of_pretrained_models'}},forum = 'pRQmRaonxf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13525/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13525/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13525/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13525/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pR5g1bBqoV',number = 10386,cdate = 1715693580949,pdate = 1727287938064,odate = 1730873928411,mdate = 1736777942432,tcdate = 1715693580949,tmdate = 1736777942432,ddate = None,content = {'title': {'value': '$\\\\boldsymbol{\\\\mu}\\\\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling'}, 'authors': {'value': ['Moritz Haas', 'Jin Xu', 'Volkan Cevher', 'Leena Chennuru Vankadara']}, 'authorids': {'value': ['~Moritz_Haas1', '~Jin_Xu7', '~Volkan_Cevher1', '~Leena_Chennuru_Vankadara2']}, 'keywords': {'value': ['Deep Learning Theory', 'Optimal Hyperparameter Transfer', 'Sharpness Aware Minimization', 'Infinite Width Limits', 'Signal Propagation Theory', 'Tensor Programs']}, 'abstract': {'value': 'Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets. As models are continually scaled up to improve performance, a rigorous understanding of SAM’s scaling behaviour is paramount. To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework. Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters. In contrast, we identify a stable parameterization with layerwise perturbation scaling, which we call *Maximal Update and Perturbation Parameterization* ($\\\\mu$P$^2$), that ensures all layers are both feature learning and effectively perturbed in the limit. Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that $\\\\mu$P$^2$ is the first parameterization to achieve hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales. Moreover, we provide an intuitive condition to derive $\\\\mu$P$^2$ for other perturbation rules like Adaptive SAM and SAM-ON, also ensuring balanced perturbation effects across all layers.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/49a197979cf584791c9213df953b34a938ab431c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhaas2024boldsymbolmumathbfp,\\ntitle={\\\\${\\\\textbackslash}boldsymbol\\\\{{\\\\textbackslash}mu\\\\}{\\\\textbackslash}mathbf\\\\{P{\\\\textasciicircum}2\\\\}\\\\$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling},\\nauthor={Moritz Haas and Jin Xu and Volkan Cevher and Leena Chennuru Vankadara},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pR5g1bBqoV}\\n}'}, 'paperhash': {'value': 'haas|\\\\boldsymbol\\\\mu\\\\mathbfp^2_effective_sharpness_aware_minimization_requires_layerwise_perturbation_scaling'}},forum = 'pR5g1bBqoV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10386/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10386/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10386/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10386/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'pR37AmwbOt',number = 14597,cdate = 1715751620153,pdate = 1727288074024,odate = 1730873967089,mdate = 1730873967106,tcdate = 1715751620153,tmdate = 1730873967106,ddate = None,content = {'title': {'value': 'Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning'}, 'authors': {'value': ['Jiadong Pan', 'Hongcheng Gao', 'Zongyu Wu', 'taihang Hu', 'Li Su', 'Qingming Huang', 'Liang Li']}, 'authorids': {'value': ['~Jiadong_Pan1', '~Hongcheng_Gao1', '~Zongyu_Wu1', '~taihang_Hu1', '~Li_Su4', '~Qingming_Huang2', '~Liang_Li3']}, 'keywords': {'value': ['Diffusion Model', 'AI Safety']}, 'abstract': {'value': 'Diffusion models (DMs) have demonstrated remarkable proficiency in producing images based on textual prompts. Numerous methods have been proposed to ensure these models generate safe images. Early methods attempt to incorporate safety filters into models to mitigate the risk of generating harmful images but such external filters do not inherently detoxify the model and can be easily bypassed. Hence, model unlearning and data cleaning are the most essential methods for maintaining the safety of models, given their impact on model parameters.\\nHowever, malicious fine-tuning can still make models prone to generating harmful or undesirable images even with these methods.\\nInspired by the phenomenon of catastrophic forgetting, we propose a training policy using contrastive learning to increase the latent space distance between clean and harmful data distribution, thereby protecting models from being fine-tuned to generate harmful images due to forgetting.\\nThe experimental results demonstrate that our methods not only maintain clean image generation capabilities before malicious fine-tuning but also effectively prevent DMs from producing harmful images after malicious fine-tuning. Our method can also be combined with other safety methods to maintain their safety against malicious fine-tuning further.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/93f012b5feddc8835a7a126bda5778236ec74ce2.pdf'}, 'supplementary_material': {'value': '/attachment/9dded155b5f38253789c1371191bf54dc652e1c9.zip'}, '_bibtex': {'value': '@inproceedings{\\npan2024leveraging,\\ntitle={Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning},\\nauthor={Jiadong Pan and Hongcheng Gao and Zongyu Wu and taihang Hu and Li Su and Qingming Huang and Liang Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pR37AmwbOt}\\n}'}, 'paperhash': {'value': 'pan|leveraging_catastrophic_forgetting_to_develop_safe_diffusion_models_against_malicious_finetuning'}},forum = 'pR37AmwbOt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14597/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14597/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14597/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14597/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pPeXYByHNd',number = 14892,cdate = 1715755022054,pdate = 1727288081947,odate = 1730873968886,mdate = 1730873968897,tcdate = 1715755022054,tmdate = 1730873968897,ddate = None,content = {'title': {'value': 'MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training'}, 'authors': {'value': ['Bo Chen', 'Zhilei Bei', 'Xingyi Cheng', 'Pan Li', 'Jie Tang', 'Le Song']}, 'authorids': {'value': ['~Bo_Chen11', '~Zhilei_Bei1', '~Xingyi_Cheng3', '~Pan_Li11', '~Jie_Tang1', '~Le_Song1']}, 'keywords': {'value': ['Computational Biology', 'Protein Language Model', 'Protein Structure Prediction', 'MSA Generative Pre-Training']}, 'abstract': {'value': 'Multiple Sequence Alignment (MSA) plays a pivotal role in unveiling the evolutionary trajectories of protein families. The accuracy of protein structure predictions is often compromised for protein sequences that lack sufficient homologous information to construct high-quality MSA. Although various methods have been proposed to generate high-quality MSA under these conditions, they fall short in comprehensively capturing the intricate co-evolutionary patterns within MSA or require guidance from external oracle models. Here we introduce MSAGPT, a novel approach to prompt protein structure predictions via MSA generative pre-training in a low-MSA regime. MSAGPT employs a simple yet effective 2D evolutionary positional encoding scheme to model the complex evolutionary patterns. Endowed by this, the flexible 1D MSA decoding framework facilitates zero- or few-shot learning. Moreover, we demonstrate leveraging the feedback from AlphaFold2 (AF2) can further enhance the model’s capacity via Rejective Fine-tuning (RFT) and Reinforcement Learning from AF2 Feedback (RLAF). Extensive experiments confirm the efficacy of MSAGPT in generating faithful and informative MSA (up to +8.5% TM-Score on few-shot scenarios). The transfer learning also demonstrates its great potential for the wide range of tasks resorting to the quality of MSA.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/890297036236e355d4b02a290569ca2193b3631e.pdf'}, 'TLDR': {'value': 'We propose a novel MSA generative pre-training framework to yield faithful and informative MSA to promote structure prediction accuracy in a low-MSA regime. Studies of transfer learning also show its great potential to benefit other protein tasks.'}, '_bibtex': {'value': '@inproceedings{\\nchen2024msagpt,\\ntitle={{MSAGPT}: Neural Prompting Protein Structure Prediction via {MSA} Generative Pre-Training},\\nauthor={Bo Chen and Zhilei Bei and Xingyi Cheng and Pan Li and Jie Tang and Le Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pPeXYByHNd}\\n}'}, 'paperhash': {'value': 'chen|msagpt_neural_prompting_protein_structure_prediction_via_msa_generative_pretraining'}},forum = 'pPeXYByHNd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14892/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14892/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14892/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14892/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'pPSWHsgqRp',number = 21167,cdate = 1715800419678,pdate = 1727288249409,odate = 1730874004929,mdate = 1730874004949,tcdate = 1715800419678,tmdate = 1730874004949,ddate = None,content = {'title': {'value': 'Smoothie: Label Free Language Model Routing'}, 'authors': {'value': ['Neel Guha', 'Mayee F Chen', 'Trevor Chow', 'Ishan S. Khare', 'Christopher Re']}, 'authorids': {'value': ['~Neel_Guha1', '~Mayee_F_Chen1', '~Trevor_Chow1', '~Ishan_S._Khare1', '~Christopher_Re1']}, 'keywords': {'value': ['large language models', 'weak supervision', 'graphical models', 'routing']}, 'TLDR': {'value': 'We propose an algorithm for learning LLM routers without labeled data.'}, 'abstract': {'value': \"Large language models (LLMs) are increasingly used in applications where LLM inputs may span many different tasks. Recent work has found that the choice of LLM is consequential, and different LLMs may be good for different input samples. Prior approaches have thus explored how engineers might select an LLM to use for each sample (i.e. _routing_). While existing routing methods mostly require training auxiliary models on human-annotated data, our work explores whether it is possible to perform _unsupervised_ routing. We propose Smoothie, a weak supervision-inspired routing approach that requires no labeled data. Given a set of outputs from different LLMs, Smoothie constructs a latent variable graphical model over embedding representations of observable LLM outputs and unknown “true” outputs. Using this graphical model, we estimate sample-dependent quality scores for each LLM, and route each sample to the LLM with the highest corresponding score. We find that Smoothie's LLM quality-scores correlate with ground-truth model quality (correctly identifying the optimal model on 9/14 tasks), and that Smoothie outperforms baselines for routing by up to 10 points accuracy.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5c0e172eaa2e9b1dfee0fa41fd6b06f1eb52c2b7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguha2024smoothie,\\ntitle={Smoothie: Label Free Language Model Routing},\\nauthor={Neel Guha and Mayee F Chen and Trevor Chow and Ishan S. Khare and Christopher Re},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pPSWHsgqRp}\\n}'}, 'paperhash': {'value': 'guha|smoothie_label_free_language_model_routing'}},forum = 'pPSWHsgqRp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21167/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21167/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21167/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21167/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pOXgdFEB7q',number = 594,cdate = 1713978348439,pdate = 1727287641498,odate = 1730873842222,mdate = 1730873842241,tcdate = 1713978348439,tmdate = 1730873842241,ddate = None,content = {'title': {'value': 'What Variables Affect Out-of-Distribution Generalization in Pretrained Models?'}, 'authors': {'value': ['Md Yousuf Harun', 'Kyungbok Lee', 'Jhair Gallardo', 'Giri Prashanth', 'Christopher Kanan']}, 'authorids': {'value': ['~Md_Yousuf_Harun1', '~Kyungbok_Lee2', '~Jhair_Gallardo1', '~Giri_Prashanth1', '~Christopher_Kanan1']}, 'keywords': {'value': ['Image Embeddings', 'Out-of-Distribution Generalization', 'Tunnel Effect', 'Neural Collapse']}, 'TLDR': {'value': 'We identify what variables matter most in out-of-distribution generalization of embeddings and we show that the tunnel effect hypothesis proposed in NeurIPS-2023 is not universal.'}, 'abstract': {'value': 'Embeddings produced by pre-trained deep neural networks (DNNs) are widely used; however, their efficacy for downstream tasks can vary widely. We study the factors influencing transferability and out-of-distribution (OOD) generalization of pre-trained DNN embeddings through the lens of the tunnel effect hypothesis, which is closely related to intermediate neural collapse. This hypothesis suggests that deeper DNN layers compress representations and hinder OOD generalization. Contrary to earlier work, our experiments show this is not a universal phenomenon. We comprehensively investigate the impact of DNN architecture, training data, image resolution, and augmentations on transferability. We identify that training with high-resolution datasets containing many classes greatly reduces representation compression and improves transferability. Our results emphasize the danger of generalizing findings from toy datasets to broader contexts.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/72595a83a1c7b4280a80b08364ed2a1ddae7cae1.pdf'}, 'supplementary_material': {'value': '/attachment/a55e924e17df2081f8641fb78f4180eb2247d560.zip'}, '_bibtex': {'value': '@inproceedings{\\nharun2024what,\\ntitle={What Variables Affect Out-of-Distribution Generalization in Pretrained Models?},\\nauthor={Md Yousuf Harun and Kyungbok Lee and Jhair Gallardo and Giri Prashanth and Christopher Kanan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pOXgdFEB7q}\\n}'}, 'paperhash': {'value': 'harun|what_variables_affect_outofdistribution_generalization_in_pretrained_models'}},forum = 'pOXgdFEB7q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission594/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission594/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission594/-/Revision', 'NeurIPS.cc/2024/Conference/Submission594/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pNnvzQsS4P',number = 13935,cdate = 1715744401034,pdate = 1727288055327,odate = 1730873962253,mdate = 1730873962284,tcdate = 1715744401034,tmdate = 1730873962284,ddate = None,content = {'title': {'value': 'KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization'}, 'authors': {'value': ['Tianyi Zhang', 'Jonah Wonkyu Yi', 'Zhaozhuo Xu', 'Anshumali Shrivastava']}, 'authorids': {'value': ['~Tianyi_Zhang6', '~Jonah_Wonkyu_Yi1', '~Zhaozhuo_Xu1', '~Anshumali_Shrivastava1']}, 'keywords': {'value': ['large language model', 'efficiency', 'kv cache', 'quantization']}, 'abstract': {'value': 'Efficient deployment of Large Language Models (LLMs) requires batching multiple requests together to improve throughput. As batch size, context length, or model size increases, the size of key and value (KV) cache quickly becomes the main contributor to GPU memory usage and the bottleneck of inference latency and throughput. Quantization has emerged as an effective technique for KV cache compression, but existing methods still fail at very low bit widths. Currently, KV cache quantization is performed per-channel or per-token independently. Our analysis shows that distinct channels of a key/value activation embedding are highly interdependent, and the joint entropy of multiple channels grows at a slower rate than the sum of their marginal entropy, which implies that per-channel independent quantization is sub-optimal. To mitigate this sub-optimality, we propose Coupled Quantization (CQ), which couples multiple key/value channels together for quantization to exploit their interdependence and encode the activations in a more information-efficient manner. Extensive experiments reveal that CQ compares favorably with existing baselines in preserving model quality, and improves inference throughput by 1.4–3.5$\\\\times$ relative to the uncompressed baseline. Furthermore, we demonstrate that CQ can preserve model quality reasonably with KV cache quantized down to 1 bit.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cc83819e3c2ee5e47a2a7f0f28eb98ada7deb1ce.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024kv,\\ntitle={{KV} Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization},\\nauthor={Tianyi Zhang and Jonah Wonkyu Yi and Zhaozhuo Xu and Anshumali Shrivastava},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pNnvzQsS4P}\\n}'}, 'paperhash': {'value': 'zhang|kv_cache_is_1_bit_per_channel_efficient_large_language_model_inference_with_coupled_quantization'}},forum = 'pNnvzQsS4P',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13935/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13935/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13935/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13935/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pMaCRgu8GV',number = 20278,cdate = 1715795481045,pdate = 1727288230044,odate = 1730874000532,mdate = 1730874000547,tcdate = 1715795481045,tmdate = 1730874000547,ddate = None,content = {'title': {'value': 'Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning'}, 'authors': {'value': ['Jonathan Cook', 'Chris Lu', 'Edward Hughes', 'Joel Z Leibo', 'Jakob Nicolaus Foerster']}, 'authorids': {'value': ['~Jonathan_Cook3', '~Chris_Lu1', '~Edward_Hughes1', '~Joel_Z_Leibo1', '~Jakob_Nicolaus_Foerster1']}, 'keywords': {'value': ['social learning', 'cultural accumulation', 'in-context learning', 'reinforcement learning']}, 'TLDR': {'value': 'We investigate cultural accumulation among reinforcement learning agents and demonstrate that it can improve upon \"single-lifetime\" learning.'}, 'abstract': {'value': 'Cultural accumulation drives the open-ended and diverse progress in capabilities spanning human history. It builds an expanding body of knowledge and skills by combining individual exploration with inter-generational information transmission. Despite its widespread success among humans, the capacity for artificial learning agents to accumulate culture remains under-explored. In particular, approaches to reinforcement learning typically strive for improvements over only a single lifetime. Generational algorithms that do exist fail to capture the open-ended, emergent nature of cultural accumulation, which allows individuals to trade-off innovation and imitation. Building on the previously demonstrated ability for reinforcement learning agents to perform social learning, we find that training setups which balance this with independent learning give rise to cultural accumulation. These accumulating agents outperform those trained for a single lifetime with the same cumulative experience. We explore this accumulation by constructing two models under two distinct notions of a generation: episodic generations, in which accumulation occurs via in-context learning and train-time generations, in which accumulation occurs via in-weights learning. In-context and in-weights cultural accumulation can be interpreted as analogous to knowledge and skill accumulation, respectively. To the best of our knowledge, this work is the first to present general models that achieve emergent cultural accumulation in reinforcement learning, opening up new avenues towards more open-ended learning systems, as well as presenting new opportunities for modelling human culture.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1e586109098febb5c94c5bd615adda318b49a15a.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncook2024artificial,\\ntitle={Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning},\\nauthor={Jonathan Cook and Chris Lu and Edward Hughes and Joel Z Leibo and Jakob Nicolaus Foerster},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pMaCRgu8GV}\\n}'}, 'paperhash': {'value': 'cook|artificial_generational_intelligence_cultural_accumulation_in_reinforcement_learning'}},forum = 'pMaCRgu8GV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20278/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20278/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20278/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20278/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pMPBxMf8T3',number = 8587,cdate = 1715664979245,pdate = 1727287884484,odate = 1730873913270,mdate = 1730873913282,tcdate = 1715664979245,tmdate = 1730873913282,ddate = None,content = {'title': {'value': 'The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing'}, 'authors': {'value': ['Yang Xu', 'Yihong Gu', 'Cong Fang']}, 'authorids': {'value': ['~Yang_Xu17', '~Yihong_Gu1', '~Cong_Fang1']}, 'keywords': {'value': ['implicit bias', 'matrix sensing', 'invariance learning', 'non-convex optimization']}, 'TLDR': {'value': 'The implicit bias of heterogeneity helps model learn invariance.'}, 'abstract': {'value': 'Models are expected to engage in invariance learning, which involves distinguishing the core relations that remain consistent across varying environments to ensure the predictions are safe, robust and fair. While existing works consider specific algorithms to realize invariance learning, we show that model has the potential to learn invariance through standard training procedures. In other words, this paper studies the implicit bias of Stochastic Gradient Descent (SGD) over heterogeneous data and shows that the implicit bias drives the model learning towards an invariant solution. We call the phenomenon the implicit invariance learning. Specifically, we theoretically investigate the multi-environment low-rank matrix sensing problem where in each environment, the signal comprises (i) a lower-rank invariant part shared across all environments; and (ii) a significantly varying environment-dependent spurious component. The key insight is, through simply employing the large step size large-batch SGD sequentially in each environment without any explicit regularization, the oscillation caused by heterogeneity can provably prevent model learning spurious signals.  The model reaches the invariant solution after certain iterations. In contrast, model learned using pooled SGD over all data would simultaneously learn both the invariant and spurious signals. Overall, we unveil another implicit bias that is a result of the symbiosis between the heterogeneity of data and modern algorithms, which is, to the best of our knowledge, first in the literature.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c0ddf06b5d5e52dfc2af13537aa87138050120f7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxu2024the,\\ntitle={The Implicit Bias of Heterogeneity towards Invariance: A Study of Multi-Environment Matrix Sensing},\\nauthor={Yang Xu and Yihong Gu and Cong Fang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pMPBxMf8T3}\\n}'}, 'paperhash': {'value': 'xu|the_implicit_bias_of_heterogeneity_towards_invariance_a_study_of_multienvironment_matrix_sensing'}},forum = 'pMPBxMf8T3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8587/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8587/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8587/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8587/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pMJFaBzoG3',number = 2787,cdate = 1715162068612,pdate = 1727287700307,odate = 1730873861047,mdate = 1730873861060,tcdate = 1715162068612,tmdate = 1730873861060,ddate = None,content = {'title': {'value': 'OT4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation'}, 'authors': {'value': ['Yaming Guo', 'Chen Zhu', 'Hengshu Zhu', 'Tieru Wu']}, 'authorids': {'value': ['~Yaming_Guo1', '~Chen_Zhu5', '~Hengshu_Zhu1', '~Tieru_Wu1']}, 'keywords': {'value': ['permutation matrix', 'orthogonal group', 'differentiable transformation', 'stochastic optimization']}, 'TLDR': {'value': 'We present a novel differentiable transformation for relaxing permutation matrices onto the orthogonal group, which enables gradient-based (stochastic) optimization of problems involving permutation matrices.'}, 'abstract': {'value': 'Optimization over permutations is typically an NP-hard problem that arises extensively in ranking, matching, tracking, etc. Birkhoff polytope-based relaxation methods have made significant advancements, particularly in penalty-free optimization and probabilistic inference. Relaxation onto the orthogonal group offers unique potential advantages such as a lower representation dimension and preservation of inner products; however, equally effective approaches remain unexplored. To bridge the gap, we present a temperature-controlled differentiable transformation that maps unconstrained vector space to the orthogonal group, where the temperature, in the limit, concentrates orthogonal matrices near permutation matrices. This transformation naturally implements a parameterization for the relaxation of permutation matrices, allowing for gradient-based optimization of problems involving permutations. Additionally, by deriving a re-parameterized gradient estimator, this transformation also provides efficient stochastic optimization over the latent permutations. Extensive experiments involving the optimization over permutation matrices validate the effectiveness of the proposed method.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c3b6f09a1ec2460ff63325a7189dd7873a26b49b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguo2024otp,\\ntitle={{OT}4P: Unlocking Effective Orthogonal Group Path for Permutation Relaxation},\\nauthor={Yaming Guo and Chen Zhu and Hengshu Zhu and Tieru Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pMJFaBzoG3}\\n}'}, 'paperhash': {'value': 'guo|ot4p_unlocking_effective_orthogonal_group_path_for_permutation_relaxation'}},forum = 'pMJFaBzoG3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2787/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2787/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2787/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2787/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pLoX8Og3bH',number = 906,cdate = 1714205544273,pdate = 1727287647322,odate = 1730873844052,mdate = 1734751536661,tcdate = 1714205544273,tmdate = 1734751536661,ddate = None,content = {'title': {'value': \"Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New UAV-View Benchmark\"}, 'authors': {'value': ['Wei Ji', 'Jingjing Li', 'Wenbo Li', 'Yilin Shen', 'Li cheng', 'Hongxia Jin']}, 'authorids': {'value': ['~Wei_Ji2', '~Jingjing_Li5', '~Wenbo_Li5', '~Yilin_Shen1', '~Li_Cheng1', '~Hongxia_Jin1']}, 'keywords': {'value': ['Computer Vision', 'Deep Learning', 'Semantic Segmentation']}, 'TLDR': {'value': 'In this work, we propose the SemiMV baseline, the first approach to apply semi-supervised learning specifically for multispectral video semantic segmentation, and introduce a new UAV-view dataset to advance research in this field.'}, 'abstract': {'value': 'Thanks to the rapid progress in RGB & thermal imaging, also known as multispectral imaging, the task of multispectral video semantic segmentation, or MVSS in short, has recently drawn significant attentions. Noticeably, it offers new opportunities in improving segmentation performance under unfavorable visual conditions such as poor light or overexposure. Unfortunately, there are currently very few datasets available, including for example MVSeg dataset that focuses purely toward eye-level view; and it features the sparse annotation nature due to the intensive demands of labeling process. To address these key challenges of the MVSS task, this paper presents two major contributions: the introduction of MVUAV, a new MVSS benchmark dataset, and the development of a dedicated semi-supervised MVSS baseline - SemiMV. Our MVUAV dataset is captured via Unmanned Aerial Vehicles (UAV), which offers a unique oblique bird’s-eye view complementary to the existing MVSS datasets; it also encompasses a broad range of day/night lighting conditions and over 30 semantic categories. In the meantime, to better leverage the sparse annotations and extra unlabeled RGB-Thermal videos, a semi-supervised learning baseline, SemiMV, is proposed to enforce consistency regularization through a dedicated Cross-collaborative Consistency Learning (C3L) module and a denoised temporal aggregation strategy. Comprehensive empirical evaluations on both MVSeg and MVUAV benchmark datasets have showcased the efficacy of our SemiMV baseline.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9c9d58797ce5526a1f0dff317e669cf54103606d.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': \"@inproceedings{\\nji2024unleashing,\\ntitle={Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New {UAV}-View Benchmark},\\nauthor={Wei Ji and Jingjing Li and Wenbo Li and Yilin Shen and Li cheng and Hongxia Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pLoX8Og3bH}\\n}\"}, 'supplementary_material': {'value': '/attachment/3f8cc41b8a160fd01781401dd9c8904614944a9c.zip'}, 'paperhash': {'value': 'ji|unleashing_multispectral_videos_potential_in_semantic_segmentation_a_semisupervised_viewpoint_and_new_uavview_benchmark'}},forum = 'pLoX8Og3bH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission906/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission906/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission906/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission906/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pK2qGRY2Hv',number = 7347,cdate = 1715618694879,pdate = 1727287843986,odate = 1730873902112,mdate = 1730873902123,tcdate = 1715618694879,tmdate = 1730873902123,ddate = None,content = {'title': {'value': 'The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure'}, 'authors': {'value': ['Tyler Sam', 'Yudong Chen', 'Christina Yu']}, 'authorids': {'value': ['~Tyler_Sam1', '~Yudong_Chen1', '~Christina_Yu1']}, 'keywords': {'value': ['Reinforcement Learning', 'Transfer Learning', 'Representation Learning']}, 'abstract': {'value': \"Many reinforcement learning (RL) algorithms are too costly to use in practice due to the large sizes $S,A$ of the problem's state and action space. To resolve this issue, we study transfer RL with latent low rank structure. We consider the problem of transferring a latent low rank representation when the source and target MDPs have transition kernels with Tucker rank $(S, d, A)$, $(S ,S , d), (d, S , A )$, or $(d , d , d )$. In each setting, we introduce the transfer-ability coefficient $\\\\alpha$ that measures the difficulty of representational transfer. Our algorithm learns latent representations in each source MDP and then exploits the linear structure to remove the dependence on $S , A $, or $SA $ in the target MDP regret bound. We complement our positive results with information theoretic lower bounds that show our algorithms (excluding the ($d, d, d$) setting) are minimax-optimal with respect to $\\\\alpha$.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/03994c43bf851c51ecc74ae7b1902121af6814e6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsam2024the,\\ntitle={The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure},\\nauthor={Tyler Sam and Yudong Chen and Christina Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pK2qGRY2Hv}\\n}'}, 'paperhash': {'value': 'sam|the_limits_of_transfer_reinforcement_learning_with_latent_lowrank_structure'}},forum = 'pK2qGRY2Hv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7347/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7347/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7347/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7347/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pJlFURyTG5',number = 10401,cdate = 1715693731617,pdate = 1727287939120,odate = 1730873928487,mdate = 1730873928498,tcdate = 1715693731617,tmdate = 1730873928498,ddate = None,content = {'title': {'value': 'Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning'}, 'authors': {'value': ['Lijun Zhang', 'Lin Li', 'Wei Wei', 'Huizhong Song', 'Yaodong Yang', 'Jiye Liang']}, 'authorids': {'value': ['~Lijun_Zhang8', '~Lin_Li17', '~Wei_Wei13', '~Huizhong_Song1', '~Yaodong_Yang1', '~Jiye_Liang1']}, 'keywords': {'value': ['Multi-agent reinforcement learning', 'policy optimization', 'safe learning', 'scalable method']}, 'TLDR': {'value': 'We develop a novel scalable multi-agent constrained policy optimization method and prove that the safety constraints and the joint policy improvement can be met when each agent adopts a sequential update scheme to optimize a $\\\\kappa$-hop policy.'}, 'abstract': {'value': 'A challenging problem in seeking to bring multi-agent reinforcement learning (MARL) techniques into real-world applications, such as autonomous driving and drone swarms, is how to control multiple agents safely and cooperatively to accomplish tasks. Most existing safe MARL methods learn the centralized value function by introducing a global state to guide safety cooperation. However, the global coupling arising from agents’ safety constraints and the exponential growth of the state-action space size limit their applicability in instant communication or computing resource-constrained systems and larger multi-agent systems.\\xa0In this paper, we develop a novel scalable\\xa0and theoretically-justified multi-agent constrained policy optimization method. This method utilizes the rigorous bounds of the trust region method and the bounds of the truncated advantage function to provide a new local policy optimization objective for each agent. Also, we prove that the safety constraints and the joint policy improvement can\\xa0be met\\xa0when each agent adopts a sequential update scheme to optimize a $\\\\kappa$-hop policy. Then, we propose a practical algorithm called Scalable MAPPO-Lagrangian (Scal-MAPPO-L). The proposed method’s effectiveness\\xa0is verified\\xa0on a collection of benchmark tasks, and the results support our theory that decentralized training with local interactions can still improve reward performance and satisfy safe constraints.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/928f1f6ddcbbdb2f197603e2a4dedf1d3eade0fa.pdf'}, 'supplementary_material': {'value': '/attachment/cd180c748ace2ac76c5d5d4cc9d97e7bebf96639.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024scalable,\\ntitle={Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning},\\nauthor={Lijun Zhang and Lin Li and Wei Wei and Huizhong Song and Yaodong Yang and Jiye Liang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pJlFURyTG5}\\n}'}, 'paperhash': {'value': 'zhang|scalable_constrained_policy_optimization_for_safe_multiagent_reinforcement_learning'}},forum = 'pJlFURyTG5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10401/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10401/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10401/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10401/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'pHiTmEsAfZ',number = 8467,cdate = 1715661359385,pdate = 1727287880438,odate = 1730873912481,mdate = 1730873912500,tcdate = 1715661359385,tmdate = 1730873912500,ddate = None,content = {'title': {'value': 'Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion'}, 'authors': {'value': ['Yongyuan Liang', 'Tingqiang Xu', 'Kaizhe Hu', 'Guangqi Jiang', 'Furong Huang', 'Huazhe Xu']}, 'authorids': {'value': ['~Yongyuan_Liang1', '~Tingqiang_Xu1', '~Kaizhe_Hu1', '~Guangqi_Jiang1', '~Furong_Huang1', '~Huazhe_Xu1']}, 'keywords': {'value': ['Diffusion Model', 'Policy Learning', 'Parameter Generation', 'Reinforcement Learning']}, 'TLDR': {'value': 'We introduce a policy network generator that utilizes conditional diffusion models for behavior-to-policy generation.'}, 'abstract': {'value': 'Can we generate a control policy for an agent using just one demonstration of desired behaviors as a prompt, as effortlessly as creating an image from a textual description?\\nIn this paper, we present **Make-An-Agent**, a novel policy parameter generator that leverages the power of conditional diffusion models for behavior-to-policy generation. Guided by behavior embeddings that encode trajectory information, our policy generator synthesizes latent parameter representations, which can then be decoded into policy networks. \\nTrained on policy network checkpoints and their corresponding trajectories, our generation model demonstrates remarkable versatility and scalability on multiple tasks and has a strong generalization ability on unseen tasks to output well-performed policies with only few-shot demonstrations as inputs. We showcase its efficacy and efficiency on various domains and tasks, including varying objectives, behaviors, and even across different robot manipulators. Beyond simulation, we directly deploy policies generated by **Make-An-Agent** onto real-world robots on locomotion tasks. Project page: https://cheryyunl.github.io/make-an-agent/.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/30950d8eec8a2456a781dff694642e9b4c2d048c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliang2024makeanagent,\\ntitle={Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion},\\nauthor={Yongyuan Liang and Tingqiang Xu and Kaizhe Hu and Guangqi Jiang and Furong Huang and Huazhe Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pHiTmEsAfZ}\\n}'}, 'paperhash': {'value': 'liang|makeanagent_a_generalizable_policy_network_generator_with_behaviorprompted_diffusion'}},forum = 'pHiTmEsAfZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8467/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8467/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8467/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8467/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pH3XAQME6c',number = 21012,cdate = 1715799591175,pdate = 1727288246855,odate = 1730874004389,mdate = 1730874004406,tcdate = 1715799591175,tmdate = 1730874004406,ddate = None,content = {'title': {'value': 'Refusal in Language Models Is Mediated by a Single Direction'}, 'authors': {'value': ['Andy Arditi', 'Oscar Balcells Obeso', 'Aaquib Syed', 'Daniel Paleka', 'Nina Rimsky', 'Wes Gurnee', 'Neel Nanda']}, 'authorids': {'value': ['~Andy_Arditi1', '~Oscar_Balcells_Obeso1', '~Aaquib_Syed1', '~Daniel_Paleka1', '~Nina_Rimsky1', '~Wes_Gurnee1', '~Neel_Nanda1']}, 'keywords': {'value': ['mechanistic interpretability', 'refusal', 'jailbreaks', 'language models', 'steering vectors', 'representation engineering']}, 'abstract': {'value': \"Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables a model's ability to refuse, with minimal effect on other capabilities. This interpretable rank-one weight edit results in an effective jailbreak technique that is simpler and more efficient than fine-tuning. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8671368b91e68640bc816aa01b23ac6335bb2bf0.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\narditi2024refusal,\\ntitle={Refusal in Language Models Is Mediated by a Single Direction},\\nauthor={Andy Arditi and Oscar Balcells Obeso and Aaquib Syed and Daniel Paleka and Nina Rimsky and Wes Gurnee and Neel Nanda},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pH3XAQME6c}\\n}'}, 'paperhash': {'value': 'arditi|refusal_in_language_models_is_mediated_by_a_single_direction'}},forum = 'pH3XAQME6c',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21012/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21012/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21012/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21012/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pGeAcYhnN5',number = 11570,cdate = 1715706807608,pdate = 1727287975264,odate = 1730873939900,mdate = 1730873939912,tcdate = 1715706807608,tmdate = 1730873939912,ddate = None,content = {'title': {'value': 'Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration'}, 'authors': {'value': ['Zhuofan Wen', 'Shangtong Gui', 'Yang Feng']}, 'authorids': {'value': ['~Zhuofan_Wen2', '~Shangtong_Gui1', '~Yang_Feng4']}, 'keywords': {'value': ['Pretrained language model', 'Speculative decoding', 'CTC decoding algorithm']}, 'TLDR': {'value': 'We propose a flexible speculation decoding framework with CTC-based draft model for context information retrieving when drafting and adaptive candidate sequence generation.'}, 'abstract': {'value': 'Inference acceleration of large language models (LLMs) has been put forward in many application scenarios and speculative decoding has shown its advantage in addressing inference acceleration. Speculative decoding usually introduces a draft model to assist the base LLM where the draft model produces drafts and the base LLM verifies the draft for acceptance or rejection. In this framework, the final inference speed is decided by the decoding speed of the draft model and the acceptance rate of the draft provided by the draft model. Currently the widely used draft models usually generate draft tokens for the next several positions in a non-autoregressive way without considering the correlations between draft tokens. Therefore, it has a high decoding speed but an unsatisfactory acceptance rate. In this paper, we focus on how to improve the performance of the draft model and aim to accelerate inference via a high acceptance rate. To this end, we propose a CTC-based draft model which strengthens the correlations between draft tokens during the draft phase, thereby generating higher-quality draft candidate sequences. Experiment results show that compared to strong baselines, the proposed method can achieve a higher acceptance rate and hence a faster inference speed.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/24afb093510474acb2e9714e67a7d28648b9dd2e.pdf'}, 'supplementary_material': {'value': '/attachment/1732507d4d60f03c8b9aa4ae8b3f8ee1cae85ef9.zip'}, '_bibtex': {'value': '@inproceedings{\\nwen2024speculative,\\ntitle={Speculative Decoding with {CTC}-based Draft Model for {LLM} Inference Acceleration},\\nauthor={Zhuofan Wen and Shangtong Gui and Yang Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pGeAcYhnN5}\\n}'}, 'paperhash': {'value': 'wen|speculative_decoding_with_ctcbased_draft_model_for_llm_inference_acceleration'}},forum = 'pGeAcYhnN5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11570/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11570/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11570/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11570/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pGR5X4e1gy',number = 18854,cdate = 1715787735056,pdate = 1727288195161,odate = 1730873992262,mdate = 1734866551851,tcdate = 1715787735056,tmdate = 1734866551851,ddate = None,content = {'title': {'value': 'Learning on Large Graphs using Intersecting Communities'}, 'authors': {'value': ['Ben Finkelshtein', 'Ismail Ilkan Ceylan', 'Michael M. Bronstein', 'Ron Levie']}, 'authorids': {'value': ['~Ben_Finkelshtein1', '~Ismail_Ilkan_Ceylan2', '~Michael_M._Bronstein1', '~Ron_Levie1']}, 'keywords': {'value': ['graph representation learning', 'graph neural networks', 'regularity lemma', 'intersecting communities', 'graphon', 'scalability']}, 'TLDR': {'value': 'We show how to approximate graphs by intersecting communities, which allows an efficient learning algorithm on large non-sparse graphs.'}, 'abstract': {'value': 'Message Passing Neural Networks (MPNNs) are a staple of graph machine learning. MPNNs iteratively update each node’s representation in an input graph by aggregating messages from the node’s neighbors, which necessitates a memory complexity of the order of the __number of graph edges__. This complexity might quickly become  prohibitive for large graphs provided they are not very sparse.  In this paper, we propose a novel approach to alleviate this problem by  approximating the input graph as an  intersecting community graph (ICG) -- a combination of intersecting cliques. The key insight is that the number of communities required to approximate a graph  __does not depend on the graph size__. We develop a new constructive version of the Weak Graph Regularity Lemma to efficiently construct an approximating ICG for any input graph. We then devise an efficient graph learning algorithm operating directly on ICG in linear memory and time with respect to the __number of nodes__ (rather than edges).  This offers a new and fundamentally different pipeline for learning on very large non-sparse graphs, whose applicability is demonstrated empirically on node classification tasks and spatio-temporal data processing.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/eb52e00ead76e6512bdb87aeb1eaa15ba3c6f652.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfinkelshtein2024learning,\\ntitle={Learning on Large Graphs using Intersecting Communities},\\nauthor={Ben Finkelshtein and Ismail Ilkan Ceylan and Michael M. Bronstein and Ron Levie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pGR5X4e1gy}\\n}'}, 'paperhash': {'value': 'finkelshtein|learning_on_large_graphs_using_intersecting_communities'}},forum = 'pGR5X4e1gy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18854/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18854/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18854/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18854/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pGOBEYcXzs',number = 15166,cdate = 1715757833355,pdate = 1727288089900,odate = 1730873971013,mdate = 1730873971031,tcdate = 1715757833355,tmdate = 1730873971031,ddate = None,content = {'title': {'value': 'Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models'}, 'authors': {'value': ['Dongwon Jo', 'Taesu Kim', 'Yulhwa Kim', 'Jae-Joon Kim']}, 'authorids': {'value': ['~Dongwon_Jo1', '~Taesu_Kim1', '~Yulhwa_Kim1', '~Jae-Joon_Kim2']}, 'keywords': {'value': ['Large Language Models', 'Binarization', 'Quantization', 'Model Compression', 'Efficient LLM']}, 'TLDR': {'value': 'BinaryMoS introduces memory-efficient token-adaptive binarization for LLMs, reducing model size and enhancing representation ability of binarized weights'}, 'abstract': {'value': 'Binarization, which converts weight parameters to binary values, has emerged as an effective strategy to reduce the size of large language models (LLMs). However, typical binarization techniques significantly diminish linguistic effectiveness of LLMs.\\nTo address this issue, we introduce a novel binarization technique called Mixture of Scales (BinaryMoS). Unlike conventional methods, BinaryMoS employs multiple scaling experts for binary weights, dynamically merging these experts for each token to adaptively generate scaling factors. This token-adaptive approach boosts the representational power of binarized LLMs by enabling contextual adjustments to the values of binary weights. Moreover, because this adaptive process only involves the scaling factors rather than the entire weight matrix, BinaryMoS maintains compression efficiency similar to traditional static binarization methods. Our experimental results reveal that BinaryMoS surpasses conventional binarization techniques in various natural language processing tasks and even outperforms 2-bit quantization methods, all while maintaining similar model size to static binarization techniques.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/df9150138771594401ce4c6df21e1cb8dcea3509.pdf'}, 'supplementary_material': {'value': '/attachment/a998ffdc4e689a84d6bb97f427dc7a28935c31db.zip'}, '_bibtex': {'value': '@inproceedings{\\njo2024mixture,\\ntitle={Mixture of Scales: Memory-Efficient Token-Adaptive Binarization for Large Language Models},\\nauthor={Dongwon Jo and Taesu Kim and Yulhwa Kim and Jae-Joon Kim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pGOBEYcXzs}\\n}'}, 'paperhash': {'value': 'jo|mixture_of_scales_memoryefficient_tokenadaptive_binarization_for_large_language_models'}},forum = 'pGOBEYcXzs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15166/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15166/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15166/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15166/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pGEY8JQ3qx',number = 14527,cdate = 1715750903496,pdate = 1727288071896,odate = 1730873966551,mdate = 1730873966574,tcdate = 1715750903496,tmdate = 1730873966574,ddate = None,content = {'title': {'value': 'Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs'}, 'authors': {'value': ['Matthew Zurek', 'Yudong Chen']}, 'authorids': {'value': ['~Matthew_Zurek1', '~Yudong_Chen1']}, 'keywords': {'value': ['reinforcement learning theory', 'average reward', 'sample complexity']}, 'abstract': {'value': 'We study the sample complexity of learning an $\\\\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\\\\widetilde{O}\\\\left(SA\\\\frac{\\\\mathsf{H}}{\\\\varepsilon^2} \\\\right)$, where $\\\\mathsf{H}$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,\\\\mathsf{H}$, and $\\\\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter $\\\\mathsf{B}$ is necessary, establish an $\\\\widetilde{O}\\\\left(SA\\\\frac{\\\\mathsf{B} + \\\\mathsf{H}}{\\\\varepsilon^2} \\\\right)$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To optimally analyze this reduction, we develop improved bounds for $\\\\gamma$-discounted MDPs, showing that $\\\\widetilde{O}\\\\left(SA\\\\frac{\\\\mathsf{H}}{(1-\\\\gamma)^2\\\\varepsilon^2} \\\\right)$ and $\\\\widetilde{O}\\\\left(SA\\\\frac{\\\\mathsf{B} + \\\\mathsf{H}}{(1-\\\\gamma)^2\\\\varepsilon^2} \\\\right)$ samples suffice to learn $\\\\varepsilon$-optimal policies in weakly communicating and in general MDPs, respectively. Both these results circumvent the well-known minimax lower bound of $\\\\widetilde{\\\\Omega}\\\\left(SA\\\\frac{1}{(1-\\\\gamma)^3\\\\varepsilon^2} \\\\right)$ for $\\\\gamma$-discounted MDPs, and establish a quadratic rather than cubic horizon dependence for a fixed MDP instance.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2ff245e09d2ec82378e2aa6ffea57a9ec01c043c.pdf'}, 'TLDR': {'value': 'We resolve the span-based sample complexity of weakly communicating average reward MDPs and initiate the study of general multichain MDPs, obtaining minimax optimal bounds and uncovering improved horizon dependence for fixed discounted MDP instances.'}, '_bibtex': {'value': '@inproceedings{\\nzurek2024spanbased,\\ntitle={Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward {MDP}s},\\nauthor={Matthew Zurek and Yudong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pGEY8JQ3qx}\\n}'}, 'paperhash': {'value': 'zurek|spanbased_optimal_sample_complexity_for_weakly_communicating_and_general_average_reward_mdps'}},forum = 'pGEY8JQ3qx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14527/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14527/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14527/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14527/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pG380vLYRU',number = 13386,cdate = 1715739056868,pdate = 1727288037786,odate = 1730873957686,mdate = 1736487149766,tcdate = 1715739056868,tmdate = 1736487149766,ddate = None,content = {'title': {'value': 'Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints'}, 'authors': {'value': ['Zhenwei Lin', 'Qi Deng']}, 'authorids': {'value': ['~Zhenwei_Lin3', '~Qi_Deng1']}, 'keywords': {'value': ['Convex optimization; Accelerated primal dual algorithm; Sparse Optimization']}, 'TLDR': {'value': 'We propose faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints.'}, 'abstract': {'value': \"In this paper, we introduce faster accelerated primal-dual algorithms for minimizing a convex function subject to strongly convex function constraints. \\nPrior to our work, the best complexity bound was $\\\\mathcal{O}(1/{\\\\varepsilon})$, regardless of the strong convexity of the constraint function.\\nIt is unclear whether the strong convexity assumption can enable even better convergence results. \\nTo address this issue, we have developed novel techniques to progressively estimate the strong convexity of the Lagrangian function.\\nOur approach, for the first time, effectively leverages the constraint strong convexity, obtaining an improved complexity of $\\\\mathcal{O}(1/\\\\sqrt{\\\\varepsilon})$. This rate matches the complexity lower bound for strongly-convex-concave saddle point optimization and is therefore order-optimal.\\nWe show the superior performance of our methods in sparsity-inducing constrained optimization, notably Google's personalized PageRank problem. Furthermore, we show that a restarted version of the proposed methods can effectively identify the optimal solution's sparsity pattern within a finite number of steps, a result that appears to have independent significance.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f2324dfdd34c5a9c9e58abb28f45c8d9326cbca7.pdf'}, 'supplementary_material': {'value': '/attachment/7f254ffb02516109c7485761ea0f4757fadbfcd4.zip'}, '_bibtex': {'value': '@inproceedings{\\nlin2024faster,\\ntitle={Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints},\\nauthor={Zhenwei Lin and Qi Deng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pG380vLYRU}\\n}'}, 'paperhash': {'value': 'lin|faster_accelerated_firstorder_methods_for_convex_optimization_with_strongly_convex_function_constraints'}},forum = 'pG380vLYRU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13386/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13386/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13386/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13386/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pEhvscmSgG',number = 19177,cdate = 1715789587530,pdate = 1727288203694,odate = 1730873994233,mdate = 1736892126819,tcdate = 1715789587530,tmdate = 1736892126819,ddate = None,content = {'title': {'value': 'Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning'}, 'authors': {'value': ['Marvin Alles', 'Philip Becker-Ehmck', 'Patrick van der Smagt', 'Maximilian Karl']}, 'authorids': {'value': ['~Marvin_Alles1', '~Philip_Becker-Ehmck1', '~Patrick_van_der_Smagt1', '~Maximilian_Karl1']}, 'keywords': {'value': ['Offline Reinforcement Learning', 'Model-Based Reinforcement Learning', 'Latent Action', 'Constrained Policy', 'Generative Model', 'Applied Reinforcement Learning']}, 'abstract': {'value': 'In offline reinforcement learning, a policy is learned using a static dataset in the absence of costly feedback from the environment. In contrast to the online setting, only using static datasets poses additional challenges, such as policies generating out-of-distribution samples. Model-based offline reinforcement learning methods try to overcome these by learning a model of the underlying dynamics of the environment and using it to guide policy search. It is beneficial but, with limited datasets, errors in the model and the issue of value overestimation among out-of-distribution states can worsen performance. Current model-based methods apply some notion of conservatism to the Bellman update, often implemented using uncertainty estimation derived from model ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP) which learns a generative model of the joint distribution of observations and actions. We cast policy learning as a constrained objective to always stay within the support of the latent action distribution, and use the generative capabilities of the model to impose an implicit constraint on the generated actions. Thereby eliminating the need to use additional uncertainty penalties on the Bellman update and significantly decreasing the number of gradient steps required to learn a policy. We empirically evaluate C-LAP on the D4RL and V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art methods, especially outperforming on datasets with visual observations.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/58e3180fda4db56e3a9b66538eb1714ad00f3837.pdf'}, '_bibtex': {'value': '@inproceedings{\\nalles2024constrained,\\ntitle={Constrained Latent Action Policies for Model-Based Offline Reinforcement Learning},\\nauthor={Marvin Alles and Philip Becker-Ehmck and Patrick van der Smagt and Maximilian Karl},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pEhvscmSgG}\\n}'}, 'paperhash': {'value': 'alles|constrained_latent_action_policies_for_modelbased_offline_reinforcement_learning'}},forum = 'pEhvscmSgG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19177/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19177/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19177/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19177/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pCVxYw6FKg',number = 17994,cdate = 1715783052345,pdate = 1727288167096,odate = 1730873987243,mdate = 1730873987254,tcdate = 1715783052345,tmdate = 1730873987254,ddate = None,content = {'title': {'value': 'The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof'}, 'authors': {'value': ['Derek Lim', 'Theo Putterman', 'Robin Walters', 'Haggai Maron', 'Stefanie Jegelka']}, 'authorids': {'value': ['~Derek_Lim1', '~Theo_Putterman1', '~Robin_Walters1', '~Haggai_Maron1', '~Stefanie_Jegelka3']}, 'keywords': {'value': ['Parameter symmetry', 'loss landscapes', 'identifiability', 'mode connectivity', 'Bayesian neural networks']}, 'TLDR': {'value': 'We develop neural network architectures with less parameter-space symmetries, and empirically study the impact of parameter symmetries on various phenomena in deep learning.'}, 'abstract': {'value': 'Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries --- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenonmena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries.  With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/33fd01ec0c4a23dc04a9d5065ee1e34c696d9042.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlim2024the,\\ntitle={The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof},\\nauthor={Derek Lim and Theo Putterman and Robin Walters and Haggai Maron and Stefanie Jegelka},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pCVxYw6FKg}\\n}'}, 'paperhash': {'value': 'lim|the_empirical_impact_of_neural_parameter_symmetries_or_lack_thereof'}},forum = 'pCVxYw6FKg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17994/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17994/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17994/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17994/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pCJ0l1JVUX',number = 1924,cdate = 1714892643084,pdate = 1727287675781,odate = 1730873853457,mdate = 1730873853474,tcdate = 1714892643084,tmdate = 1730873853474,ddate = None,content = {'title': {'value': 'Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba'}, 'authors': {'value': ['Haoye Dong', 'Aviral Chharia', 'Wenbo Gou', 'Francisco Vicente Carrasco', 'Fernando De la Torre']}, 'authorids': {'value': ['~Haoye_Dong1', '~Aviral_Chharia2', '~Wenbo_Gou1', '~Francisco_Vicente_Carrasco1', '~Fernando_De_la_Torre2']}, 'keywords': {'value': ['3D Hand Reconstruction', 'Mamba', 'State Space Model']}, 'abstract': {'value': \"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, yet they do not fully achieve robust and accurate performance, primarily due to inefficiently modeling spatial relations between joints. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to efficiently learn the spatial relationships between joints for improving reconstruction performance. Specifically, we design a Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5\\\\% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space features and jointly considers global and local features to improve performance. Experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. At the time of this paper's acceptance, Hamba holds the top position, Rank 1, in two competition leaderboards on 3D hand reconstruction.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/52f78a66bec9fab9154b68eb2413119039daa6f5.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024hamba,\\ntitle={Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba},\\nauthor={Haoye Dong and Aviral Chharia and Wenbo Gou and Francisco Vicente Carrasco and Fernando De la Torre},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pCJ0l1JVUX}\\n}'}, 'paperhash': {'value': 'dong|hamba_singleview_3d_hand_reconstruction_with_graphguided_biscanning_mamba'}},forum = 'pCJ0l1JVUX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1924/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1924/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1924/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1924/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'pC44UMwy2v',number = 16884,cdate = 1715775218923,pdate = 1727288139994,odate = 1730873982165,mdate = 1735191692619,tcdate = 1715775218923,tmdate = 1735191692619,ddate = None,content = {'title': {'value': 'Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought'}, 'authors': {'value': ['Qiguang Chen', 'Libo Qin', 'Jiaqi WANG', 'Jingxuan Zhou', 'Wanxiang Che']}, 'authorids': {'value': ['~Qiguang_Chen1', '~Libo_Qin1', '~Jiaqi_WANG11', '~Jingxuan_Zhou2', '~Wanxiang_Che1']}, 'keywords': {'value': ['Chain-of-Thought', 'Reasoning Granularity', 'Reasoning Boundary']}, 'abstract': {'value': 'Chain-of-Thought (CoT) reasoning has emerged as a promising approach for enhancing the performance of large language models (LLMs) on complex reasoning tasks. Recently, a series of studies attempt to explain the mechanisms underlying CoT, aiming to deepen the understanding of its efficacy. Nevertheless, the existing research faces two major challenges: (1) a lack of quantitative metrics to assess CoT capabilities and (2) a dearth of guidance on optimizing CoT performance. Motivated by this, in this work, we introduce a novel reasoning boundary framework (RBF) to address these challenges. To solve the lack of quantification, we first define a reasoning boundary (RB) to quantify the upper-bound of CoT and establish a combination law for RB, enabling a practical quantitative approach applicable to various real-world CoT tasks. To address the lack of optimization, we propose three categories of RBs. We further optimize these categories with combination laws focused on RB promotion and reasoning path optimization for CoT improvement. Through extensive experiments on 27 models and 5 tasks, the study validates the existence and rationality of the proposed framework. Furthermore, it explains the effectiveness of 10 CoT strategies and guides optimization from two perspectives. We hope this work can provide a comprehensive understanding of the boundaries and optimization strategies for reasoning in LLMs. Our code and data are available at https://github.com/LightChen233/reasoning-boundary.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/47a165ca745dea00bf9fe4ba52210932fb6d1787.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024unlocking,\\ntitle={Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought},\\nauthor={Qiguang Chen and Libo Qin and Jiaqi WANG and Jingxuan Zhou and Wanxiang Che},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pC44UMwy2v}\\n}'}, 'paperhash': {'value': 'chen|unlocking_the_capabilities_of_thought_a_reasoning_boundary_framework_to_quantify_and_optimize_chainofthought'}},forum = 'pC44UMwy2v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16884/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16884/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16884/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16884/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'pASJxzMJb7',number = 9277,cdate = 1715677697499,pdate = 1727287905898,odate = 1730873919420,mdate = 1737028200833,tcdate = 1715677697499,tmdate = 1737028200833,ddate = None,content = {'title': {'value': 'Zipfian Whitening'}, 'authors': {'value': ['Sho Yokoi', 'Han Bao', 'Hiroto Kurita', 'Hidetoshi Shimodaira']}, 'authorids': {'value': ['~Sho_Yokoi1', '~Han_Bao2', '~Hiroto_Kurita1', '~Hidetoshi_Shimodaira1']}, 'keywords': {'value': ['representation learning', 'word embeddings', 'isotropy', 'natural language processing']}, 'TLDR': {'value': 'We propose a method to correct anisotropy in the word embedding space by accounting for word frequency; then justify our approach from the perspectives of generative models, word embedding norms, and errors in long-tail distributions.'}, 'abstract': {'value': \"The word embedding space in neural models is skewed, and correcting this can improve task performance.\\nWe point out that most approaches for modeling, correcting, and measuring the symmetry of an embedding space implicitly assume that the word frequencies are *uniform*; in reality, word frequencies follow a highly non-uniform distribution, known as *Zipf's law*.\\nSurprisingly, simply performing PCA whitening weighted by the empirical word frequency that follows Zipf's law significantly improves task performance, surpassing established baselines.\\nFrom a theoretical perspective, both our approach and existing methods can be clearly categorized: word representations are distributed according to an exponential family with either uniform or Zipfian base measures.\\nBy adopting the latter approach, we can naturally emphasize informative low-frequency words in terms of their vector norm, which becomes evident from the information-geometric perspective (Oyama et al., EMNLP 2023), and in terms of the loss functions for imbalanced classification (Menon et al. ICLR 2021).\\nAdditionally, our theory corroborates that popular natural language processing methods, such as skip-gram negative sampling (Mikolov et al., NIPS 2013), WhiteningBERT (Huang et al., Findings of EMNLP 2021), and headless language models (Godey et al., ICLR 2024), work well just because their word embeddings encode the empirical word frequency into the underlying probabilistic model.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2e52007f91166840e85e1e5b1d50979bc791b3fe.pdf'}, 'supplementary_material': {'value': '/attachment/467a9702d4364b40ecc3a9c2c193c2348086aaeb.zip'}, '_bibtex': {'value': '@inproceedings{\\nyokoi2024zipfian,\\ntitle={Zipfian Whitening},\\nauthor={Sho Yokoi and Han Bao and Hiroto Kurita and Hidetoshi Shimodaira},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=pASJxzMJb7}\\n}'}, 'paperhash': {'value': 'yokoi|zipfian_whitening'}},forum = 'pASJxzMJb7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9277/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9277/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9277/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9277/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p54CYwdjVP',number = 4870,cdate = 1715476703570,pdate = 1727287763464,odate = 1730873879755,mdate = 1730873879773,tcdate = 1715476703570,tmdate = 1730873879773,ddate = None,content = {'title': {'value': 'A Globally Optimal Portfolio for m-Sparse Sharpe Ratio Maximization'}, 'authors': {'value': ['Yizun Lin', 'Zhao-Rong Lai', 'Cheng Li']}, 'authorids': {'value': ['~Yizun_Lin1', '~Zhao-Rong_Lai1', '~Cheng_Li24']}, 'keywords': {'value': ['Sharpe ratio', '$\\\\ell_0$ constraint', 'proximal gradient algorithm', 'global optimality']}, 'TLDR': {'value': 'We exploit the Kurdyka-Lojasiewicz property to develop an efficient proximal gradient algorithm that leads to a portfolio which achieves the globally optimal m-sparse Sharpe ratio.'}, 'abstract': {'value': 'The Sharpe ratio is an important and widely-used risk-adjusted return in financial engineering. In modern portfolio management, one may require an m-sparse (no more than m active assets) portfolio to save managerial and financial costs. However, few existing methods can optimize the Sharpe ratio with the m-sparse constraint, due to the nonconvexity and the complexity of this constraint. We propose to convert the m-sparse fractional optimization problem into an equivalent m-sparse quadratic programming problem. The semi-algebraic property of the resulting objective function allows us to exploit the Kurdyka-Lojasiewicz property to develop an efficient Proximal Gradient Algorithm (PGA) that leads to a portfolio which achieves the globally optimal m-sparse Sharpe ratio under certain conditions. The convergence rates of PGA are also provided. To the best of our knowledge, this is the first proposal that achieves a globally optimal m-sparse Sharpe ratio with a theoretically-sound guarantee.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/4af49110b8c57c67581b78c99890cbd38214e046.zip'}, 'pdf': {'value': '/pdf/46ac508deb5379c54e47d2192ddac171e2014a47.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024a,\\ntitle={A Globally Optimal Portfolio for m-Sparse Sharpe Ratio Maximization},\\nauthor={Yizun Lin and Zhao-Rong Lai and Cheng Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p54CYwdjVP}\\n}'}, 'paperhash': {'value': 'lin|a_globally_optimal_portfolio_for_msparse_sharpe_ratio_maximization'}},forum = 'p54CYwdjVP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4870/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4870/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4870/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4870/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p50Dyqk0GX',number = 4,cdate = 1713816058333,pdate = 1727287627500,odate = 1730873836641,mdate = 1730873837349,tcdate = 1713816058333,tmdate = 1730873837349,ddate = None,content = {'title': {'value': 'Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models'}, 'authors': {'value': ['Kaican Li', 'Weiyan Xie', 'Yongxiang Huang', 'Didan Deng', 'Lanqing HONG', 'Zhenguo Li', 'Ricardo Silva', 'Nevin L. Zhang']}, 'authorids': {'value': ['~Kaican_Li1', '~Weiyan_Xie1', '~Yongxiang_Huang1', '~Didan_Deng1', '~Lanqing_HONG1', '~Zhenguo_Li1', '~Ricardo_Silva1', '~Nevin_L._Zhang1']}, 'keywords': {'value': ['robustness', 'fine-tuning zero-shot models', 'CLIP', 'concept descriptions']}, 'abstract': {'value': 'Fine-tuning foundation models often compromises their robustness to distribution shifts. To remedy this, most robust fine-tuning methods aim to preserve the pre-trained features. However, not all pre-trained features are robust and those methods are largely indifferent to which ones to preserve. We propose dual risk minimization (DRM), which combines empirical risk minimization with worst-case risk minimization, to better preserve the core features of downstream tasks. In particular, we utilize core-feature descriptions generated by LLMs to induce core-based zero-shot predictions which then serve as proxies to estimate the worst-case risk. DRM balances two crucial aspects of model robustness: expected performance and worst-case performance, establishing a new state of the art on various real-world benchmarks. DRM significantly improves the out-of-distribution performance of CLIP ViT-L/14@336 on ImageNet (75.9$\\\\to$77.1), WILDS-iWildCam (47.1$\\\\to$51.8), and WILDS-FMoW (50.7$\\\\to$53.1); opening up new avenues for robust fine-tuning. Our code is available at https://github.com/vaynexie/DRM.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Combining empirical and worst-case risk minimization enhances the robustness of fine-tuned CLIP models.'}, 'pdf': {'value': '/pdf/37aa594599d17d50f06646d1223c501116d57857.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024dual,\\ntitle={Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models},\\nauthor={Kaican Li and Weiyan Xie and Yongxiang Huang and Didan Deng and Lanqing HONG and Zhenguo Li and Ricardo Silva and Nevin L. Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p50Dyqk0GX}\\n}'}, 'paperhash': {'value': 'li|dual_risk_minimization_towards_nextlevel_robustness_in_finetuning_zeroshot_models'}},forum = 'p50Dyqk0GX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p4a1nSvwD7',number = 3569,cdate = 1715305542560,pdate = 1727287724865,odate = 1730873867795,mdate = 1730873867813,tcdate = 1715305542560,tmdate = 1730873867813,ddate = None,content = {'title': {'value': 'From Dictionary to Tensor: A Scalable Multi-View Subspace Clustering Framework with Triple Information Enhancement'}, 'authors': {'value': ['Zhibin Gu', 'Songhe Feng']}, 'authorids': {'value': ['~Zhibin_Gu1', '~Songhe_Feng1']}, 'keywords': {'value': ['Multi-view clustering', 'Low-rank tensor representation', 'Anchor hypergraph Laplacian regularization']}, 'abstract': {'value': 'While Tensor-based Multi-view Subspace Clustering (TMSC) has garnered significant attention for its capacity to effectively capture high-order correlations among multiple views, three notable limitations in current TMSC methods necessitate consideration: 1) high computational complexity and reliance on dictionary completeness resulting from using observed data as the dictionary, 2) inaccurate subspace representation stemming from the oversight of local geometric information and 3) under-penalization of noise-related singular values within tensor data caused by treating all singular values equally. To address these limitations, this paper presents a \\\\textbf{S}calable TMSC framework with \\\\textbf{T}riple inf\\\\textbf{O}rmatio\\\\textbf{N} \\\\textbf{E}nhancement (\\\\textbf{STONE}). Notably, an enhanced anchor dictionary learning mechanism has been utilized to recover the low-rank anchor structure, resulting in reduced computational complexity and increased resilience, especially in scenarios with inadequate dictionaries. Additionally, we introduce an anchor hypergraph Laplacian regularizer to preserve the inherent geometry of the data within the subspace representation. Simultaneously, an improved hyperbolic tangent function has been employed as a precise approximation for tensor rank, effectively capturing the significant variations in singular values. Extensive experimentation on a variety of datasets demonstrates that our approach surpasses SOTA methods in both effectiveness and efficiency.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a9b52ea6298c33aa1b741a10daf60f7b9a8b2a08.pdf'}, 'supplementary_material': {'value': '/attachment/47b5a6f1560f7e297ee31c96c5aabcb32bb20c7b.zip'}, '_bibtex': {'value': '@inproceedings{\\ngu2024from,\\ntitle={From Dictionary to Tensor: A Scalable Multi-View Subspace Clustering Framework with Triple Information Enhancement},\\nauthor={Zhibin Gu and Songhe Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p4a1nSvwD7}\\n}'}, 'paperhash': {'value': 'gu|from_dictionary_to_tensor_a_scalable_multiview_subspace_clustering_framework_with_triple_information_enhancement'}},forum = 'p4a1nSvwD7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3569/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3569/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3569/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3569/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p43ObIwJFW',number = 9995,cdate = 1715688891343,pdate = 1727287927007,odate = 1730873925278,mdate = 1730873925292,tcdate = 1715688891343,tmdate = 1730873925292,ddate = None,content = {'title': {'value': 'Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way'}, 'authors': {'value': ['Ming Chen', 'Jie Chun', 'Shang Xiang', 'Luona Wei', 'Yonghao Du', 'Qian Wan', 'Yuning Chen', 'Yingwu Chen']}, 'authorids': {'value': ['~Ming_Chen17', '~Jie_Chun1', '~Shang_Xiang1', '~Luona_Wei1', '~Yonghao_Du1', '~Qian_Wan2', '~Yuning_Chen2', '~Yingwu_Chen2']}, 'keywords': {'value': ['quadratic unconstrained binary optimization', 'combinational optimization', 'machine learning', 'classification', 'neural solver']}, 'TLDR': {'value': 'We propose a neural solver to solve quadratic unconstrained binary optimization in a classification way, which can achieve near-optimal solutions in milliseconds,  even for instances comprising thousands of variables.'}, 'abstract': {'value': 'The quadratic unconstrained binary optimization (QUBO) is a well-known NP-hard problem that takes an $n\\\\times n$ matrix $Q$ as input and decides an $n$-dimensional 0-1 vector $x$, to optimize a quadratic function. Existing learning-based models that always formulate the solution process as sequential decisions suffer from high computational overload. To overcome this issue, we propose a neural solver called the Value Classification Model (VCM) that formulates the solution process from a classification perspective. It applies a Depth Value Network (DVN) based on graph convolution that exploits the symmetry property in $Q$ to auto-grasp value features. These features are then fed into a Value Classification Network (VCN) which directly generates classification solutions. Trained by a highly efficient model-tailored Greedy-guided Self Trainer (GST) which does not require any priori optimal labels, VCM significantly outperforms competitors in both computational efficiency and solution quality with a remarkable generalization ability. It can achieve near-optimal solutions in milliseconds with an average optimality gap of just 0.362\\\\% on benchmarks with up to 2500 variables. Notably, a VCM trained at a specific DVN depth can steadily find better solutions by simply extending the testing depth, which narrows the gap to 0.034\\\\% on benchmarks. To our knowledge, this is the first learning-based model to reach such a performance.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5ac36f107f507f095117ea3f08a401ea0fc342a9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024learning,\\ntitle={Learning to Solve Quadratic Unconstrained Binary Optimization in a Classification Way},\\nauthor={Ming Chen and Jie Chun and Shang Xiang and Luona Wei and Yonghao Du and Qian Wan and Yuning Chen and Yingwu Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p43ObIwJFW}\\n}'}, 'paperhash': {'value': 'chen|learning_to_solve_quadratic_unconstrained_binary_optimization_in_a_classification_way'}},forum = 'p43ObIwJFW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9995/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9995/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9995/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9995/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'p3tSEFMwpG',number = 20425,cdate = 1715796318983,pdate = 1727288233610,odate = 1730874001227,mdate = 1735829321874,tcdate = 1715796318983,tmdate = 1735829321874,ddate = None,content = {'title': {'value': 'Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data'}, 'authors': {'value': ['Kai Helli', 'David Schnurr', 'Noah Hollmann', 'Samuel Müller', 'Frank Hutter']}, 'authorids': {'value': ['~Kai_Helli1', '~David_Schnurr1', '~Noah_Hollmann1', '~Samuel_Müller1', '~Frank_Hutter1']}, 'keywords': {'value': ['Temporal Distribution Shifts', 'In-Context Learning', 'Bayesian Inference', 'Prior-Data Fitted Networks', 'Temporal Domain Generalization', 'Structural Causal Models', 'TabPFN', 'Tabular Data Modeling', 'Out-Of-Distribution Generalization', 'Domain Generalization', 'Meta-Learning', 'Concept Drift']}, 'abstract': {'value': \"While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present Drift-Resilient TabPFN, an approach using In-Context Learning via a Prior-Data Fitted Network, to address distribution shifts in tabular data, outperforming existing methods in terms of performance and calibration.'}, 'pdf': {'value': '/pdf/b3a5a1ac99a58bf02a2ed186dff50e980bba1c89.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhelli2024driftresilient,\\ntitle={Drift-Resilient Tab{PFN}: In-Context Learning Temporal Distribution Shifts on Tabular Data},\\nauthor={Kai Helli and David Schnurr and Noah Hollmann and Samuel M{\\\\\"u}ller and Frank Hutter},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p3tSEFMwpG}\\n}'}, 'paperhash': {'value': 'helli|driftresilient_tabpfn_incontext_learning_temporal_distribution_shifts_on_tabular_data'}},forum = 'p3tSEFMwpG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20425/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20425/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20425/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20425/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p3nPHMpx04',number = 5788,cdate = 1715569921998,pdate = 1727287795132,odate = 1730873888244,mdate = 1736297275208,tcdate = 1715569921998,tmdate = 1736297275208,ddate = None,content = {'title': {'value': 'A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation'}, 'authors': {'value': ['Tomoya Sakai', 'Haoxiang Qiu', 'Takayuki Katsuki', 'Daiki Kimura', 'Takayuki Osogami', 'Tadanobu Inoue']}, 'authorids': {'value': ['~Tomoya_Sakai2', '~Haoxiang_Qiu1', '~Takayuki_Katsuki2', '~Daiki_Kimura1', '~Takayuki_Osogami1', '~Tadanobu_Inoue1']}, 'keywords': {'value': ['few-shot learning', 'semantic segmentation', 'catastrophic forgetting']}, 'abstract': {'value': 'The goal of *generalized* few-shot semantic segmentation (GFSS) is to recognize *novel-class* objects through training with a few annotated examples and the *base-class* model that learned the knowledge about the base classes.\\nUnlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting.\\nCurrent GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning.\\nHowever, we found that a simple rule and standard supervised learning substantially improve the GFSS performance.\\nIn this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above.\\nAlso, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes.\\nThrough numerical experiments, we demonstrated the effectiveness of our method.\\nIt improved in novel-class segmentation performance in the $1$-shot scenario by $6.1$% on the PASCAL-$5^i$ dataset, $4.7$% on the PASCAL-$10^i$ dataset, and $1.0$% on the COCO-$20^i$ dataset.\\nOur code is publicly available at https://github.com/IBM/BCM.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4ec00abcd1e4ddc0c0fea4aeb74b8e5839a3fa8d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsakai2024a,\\ntitle={A Surprisingly Simple Approach to Generalized Few-Shot Semantic Segmentation},\\nauthor={Tomoya Sakai and Haoxiang Qiu and Takayuki Katsuki and Daiki Kimura and Takayuki Osogami and Tadanobu Inoue},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p3nPHMpx04}\\n}'}, 'TLDR': {'value': 'A simple approach without resorting to, e.g., complicated modules and meta-learning improved GFSS performance.'}, 'paperhash': {'value': 'sakai|a_surprisingly_simple_approach_to_generalized_fewshot_semantic_segmentation'}},forum = 'p3nPHMpx04',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5788/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5788/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5788/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5788/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'p3hNrpeWMe',number = 8157,cdate = 1715654563829,pdate = 1727287870706,odate = 1730873909950,mdate = 1730873909962,tcdate = 1715654563829,tmdate = 1730873909962,ddate = None,content = {'title': {'value': 'A Walsh Hadamard Derived Linear Vector Symbolic Architecture'}, 'authors': {'value': ['Mohammad Mahmudul Alam', 'Alexander Oberle', 'Edward Raff', 'Stella Biderman', 'Tim Oates', 'James Holt']}, 'authorids': {'value': ['~Mohammad_Mahmudul_Alam1', '~Alexander_Oberle1', '~Edward_Raff1', '~Stella_Biderman1', '~Tim_Oates2', '~James_Holt1']}, 'keywords': {'value': ['Vector Symbolic Architectures', 'Holographic Reduced Representations', 'Hadamard Transformation', 'HRR', 'VTB', 'MAP', 'HLB']}, 'TLDR': {'value': 'Starting from the Hadamard transform we develop a simple method for neuro-symbolic manipulation of vectors that has desirable properties for deep learning.'}, 'abstract': {'value': \"Vector Symbolic Architectures (VSAs) are one approach to developing Neuro-symbolic AI, where two vectors in $\\\\mathbb{R}^d$ are 'bound' together to produce a new vector in the same space. VSAs support the commutativity and associativity of this binding operation, along with an inverse operation, allowing one to construct symbolic-style manipulations over real-valued vectors. Most VSAs were developed before deep learning and automatic differentiation became popular and instead focused on efficacy in hand-designed systems. In this work, we introduce the Hadamard-derived linear Binding (HLB), which is designed to have favorable computational efficiency, and efficacy in classic VSA tasks, and perform well in differentiable systems.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/87b4b1c14628316f12b4dc3487e0802deba8e72b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nalam2024a,\\ntitle={A Walsh Hadamard Derived Linear Vector Symbolic Architecture},\\nauthor={Mohammad Mahmudul Alam and Alexander Oberle and Edward Raff and Stella Biderman and Tim Oates and James Holt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p3hNrpeWMe}\\n}'}, 'paperhash': {'value': 'alam|a_walsh_hadamard_derived_linear_vector_symbolic_architecture'}},forum = 'p3hNrpeWMe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8157/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8157/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8157/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8157/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p3gMGkHMkM',number = 1654,cdate = 1714736430283,pdate = 1727287667522,odate = 1730873850749,mdate = 1736874355037,tcdate = 1714736430283,tmdate = 1736874355037,ddate = None,content = {'title': {'value': 'Particle Semi-Implicit Variational Inference'}, 'authors': {'value': ['Jen Ning Lim', 'Adam Michael Johansen']}, 'authorids': {'value': ['~Jen_Ning_Lim1', '~Adam_Michael_Johansen1']}, 'keywords': {'value': ['variational inference', 'gradient flow']}, 'abstract': {'value': 'Semi-implicit variational inference (SIVI) enriches the expressiveness of variational\\nfamilies by utilizing a kernel and a mixing distribution to hierarchically define the\\nvariational distribution. Existing SIVI methods parameterize the mixing distribution\\nusing implicit distributions, leading to intractable variational densities. As a result,\\ndirectly maximizing the evidence lower bound (ELBO) is not possible, so they\\nresort to one of the following: optimizing bounds on the ELBO, employing costly\\ninner-loop Markov chain Monte Carlo runs, or solving minimax objectives. In this\\npaper, we propose a novel method for SIVI called Particle Variational Inference\\n(PVI) which employs empirical measures to approximate the optimal mixing\\ndistributions characterized as the minimizer of a free energy functional. PVI arises\\nnaturally as a particle approximation of a Euclidean–Wasserstein gradient flow and,\\nunlike prior works, it directly optimizes the ELBO whilst making no parametric\\nassumption about the mixing distribution. Our empirical results demonstrate that\\nPVI performs favourably compared to other SIVI methods across various tasks.\\nMoreover, we provide a theoretical analysis of the behaviour of the gradient flow\\nof a related free energy functional: establishing the existence and uniqueness of\\nsolutions as well as propagation of chaos results.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5eb4c3bb21789c23444061ea4b22d94c6a4852a1.pdf'}, 'supplementary_material': {'value': '/attachment/59c31b7c9fb7e0ba3afd41d0675dc1fcb6d1b56e.zip'}, '_bibtex': {'value': '@inproceedings{\\nlim2024particle,\\ntitle={Particle Semi-Implicit Variational Inference},\\nauthor={Jen Ning Lim and Adam Michael Johansen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p3gMGkHMkM}\\n}'}, 'paperhash': {'value': 'lim|particle_semiimplicit_variational_inference'}},forum = 'p3gMGkHMkM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1654/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1654/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1654/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1654/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p37NlKi9vl',number = 7185,cdate = 1715614514477,pdate = 1727287838212,odate = 1730873900259,mdate = 1730873900273,tcdate = 1715614514477,tmdate = 1730873900273,ddate = None,content = {'title': {'value': 'Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization'}, 'authors': {'value': ['Davide Buffelli', 'Jamie McGowan', 'Wangkun Xu', 'Alexandru Cioba', 'Da-shan Shiu', 'Guillaume Hennequin', 'Alberto Bernacchia']}, 'authorids': {'value': ['~Davide_Buffelli1', '~Jamie_McGowan1', '~Wangkun_Xu1', '~Alexandru_Cioba1', '~Da-shan_Shiu1', '~Guillaume_Hennequin1', '~Alberto_Bernacchia1']}, 'keywords': {'value': ['Gauss-Newton', 'Optimization', 'Deep Learning', 'Reversible Neural Networks']}, 'abstract': {'value': 'Second-order optimization has been shown to accelerate the training of deep neural networks in many applications, often yielding faster progress per iteration on the training loss compared to first-order optimizers. However, the generalization properties of second-order methods are still being debated. Theoretical investigations have proved difficult to carry out outside the tractable settings of heavily simplified model classes - thus, the relevance of existing theories to practical deep learning applications remains unclear. Similarly, empirical studies in large-scale models and real datasets are significantly confounded by the necessity to approximate second-order updates in practice. It is often unclear whether the observed generalization behaviour arises specifically from the second-order nature of the parameter updates, or instead reflects the specific structured (e.g. Kronecker) approximations used or any damping-based interpolation towards first-order updates. Here, we show for the first time that exact Gauss-Newton (GN) updates take on a tractable form in a class of deep reversible architectures that are sufficiently expressive to be meaningfully applied to common benchmark datasets. We exploit this novel setting to study the training and generalization properties of the GN optimizer. We find that exact GN generalizes poorly. In the mini-batch training setting, this manifests as rapidly saturating progress even on the training loss, with parameter updates found to overfit each mini-batch without producing the features that would support generalization to other mini-batches. In contrast to previous work, we show that our experiments run in the feature learning regime, in which the neural tangent kernel (NTK) changes during the course of training. However, changes in the NTK are not associated with any significant change in neural representations, explaining the lack of generalization.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/86472b35b8afcaf820a9cce70de715247435a5a4.pdf'}, 'supplementary_material': {'value': '/attachment/dab218a4efd4dc2767b90fed555c323b8cf41d22.zip'}, '_bibtex': {'value': '@inproceedings{\\nbuffelli2024exact,\\ntitle={Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization},\\nauthor={Davide Buffelli and Jamie McGowan and Wangkun Xu and Alexandru Cioba and Da-shan Shiu and Guillaume Hennequin and Alberto Bernacchia},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p37NlKi9vl}\\n}'}, 'paperhash': {'value': 'buffelli|exact_tractable_gaussnewton_optimization_in_deep_reversible_architectures_reveal_poor_generalization'}},forum = 'p37NlKi9vl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7185/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7185/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7185/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7185/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p2PO2PUPFY',number = 2209,cdate = 1714995309714,pdate = 1727287683823,odate = 1730873856336,mdate = 1730873856375,tcdate = 1714995309714,tmdate = 1730873856375,ddate = None,content = {'title': {'value': '$\\\\text{Di}^2\\\\text{Pose}$: Discrete Diffusion Model for Occluded 3D Human Pose Estimation'}, 'authors': {'value': ['Weiquan Wang', 'Jun Xiao', 'Chunping Wang', 'Wei Liu', 'Zhao Wang', 'Long Chen']}, 'authorids': {'value': ['~Weiquan_Wang1', '~Jun_Xiao1', '~Chunping_Wang1', '~Wei_Liu3', '~Zhao_Wang5', '~Long_Chen8']}, 'keywords': {'value': ['Occluded 3D human pose estimation; Discrete diffusion model']}, 'abstract': {'value': 'Diffusion models have demonstrated their effectiveness in addressing the inherent uncertainty and indeterminacy in monocular 3D human pose estimation (HPE). \\nDespite their strengths, the need for large search spaces and the corresponding demand for substantial training data make these models prone to generating biomechanically unrealistic poses. \\nThis challenge is particularly noticeable in occlusion scenarios, where the complexity of inferring 3D structures from 2D images intensifies. \\nIn response to these limitations, we introduce the **Di**screte **Di**ffusion **Pose** (**$\\\\text{Di}^2\\\\text{Pose}$**), a novel framework designed for occluded 3D HPE that capitalizes on the benefits of a discrete diffusion model. \\nSpecifically, **$\\\\text{Di}^2\\\\text{Pose}$** employs a two-stage process: it first converts 3D poses into a discrete representation through a pose quantization step, which is subsequently modeled in latent space through a discrete diffusion process. \\nThis methodological innovation restrictively confines the search space towards physically viable configurations and enhances the model’s capability to comprehend how occlusions affect human pose within the latent space. \\nExtensive evaluations conducted on various benchmarks (e.g., Human3.6M, 3DPW, and 3DPW-Occ) have demonstrated its effectiveness.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2f464387bc4d5c53f929c419cd4f83dc55f1e5a3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024textditextpose,\\ntitle={\\\\${\\\\textbackslash}text\\\\{Di\\\\}{\\\\textasciicircum}2{\\\\textbackslash}text\\\\{Pose\\\\}\\\\$: Discrete Diffusion Model for Occluded 3D Human Pose Estimation},\\nauthor={Weiquan Wang and Jun Xiao and Chunping Wang and Wei Liu and Zhao Wang and Long Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p2PO2PUPFY}\\n}'}, 'supplementary_material': {'value': '/attachment/c196b73a791d1437868bdb316f6969c1250c31a1.zip'}, 'paperhash': {'value': 'wang|\\\\textdi^2\\\\textpose_discrete_diffusion_model_for_occluded_3d_human_pose_estimation'}},forum = 'p2PO2PUPFY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2209/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2209/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2209/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2209/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p1ft33Mu3J',number = 20163,cdate = 1715794810190,pdate = 1727288226919,odate = 1730873999640,mdate = 1747068378299,tcdate = 1715794810190,tmdate = 1747068378299,ddate = None,content = {'title': {'value': 'Linear Transformers are Versatile In-Context Learners'}, 'authors': {'value': ['Max Vladymyrov', 'Johannes von Oswald', 'Mark Sandler', 'Rong Ge']}, 'authorids': {'value': ['~Max_Vladymyrov1', '~Johannes_von_Oswald2', '~Mark_Sandler1', '~Rong_Ge1']}, 'keywords': {'value': ['Linear Transformers', 'In-Context Learning', 'Noisy Linear Regression', 'Model Selection', 'Mesa-optimization']}, 'abstract': {'value': 'Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that each layer of a linear transformer maintains a weight vector for an implicit linear regression problem and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We analyze this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our findings show that even linear transformers possess the surprising ability to discover sophisticated optimization strategies.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper shows that linear transformers can implicitly learn sophisticated optimization algorithms, particularly for noisy linear regression, exceeding the performance of traditional methods.'}, 'pdf': {'value': '/pdf/f0bb66de2cdfa918523f8384547b5be538489651.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvladymyrov2024linear,\\ntitle={Linear Transformers are Versatile In-Context Learners},\\nauthor={Max Vladymyrov and Johannes von Oswald and Mark Sandler and Rong Ge},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p1ft33Mu3J}\\n}'}, 'paperhash': {'value': 'vladymyrov|linear_transformers_are_versatile_incontext_learners'}},forum = 'p1ft33Mu3J',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20163/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20163/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20163/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20163/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p1LpXNPmIa',number = 2058,cdate = 1714962978097,pdate = 1727287679557,odate = 1730873854511,mdate = 1730873854530,tcdate = 1714962978097,tmdate = 1730873854530,ddate = None,content = {'title': {'value': 'PromptFix: You Prompt and We Fix the Photo'}, 'authors': {'value': ['Yongsheng Yu', 'Ziyun Zeng', 'Hang Hua', 'Jianlong Fu', 'Jiebo Luo']}, 'authorids': {'value': ['~Yongsheng_Yu1', '~Ziyun_Zeng2', '~Hang_Hua1', '~Jianlong_Fu1', '~Jiebo_Luo1']}, 'keywords': {'value': ['diffusion model', 'image processing']}, 'abstract': {'value': \"Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f858bc374c4a90f1e788a8dcdafcda856fa9915a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyu2024promptfix,\\ntitle={PromptFix: You Prompt and We Fix the Photo},\\nauthor={Yongsheng Yu and Ziyun Zeng and Hang Hua and Jianlong Fu and Jiebo Luo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p1LpXNPmIa}\\n}'}, 'paperhash': {'value': 'yu|promptfix_you_prompt_and_we_fix_the_photo'}},forum = 'p1LpXNPmIa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2058/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2058/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2058/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2058/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'p0BBKhD5aI',number = 12692,cdate = 1715726559130,pdate = 1727288014488,odate = 1730873950984,mdate = 1732036798596,tcdate = 1715726559130,tmdate = 1732036798596,ddate = None,content = {'title': {'value': 'Infinite Limits of Multi-head Transformer Dynamics'}, 'authors': {'value': ['Blake Bordelon', 'Hamza Tahir Chaudhry', 'Cengiz Pehlevan']}, 'authorids': {'value': ['~Blake_Bordelon1', '~Hamza_Tahir_Chaudhry1', '~Cengiz_Pehlevan2']}, 'keywords': {'value': ['infinite limits', 'transformers', 'mean field theory', 'learning dynamics']}, 'TLDR': {'value': 'A theory of various large model size limits for transformers'}, 'abstract': {'value': 'In this work we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations which admit well defined infinite width and depth limits that allow the attention layers to update throughout training, a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite heads, infinite key/query dimension, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and show they maintain the correct scale of updates for both SGD and Adam.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a585f90934bd1a7f438b6cf6acb2ada2329f9c29.pdf'}, 'supplementary_material': {'value': '/attachment/da69fefd99f6d26882846fc5e8b597b2885178b5.zip'}, '_bibtex': {'value': '@inproceedings{\\nbordelon2024infinite,\\ntitle={Infinite Limits of Multi-head Transformer Dynamics},\\nauthor={Blake Bordelon and Hamza Tahir Chaudhry and Cengiz Pehlevan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=p0BBKhD5aI}\\n}'}, 'paperhash': {'value': 'bordelon|infinite_limits_of_multihead_transformer_dynamics'}},forum = 'p0BBKhD5aI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12692/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12692/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12692/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12692/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oyl2Fnzune',number = 10862,cdate = 1715698408347,pdate = 1727287953448,odate = 1730873932979,mdate = 1735139870214,tcdate = 1715698408347,tmdate = 1735139870214,ddate = None,content = {'title': {'value': 'Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE'}, 'authors': {'value': ['Xun Zhu', 'Ying Hu', 'Fanbin Mo', 'Miao Li', 'Ji Wu']}, 'authorids': {'value': ['~Xun_Zhu1', '~Ying_Hu4', '~Fanbin_Mo1', '~Miao_Li10', '~Ji_Wu3']}, 'keywords': {'value': ['medical generalist foundation model', 'multi-task learning', 'mixture-of-experts']}, 'abstract': {'value': 'Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/MSIIP/Uni-Med.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10ce18d0d85e41e7de3cde824ce12541b90ae181.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024unimed,\\ntitle={Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE},\\nauthor={Xun Zhu and Ying Hu and Fanbin Mo and Miao Li and Ji Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oyl2Fnzune}\\n}'}, 'paperhash': {'value': 'zhu|unimed_a_unified_medical_generalist_foundation_model_for_multitask_learning_via_connectormoe'}},forum = 'oyl2Fnzune',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10862/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10862/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10862/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10862/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'oyiBLfNJvY',number = 11947,cdate = 1715713645085,pdate = 1727287989152,odate = 1730873943987,mdate = 1730873943998,tcdate = 1715713645085,tmdate = 1730873943998,ddate = None,content = {'title': {'value': 'Exploration by Learning Diverse Skills through Successor State Representations'}, 'authors': {'value': ['Paul-Antoine LE TOLGUENEC', 'Yann BESSE', 'Florent Teichteil-Königsbuch', 'Dennis George Wilson', 'Emmanuel Rachelson']}, 'authorids': {'value': ['~Paul-Antoine_LE_TOLGUENEC1', '~Yann_BESSE1', '~Florent_Teichteil-Königsbuch1', '~Dennis_George_Wilson1', '~Emmanuel_Rachelson1']}, 'keywords': {'value': ['Reinforcement Learning', 'Exploration', 'Deep Learning']}, 'abstract': {'value': 'The ability to perform different skills can encourage agents to explore. In this work, we aim to construct a set of diverse skills that uniformly cover the state space. We propose a formalization of this search for diverse skills, building on a previous definition based on the mutual information between states and skills. We consider the distribution of states reached by a policy conditioned on each skill and leverage the successor state representation to maximize the difference between these skill distributions. We call this approach LEADS: Learning Diverse Skills through Successor State Representations. We demonstrate our approach on a set of maze navigation and robotic control tasks which show that our method is capable of constructing a diverse set of skills which exhaustively cover the state space without relying on reward or exploration bonuses. Our findings demonstrate that this new formalization promotes more robust and efficient exploration by combining mutual information maximization and exploration bonuses.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/dd7daaa95d0d7d28d2b3debd6bc2adb0031ae0f9.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ntolguenec2024exploration,\\ntitle={Exploration by Learning Diverse Skills through Successor State Representations},\\nauthor={Paul-Antoine LE TOLGUENEC and Yann BESSE and Florent Teichteil-K{\\\\\"o}nigsbuch and Dennis George Wilson and Emmanuel Rachelson},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oyiBLfNJvY}\\n}'}, 'paperhash': {'value': 'tolguenec|exploration_by_learning_diverse_skills_through_successor_state_representations'}},forum = 'oyiBLfNJvY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11947/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11947/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11947/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11947/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'owuEcT6BTl',number = 21435,cdate = 1715801815850,pdate = 1727288255090,odate = 1730874005672,mdate = 1734551151648,tcdate = 1715801815850,tmdate = 1734551151648,ddate = None,content = {'title': {'value': 'Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space'}, 'authors': {'value': ['Core Francisco Park', 'Maya Okawa', 'Andrew Lee', 'Ekdeep Singh Lubana', 'Hidenori Tanaka']}, 'authorids': {'value': ['~Core_Francisco_Park1', '~Maya_Okawa1', '~Andrew_Lee2', '~Ekdeep_Singh_Lubana1', '~Hidenori_Tanaka1']}, 'keywords': {'value': ['Learning Dynamics', 'Compositional Generalization', 'Emergent Abilities', 'Diffusion Models', 'Mechanistic Interpretability']}, 'TLDR': {'value': 'We find that compositional generalization abilities of diffusion models emerge suddenly and robustly, while models might not actively exhibit this ability.'}, 'abstract': {'value': 'Modern generative models demonstrate impressive capabilities, likely stemming from an ability to identify and manipulate abstract concepts underlying their training data. However, fundamental questions remain: what determines the concepts a model learns, the order in which it learns them, and its ability to manipulate those concepts? To address these questions, we propose analyzing a model’s learning dynamics via a framework we call the concept space, where each axis represents an independent concept underlying the data generating process. By characterizing learning dynamics in this space, we identify how the speed at which a concept is learned, and hence the order of concept learning, is controlled by properties of the data we term concept signal. Further, we observe moments of sudden turns in the direction of a model’s learning dynamics in concept space. Surprisingly, these points precisely correspond to the emergence of hidden capabilities, i.e., where latent interventions show the model possesses the capability to manipulate a concept, but these capabilities cannot yet be elicited via naive input prompting. While our results focus on synthetically defined toy datasets, we hypothesize a general claim on emergence of hidden capabilities may hold: generative models possess latent capabilities that emerge suddenly and consistently during training, though a model might not exhibit these capabilities under naive input prompting.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cd344c6f2d8a5970e4d4267156a7cfb867521da6.pdf'}, '_bibtex': {'value': '@inproceedings{\\npark2024emergence,\\ntitle={Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept Space},\\nauthor={Core Francisco Park and Maya Okawa and Andrew Lee and Ekdeep Singh Lubana and Hidenori Tanaka},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=owuEcT6BTl}\\n}'}, 'paperhash': {'value': 'park|emergence_of_hidden_capabilities_exploring_learning_dynamics_in_concept_space'}},forum = 'owuEcT6BTl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21435/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21435/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21435/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21435/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'owHj0G15cd',number = 15189,cdate = 1715758037467,pdate = 1727288090649,odate = 1730873971323,mdate = 1736847346127,tcdate = 1715758037467,tmdate = 1736847346127,ddate = None,content = {'title': {'value': 'Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits'}, 'authors': {'value': ['Tian Huang', 'Shengbo Wang', 'Ke Li']}, 'authorids': {'value': ['~Tian_Huang3', '~Shengbo_Wang2', '~Ke_Li5']}, 'keywords': {'value': ['multi-objective optimization', 'dueling bandit', 'interactive multi-objective optimization', 'preference learning']}, 'abstract': {'value': 'The ultimate goal of multi-objective optimization (MO) is to assist human decision-makers (DMs) in identifying solutions of interest (SOI) that optimally reconcile multiple objectives according to their preferences. Preference-based evolutionary MO (PBEMO) has emerged as a promising framework that progressively approximates SOI by involving human in the optimization-cum-decision-making process. Yet, current PBEMO approaches are prone to be inefficient and misaligned with the DM’s true aspirations, especially when inadvertently exploiting mis-calibrated reward models. This is further exacerbated when considering the stochastic nature of human feedback. This paper proposes a novel framework that navigates MO to SOI by directly leveraging human feedback without being restricted by a predefined reward model nor cumbersome model selection. Specifically, we developed a clustering-based stochastic dueling bandits algorithm that strategically scales well to high-dimensional dueling bandits, and achieves a regret of $\\\\mathcal{O}(K^2\\\\log T)$, where $K$ is the number of clusters and $T$ is the number of rounds. The learned preferences are then transformed into a unified probabilistic format that can be readily adapted to prevalent EMO algorithms. This also leads to a principled termination criterion that strategically manages human cognitive loads and computational budget. Experiments on $48$ benchmark test problems, including synthetic problems, RNA inverse design and protein structure prediction, fully demonstrate the effectiveness of our proposed approach.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/93d75c0607de645189e6114e7cc0f9816082393a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024direct,\\ntitle={Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits},\\nauthor={Tian Huang and Shengbo Wang and Ke Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=owHj0G15cd}\\n}'}, 'paperhash': {'value': 'huang|direct_preferencebased_evolutionary_multiobjective_optimization_with_dueling_bandits'}},forum = 'owHj0G15cd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15189/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15189/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15189/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15189/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ouoBW2PXFQ',number = 9759,cdate = 1715685293869,pdate = 1727287920124,odate = 1730873923239,mdate = 1734590128010,tcdate = 1715685293869,tmdate = 1734590128010,ddate = None,content = {'title': {'value': 'CALANet: Cheap All-Layer Aggregation for Human Activity Recognition'}, 'authors': {'value': ['Jaegyun Park', 'Dae-Won Kim', 'Jaesung Lee']}, 'authorids': {'value': ['~Jaegyun_Park2', '~Dae-Won_Kim1', '~Jaesung_Lee2']}, 'keywords': {'value': ['Human activity recognition', 'Wearable sensors', 'Neural networks', 'Real-time systems']}, 'TLDR': {'value': 'We proposed CALANet that allows the classifier to aggregate features for all layer while maintaining the efficiency of existing real-time HAR models.'}, 'abstract': {'value': 'With the steady growth of sensing technology and wearable devices, sensor-based human activity recognition has become essential in widespread applications, such as healthcare monitoring and fitness tracking, where accurate and real-time systems are required. \\nTo achieve real-time response, recent studies have focused on lightweight neural network models.\\nSpecifically, they designed the network architectures by restricting the number of layers shallowly or connections of each layer.\\nHowever, these approaches suffer from limited accuracy because the classifier only uses the features at the last layer.\\nIn this study, we propose a cheap all-layer aggregation network, CALANet, for accuracy improvement while maintaining the efficiency of existing real-time HAR models.\\nSpecifically, CALANet allows the classifier to aggregate the features for all layers, resulting in a performance gain.\\nIn addition, this work proves that the theoretical computation cost of CALANet is equivalent to that of conventional networks. \\nEvaluated on seven publicly available datasets, CALANet outperformed existing methods, achieving state-of-the-art performance. \\nThe source codes of the CALANet are publicly available at https://github.com/jgpark92/CALANet.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f165b0565c501ec05544215e221a6ea53e672cfa.pdf'}, 'supplementary_material': {'value': '/attachment/72a4cc2f97451f7a7534aa09bb9b7032a193c6d1.zip'}, '_bibtex': {'value': '@inproceedings{\\npark2024calanet,\\ntitle={{CALAN}et: Cheap All-Layer Aggregation for Human Activity Recognition},\\nauthor={Jaegyun Park and Dae-Won Kim and Jaesung Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ouoBW2PXFQ}\\n}'}, 'paperhash': {'value': 'park|calanet_cheap_alllayer_aggregation_for_human_activity_recognition'}},forum = 'ouoBW2PXFQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9759/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9759/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9759/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9759/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'otxOtsWCMb',number = 1717,cdate = 1714759432778,pdate = 1727287669252,odate = 1730873851361,mdate = 1730873851381,tcdate = 1714759432778,tmdate = 1730873851381,ddate = None,content = {'title': {'value': 'From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos'}, 'authors': {'value': ['Matthew Wallingford', 'Anand Bhattad', 'Aditya Kusupati', 'Vivek Ramanujan', 'Matt Deitke', 'Aniruddha Kembhavi', 'Roozbeh Mottaghi', 'Wei-Chiu Ma', 'Ali Farhadi']}, 'authorids': {'value': ['~Matthew_Wallingford1', '~Anand_Bhattad1', '~Aditya_Kusupati1', '~Vivek_Ramanujan1', '~Matt_Deitke1', '~Aniruddha_Kembhavi1', '~Roozbeh_Mottaghi1', '~Wei-Chiu_Ma1', '~Ali_Farhadi3']}, 'keywords': {'value': ['Novel View Synthesis', '3D', 'Video', '360 Video', 'Large-Scale', 'Data', 'Scene Generation']}, 'TLDR': {'value': 'Empowered by the largest real-world, multi-view dataset to date, we present the first model to reasonably synthesize real-world 3D scenes and reconstruct their geometry conditioned on a single image.'}, 'abstract': {'value': \"Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.  In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e92b581e47ffbaa019c305f5181802de017fd66e.pdf'}, 'supplementary_material': {'value': '/attachment/bd85b6068d813279b5a145c8c87a9a185320139f.zip'}, '_bibtex': {'value': '@inproceedings{\\nwallingford2024from,\\ntitle={From an Image to a Scene: Learning to Imagine the World from a Million 360{\\\\textdegree} Videos},\\nauthor={Matthew Wallingford and Anand Bhattad and Aditya Kusupati and Vivek Ramanujan and Matt Deitke and Aniruddha Kembhavi and Roozbeh Mottaghi and Wei-Chiu Ma and Ali Farhadi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=otxOtsWCMb}\\n}'}, 'paperhash': {'value': 'wallingford|from_an_image_to_a_scene_learning_to_imagine_the_world_from_a_million_360_videos'}},forum = 'otxOtsWCMb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1717/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1717/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1717/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1717/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'otZPBS0un6',number = 5746,cdate = 1715568021746,pdate = 1727287793695,odate = 1730873887844,mdate = 1730873887855,tcdate = 1715568021746,tmdate = 1730873887855,ddate = None,content = {'title': {'value': 'FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge'}, 'authors': {'value': ['Hanzhe LI', 'Jiaran Zhou', 'Yuezun Li', 'Baoyuan Wu', 'Bin Li', 'Junyu Dong']}, 'authorids': {'value': ['~Hanzhe_LI2', '~Jiaran_Zhou1', '~Yuezun_Li1', '~Baoyuan_Wu1', '~Bin_Li16', '~Junyu_Dong1']}, 'keywords': {'value': ['DeepFake Detection', 'Security and Privacy', 'Multimedia Forensics']}, 'abstract': {'value': 'Generating synthetic fake faces, known as pseudo-fake faces, is an effective way to improve the generalization of DeepFake detection. Existing methods typically generate these faces by blending real or fake faces in spatial domain. While these methods have shown promise, they overlook the simulation of frequency distribution in pseudo-fake faces, limiting the learning of generic forgery traces in-depth. To address this, this paper introduces {\\\\em FreqBlender}, a new method that can generate pseudo-fake faces by blending frequency knowledge. Concretely, we investigate the major frequency components and propose a Frequency Parsing Network to adaptively partition frequency components related to forgery traces. Then we blend this frequency knowledge from fake faces into real faces to generate pseudo-fake faces. Since there is no ground truth for frequency components, we describe a dedicated training strategy by leveraging the inner correlations among different frequency knowledge to instruct the learning process. Experimental results demonstrate the effectiveness of our method in enhancing DeepFake detection, making it a potential plug-and-play strategy for other methods.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/069e8992a7029f0458ddc78b200f7894c856f29b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024freqblender,\\ntitle={FreqBlender: Enhancing DeepFake Detection by Blending Frequency Knowledge},\\nauthor={Hanzhe LI and Jiaran Zhou and Yuezun Li and Baoyuan Wu and Bin Li and Junyu Dong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=otZPBS0un6}\\n}'}, 'paperhash': {'value': 'li|freqblender_enhancing_deepfake_detection_by_blending_frequency_knowledge'}},forum = 'otZPBS0un6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5746/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5746/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5746/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5746/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'os14qXhy55',number = 6823,cdate = 1715605645141,pdate = 1727287826535,odate = 1730873896527,mdate = 1730873896549,tcdate = 1715605645141,tmdate = 1730873896549,ddate = None,content = {'title': {'value': 'OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries'}, 'authors': {'value': ['Yuhang Lu', 'Xinge ZHU', 'Tai Wang', 'Yuexin Ma']}, 'authorids': {'value': ['~Yuhang_Lu2', '~Xinge_ZHU2', '~Tai_Wang2', '~Yuexin_Ma2']}, 'keywords': {'value': ['3D Scene Understanding; Occupancy Prediction; Octree']}, 'abstract': {'value': 'Occupancy prediction has increasingly garnered attention in recent years for its fine-grained understanding of 3D scenes. Traditional approaches typically rely on dense, regular grid representations, which often leads to excessive computational demands and a loss of spatial details for small objects. This paper introduces OctreeOcc, an innovative 3D occupancy prediction framework that leverages the octree representation to adaptively capture valuable information in 3D, offering variable granularity to accommodate object shapes and semantic regions of varying sizes and complexities. In particular, we incorporate image semantic information to improve the accuracy of initial octree structures and design an effective rectification mechanism to refine the octree structure iteratively. Our extensive evaluations show that OctreeOcc not only surpasses state-of-the-art methods in occupancy prediction, but also achieves a 15%-24% reduction in computational overhead compared to dense-grid-based methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/11a4477db5fea940dca91084f19ce4645c3cd0fa.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlu2024octreeocc,\\ntitle={OctreeOcc: Efficient and Multi-Granularity Occupancy Prediction Using Octree Queries},\\nauthor={Yuhang Lu and Xinge ZHU and Tai Wang and Yuexin Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=os14qXhy55}\\n}'}, 'paperhash': {'value': 'lu|octreeocc_efficient_and_multigranularity_occupancy_prediction_using_octree_queries'}},forum = 'os14qXhy55',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6823/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6823/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6823/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6823/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'orxQccN8Fm',number = 14483,cdate = 1715750400373,pdate = 1727288070889,odate = 1730873966160,mdate = 1734717419024,tcdate = 1715750400373,tmdate = 1734717419024,ddate = None,content = {'title': {'value': 'Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment'}, 'authors': {'value': ['Jiaxiang Li', 'Siliang Zeng', 'Hoi To Wai', 'Chenliang Li', 'Alfredo Garcia', 'Mingyi Hong']}, 'authorids': {'value': ['~Jiaxiang_Li1', '~Siliang_Zeng1', '~Hoi_To_Wai1', '~Chenliang_Li3', '~Alfredo_Garcia1', '~Mingyi_Hong1']}, 'keywords': {'value': ['Large language models', 'Fine-tune', 'alignment', 'Reinforcement learning']}, 'abstract': {'value': 'Aligning human preference and value is an important requirement for contemporary foundation models. State-of-the-art techniques such as Reinforcement Learning from Human Feedback (RLHF) often consist of two stages: 1) supervised fine-tuning (SFT), where the model is fine-tuned by learning from human demonstration data; 2) Preference learning, where preference data is used to learn a reward model, which is in turn used by a reinforcement learning (RL) step to fine-tune the model. Such reward model serves as a proxy to human preference, and it is critical to guide the RL step towards improving the model quality. In this work, we argue that the SFT stage significantly benefits from learning a reward model as well. Instead of using the human demonstration data directly via supervised learning, we propose to leverage an Inverse Reinforcement Learning (IRL) technique to {\\\\it simultaneously} build an reward model and a policy model. This approach leads to new SFT algorithms that are not only efficient to implement, but are robust to the presence of low-quality supervised learning data. Moreover, we discover a connection between the proposed IRL based approach, and a recent line of works called Self-Play Fine-tune (SPIN, \\\\cite{chen2024self}). Theoretically, we show that the proposed algorithms converge to the stationary solutions of the IRL problem. Empirically, we align 1B and 7B models using proposed methods and evaluate them on a reward benchmark model and the HuggingFace Open LLM Leaderboard. The proposed methods show significant performance improvement over existing SFT approaches. Our results indicate that it is beneficial to leverage reward learning throughout the entire alignment process. Our code is available at \\\\url{https://github.com/JasonJiaxiangLi/Reward_learning_SFT}.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/24345326e081292ae57e5fa1b701d8151b01f32f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024getting,\\ntitle={Getting More Juice Out of the {SFT} Data: Reward Learning from Human Demonstration Improves {SFT} for {LLM} Alignment},\\nauthor={Jiaxiang Li and Siliang Zeng and Hoi To Wai and Chenliang Li and Alfredo Garcia and Mingyi Hong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=orxQccN8Fm}\\n}'}, 'TLDR': {'value': 'Reward Learning from Human Demonstration Improves SFT for LLM Alignment'}, 'paperhash': {'value': 'li|getting_more_juice_out_of_the_sft_data_reward_learning_from_human_demonstration_improves_sft_for_llm_alignment'}},forum = 'orxQccN8Fm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14483/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14483/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14483/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14483/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oqdy2EFrja',number = 1768,cdate = 1714799141435,pdate = 1727287670906,odate = 1730873851890,mdate = 1730873851910,tcdate = 1714799141435,tmdate = 1730873851910,ddate = None,content = {'title': {'value': 'Compositional 3D-aware Video Generation with LLM Director'}, 'authors': {'value': ['Hanxin Zhu', 'Tianyu He', 'Anni Tang', 'Junliang Guo', 'Zhibo Chen', 'Jiang Bian']}, 'authorids': {'value': ['~Hanxin_Zhu1', '~Tianyu_He1', '~Anni_Tang1', '~Junliang_Guo1', '~Zhibo_Chen1', '~Jiang_Bian1']}, 'keywords': {'value': ['3D-aware Video Generation; LLM; Compositional']}, 'abstract': {'value': 'Significant progress has been made in text-to-video generation through the use of powerful generative models and large-scale internet data. However, substantial challenges remain in precisely controlling individual elements within the generated video, such as the movement and appearance of specific characters and the manipulation of viewpoints. In this work, we propose a novel paradigm that generates each element in 3D representation separately and then composites them with priors from Large Language Models (LLMs) and 2D diffusion models. Specifically, given an input textual query, our scheme consists of four stages: 1) we leverage the LLMs as the director to first decompose the complex query into several sub-queries, where each sub-query describes each element of the generated video; 2) to generate each element, pre-trained models are invoked by the LLMs to obtain the corresponding 3D representation; 3) to composite the generated 3D representations, we prompt multi-modal LLMs to produce coarse guidance on the scale, location, and trajectory of different objects; 4) to make the results adhere to natural distribution, we further leverage 2D diffusion priors and use score distillation sampling to refine the composition. Extensive experiments demonstrate that our method can generate high-fidelity videos from text with flexible control over each element.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/208a96eeea481035e495ffccce283af8e1625a86.pdf'}, 'supplementary_material': {'value': '/attachment/f503358b9a24890ebeeef71682c24641703b4cbc.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024compositional,\\ntitle={Compositional 3D-aware Video Generation with {LLM} Director},\\nauthor={Hanxin Zhu and Tianyu He and Anni Tang and Junliang Guo and Zhibo Chen and Jiang Bian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oqdy2EFrja}\\n}'}, 'paperhash': {'value': 'zhu|compositional_3daware_video_generation_with_llm_director'}},forum = 'oqdy2EFrja',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1768/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1768/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1768/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1768/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'opt72TYzwZ',number = 14963,cdate = 1715755637253,pdate = 1727288083958,odate = 1730873969538,mdate = 1730873969558,tcdate = 1715755637253,tmdate = 1730873969558,ddate = None,content = {'title': {'value': 'Optimal ablation for interpretability'}, 'authors': {'value': ['Maximilian Li', 'Lucas Janson']}, 'authorids': {'value': ['~Maximilian_Li1', '~Lucas_Janson2']}, 'keywords': {'value': ['mechanistic intepretability', 'model internals', 'ablation', 'activation patching', 'automatic circuit discovery', 'causal tracing', 'tuned lens']}, 'abstract': {'value': 'Interpretability studies often involve tracing the flow of information through machine learning models to identify specific model components that perform relevant computations for tasks of interest. Prior work quantifies the importance of a model component on a particular task by measuring the impact of performing ablation on that component, or simulating model inference with the component disabled.\\n We propose a new method, optimal ablation (OA), and show that OA-based component importance has theoretical and empirical advantages over measuring importance via other ablation methods. We also show that OA-based component importance can benefit several downstream interpretability tasks, including circuit discovery, localization of factual recall, and latent prediction.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2575d807ee7d657841c4bdd5cfdaed85cb895f09.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024optimal,\\ntitle={Optimal ablation for interpretability},\\nauthor={Maximilian Li and Lucas Janson},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=opt72TYzwZ}\\n}'}, 'paperhash': {'value': 'li|optimal_ablation_for_interpretability'}},forum = 'opt72TYzwZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14963/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14963/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14963/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14963/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'opaRhDvQRD',number = 4054,cdate = 1715352941247,pdate = 1727287738663,odate = 1730873871776,mdate = 1730873871795,tcdate = 1715352941247,tmdate = 1730873871795,ddate = None,content = {'title': {'value': 'Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning'}, 'authors': {'value': ['Wang Xinrui', 'Chuanxing Geng', 'Wenhai Wan', 'Shao-Yuan Li', 'Songcan Chen']}, 'authorids': {'value': ['~Wang_Xinrui1', '~Chuanxing_Geng1', '~Wenhai_Wan1', '~Shao-Yuan_Li1', '~Songcan_Chen1']}, 'keywords': {'value': ['online continual learning', 'data stream', 'model throughput']}, 'TLDR': {'value': 'Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning'}, 'abstract': {'value': \"Online continual learning (OCL) requires the models to learn from constant, endless streams of data. While significant efforts have been made in this field, most were focused on mitigating the \\\\textit{catastrophic forgetting} issue to achieve better classification ability, at the cost of a much heavier training workload. They overlooked that in real-world scenarios, e.g., in high-speed data stream environments, data do not pause to accommodate slow models. In this paper, we emphasize that \\\\textit{model throughput}-- defined as the maximum number of training samples that a model can process within a unit of time --  is equally important. It directly limits how much data a model can utilize and presents a challenging dilemma for current methods. With this understanding, we revisit key challenges in OCL from both empirical and theoretical perspectives, highlighting two critical issues beyond the well-documented catastrophic forgetting: (\\\\romannumeral1) Model's ignorance: the single-pass nature of OCL challenges models to learn effective features within constrained training time and storage capacity, leading to a trade-off between effective learning and model throughput; (\\\\romannumeral2) Model's myopia: the local learning nature of OCL on the current task leads the model to adopt overly simplified, task-specific features and \\\\textit{excessively sparse classifier}, resulting in the gap between the optimal solution for the current task and the global objective. To tackle these issues, we propose the Non-sparse Classifier Evolution framework (NsCE) to facilitate effective global discriminative feature learning with minimal time cost. NsCE integrates non-sparse maximum separation regularization and targeted experience replay techniques with the help of pre-trained models, enabling rapid acquisition of new globally discriminative features. Extensive experiments demonstrate the substantial improvements of our framework in performance, throughput and real-world practicality.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f234665a6b29bf4968da01a5adc0303e595efb5c.pdf'}, 'supplementary_material': {'value': '/attachment/5b5389f97aa0f6cd572584536296184395d9b2a3.zip'}, '_bibtex': {'value': '@inproceedings{\\nxinrui2024forgetting,\\ntitle={Forgetting, Ignorance or Myopia: Revisiting Key Challenges in Online Continual Learning},\\nauthor={Wang Xinrui and Chuanxing Geng and Wenhai Wan and Shao-Yuan Li and Songcan Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=opaRhDvQRD}\\n}'}, 'paperhash': {'value': 'xinrui|forgetting_ignorance_or_myopia_revisiting_key_challenges_in_online_continual_learning'}},forum = 'opaRhDvQRD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4054/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4054/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4054/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4054/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'omyzrkacme',number = 16261,cdate = 1715768518301,pdate = 1727288121508,odate = 1730873978090,mdate = 1730873978103,tcdate = 1715768518301,tmdate = 1730873978103,ddate = None,content = {'title': {'value': 'Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality'}, 'authors': {'value': ['Antoine Scheid', 'Aymeric Capitaine', 'Etienne Boursier', 'Eric Moulines', 'Michael Jordan', 'Alain Oliviero Durmus']}, 'authorids': {'value': ['~Antoine_Scheid1', '~Aymeric_Capitaine1', '~Etienne_Boursier1', '~Eric_Moulines1', '~Michael_Jordan1', '~Alain_Oliviero_Durmus1']}, 'keywords': {'value': ['Online Learning', 'Bandits', 'Algorithmic Game Theory', 'Externality', 'Policy Design', 'Two-Players Game']}, 'abstract': {'value': 'In Economics, the concept of externality refers to any indirect effect resulting from an interaction between players and affecting a third party without compensation. Most of the models within which externality has been studied assume that agents have perfect knowledge of their environment and preferences. This is a major hindrance to the practical implementation of many proposed solutions. To adress this issue, we consider a two-players bandit game setting where the actions of one of the player affect the other one. Building upon this setup, we extend the Coase theorem [Coase, 2013], which suggests that the optimal approach for maximizing the social welfare in the presence of externality is to establish property rights, i.e., enabling transfers and bargaining between the players. Nonetheless, this fundamental result relies on the assumption that bargainers possess perfect knowledge of the underlying game. We first demonstrate that in the absence of property rights in the considered online scenario, the social welfare breaks down. We then provide a policy for the players, which allows them to learn a bargaining strategy which maximizes the total welfare, recovering the Coase theorem under uncertainty.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cfe82f182f81ae3a6a8939ea1eda0a5b491dcba6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nscheid2024learning,\\ntitle={Learning to Mitigate Externalities: the Coase Theorem with Hindsight Rationality},\\nauthor={Antoine Scheid and Aymeric Capitaine and Etienne Boursier and Eric Moulines and Michael Jordan and Alain Oliviero Durmus},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=omyzrkacme}\\n}'}, 'paperhash': {'value': 'scheid|learning_to_mitigate_externalities_the_coase_theorem_with_hindsight_rationality'}},forum = 'omyzrkacme',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16261/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16261/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16261/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16261/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'om2Aa0gUha',number = 2264,cdate = 1715010907002,pdate = 1727287685357,odate = 1730873856857,mdate = 1730873856877,tcdate = 1715010907002,tmdate = 1730873856877,ddate = None,content = {'title': {'value': 'Policy Mirror Descent with Lookahead'}, 'authors': {'value': ['Kimon Protopapas', 'Anas Barakat']}, 'authorids': {'value': ['~Kimon_Protopapas1', '~Anas_Barakat1']}, 'keywords': {'value': ['Policy mirror descent', 'policy gradient methods', 'policy iteration', 'multi-step greedy policy improvement', 'policy optimization']}, 'abstract': {'value': 'Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\\\\gamma$, we show that $h$-PMD which generalizes the standard PMD enjoys a faster dimension-free $\\\\gamma^h$-linear convergence rate, contingent on the computation of multi-step greedy policies. We propose an inexact version of $h$-PMD where lookahead action values are estimated. Under a generative model, we establish a sample complexity for $h$-PMD which improves over prior work. Finally, we extend our result to linear function approximation to scale to large state spaces. Under suitable assumptions, our sample complexity only involves dependence on the dimension of the feature map space instead of the state space size.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3e67678b563dbcb1a4c125114a24df6e808c3c36.pdf'}, 'supplementary_material': {'value': '/attachment/d8d2a06b9bfa06761f3fd82bb38b717d1083cb29.zip'}, '_bibtex': {'value': '@inproceedings{\\nprotopapas2024policy,\\ntitle={Policy Mirror Descent with Lookahead},\\nauthor={Kimon Protopapas and Anas Barakat},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=om2Aa0gUha}\\n}'}, 'paperhash': {'value': 'protopapas|policy_mirror_descent_with_lookahead'}},forum = 'om2Aa0gUha',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2264/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2264/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2264/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2264/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ojLIEQ0j9T',number = 6533,cdate = 1715597556761,pdate = 1727287817653,odate = 1730873894217,mdate = 1736813234990,tcdate = 1715597556761,tmdate = 1736813234990,ddate = None,content = {'title': {'value': 'Shaping the distribution of neural responses with interneurons in a recurrent circuit model'}, 'authors': {'value': ['David Lipshutz', 'Eero P Simoncelli']}, 'authorids': {'value': ['~David_Lipshutz1', '~Eero_P_Simoncelli1']}, 'keywords': {'value': ['Efficient coding', 'optimal transport', 'Gaussianization', 'Hebbian plasticity', 'gain modulation', 'neural adaptation']}, 'TLDR': {'value': 'Motivated by the efficient coding hypothesis, we propose a normative circuit model with local interneurons that learns to reshape its inputs so that its responses follow a target density.'}, 'abstract': {'value': \"Efficient coding theory posits that sensory circuits transform natural signals into neural representations that maximize information transmission subject to resource constraints. Local interneurons are thought to play an important role in these transformations, shaping patterns of circuit activity to facilitate and direct information flow. However, the relationship between these coordinated, nonlinear, circuit-level transformations and the properties of interneurons (e.g., connectivity, activation functions) remains unknown. Here, we propose a normative computational model that establishes such a relationship. Our model is derived from an optimal transport objective that conceptualizes the circuit's input-response function as transforming the inputs to achieve a target response distribution. The circuit, which is comprised of primary neurons that are recurrently connected to a set of local interneurons, continuously optimizes this objective by dynamically adjusting both the synaptic connections between neurons as well as the interneuron activation functions. In an application motivated by redundancy reduction theory, we demonstrate that when the inputs are natural image statistics and the target distribution is a spherical Gaussian, the circuit learns a nonlinear transformation that significantly reduces statistical dependencies in neural responses. Overall, our results provide a framework in which the distribution of circuit responses is systematically and nonlinearly controlled by adjustment of interneuron connectivity and activation functions.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3302d93e32df1925b6db9ae4decff9464e0cd59b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlipshutz2024shaping,\\ntitle={Shaping the distribution of neural responses with interneurons in a recurrent circuit model},\\nauthor={David Lipshutz and Eero P Simoncelli},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ojLIEQ0j9T}\\n}'}, 'paperhash': {'value': 'lipshutz|shaping_the_distribution_of_neural_responses_with_interneurons_in_a_recurrent_circuit_model'}},forum = 'ojLIEQ0j9T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6533/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6533/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6533/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6533/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ojIhvhQBAQ',number = 13300,cdate = 1715738063781,pdate = 1727288034876,odate = 1730873956911,mdate = 1736265586841,tcdate = 1715738063781,tmdate = 1736265586841,ddate = None,content = {'title': {'value': 'Efficient Discrepancy Testing for Learning with Distribution Shift'}, 'authors': {'value': ['Gautam Chandrasekaran', 'Adam Klivans', 'Vasilis Kontonis', 'Konstantinos Stavropoulos', 'Arsen Vasilyan']}, 'authorids': {'value': ['~Gautam_Chandrasekaran1', '~Adam_Klivans1', '~Vasilis_Kontonis1', '~Konstantinos_Stavropoulos1', '~Arsen_Vasilyan1']}, 'keywords': {'value': ['pac learning', 'testable learning', 'distribution shift', 'distribution testing', 'discrepancy distance']}, 'abstract': {'value': 'A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance. While in general hard to compute, here we provide the first set of provably efficient algorithms for testing *localized* discrepancy distance, where discrepancy is computed with respect to a fixed output classifier.  These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023).\\n\\nOur approach generalizes and improves all prior work on TDS learning: (1) we obtain *universal* learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits. Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets. Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7dbbbce4bb0597fffe02bc96c8bab077911404fd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchandrasekaran2024efficient,\\ntitle={Efficient Discrepancy Testing for Learning with Distribution Shift},\\nauthor={Gautam Chandrasekaran and Adam Klivans and Vasilis Kontonis and Konstantinos Stavropoulos and Arsen Vasilyan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ojIhvhQBAQ}\\n}'}, 'paperhash': {'value': 'chandrasekaran|efficient_discrepancy_testing_for_learning_with_distribution_shift'}},forum = 'ojIhvhQBAQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13300/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13300/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13300/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13300/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ohvXBIPV7e',number = 6737,cdate = 1715603540225,pdate = 1727287824036,odate = 1730873895679,mdate = 1730873895697,tcdate = 1715603540225,tmdate = 1730873895697,ddate = None,content = {'title': {'value': 'CSPG: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search'}, 'authors': {'value': ['Ming Yang', 'Yuzheng Cai', 'Weiguo Zheng']}, 'authorids': {'value': ['~Ming_Yang27', '~Yuzheng_Cai1', '~Weiguo_Zheng1']}, 'keywords': {'value': ['similarity search', 'approximate nearest neighbor search', 'high-dimensional space', 'graph index']}, 'abstract': {'value': 'The state-of-the-art approximate nearest neighbor search (ANNS) algorithm builds a large proximity graph on the dataset and performs a greedy beam search, which may bring many unnecessary explorations. We develop a novel framework, namely *corssing sparse proximity graph (CSPG)*, based on random partitioning of the dataset. It produces a smaller sparse proximity graph for each partition and routing vectors that bind all the partitions. An efficient two-staged approach is designed for exploring *CSPG*, with fast approaching and cross-partition expansion. We theoretically prove that *CSPG* can accelerate the existing graph-based ANNS algorithms by reducing unnecessary explorations. In addition, we conduct extensive experiments on benchmark datasets. The experimental results confirm that the existing graph-based methods can be significantly outperformed by incorporating *CSPG*, achieving 1.5x to 2x speedups of *QPS* in almost all recalls.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A graph-based schema for ANNS'}, 'pdf': {'value': '/pdf/5a92e17bff7f7b824c429943475cdb44a59be721.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024cspg,\\ntitle={{CSPG}: Crossing Sparse Proximity Graphs for Approximate Nearest Neighbor Search},\\nauthor={Ming Yang and Yuzheng Cai and Weiguo Zheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ohvXBIPV7e}\\n}'}, 'paperhash': {'value': 'yang|cspg_crossing_sparse_proximity_graphs_for_approximate_nearest_neighbor_search'}},forum = 'ohvXBIPV7e',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6737/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6737/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6737/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6737/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ohi00YhT3T',number = 1209,cdate = 1714472884432,pdate = 1727287654840,odate = 1730873846421,mdate = 1734579207988,tcdate = 1714472884432,tmdate = 1734579207988,ddate = None,content = {'title': {'value': 'Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction'}, 'authors': {'value': ['Guobin Shen', 'Dongcheng Zhao', 'Xiang He', 'Linghao Feng', 'Yiting Dong', 'Jihang Wang', 'Qian Zhang', 'Yi Zeng']}, 'authorids': {'value': ['~Guobin_Shen1', '~Dongcheng_Zhao2', '~Xiang_He3', '~Linghao_Feng1', '~Yiting_Dong1', '~Jihang_Wang1', '~Qian_Zhang13', '~Yi_Zeng1']}, 'keywords': {'value': ['Neural decoding', 'Mind Reader', 'Visual Reconstruction', 'Multimodal Large Model', 'Concept Localization']}, 'abstract': {'value': 'Decoding non-invasive brain recordings is pivotal for advancing our understanding of human cognition but faces challenges due to individual differences and complex neural signal representations. Traditional methods often require customized models and extensive trials, lacking interpretability in visual reconstruction tasks. Our framework integrates 3D brain structures with visual semantics using a *Vision Transformer 3D*. This unified feature extractor efficiently aligns fMRI features with multiple levels of visual embeddings, eliminating the need for subject-specific models and allowing extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with Large Language Models (LLMs). Additionally, we have enhanced the fMRI dataset with diverse fMRI-image-related textual data to support multimodal large model development. Integrating with LLMs enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. Our approach demonstrates superior performance across these tasks, precisely identifying language-based concepts within brain signals, enhancing interpretability, and providing deeper insights into neural processes. These advances significantly broaden the applicability of non-invasive brain decoding in neuroscience and human-computer interaction, setting the stage for advanced brain-computer interfaces and cognitive models.'}, 'pdf': {'value': '/pdf/ba7b93f7d624a68efb8b3859bb55e3ab19511123.pdf'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/776d5156552946adfb0496f7911d7e76a5ae5c57.zip'}, '_bibtex': {'value': '@inproceedings{\\nshen2024neurovision,\\ntitle={Neuro-Vision to Language: Enhancing Brain Recording-based Visual Reconstruction and Language Interaction},\\nauthor={Guobin Shen and Dongcheng Zhao and Xiang He and Linghao Feng and Yiting Dong and Jihang Wang and Qian Zhang and Yi Zeng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ohi00YhT3T}\\n}'}, 'paperhash': {'value': 'shen|neurovision_to_language_enhancing_brain_recordingbased_visual_reconstruction_and_language_interaction'}},forum = 'ohi00YhT3T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1209/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1209/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1209/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1209/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'ogk236hsJM',number = 9953,cdate = 1715688395295,pdate = 1727287925747,odate = 1730873924984,mdate = 1730873925002,tcdate = 1715688395295,tmdate = 1730873925002,ddate = None,content = {'title': {'value': 'One-Step Diffusion Distillation through Score Implicit Matching'}, 'authors': {'value': ['Weijian Luo', 'Zemin Huang', 'Zhengyang Geng', 'J Zico Kolter', 'Guo-Jun Qi']}, 'authorids': {'value': ['~Weijian_Luo1', '~Zemin_Huang1', '~Zhengyang_Geng1', '~J_Zico_Kolter1', '~Guo-Jun_Qi1']}, 'keywords': {'value': ['Diffusion mode', 'Diffusion Distillation', 'Text-to-Image Generation', 'Generative Adversarial Network']}, 'TLDR': {'value': 'The submission propose an general one-step diffusion distillation framework that induces efficient and stable instances with very strong performances.'}, 'abstract': {'value': 'Despite their strong performances on many generative tasks, diffusion models require a large number of sampling steps in order to generate realistic samples.  This has motivated the community to develop effective methods to distill pre-trained diffusion models into more efficient models, but these methods still typically require few-step inference or perform substantially worse than the underlying model.  In this paper, we present Score Implicit Matching (SIM) a new approach to distilling pre-trained diffusion models into single-step generator models, while maintaining almost the same sample generation ability as the original model as well as being data-free with no need of training samples for distillation.  The method rests upon the fact that, although the traditional score-based loss is intractable to minimize for generator models, under certain conditions we \\\\emph{can} efficiently compute the \\\\emph{gradients} for a wide class of score-based divergences between a diffusion model and a generator. SIM shows strong empirical performances for one-step generators: on the CIFAR10 dataset, it achieves an FID of 2.06 for unconditional generation and 1.96 for class-conditional generation.  Moreover, by applying SIM to a leading transformer-based diffusion model, we distill a single-step generator for text-to-image (T2I) generation that attains an aesthetic score of 6.42 with no performance decline over the original multi-step counterpart, clearly outperforming the other one-step generators including SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We will release this industry-ready one-step transformer-based T2I generator along with this paper.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6737446f78cac3f1ee056d202bc0989af0d0522d.pdf'}, 'supplementary_material': {'value': '/attachment/b77b751f06109aa1d01c7642fa8d226927c9770b.zip'}, '_bibtex': {'value': '@inproceedings{\\nluo2024onestep,\\ntitle={One-Step Diffusion Distillation through Score Implicit Matching},\\nauthor={Weijian Luo and Zemin Huang and Zhengyang Geng and J Zico Kolter and Guo-Jun Qi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ogk236hsJM}\\n}'}, 'paperhash': {'value': 'luo|onestep_diffusion_distillation_through_score_implicit_matching'}},forum = 'ogk236hsJM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9953/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9953/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9953/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9953/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ogaeChzbKu',number = 19736,cdate = 1715792545789,pdate = 1727288216418,odate = 1730873997420,mdate = 1730873997442,tcdate = 1715792545789,tmdate = 1730873997442,ddate = None,content = {'title': {'value': 'RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees'}, 'authors': {'value': ['Xun Xian', 'Ganghua Wang', 'Xuan Bi', 'Jayanth Srinivasa', 'Ashish Kundu', 'Mingyi Hong', 'Jie Ding']}, 'authorids': {'value': ['~Xun_Xian1', '~Ganghua_Wang1', '~Xuan_Bi1', '~Jayanth_Srinivasa1', '~Ashish_Kundu1', '~Mingyi_Hong1', '~Jie_Ding2']}, 'keywords': {'value': ['Copyright Protection', 'Privacy']}, 'abstract': {'value': 'Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, referred to as RAW.\\nAs a departure from existing encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark.\\nThe proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework also provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of adversarial attacks targeting watermark removal. \\nExperiments on a diverse range of images generated by state-of-the-art diffusion models demonstrate substantially improved watermark encoding speed and watermark detection performance, under adversarial attacks, while maintaining image quality. Our code is publicly available [here](https://github.com/jeremyxianx/RAWatermark).'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f99919d37a1f3f7d9ec02d7961b37ab55afab6e4.pdf'}, 'supplementary_material': {'value': '/attachment/28722f6d4f5d7d79a717a44c14f08854f8d406c4.zip'}, '_bibtex': {'value': '@inproceedings{\\nxian2024raw,\\ntitle={{RAW}: A Robust and Agile Plug-and-Play Watermark Framework for {AI}-Generated Images with Provable Guarantees},\\nauthor={Xun Xian and Ganghua Wang and Xuan Bi and Jayanth Srinivasa and Ashish Kundu and Mingyi Hong and Jie Ding},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ogaeChzbKu}\\n}'}, 'paperhash': {'value': 'xian|raw_a_robust_and_agile_plugandplay_watermark_framework_for_aigenerated_images_with_provable_guarantees'}},forum = 'ogaeChzbKu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19736/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19736/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19736/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19736/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ofjTu2ktxO',number = 19016,cdate = 1715788739859,pdate = 1727288199507,odate = 1730873993165,mdate = 1731566908860,tcdate = 1715788739859,tmdate = 1731566908860,ddate = None,content = {'title': {'value': 'Carrot and Stick: Eliciting Comparison Data and Beyond'}, 'authors': {'value': ['Yiling Chen', 'Shi Feng', 'Fang-Yi Yu']}, 'authorids': {'value': ['~Yiling_Chen1', '~Shi_Feng2', '~Fang-Yi_Yu1']}, 'keywords': {'value': ['information elicitation', 'mechanism design', 'crowdsourcing', 'ranking data', 'social network']}, 'abstract': {'value': 'Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment. Our design leverages on the strong stochastic transitivity for comparison data to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.\\n\\nWe further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents’ private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/da688323a2458b8e49fde4b8ea0e36903ed8a8ba.zip'}, 'pdf': {'value': '/pdf/d69667a18fd55e4b5d8cf8e0909d7823d25e80ec.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024carrot,\\ntitle={Carrot and Stick: Eliciting Comparison Data and Beyond},\\nauthor={Yiling Chen and Shi Feng and Fang-Yi Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ofjTu2ktxO}\\n}'}, 'paperhash': {'value': 'chen|carrot_and_stick_eliciting_comparison_data_and_beyond'}},forum = 'ofjTu2ktxO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19016/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19016/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19016/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19016/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oe7MfqFK1M',number = 3759,cdate = 1715323253397,pdate = 1727287729967,odate = 1730873869194,mdate = 1730873869213,tcdate = 1715323253397,tmdate = 1730873869213,ddate = None,content = {'title': {'value': 'Recovering Complete Actions for Cross-dataset Skeleton Action Recognition'}, 'authors': {'value': ['Hanchao Liu', 'Yujiang Li', 'Tai-Jiang Mu', 'Shi-min Hu']}, 'authorids': {'value': ['~Hanchao_Liu3', '~Yujiang_Li2', '~Tai-Jiang_Mu2', '~Shi-min_Hu1']}, 'keywords': {'value': ['skeleton action recognition', 'domain generalization', 'data augmentation']}, 'abstract': {'value': 'Despite huge progress in skeleton-based action recognition, its generalizability to different domains remains a challenging issue. \\nIn this paper, to solve the skeleton action generalization problem, we present a recover-and-resample augmentation framework based on a novel complete action prior. We observe that human daily actions are confronted with temporal mismatch across different datasets, as they are usually partial observations of their complete action sequences. By recovering complete actions and resampling from these full sequences, we can generate strong augmentations for unseen domains. At the same time, we discover the nature of general action completeness within large datasets, indicated by the per-frame diversity over time. This allows us to exploit two assets of transferable knowledge that can be shared across action samples and be helpful for action completion: boundary poses for determining the action start, and linear temporal transforms for capturing global action patterns. Therefore, we formulate the recovering stage as a two-step stochastic action completion with boundary pose-conditioned extrapolation followed by smooth linear transforms. Both the boundary poses and linear transforms can be efficiently learned from the whole dataset via clustering. We validate our approach on a cross-dataset setting with three skeleton action datasets, outperforming other domain generalization approaches by a considerable margin.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present a novel recover-and-resample augmentation framework based on complete action prior for skeleton action generalization task.'}, 'pdf': {'value': '/pdf/ed3d5688d70d03ec751dfe16c56d0722ef8c2528.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024recovering,\\ntitle={Recovering Complete Actions for Cross-dataset Skeleton Action Recognition},\\nauthor={Hanchao Liu and Yujiang Li and Tai-Jiang Mu and Shi-min Hu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oe7MfqFK1M}\\n}'}, 'paperhash': {'value': 'liu|recovering_complete_actions_for_crossdataset_skeleton_action_recognition'}},forum = 'oe7MfqFK1M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3759/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3759/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3759/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3759/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oe5ZEqTOaz',number = 15464,cdate = 1715760544175,pdate = 1727288098745,odate = 1730873973147,mdate = 1730873973158,tcdate = 1715760544175,tmdate = 1730873973158,ddate = None,content = {'title': {'value': 'Classifier-guided Gradient Modulation for Enhanced Multimodal Learning'}, 'authors': {'value': ['Zirun Guo', 'Tao Jin', 'Jingyuan Chen', 'Zhou Zhao']}, 'authorids': {'value': ['~Zirun_Guo1', '~Tao_Jin2', '~Jingyuan_Chen3', '~Zhou_Zhao3']}, 'keywords': {'value': ['balanced multimodal learning', 'gradient modulation']}, 'abstract': {'value': 'Multimodal learning has developed very fast in recent years. However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities. Existing methods to balance the training process always have some limitations on the loss functions, optimizers and the number of modalities and only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. To solve these problems, in this paper, we present a novel method to balance multimodal learning with **C**lassifier-**G**uided **G**radient **M**odulation (CGGM), considering both the magnitude and directions of the gradients. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Our code is available at https://github.com/zrguo/CGGM.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/984068053cc1a70d884e9e39819ffd4cca9038bc.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguo2024classifierguided,\\ntitle={Classifier-guided Gradient Modulation for Enhanced Multimodal Learning},\\nauthor={Zirun Guo and Tao Jin and Jingyuan Chen and Zhou Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oe5ZEqTOaz}\\n}'}, 'paperhash': {'value': 'guo|classifierguided_gradient_modulation_for_enhanced_multimodal_learning'}},forum = 'oe5ZEqTOaz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15464/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15464/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15464/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15464/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'ocxVXe5XN1',number = 19369,cdate = 1715790567818,pdate = 1727288208151,odate = 1730873995186,mdate = 1730873995203,tcdate = 1715790567818,tmdate = 1730873995203,ddate = None,content = {'title': {'value': 'Generalization Bounds via Conditional $f$-Information'}, 'authors': {'value': ['Ziqiao Wang', 'Yongyi Mao']}, 'authorids': {'value': ['~Ziqiao_Wang1', '~Yongyi_Mao2']}, 'keywords': {'value': ['information-theoretic generalization bound', 'generalization', 'f-divergence']}, 'TLDR': {'value': 'We use novel methods to derive information-theoretic generalization bounds via conditional $f$-divergences.'}, 'abstract': {'value': 'In this work, we introduce novel information-theoretic generalization bounds using the conditional $f$-information framework, an extension of the traditional conditional mutual information (MI) framework. We provide a generic approach to derive generalization bounds via $f$-information in the supersample setting, applicable to both bounded and unbounded loss functions. Unlike previous MI-based bounds, our proof strategy does not rely on upper bounding the cumulant-generating function (CGF) in the variational formula of MI. Instead, we set the CGF or its upper bound to zero by carefully selecting  the measurable function invoked in the variational formula. Although some of our techniques are partially inspired by recent advances in the coin-betting framework (e.g., Jang et al. (2023)), our results are independent of any previous findings from regret guarantees of online gambling algorithms. Additionally, our newly derived MI-based bound recovers many previous results and improves our understanding of their potential limitations. Finally, we empirically compare various $f$-information measures for generalization, demonstrating the improvement of our new bounds over the previous bounds.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e8d8ef04ce6ec6fe37962a01ec52a12103f41a29.pdf'}, 'supplementary_material': {'value': '/attachment/e1e134b5abc564c0a0a8b27b1fb4d5ac3f04fd02.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024generalization,\\ntitle={Generalization Bounds via Conditional \\\\$f\\\\$-Information},\\nauthor={Ziqiao Wang and Yongyi Mao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ocxVXe5XN1}\\n}'}, 'paperhash': {'value': 'wang|generalization_bounds_via_conditional_finformation'}},forum = 'ocxVXe5XN1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19369/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19369/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19369/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19369/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'obUXeUMmq1',number = 8805,cdate = 1715669965451,pdate = 1727287891338,odate = 1730873915038,mdate = 1730873915058,tcdate = 1715669965451,tmdate = 1730873915058,ddate = None,content = {'title': {'value': 'Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective'}, 'authors': {'value': ['Haixiang Sun', 'Ye Shi']}, 'authorids': {'value': ['~Haixiang_Sun1', '~Ye_Shi1']}, 'keywords': {'value': ['Deep Equilibrium Models', 'Neural Collapse', 'Imbalance learning']}, 'abstract': {'value': \"Deep Equilibrium Model (DEQ), which serves as a typical implicit neural network, emphasizes their memory efficiency and competitive performance compared to explicit neural networks. However, there has been relatively limited theoretical analysis on the representation of DEQ. In this paper, we utilize the Neural Collapse ($\\\\mathcal{NC}$) as a tool to systematically analyze the representation of DEQ under both balanced and imbalanced conditions. $\\\\mathcal{NC}$ is an interesting phenomenon in the neural network training process that characterizes the geometry of class features and classifier weights. While extensively studied in traditional explicit neural networks, the $\\\\mathcal{NC}$ phenomenon has not received substantial attention in the context of implicit neural networks. \\nWe theoretically show that $\\\\mathcal{NC}$ exists in DEQ under balanced conditions. Moreover, in imbalanced settings, despite the presence of minority collapse, DEQ demonstrated advantages over explicit neural networks. These advantages include the convergence of extracted features to the vertices of a simplex equiangular tight frame and self-duality properties under mild conditions, highlighting DEQ's superiority in handling imbalanced datasets. Finally, we validate our theoretical analyses through experiments in both balanced and imbalanced scenarios.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1a16568f6e76ba0c102e0389408ff6409d097dcb.pdf'}, 'TLDR': {'value': 'This paper analyzes the representation of DEQ from the Neural Collapse perspective under both balanced and imbalanced conditions.'}, '_bibtex': {'value': '@inproceedings{\\nsun2024understanding,\\ntitle={Understanding Representation of Deep Equilibrium Models from Neural Collapse Perspective},\\nauthor={Haixiang Sun and Ye Shi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=obUXeUMmq1}\\n}'}, 'paperhash': {'value': 'sun|understanding_representation_of_deep_equilibrium_models_from_neural_collapse_perspective'}},forum = 'obUXeUMmq1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8805/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8805/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8805/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8805/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oZy4a11SUg',number = 10476,cdate = 1715694531994,pdate = 1727287941821,odate = 1730873929396,mdate = 1730873929415,tcdate = 1715694531994,tmdate = 1730873929415,ddate = None,content = {'title': {'value': 'Boosting the Potential of Large Language Models with an Intelligent Information Assistant'}, 'authors': {'value': ['Yujia Zhou', 'Zheng Liu', 'Zhicheng Dou']}, 'authorids': {'value': ['~Yujia_Zhou1', '~Zheng_Liu4', '~Zhicheng_Dou1']}, 'keywords': {'value': ['information retrieval', 'large language model', 'information assistant']}, 'abstract': {'value': 'The emergence of Large Language Models (LLMs) has significantly advanced natural language processing, but these models often generate factually incorrect information, known as \"hallucination.\" Initial retrieval-augmented generation (RAG) methods like the \"Retrieve-Read\" framework was inadequate for complex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised Fine-Tuning (SFT) methods improved performance but required frequent retraining and risked altering foundational LLM capabilities. To cope with these challenges, we propose Assistant-based Retrieval-Augmented Generation (AssistRAG), integrating an intelligent information assistant within LLMs. This assistant manages memory and knowledge through tool usage, action execution, memory building, and plan specification. Using a two-phase training approach—Curriculum Assistant Learning and Reinforced Preference Optimization—AssistRAG enhances information retrieval and decision-making. Experiments show AssistRAG significantly outperforms benchmarks, especially benefiting less advanced LLMs, by providing superior reasoning capabilities and accurate responses.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a54fd782fb5a60604650797b3d2e049edfe0dc60.pdf'}, 'supplementary_material': {'value': '/attachment/54a1dbf015befccf979634ecabfb7054eb815760.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024boosting,\\ntitle={Boosting the Potential of Large Language Models with an Intelligent Information Assistant},\\nauthor={Yujia Zhou and Zheng Liu and Zhicheng Dou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oZy4a11SUg}\\n}'}, 'paperhash': {'value': 'zhou|boosting_the_potential_of_large_language_models_with_an_intelligent_information_assistant'}},forum = 'oZy4a11SUg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10476/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10476/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10476/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10476/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oYyEsVz6DX',number = 6325,cdate = 1715590328788,pdate = 1727287811188,odate = 1730873892347,mdate = 1730873892360,tcdate = 1715590328788,tmdate = 1730873892360,ddate = None,content = {'title': {'value': 'Measuring Per-Unit Interpretability at Scale Without Humans'}, 'authors': {'value': ['Roland S. Zimmermann', 'David Klindt', 'Wieland Brendel']}, 'authorids': {'value': ['~Roland_S._Zimmermann1', '~David_Klindt1', '~Wieland_Brendel1']}, 'keywords': {'value': ['interpretability', 'explainability', 'deep learning', 'neural networks', 'analysis', 'activation maximization', 'alignment', 'evaluation']}, 'TLDR': {'value': 'We introduce the first scalable method to measure the per-unit interpretability in vision neural networks and demonstrate its alignment to human judgements.'}, 'abstract': {'value': \"In today’s era, whatever we can measure at scale, we can optimize. So far, measuring the interpretability of units in deep neural networks (DNNs) for computer vision still requires direct human evaluation and is not scalable. As a result, the inner workings of DNNs remain a mystery despite the remarkable progress we have seen in their applications. In this work, we introduce the first scalable method to measure the per-unit interpretability in vision DNNs. This method does not require any human evaluations, yet its prediction correlates well with existing human interpretability measurements. We validate its predictive power through an interventional human psychophysics study. We demonstrate the usefulness of this measure by performing previously infeasible experiments: (1) A large-scale interpretability analysis across more than 70 million units from 835 computer vision models, and (2) an extensive analysis of how units transform during training. We find an anticorrelation between a model's downstream classification performance and per-unit interpretability, which is also observable during model training. Furthermore, we see that a layer's location and width influence its interpretability.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1f3c601e4a12a2ae5db3cfa5d505568abbed1f6b.pdf'}, 'supplementary_material': {'value': '/attachment/702ea8de49793984e5ea356c6625eb27385a1ce1.zip'}, '_bibtex': {'value': '@inproceedings{\\nzimmermann2024measuring,\\ntitle={Measuring Per-Unit Interpretability at Scale Without Humans},\\nauthor={Roland S. Zimmermann and David Klindt and Wieland Brendel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oYyEsVz6DX}\\n}'}, 'paperhash': {'value': 'zimmermann|measuring_perunit_interpretability_at_scale_without_humans'}},forum = 'oYyEsVz6DX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6325/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6325/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6325/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6325/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'oXHyYHp4Zb',number = 8017,cdate = 1715650517781,pdate = 1727287865700,odate = 1730873908523,mdate = 1730873908538,tcdate = 1715650517781,tmdate = 1730873908538,ddate = None,content = {'title': {'value': 'SparseLLM: Towards Global Pruning of Pre-trained Language Models'}, 'authors': {'value': ['Guangji Bai', 'Yijiang Li', 'Chen Ling', 'Kibaek Kim', 'Liang Zhao']}, 'authorids': {'value': ['~Guangji_Bai1', '~Yijiang_Li2', '~Chen_Ling3', '~Kibaek_Kim1', '~Liang_Zhao6']}, 'keywords': {'value': ['Large Language Model', 'Pruning']}, 'TLDR': {'value': 'This paper proposed a novel framework for global pruning of large language models, via considering the LLMs as a chain of modules and introducing auxiliary variables.'}, 'abstract': {'value': \"The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose *SparseLLM*, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods. Our source code is publicly available at https://github.com/BaiTheBest/SparseLLM.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fccc47c2f07ff5d77b691da1a0395609ed3b4e9f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbai2024sparsellm,\\ntitle={Sparse{LLM}: Towards Global Pruning of Pre-trained Language Models},\\nauthor={Guangji Bai and Yijiang Li and Chen Ling and Kibaek Kim and Liang Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oXHyYHp4Zb}\\n}'}, 'paperhash': {'value': 'bai|sparsellm_towards_global_pruning_of_pretrained_language_models'}},forum = 'oXHyYHp4Zb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8017/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8017/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8017/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8017/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oXCmwwkQTZ',number = 10919,cdate = 1715699054627,pdate = 1727287954932,odate = 1730873933328,mdate = 1730873933344,tcdate = 1715699054627,tmdate = 1730873933344,ddate = None,content = {'title': {'value': 'Implicit Regularization Paths of Weighted Neural Representations'}, 'authors': {'value': ['Jin-Hong Du', 'Pratik Patil']}, 'authorids': {'value': ['~Jin-Hong_Du1', '~Pratik_Patil1']}, 'keywords': {'value': ['pretrained representations', 'implicit regularization', 'random features', 'kernel regression', 'neural tangent kernel', 'cross-validation']}, 'abstract': {'value': 'We study the implicit regularization effects induced by (observation) weighting of pretrained features.\\nFor weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels.\\nSpecifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms.\\nThese paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features.\\nFor the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil and Du (2023).\\nWe also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity.\\nAs a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/afbfa56e2585dc77250b4794887ebc050756e118.pdf'}, 'supplementary_material': {'value': '/attachment/7eaa4df2041c4a47746533290fd68dbe5f775b14.zip'}, 'TLDR': {'value': 'We explore the implicit regularization induced by weighted and/or subsampled pretrained neural features, offering theory and practical methods to manage high-dimensional data for better model fitting.'}, '_bibtex': {'value': '@inproceedings{\\ndu2024implicit,\\ntitle={Implicit Regularization Paths of Weighted Neural Representations},\\nauthor={Jin-Hong Du and Pratik Patil},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oXCmwwkQTZ}\\n}'}, 'paperhash': {'value': 'du|implicit_regularization_paths_of_weighted_neural_representations'}},forum = 'oXCmwwkQTZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10919/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10919/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10919/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10919/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oX6aIl9f0Y',number = 18826,cdate = 1715787573067,pdate = 1727288194368,odate = 1730873992131,mdate = 1730873992144,tcdate = 1715787573067,tmdate = 1730873992144,ddate = None,content = {'title': {'value': 'Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions'}, 'authors': {'value': ['Hilal Asi', 'Daogao Liu', 'Kevin Tian']}, 'authorids': {'value': ['~Hilal_Asi1', '~Daogao_Liu1', '~Kevin_Tian4']}, 'keywords': {'value': ['Stochastic Convex Optimization', 'Heavy-Tailed Distributions', 'Differential Privacy']}, 'abstract': {'value': 'We study the problem of differentially private stochastic convex optimization (DP-SCO) with heavy-tailed gradients, where we assume a $k^{\\\\text{th}}$-moment bound on the Lipschitz constants of sample functions, rather than a uniform bound. We propose a new reduction-based approach that enables us to obtain the first optimal rates (up to logarithmic factors) in the heavy-tailed setting, achieving error $G_2 \\\\cdot \\\\frac 1 {\\\\sqrt n} + G_k \\\\cdot (\\\\frac{\\\\sqrt d}{n\\\\epsilon})^{1 - \\\\frac 1 k}$ under $(\\\\epsilon, \\\\delta)$-approximate differential privacy, up to a mild $\\\\textup{polylog}(\\\\frac{1}{\\\\delta})$ factor, where $G_2^2$ and $G_k^k$ are the $2^{\\\\text{nd}}$ and $k^{\\\\text{th}}$ moment bounds on sample Lipschitz constants, nearly-matching a lower bound of [LR23].\\nWe then give a suite of private algorithms for DP-SCO with heavy-tailed gradients improving our basic result under additional assumptions, including an optimal algorithm under a known-Lipschitz constant assumption, a near-linear time algorithm for smooth functions, and an optimal linear time algorithm for smooth generalized linear models.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We give the first algorithm attaining near-optimal error rates for DP-SCO assuming heavy-tailed gradients, and several improvements in structured cases.'}, 'pdf': {'value': '/pdf/0c7ca297622a168d2601aed45be8718d346802d2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nasi2024private,\\ntitle={Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions},\\nauthor={Hilal Asi and Daogao Liu and Kevin Tian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oX6aIl9f0Y}\\n}'}, 'paperhash': {'value': 'asi|private_stochastic_convex_optimization_with_heavy_tails_nearoptimality_from_simple_reductions'}},forum = 'oX6aIl9f0Y',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18826/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18826/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18826/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18826/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oWAItGB8LJ',number = 964,cdate = 1714271584158,pdate = 1727287648772,odate = 1730873844447,mdate = 1730873844465,tcdate = 1714271584158,tmdate = 1730873844465,ddate = None,content = {'title': {'value': 'BiDM: Pushing the Limit of Quantization for Diffusion Models'}, 'authors': {'value': ['Xingyu Zheng', 'Xianglong Liu', 'Yichen Bian', 'Xudong Ma', 'Yulun Zhang', 'Jiakai Wang', 'Jinyang Guo', 'Haotong Qin']}, 'authorids': {'value': ['~Xingyu_Zheng1', '~Xianglong_Liu3', '~Yichen_Bian1', '~Xudong_Ma3', '~Yulun_Zhang1', '~Jiakai_Wang1', '~Jinyang_Guo1', '~Haotong_Qin1']}, 'keywords': {'value': ['Diffusion Model', 'Model Binarization', 'Low-bit Quantization', 'Machine Learning']}, 'abstract': {'value': 'Diffusion models (DMs) have been significantly developed and widely used in various applications due to their excellent generative qualities. However, the expensive computation and massive parameters of DMs hinder their practical use in resource-constrained scenarios. As one of the effective compression approaches, quantization allows DMs to achieve storage saving and inference acceleration by reducing bit-width while maintaining generation performance.  However, as the most extreme quantization form, 1-bit binarization causes the generation performance of DMs to face severe degradation or even collapse. This paper proposes a novel method, namely BiDM, for fully binarizing weights and activations of DMs, pushing quantization to the 1-bit limit. From a temporal perspective, we introduce the Timestep-friendly Binary Structure (TBS), which uses learnable activation binarizers and cross-timestep feature connections to address the highly timestep-correlated activation features of DMs. From a spatial perspective, we propose Space Patched Distillation (SPD) to address the difficulty of matching binary features during distillation, focusing on the spatial locality of image generation tasks and noise estimation networks. As the first work to fully binarize DMs, the W1A1 BiDM on the LDM-4 model for LSUN-Bedrooms 256$\\\\times$256 achieves a remarkable FID of 22.74, significantly outperforming the current state-of-the-art general binarization methods with an FID of 59.44 and invalid generative samples, and achieves up to excellent 28.0 times storage and 52.7 times OPs savings.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'In this paper, we propose BiDM, pushing the quantization of the diffusion model to the limit with fully binarized weights and activations.'}, 'pdf': {'value': '/pdf/6d17445f162ca58b230060f58daa438dbe160260.pdf'}, 'supplementary_material': {'value': '/attachment/4c403f26c27dc94583dc373308e56bcc854150c2.zip'}, '_bibtex': {'value': '@inproceedings{\\nzheng2024bidm,\\ntitle={Bi{DM}: Pushing the Limit of Quantization for Diffusion Models},\\nauthor={Xingyu Zheng and Xianglong Liu and Yichen Bian and Xudong Ma and Yulun Zhang and Jiakai Wang and Jinyang Guo and Haotong Qin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oWAItGB8LJ}\\n}'}, 'paperhash': {'value': 'zheng|bidm_pushing_the_limit_of_quantization_for_diffusion_models'}},forum = 'oWAItGB8LJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission964/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission964/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission964/-/Revision', 'NeurIPS.cc/2024/Conference/Submission964/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oUXiNX5KRm',number = 6487,cdate = 1715595304113,pdate = 1727287816061,odate = 1730873893758,mdate = 1730873893785,tcdate = 1715595304113,tmdate = 1730873893785,ddate = None,content = {'title': {'value': 'Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators'}, 'authors': {'value': ['Benedikt Alkin', 'Andreas Fürst', 'Simon Lucas Schmid', 'Lukas Gruber', 'Markus Holzleitner', 'Johannes Brandstetter']}, 'authorids': {'value': ['~Benedikt_Alkin1', '~Andreas_Fürst1', '~Simon_Lucas_Schmid1', '~Lukas_Gruber2', '~Markus_Holzleitner1', '~Johannes_Brandstetter1']}, 'keywords': {'value': ['neural operator', 'computational fluid dynamics', 'Lagrangian simulations', 'transformers', 'latent space modeling']}, 'TLDR': {'value': 'We introduce Universal Physics Transformers, an efficiently scalable neural operator framework to model a wide range of spatio-temporal problems – for Lagrangian and Eulerian discretization schemes.'}, 'abstract': {'value': 'Neural operators, serving as physics surrogate models, have recently gained increased interest. With ever increasing problem complexity, the natural question arises: what is an efficient way to scale neural operators to larger and more complex simulations - most importantly by taking into account different types of simulation datasets. This is of special interest since, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. Whereas the flexibility of transformers has enabled unified architectures across domains, neural operators mostly follow a problem specific design, where GNNs are commonly used for Lagrangian simulations and grid-based models predominate Eulerian simulations. \\n\\nWe introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space representation at any point in space-time. We demonstrate diverse applicability and efficacy of UPTs in mesh-based fluid simulations, and steady-state Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4ffed9700454b3329a91e1a3f4304eef2c75e090.pdf'}, 'supplementary_material': {'value': '/attachment/ffbee57cf57b024cc7564d74926efc865fcb2da6.zip'}, '_bibtex': {'value': '@inproceedings{\\nalkin2024universal,\\ntitle={Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators},\\nauthor={Benedikt Alkin and Andreas F{\\\\\"u}rst and Simon Lucas Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oUXiNX5KRm}\\n}'}, 'paperhash': {'value': 'alkin|universal_physics_transformers_a_framework_for_efficiently_scaling_neural_operators'}},forum = 'oUXiNX5KRm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6487/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6487/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6487/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6487/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oTzydUKWpq',number = 15853,cdate = 1715764303095,pdate = 1727288108946,odate = 1730873975152,mdate = 1730873975169,tcdate = 1715764303095,tmdate = 1730873975169,ddate = None,content = {'title': {'value': 'Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level'}, 'authors': {'value': ['Runlin Lei', 'Yuwei Hu', 'Yuchen Ren', 'Zhewei Wei']}, 'authorids': {'value': ['~Runlin_Lei1', '~Yuwei_Hu1', '~Yuchen_Ren5', '~Zhewei_Wei1']}, 'keywords': {'value': ['Graph Neural Networks', 'Graph Adversarial Attack', 'Graph Injection Attack']}, 'TLDR': {'value': 'We study text-level Graph Injection Attacks beyond the embedding level, revealing new challenges and insights about Graph Injection Attack designs.'}, 'abstract': {'value': 'Graph Neural Networks (GNNs) excel across various applications but remain vulnerable to adversarial attacks, particularly Graph Injection Attacks (GIAs), which inject malicious nodes into the original graph and pose realistic threats.\\nText-attributed graphs (TAGs), where nodes are associated with textual features, are crucial due to their prevalence in real-world applications and are commonly used to evaluate these vulnerabilities.\\nHowever, existing research only focuses on embedding-level GIAs, which inject node embeddings rather than actual textual content, limiting their applicability and simplifying detection.\\nIn this paper, we pioneer the exploration of GIAs at the text level, presenting three novel attack designs that inject textual content into the graph.\\nThrough theoretical and empirical analysis, we demonstrate that text interpretability, a factor previously overlooked at the embedding level, plays a crucial role in attack strength. \\nAmong the designs we investigate, the Word-frequency-based Text-level GIA (WTGIA) is particularly notable for its balance between performance and interpretability. \\nDespite the success of WTGIA, we discover that defenders can easily enhance their defenses with customized text embedding methods or large language model (LLM)--based predictors. \\nThese insights underscore the necessity for further research into the potential and practical significance of text-level GIAs.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10dce76d299f9d6b51075cc4062b9c252a021a21.pdf'}, 'supplementary_material': {'value': '/attachment/9198cd5b41f74080b2f9658979984c14d4bb1ea0.zip'}, '_bibtex': {'value': '@inproceedings{\\nlei2024intruding,\\ntitle={Intruding with Words: Towards Understanding Graph Injection Attacks at the Text Level},\\nauthor={Runlin Lei and Yuwei Hu and Yuchen Ren and Zhewei Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oTzydUKWpq}\\n}'}, 'paperhash': {'value': 'lei|intruding_with_words_towards_understanding_graph_injection_attacks_at_the_text_level'}},forum = 'oTzydUKWpq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15853/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15853/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15853/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15853/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oTv6Qa12G0',number = 15947,cdate = 1715765134825,pdate = 1727288112170,odate = 1730873976107,mdate = 1730873976119,tcdate = 1715765134825,tmdate = 1730873976119,ddate = None,content = {'title': {'value': 'A theoretical design of concept sets: improving the predictability of concept bottleneck models'}, 'authors': {'value': ['Max Ruiz Luyten', 'Mihaela van der Schaar']}, 'authorids': {'value': ['~Max_Ruiz_Luyten1', '~Mihaela_van_der_Schaar2']}, 'keywords': {'value': ['Representation Learning', 'Embedding Approaches', 'Learning Theory', 'Neural Abstract Machines', 'Nonlinear Dimensionality Reduction and Manifold Learning', 'One-Shot/Low-Shot Learning Approaches']}, 'TLDR': {'value': \"We present a theoretical analysis of Concept Bottleneck Models, which motivates a principled generation method for concept sets. We empirically verify its impact on CBMs' sample efficiency and robustness.\"}, 'abstract': {'value': \"Concept-based learning, a promising approach in machine learning, emphasizes the value of high-level representations called concepts. However, despite growing interest in concept-bottleneck models (CBMs), there is a lack of clear understanding regarding the properties of concept sets and their impact on model performance. In this work, we define concepts within the machine learning context, highlighting their core properties: 'expressiveness' and 'model-aware inductive bias', and we make explicit the underlying assumption of CBMs. We establish theoretical results for concept-bottleneck models (CBMs), revealing how these properties guide the design of concept sets that optimize model performance. Specifically, we demonstrate that well-chosen concept sets can improve sample efficiency and out-of-distribution robustness in the appropriate regimes. Based on these insights, we propose a method to effectively identify informative and non-redundant concepts. We validate our approach with experiments on CIFAR-10 and MetaShift, showing that concept-bottleneck models outperform the foundational embedding counterpart, particularly in low-data regimes and under distribution shifts. We also examine failure modes and discuss how they can be tackled.\"}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/05073046af476d5ad0927216e0e08f986cf9b582.pdf'}, '_bibtex': {'value': '@inproceedings{\\nluyten2024a,\\ntitle={A theoretical design of concept sets: improving the predictability of concept bottleneck models},\\nauthor={Max Ruiz Luyten and Mihaela van der Schaar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oTv6Qa12G0}\\n}'}, 'paperhash': {'value': 'luyten|a_theoretical_design_of_concept_sets_improving_the_predictability_of_concept_bottleneck_models'}},forum = 'oTv6Qa12G0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15947/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15947/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15947/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15947/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oTZYhOAMhX',number = 7269,cdate = 1715616607948,pdate = 1727287841320,odate = 1730873901123,mdate = 1730873901157,tcdate = 1715616607948,tmdate = 1730873901157,ddate = None,content = {'title': {'value': 'Identify Then Recommend: Towards Unsupervised Group Recommendation'}, 'authors': {'value': ['Yue Liu', 'Shihao Zhu', 'Tianyuan Yang', 'Jian Ma', 'Wenliang Zhong']}, 'authorids': {'value': ['~Yue_Liu10', '~Shihao_Zhu1', '~Tianyuan_Yang1', '~Jian_Ma9', '~Wenliang_Zhong1']}, 'keywords': {'value': ['Unsupervised Learning', 'Self-supervised Learning', 'Clustering', 'Recommendation']}, 'abstract': {'value': 'Group Recommendation (GR), which aims to recommend items to groups of users, has become a promising and practical direction for recommendation systems. This paper points out two issues of the state-of-the-art GR models. (1) The pre-defined and fixed number of user groups is inadequate for real-time industrial recommendation systems, where the group distribution can shift dynamically. (2) The training schema of existing GR methods is supervised, necessitating expensive user-group and group-item labels, leading to significant annotation costs. To this end, we present a novel unsupervised group recommendation framework named $\\\\underline{\\\\text{I}}$dentify $\\\\underline{\\\\text{T}}$hen $\\\\underline{\\\\text{R}}$ecommend ($\\\\underline{\\\\text{ITR}}$), where it first identifies the user groups in an unsupervised manner even without the pre-defined number of groups, and then two pre-text tasks are designed to conduct self-supervised group recommendation. Concretely, at the group identification stage, we first estimate the adaptive density of each user point, where areas with higher densities are more likely to be recognized as group centers. Then, a heuristic merge-and-split strategy is designed to discover the user groups and decision boundaries. Subsequently, at the self-supervised learning stage, the pull-and-repulsion pre-text task is proposed to optimize the user-group distribution. Besides, the pseudo group recommendation pre-text task is designed to assist the recommendations. Extensive experiments demonstrate the superiority and effectiveness of ITR on both user recommendation (e.g., 22.22\\\\% NDCG@5 $\\\\uparrow$) and group recommendation (e.g., 22.95\\\\% NDCG@5 $\\\\uparrow$). Furthermore, we deploy ITR on the industrial recommender and achieve promising results.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8ae6f2c622fb70c48215177ba3d62424f497ccda.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024identify,\\ntitle={Identify Then Recommend: Towards Unsupervised Group Recommendation},\\nauthor={Yue Liu and Shihao Zhu and Tianyuan Yang and Jian Ma and Wenliang Zhong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oTZYhOAMhX}\\n}'}, 'paperhash': {'value': 'liu|identify_then_recommend_towards_unsupervised_group_recommendation'}},forum = 'oTZYhOAMhX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7269/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7269/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7269/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7269/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oTEttMIymz',number = 3378,cdate = 1715265511792,pdate = 1727287717828,odate = 1730873865574,mdate = 1730873865593,tcdate = 1715265511792,tmdate = 1730873865593,ddate = None,content = {'title': {'value': 'Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis'}, 'authors': {'value': ['Liang Han', 'Junsheng Zhou', 'Yu-Shen Liu', 'Zhizhong Han']}, 'authorids': {'value': ['~Liang_Han3', '~Junsheng_Zhou3', '~Yu-Shen_Liu1', '~Zhizhong_Han2']}, 'keywords': {'value': ['Sparse View Synthesis', 'Gaussian Splatting', 'View Consistency']}, 'abstract': {'value': 'Novel view synthesis from sparse inputs is a vital yet challenging task in 3D computer vision. Previous methods explore 3D Gaussian Splatting with neural priors (e.g. depth priors) as an additional supervision, demonstrating promising quality and efficiency compared to the NeRF based methods. However, the neural priors from 2D pretrained models are often noisy and blurry, which struggle to precisely guide the learning of radiance fields. In this paper, We propose a novel method for synthesizing novel views from sparse views with Gaussian Splatting that does not require external prior as supervision. Our key idea lies in exploring the self-supervisions inherent in the binocular stereo consistency between each pair of binocular images constructed with disparity-guided image warping. To this end, we additionally introduce a Gaussian opacity constraint which regularizes the Gaussian locations and avoids Gaussian redundancy for\\nimproving the robustness and efficiency of inferring 3D Gaussians from sparse views. Extensive experiments on the LLFF, DTU, and Blender datasets demonstrate that our method significantly outperforms the state-of-the-art methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5fffa3fc11d94b1534454d017683ac8fe327445a.pdf'}, 'supplementary_material': {'value': '/attachment/8c8cbc732d1a2db01ef49e439971c2dce8ab3538.zip'}, '_bibtex': {'value': '@inproceedings{\\nhan2024binocularguided,\\ntitle={Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse View Synthesis},\\nauthor={Liang Han and Junsheng Zhou and Yu-Shen Liu and Zhizhong Han},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oTEttMIymz}\\n}'}, 'paperhash': {'value': 'han|binocularguided_3d_gaussian_splatting_with_view_consistency_for_sparse_view_synthesis'}},forum = 'oTEttMIymz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3378/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3378/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3378/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3378/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oSOVME9kl2',number = 6575,cdate = 1715598948517,pdate = 1727287819118,odate = 1730873894474,mdate = 1730873894492,tcdate = 1715598948517,tmdate = 1730873894492,ddate = None,content = {'title': {'value': 'Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems'}, 'authors': {'value': ['Bingcong Li', 'Liang Zhang', 'Niao He']}, 'authorids': {'value': ['~Bingcong_Li1', '~Liang_Zhang6', '~Niao_He3']}, 'keywords': {'value': ['sharpness-aware minimization', 'LoRA', 'implicit regularization', 'finetuning', 'language models']}, 'abstract': {'value': 'Sharpness-aware minimization (SAM) improves generalization of various deep learning tasks. Motivated by popular architectures such as LoRA, we explore the implicit regularization of SAM for scale-invariant problems involving two groups of variables. Instead of focusing on commonly used sharpness, this work introduces a concept termed *balancedness*, defined as the difference between the squared norm of two variables. This allows us to depict richer global behaviors of SAM. In particular, our theoretical and empirical findings reveal that i) SAM promotes balancedness; and ii) the regularization on balancedness is *data-responsive* -- outliers have stronger impact. \\nThe latter coincides with empirical observations that SAM outperforms SGD in the presence of outliers. \\nLeveraging the implicit regularization, we develop a resource-efficient SAM variant, balancedness-aware regularization (BAR), tailored for scale-invariant problems such as finetuning language models with LoRA. BAR saves 95% computational overhead of SAM, with enhanced test performance across various tasks on RoBERTa, GPT2, and OPT-1.3B.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We leverage implicit regularization to enhance the computational efficiency of sharpness-aware minimization (SAM).'}, 'pdf': {'value': '/pdf/333bb561496ca87ac566d937ee37ac97cd20950e.pdf'}, 'supplementary_material': {'value': '/attachment/ad7d92c6f46e2030c4c47365d8af22966ff21284.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024implicit,\\ntitle={Implicit Regularization of Sharpness-Aware Minimization for Scale-Invariant Problems},\\nauthor={Bingcong Li and Liang Zhang and Niao He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oSOVME9kl2}\\n}'}, 'paperhash': {'value': 'li|implicit_regularization_of_sharpnessaware_minimization_for_scaleinvariant_problems'}},forum = 'oSOVME9kl2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6575/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6575/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6575/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6575/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oQ1Zj9iH88',number = 10087,cdate = 1715690144893,pdate = 1727287929502,odate = 1730873926063,mdate = 1736604726562,tcdate = 1715690144893,tmdate = 1736604726562,ddate = None,content = {'title': {'value': 'Penalty-based Methods for Simple Bilevel Optimization under Hölderian Error Bounds'}, 'authors': {'value': ['Pengyu Chen', 'Xu Shi', 'Rujun Jiang', 'Jiulin Wang']}, 'authorids': {'value': ['~Pengyu_Chen3', '~Xu_Shi1', '~Rujun_Jiang1', '~Jiulin_Wang1']}, 'keywords': {'value': ['Simple Bilevel Optimization', 'Hölderian Error Bound', 'Penalization', 'Complexity']}, 'TLDR': {'value': 'This paper proposes a novel penalty-based formulation to solve simple bilevel problems with lower complexity results.'}, 'abstract': {'value': 'This paper investigates simple bilevel optimization problems where we minimize a convex upper-level objective over the optimal solution set of a convex lower-level objective. Existing methods for such problems either only guarantee asymptotic convergence, have slow sublinear rates, or require strong assumptions. To address these challenges, we propose a penalization framework that delineates the relationship between approximate solutions of the original problem and its reformulated counterparts. This framework accommodates varying assumptions regarding smoothness and convexity, enabling the application of specific methods with different complexity results. \\nSpecifically, when both upper- and lower-level objectives are composite convex functions, under an $\\\\alpha$-Hölderian error bound condition and certain mild assumptions, our algorithm attains an $(\\\\epsilon,\\\\epsilon^{\\\\beta})$-optimal solution of the original problem for any $\\\\beta> 0$ within $\\\\mathcal{O}\\\\left(\\\\sqrt{{1}/{\\\\epsilon^{\\\\max\\\\\\\\{\\\\alpha,\\\\beta\\\\\\\\}}}}\\\\right)$ iterations. The result can be improved further if the smooth part of the upper-level objective is strongly convex. We also establish complexity results when the upper- and lower-level objectives are general nonsmooth functions. Numerical experiments demonstrate the effectiveness of our algorithms.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/296eab284c5b6c73d592d0e9b1bc1ebded68b49c.pdf'}, 'supplementary_material': {'value': '/attachment/b7cbcd6fd5b4c5860fd7cf08570e56d2e1be4b9f.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024penaltybased,\\ntitle={Penalty-based Methods for Simple Bilevel Optimization under H\\\\\"olderian Error Bounds},\\nauthor={Pengyu Chen and Xu Shi and Rujun Jiang and Jiulin Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oQ1Zj9iH88}\\n}'}, 'paperhash': {'value': 'chen|penaltybased_methods_for_simple_bilevel_optimization_under_hölderian_error_bounds'}},forum = 'oQ1Zj9iH88',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10087/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10087/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10087/-/Revision', 'NeurIPS.cc/2024/Conference/-/Desk_Rejected_Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10087/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oPvBnPTbQv',number = 2271,cdate = 1715012456239,pdate = 1727287685552,odate = 1730873857010,mdate = 1730873857028,tcdate = 1715012456239,tmdate = 1730873857028,ddate = None,content = {'title': {'value': 'Referencing Where to Focus: Improving Visual Grounding with Referential Query'}, 'authors': {'value': ['Yabing Wang', 'Zhuotao Tian', 'Qingpei Guo', 'Zheng Qin', 'Sanping Zhou', 'Ming Yang', 'Le Wang']}, 'authorids': {'value': ['~Yabing_Wang1', '~Zhuotao_Tian1', '~Qingpei_Guo1', '~Zheng_Qin3', '~Sanping_Zhou1', '~Ming_Yang2', '~Le_Wang8']}, 'keywords': {'value': ['Visual Grounding']}, 'TLDR': {'value': 'Our proposed RefFormer can iteratively refine the target-related context and generate referential queries, which provide the decoder with prior context.'}, 'abstract': {'value': 'Visual Grounding aims to localize the referring object in an image given a natural language expression. Recent advancements in DETR-based visual grounding methods have attracted considerable attention, as they directly predict the coordinates of the target object without relying on additional efforts, such as pre-generated proposal candidates or pre-defined anchor boxes. However, existing research primarily focuses on designing stronger multi-modal decoder, which typically generates learnable queries by random initialization or by using linguistic embeddings.  This vanilla query generation approach inevitably increases the learning difficulty for the model, as it does not involve any target-related information at the beginning of decoding. Furthermore, they only use the deepest image feature during the query learning process, overlooking the importance of features from other levels. To address these issues, we propose a novel approach, called RefFormer. It consists of the query adaption module that can be seamlessly integrated into CLIP and generate the referential query to provide the prior context for decoder, along with a task-specific decoder. By incorporating the referential query into the decoder, we can effectively mitigate the learning difficulty of the decoder, and accurately concentrate on the target object. Additionally, our proposed query adaption module can also act as an adapter, preserving the rich knowledge within CLIP without the need to tune the parameters of the backbone network. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method, outperforming state-of-the-art approaches on five visual grounding benchmarks.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4ee331051d6635715c830b6da249a25520489f44.pdf'}, 'supplementary_material': {'value': '/attachment/ad03a61584bbd5986a51e2d2808f584c9bcfb1ed.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024referencing,\\ntitle={Referencing Where to Focus: Improving Visual Grounding with Referential Query},\\nauthor={Yabing Wang and Zhuotao Tian and Qingpei Guo and Zheng Qin and Sanping Zhou and Ming Yang and Le Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oPvBnPTbQv}\\n}'}, 'paperhash': {'value': 'wang|referencing_where_to_focus_improving_visual_grounding_with_referential_query'}},forum = 'oPvBnPTbQv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2271/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2271/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2271/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2271/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oPFjhl6DpR',number = 12694,cdate = 1715726565200,pdate = 1727288014608,odate = 1730873951067,mdate = 1730873951102,tcdate = 1715726565200,tmdate = 1730873951102,ddate = None,content = {'title': {'value': 'Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation'}, 'authors': {'value': ['Shangding Gu', 'Laixi Shi', 'Yuhao Ding', 'Alois Knoll', 'Costas Spanos', 'Adam Wierman', 'Ming Jin']}, 'authorids': {'value': ['~Shangding_Gu1', '~Laixi_Shi1', '~Yuhao_Ding2', '~Alois_Knoll1', '~Costas_Spanos1', '~Adam_Wierman1', '~Ming_Jin2']}, 'keywords': {'value': ['Safe Reinforcement Learning', 'Sample Manipulation', 'Efficient Learning']}, 'abstract': {'value': 'Safe reinforcement learning (RL) is crucial for deploying RL agents in real-world applications, as it aims to maximize long-term rewards while satisfying safety constraints. However, safe RL often suffers from sample inefficiency, requiring extensive interactions with the environment to learn a safe policy. We propose Efficient Safe Policy Optimization (ESPO), a novel approach that enhances the efficiency of safe RL through sample manipulation. ESPO employs an optimization framework with three modes: maximizing rewards, minimizing costs, and balancing the trade-off between the two. By dynamically adjusting the sampling process based on the observed conflict between reward and safety gradients, ESPO theoretically guarantees convergence, optimization stability, and improved sample complexity bounds. Experiments on the Safety-MuJoCo and Omnisafe benchmarks demonstrate that ESPO significantly outperforms existing primal-based and primal-dual-based baselines in terms of reward maximization and constraint satisfaction. Moreover, ESPO achieves substantial gains in sample efficiency, requiring 25--29\\\\% fewer samples than baselines, and reduces training time by 21--38\\\\%.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ba79b8360e1e32df1bc174e2a4c138266533424a.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngu2024enhancing,\\ntitle={Enhancing Efficiency of Safe Reinforcement Learning via Sample Manipulation},\\nauthor={Shangding Gu and Laixi Shi and Yuhao Ding and Alois Knoll and Costas Spanos and Adam Wierman and Ming Jin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oPFjhl6DpR}\\n}'}, 'paperhash': {'value': 'gu|enhancing_efficiency_of_safe_reinforcement_learning_via_sample_manipulation'}},forum = 'oPFjhl6DpR',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12694/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12694/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12694/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12694/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oNMnR0NJ2e',number = 18440,cdate = 1715785540816,pdate = 1727288183768,odate = 1730873989830,mdate = 1730873989848,tcdate = 1715785540816,tmdate = 1730873989848,ddate = None,content = {'title': {'value': 'A Label is Worth A Thousand Images in Dataset Distillation'}, 'authors': {'value': ['Tian Qin', 'Zhiwei Deng', 'David Alvarez-Melis']}, 'authorids': {'value': ['~Tian_Qin3', '~Zhiwei_Deng3', '~David_Alvarez-Melis1']}, 'keywords': {'value': ['Dataset distillation', 'Data condensation', 'Synthetic data generation', 'Data-efficient learning']}, 'TLDR': {'value': 'Informative soft labels is the key behind the success of dataset distillation methods.'}, 'abstract': {'value': 'Data *quality* is a crucial factor in the performance of machine learning models, a principle that dataset distillation methods exploit by compressing training datasets into much smaller counterparts that maintain similar downstream performance. Understanding how and why data distillation methods work is vital not only for improving these methods but also for revealing fundamental characteristics of \"good” training data. However, a major challenge in achieving this goal is the observation that distillation approaches, which rely on sophisticated but mostly disparate methods to generate synthetic data, have little in common with each other. In this work, we highlight a largely overlooked aspect common to most of these methods: the use of soft (probabilistic) labels. Through a series of ablation experiments, we study the role of soft labels in depth. Our results reveal that the main factor explaining the performance of state-of-the-art distillation methods is not the specific techniques used to generate synthetic data but rather the use of soft labels. Furthermore, we demonstrate that not all soft labels are created equal; they must contain *structured information* to be beneficial. We also provide empirical scaling laws that characterize the effectiveness of soft labels as a function of images-per-class in the distilled dataset and establish an empirical Pareto frontier for data-efficient learning. Combined, our findings challenge conventional wisdom in dataset distillation, underscore the importance of soft labels in learning, and suggest new directions for improving distillation methods. Code for all experiments is available at https://github.com/sunnytqin/no-distillation.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e453b8d02edbaab707276479e85ca31daeade255.pdf'}, 'supplementary_material': {'value': '/attachment/f0c912e0794b86294477c96a2e59d41adccaa847.zip'}, '_bibtex': {'value': '@inproceedings{\\nqin2024a,\\ntitle={A Label is Worth A Thousand Images in Dataset Distillation},\\nauthor={Tian Qin and Zhiwei Deng and David Alvarez-Melis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oNMnR0NJ2e}\\n}'}, 'paperhash': {'value': 'qin|a_label_is_worth_a_thousand_images_in_dataset_distillation'}},forum = 'oNMnR0NJ2e',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18440/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18440/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18440/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18440/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oMHpejyGdx',number = 9066,cdate = 1715674043611,pdate = 1727287899857,odate = 1730873917145,mdate = 1734703847953,tcdate = 1715674043611,tmdate = 1734703847953,ddate = None,content = {'title': {'value': 'Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models'}, 'authors': {'value': ['Cong Wan', 'Yuhang He', 'Xiang Song', 'Yihong Gong']}, 'authorids': {'value': ['~Cong_Wan1', '~Yuhang_He2', '~Xiang_Song2', '~Yihong_Gong1']}, 'keywords': {'value': ['Adversarial perturbations', 'customized diffusion models', 'privacy protection', 'prompt distribution modeling']}, 'TLDR': {'value': 'This paper proposes a Prompt-agnostic Adversarial Perturbation method for customized text-to-image diffusion models by modeling the prompt distribution and generating perturbations to maximize expected disturbance over sampled prompts.'}, 'abstract': {'value': 'Diffusion models have revolutionized customized text-to-image generation, allowing for efficient synthesis of photos from personal data with textual descriptions. However, these advancements bring forth risks including privacy breaches and unauthorized replication of artworks. Previous researches primarily center around using “prompt-specific methods” to generate adversarial examples to protect personal images, yet the effectiveness of existing methods is hindered by constrained adaptability to different prompts.\\nIn this paper, we introduce a Prompt-Agnostic Adversarial Perturbation (PAP) method for customized diffusion models. PAP first models the prompt distribution using a Laplace Approximation, and then produces prompt-agnostic perturbations by maximizing a disturbance expectation based on the modeled distribution.\\nThis approach effectively tackles the prompt-agnostic attacks, leading to improved defense stability.\\nExtensive experiments in face privacy and artistic style protection, demonstrate the superior generalization of our method in comparison to existing techniques.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9a93ace094b2a0a79ded25792fcdd8fdacf875e7.pdf'}, 'supplementary_material': {'value': '/attachment/cb14949ddc54aee0cc2058599c7d1229c3815aff.zip'}, '_bibtex': {'value': '@inproceedings{\\nwan2024promptagnostic,\\ntitle={Prompt-Agnostic Adversarial Perturbation for Customized Diffusion Models},\\nauthor={Cong Wan and Yuhang He and Xiang Song and Yihong Gong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oMHpejyGdx}\\n}'}, 'paperhash': {'value': 'wan|promptagnostic_adversarial_perturbation_for_customized_diffusion_models'}},forum = 'oMHpejyGdx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9066/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9066/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9066/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9066/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oLoqHRbXYE',number = 10759,cdate = 1715697379969,pdate = 1727287950512,odate = 1730873932006,mdate = 1730873932027,tcdate = 1715697379969,tmdate = 1730873932027,ddate = None,content = {'title': {'value': 'Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models'}, 'authors': {'value': ['Yuchen Hu', 'Chen Chen', 'Chao-Han Huck Yang', 'Chengwei Qin', 'Pin-Yu Chen', 'EngSiong Chng', 'Chao Zhang']}, 'authorids': {'value': ['~Yuchen_Hu1', '~Chen_Chen56', '~Chao-Han_Huck_Yang1', '~Chengwei_Qin1', '~Pin-Yu_Chen1', '~EngSiong_Chng1', '~Chao_Zhang20']}, 'keywords': {'value': ['Automatic speech recognition', 'speech foundation models', 'unsupervised domain adaptation']}, 'abstract': {'value': 'We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks. Our code aims to open source to the research communities.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/079a83ef1d5b8c3d225dd3ed926e2ec0289db3be.pdf'}, 'supplementary_material': {'value': '/attachment/0a9dc027cc5eabb8df5e7b0737398f9b68de699a.zip'}, '_bibtex': {'value': '@inproceedings{\\nhu2024selftaught,\\ntitle={Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models},\\nauthor={Yuchen Hu and Chen Chen and Chao-Han Huck Yang and Chengwei Qin and Pin-Yu Chen and EngSiong Chng and Chao Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oLoqHRbXYE}\\n}'}, 'paperhash': {'value': 'hu|selftaught_recognizer_toward_unsupervised_adaptation_for_speech_foundation_models'}},forum = 'oLoqHRbXYE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10759/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10759/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10759/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10759/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oLcPadFrY3',number = 9766,cdate = 1715685388390,pdate = 1727287920335,odate = 1730873923311,mdate = 1730873923329,tcdate = 1715685388390,tmdate = 1730873923329,ddate = None,content = {'title': {'value': 'AdaPKC: PeakConv with Adaptive Peak Receptive Field for Radar Semantic Segmentation'}, 'authors': {'value': ['Teng Li', 'Liwen Zhang', 'Youcheng Zhang', 'ZijunHu', 'Pengcheng Pi', 'Zongqing Lu', 'Qingmin Liao', 'Zhe Ma']}, 'authorids': {'value': ['~Teng_Li7', '~Liwen_Zhang5', '~Youcheng_Zhang1', '~ZijunHu1', '~Pengcheng_Pi1', '~Zongqing_Lu3', '~Qingmin_Liao1', '~Zhe_Ma2']}, 'keywords': {'value': ['Radar Semantic Segmentation', 'Adaptive Peak Convolution']}, 'TLDR': {'value': 'A more robust novel convolution operator tailored for radar signals.'}, 'abstract': {'value': 'Deep learning-based radar detection technology is receiving increasing attention in areas such as autonomous driving, UAV surveillance, and marine monitoring. Among recent efforts, PeakConv (PKC) provides a solution that can retain the peak response characteristics of radar signals and play the characteristics of deep convolution, thereby improving the effect of radar semantic segmentation (RSS). However, due to the use of a pre-set fixed peak receptive field sampling rule, PKC still has limitations in dealing with problems such as inconsistency of target frequency domain response broadening, non-homogeneous and time-varying characteristic of noise/clutter distribution. Therefore, this paper proposes an idea of adaptive peak receptive field, and upgrades PKC to AdaPKC based on this idea. Beyond that, a novel fine-tuning technology to further boost the performance of AdaPKC-based RSS networks is presented. Through experimental verification using various real-measured radar data (including publicly available low-cost millimeter-wave radar dataset for autonomous driving and self-collected Ku-band surveillance radar dataset), we found that the performance of AdaPKC-based models surpasses other SoTA methods in RSS tasks. The code is available at https://github.com/lihua199710/AdaPKC.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9a29228d2f050bf7c0a7f473c4d93ef1491f7152.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024adapkc,\\ntitle={Ada{PKC}: PeakConv with Adaptive Peak Receptive Field for Radar Semantic Segmentation},\\nauthor={Teng Li and Liwen Zhang and Youcheng Zhang and ZijunHu and Pengcheng Pi and Zongqing Lu and Qingmin Liao and Zhe Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oLcPadFrY3}\\n}'}, 'paperhash': {'value': 'li|adapkc_peakconv_with_adaptive_peak_receptive_field_for_radar_semantic_segmentation'}},forum = 'oLcPadFrY3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9766/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9766/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9766/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9766/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oFgTScAsBr',number = 7335,cdate = 1715618259559,pdate = 1727287843597,odate = 1730873901960,mdate = 1730873901981,tcdate = 1715618259559,tmdate = 1730873901981,ddate = None,content = {'title': {'value': 'Masked Pre-training Enables Universal Zero-shot Denoiser'}, 'authors': {'value': ['Xiaoxiao Ma', 'Zhixiang Wei', 'Yi Jin', 'Pengyang Ling', 'Tianle Liu', 'Ben Wang', 'Junkang Dai', 'Huaian Chen']}, 'authorids': {'value': ['~Xiaoxiao_Ma3', '~Zhixiang_Wei1', '~Yi_Jin1', '~Pengyang_Ling1', '~Tianle_Liu2', '~Ben_Wang6', '~Junkang_Dai1', '~Huaian_Chen1']}, 'keywords': {'value': ['image restoration', 'image denoising，self-supervised learning']}, 'TLDR': {'value': 'An efficient yet novel approach for zero-shot denoising'}, 'abstract': {'value': 'In this work, we observe that model trained on vast general images via masking strategy, has been naturally embedded with their distribution knowledge, thus spontaneously attains the underlying potential for strong image denoising.\\nBased on this observation, we propose a novel zero-shot denoising paradigm, i.e., $\\\\textbf{M}$asked $\\\\textbf{P}$re-train then $\\\\textbf{I}$terative fill ($\\\\textbf{MPI}$).\\nMPI first trains model via masking and then employs pre-trained weight for high-quality zero-shot image denoising on a single noisy image.\\nConcretely, MPI comprises two key procedures:\\n$\\\\textbf{1) Masked Pre-training}$ involves training model to reconstruct massive natural images with random masking for generalizable representations, gathering the potential for valid zero-shot denoising on images with varying noise degradation and even in distinct image types.\\n$\\\\textbf{2) Iterative filling}$ exploits pre-trained knowledge for effective zero-shot denoising. It iteratively optimizes the image by leveraging pre-trained weights, focusing on alternate reconstruction of different image parts, and gradually assembles fully denoised image within limited number of iterations.\\nComprehensive experiments across various noisy scenarios underscore the notable advances of MPI over previous approaches with a marked reduction in inference time.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/264ce97f1508a2a3605cf04af7e7ebd47bcf6786.pdf'}, 'supplementary_material': {'value': '/attachment/7dfccf01ca546a6848a2c0db1445c2eaef47bd3d.zip'}, '_bibtex': {'value': '@inproceedings{\\nma2024masked,\\ntitle={Masked Pre-training Enables Universal Zero-shot Denoiser},\\nauthor={Xiaoxiao Ma and Zhixiang Wei and Yi Jin and Pengyang Ling and Tianle Liu and Ben Wang and Junkang Dai and Huaian Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oFgTScAsBr}\\n}'}, 'paperhash': {'value': 'ma|masked_pretraining_enables_universal_zeroshot_denoiser'}},forum = 'oFgTScAsBr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7335/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7335/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7335/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7335/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'oEVsxVdush',number = 14814,cdate = 1715754000735,pdate = 1727288080133,odate = 1730873968524,mdate = 1736931594665,tcdate = 1715754000735,tmdate = 1736931594665,ddate = None,content = {'title': {'value': 'Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products'}, 'authors': {'value': ['Bethia Sun', 'Maurice Pagnucco', 'Yang Song']}, 'authorids': {'value': ['~Bethia_Sun1', '~Maurice_Pagnucco1', '~Yang_Song4']}, 'keywords': {'value': ['representation learning', 'compositional representations', 'disentanglement', 'tensor product representations', 'compositionality']}, 'abstract': {'value': \"Since the inception of the classicalist vs. connectionist debate, it has been argued that the ability to systematically combine  symbol-like entities into compositional representations is crucial for human intelligence. In connectionist systems, the field of disentanglement has gained prominence for its ability to produce explicitly compositional representations; however, it relies on a fundamentally *symbolic, concatenative* representation of compositional structure that clashes with the *continuous, distributed* foundations of deep learning. To resolve this tension, we extend Smolensky's Tensor Product Representation (TPR) and introduce *Soft TPR*, a representational form that encodes compositional structure in an inherently *distributed, flexible* manner, along with *Soft TPR Autoencoder*, a theoretically-principled architecture designed specifically to learn Soft TPRs.  Comprehensive evaluations in the visual representation learning domain demonstrate that the Soft TPR framework consistently outperforms conventional disentanglement alternatives -- achieving state-of-the-art disentanglement, boosting representation learner convergence, and delivering superior sample efficiency and low-sample regime performance in downstream tasks. These findings highlight the promise of a *distributed* and *flexible* approach to representing compositional structure by potentially enhancing alignment with the core principles of deep learning over the conventional symbolic approach.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4486f55448e109d9651e356f158d770cc5786393.pdf'}, 'supplementary_material': {'value': '/attachment/ee3b9dd34eabcba5698f62767ca6e11b7e95e1a8.zip'}, '_bibtex': {'value': '@inproceedings{\\nsun2024soft,\\ntitle={Soft Tensor Product Representations for Fully Continuous, Compositional Visual Representations},\\nauthor={Bethia Sun and Maurice Pagnucco and Yang Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oEVsxVdush}\\n}'}, 'paperhash': {'value': 'sun|fully_distributed_flexible_compositional_visual_representations_via_soft_tensor_products'}},forum = 'oEVsxVdush',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14814/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14814/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14814/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14814/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oEKFPSOWpp',number = 21619,cdate = 1715802755788,pdate = 1727288258863,odate = 1730874006363,mdate = 1730874006378,tcdate = 1715802755788,tmdate = 1730874006378,ddate = None,content = {'title': {'value': 'NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design'}, 'authors': {'value': ['Ruizhi Liu', 'ZhishengZeng', 'Shizhe Ding', 'Jingyan Sui', 'Xingquan Li', 'Dongbo Bu']}, 'authorids': {'value': ['~Ruizhi_Liu1', '~ZhishengZeng1', '~Shizhe_Ding2', '~Jingyan_Sui1', '~Xingquan_Li1', '~Dongbo_Bu1']}, 'keywords': {'value': ['Global routing', 'chip design', 'neural network', 'Steiner tree', 'deep learning', 'congestion']}, 'TLDR': {'value': 'We propose NeuralSteiner, a learning-based method to optimize overflow and wirelength simultaneously for global routing problem in chip design.'}, 'abstract': {'value': 'Global routing plays a critical role in modern chip design. The routing paths generated by global routers often form a rectilinear Steiner tree (RST). Recent advances from the machine learning community have shown the power of learning-based route generation; however, the yielded routing paths by the existing approaches often suffer from considerable overflow, thus greatly hindering their application in practice.\\nWe propose NeuralSteiner, an accurate approach to overflow-avoiding global routing in chip design. The key idea of NeuralSteiner approach is to learn Steiner trees: we first predict the locations of highly likely Steiner points by adopting a neural network considering full-net spatial and overflow information, then select appropriate points by running a graph-based post-processing algorithm, and finally connect these points with the input pins to yield overflow-avoiding RSTs. NeuralSteiner offers two advantages over previous learning-based models. First, by using the learning scheme, NeuralSteiner ensures the connectivity of generated routes while significantly reducing congestion. Second, NeuralSteiner can effectively scale to large nets and transfer to unseen chip designs without any modifications or fine-tuning.  Extensive experiments over public large-scale benchmarks reveal that, compared with the state-of-the-art deep generative methods, NeuralSteiner achieves up to a 99.8\\\\% reduction in overflow while speeding up the generation and maintaining a slight wirelength loss within only 1.8\\\\%.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f1d7976a3ee11bea310717733176d314fced0fd9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024neuralsteiner,\\ntitle={NeuralSteiner: Learning Steiner Tree for Overflow-avoiding Global Routing in Chip Design},\\nauthor={Ruizhi Liu and ZhishengZeng and Shizhe Ding and Jingyan Sui and Xingquan Li and Dongbo Bu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oEKFPSOWpp}\\n}'}, 'paperhash': {'value': 'liu|neuralsteiner_learning_steiner_tree_for_overflowavoiding_global_routing_in_chip_design'}},forum = 'oEKFPSOWpp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21619/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21619/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21619/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21619/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'oDeqjIM9Sk',number = 19100,cdate = 1715789180249,pdate = 1727288201865,odate = 1730873993819,mdate = 1747068378590,tcdate = 1715789180249,tmdate = 1747068378590,ddate = None,content = {'title': {'value': 'Weight decay induces low-rank attention layers'}, 'authors': {'value': ['Seijin Kobayashi', 'Yassir Akram', 'Johannes von Oswald']}, 'authorids': {'value': ['~Seijin_Kobayashi1', '~Yassir_Akram1', '~Johannes_von_Oswald2']}, 'keywords': {'value': ['weight decay', 'attention']}, 'abstract': {'value': 'The effect of regularizers such as weight decay when training deep neural networks is not well understood. We study the influence of weight decay as well as $L2$-regularization when training neural network models in which parameter matrices interact multiplicatively. This combination is of particular interest as this parametrization is common in attention layers, the workhorse of transformers. Here, key-query, as well as value-projection parameter matrices, are multiplied directly with each other: $W_K^TW_Q$ and $PW_V$. \\nWe extend previous results and show on one hand that any local minimum of a $L2$-regularized loss of the form $L(AB^\\\\top) + \\\\lambda (\\\\|A\\\\|^2 + \\\\|B\\\\|^2)$ coincides with a minimum of the nuclear norm-regularized loss $L(AB^\\\\top) + \\\\lambda\\\\|AB^\\\\top\\\\|_*$, and on the other hand that the 2 losses become identical exponentially quickly during training. We thus complement existing works linking $L2$-regularization with low-rank regularization, and in particular, explain why such regularization on the matrix product affects early stages of training.\\nBased on these theoretical insights, we verify empirically that the key-query and value-projection matrix products $W_K^TW_Q, PW_V$ within attention layers, when optimized with weight decay, as usually done in vision tasks and language modelling, indeed induce a significant reduction in the rank of $W_K^TW_Q$ and $PW_V$, even in fully online training.\\nWe find that, in accordance with existing work, inducing low rank in attention matrix products can damage language model performance, and observe advantages when decoupling weight decay in attention layers from the rest of the parameters.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4e220deb56a61753afa3a671a615d61b3233a361.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nkobayashi2024weight,\\ntitle={Weight decay induces low-rank attention layers},\\nauthor={Seijin Kobayashi and Yassir Akram and Johannes von Oswald},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oDeqjIM9Sk}\\n}'}, 'paperhash': {'value': 'kobayashi|weight_decay_induces_lowrank_attention_layers'}},forum = 'oDeqjIM9Sk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19100/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19100/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19100/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19100/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'oCGkSH7ys2',number = 6340,cdate = 1715590534573,pdate = 1727287811632,odate = 1730873892544,mdate = 1730873892563,tcdate = 1715590534573,tmdate = 1730873892563,ddate = None,content = {'title': {'value': 'Self-playing Adversarial Language Game Enhances LLM Reasoning'}, 'authors': {'value': ['Pengyu Cheng', 'Tianhao Hu', 'Han Xu', 'Zhisong Zhang', 'Yong Dai', 'Lei Han', 'nan du', 'Xiaolong Li']}, 'authorids': {'value': ['~Pengyu_Cheng1', '~Tianhao_Hu2', '~Han_Xu11', '~Zhisong_Zhang1', '~Yong_Dai1', '~Lei_Han1', '~nan_du3', '~Xiaolong_Li4']}, 'keywords': {'value': ['Self-play', 'Large Language Models', 'LLM Reasoning']}, 'abstract': {'value': \"We explore the potential of self-play training for large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate around a target word only visible to the attacker. The attacker aims to induce the defender to speak the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players must have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by Self-Playing this Adversarial language Game (SPAG). With this goal, we select several open-source LLMs and let each act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performances uniformly improve on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLMs' reasoning abilities. The code is available at https://github.com/Linear95/SPAG.\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5346da27aaf48030be064c5b97d66589dd9dfd0c.pdf'}, 'supplementary_material': {'value': '/attachment/6bf65db077e8378c0d82032f3abd5b70faadafb8.zip'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024selfplaying,\\ntitle={Self-playing Adversarial Language Game Enhances {LLM} Reasoning},\\nauthor={Pengyu Cheng and Tianhao Hu and Han Xu and Zhisong Zhang and Yong Dai and Lei Han and nan du and Xiaolong Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oCGkSH7ys2}\\n}'}, 'paperhash': {'value': 'cheng|selfplaying_adversarial_language_game_enhances_llm_reasoning'}},forum = 'oCGkSH7ys2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6340/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6340/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6340/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6340/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'oBvaZJ1C71',number = 19928,cdate = 1715793493284,pdate = 1727288221322,odate = 1730873998392,mdate = 1730873998403,tcdate = 1715793493284,tmdate = 1730873998403,ddate = None,content = {'title': {'value': 'GAVEL: Generating Games via Evolution and Language Models'}, 'authors': {'value': ['Graham Todd', 'Alexander George Padula', 'Matthew Stephenson', 'Eric Piette', 'Dennis J. N. J. Soemers', 'Julian Togelius']}, 'authorids': {'value': ['~Graham_Todd1', '~Alexander_George_Padula1', '~Matthew_Stephenson2', '~Eric_Piette1', '~Dennis_J._N._J._Soemers1', '~Julian_Togelius1']}, 'keywords': {'value': ['games', 'llms', 'language models', 'evolution', 'quality diversity', 'pcg']}, 'abstract': {'value': 'Automatically generating novel and interesting games is a complex task. Challenges include representing game rules in a computationally workable form, searching through the large space of potential games under most such representations, and accurately evaluating the originality and quality of previously unseen games. Prior work in automated game generation has largely focused on relatively restricted rule representations and relied on domain-specific heuristics. In this work, we explore the generation of novel games in the comparatively expansive Ludii game description language, which encodes the rules of over 1000 board games in a variety of styles and modes of play. We draw inspiration from recent advances in large language models and evolutionary computation in order to train a model that intelligently mutates and recombines games and mechanics expressed as code. We demonstrate both quantitatively and qualitatively that our approach is capable of generating new and interesting games, including in regions of the potential rules space not covered by existing games in the Ludii dataset.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We use evolutionary computation and large language models to generate new and interesting board games'}, 'pdf': {'value': '/pdf/5be0e53d8175bf2545791d8024cd437cf7a20fff.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntodd2024gavel,\\ntitle={{GAVEL}: Generating Games via Evolution and Language Models},\\nauthor={Graham Todd and Alexander George Padula and Matthew Stephenson and Eric Piette and Dennis J. N. J. Soemers and Julian Togelius},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=oBvaZJ1C71}\\n}'}, 'paperhash': {'value': 'todd|gavel_generating_games_via_evolution_and_language_models'}},forum = 'oBvaZJ1C71',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19928/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19928/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19928/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19928/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o9Lkiv1qpc',number = 11041,cdate = 1715700408637,pdate = 1727287959017,odate = 1730873934571,mdate = 1734693489685,tcdate = 1715700408637,tmdate = 1734693489685,ddate = None,content = {'title': {'value': 'Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model'}, 'authors': {'value': ['Min Zhao', 'Hongzhou Zhu', 'Chendong Xiang', 'Kaiwen Zheng', 'Chongxuan Li', 'Jun Zhu']}, 'authorids': {'value': ['~Min_Zhao3', '~Hongzhou_Zhu1', '~Chendong_Xiang1', '~Kaiwen_Zheng2', '~Chongxuan_Li1', '~Jun_Zhu2']}, 'keywords': {'value': ['diffusion model', 'image-to-video diffusion model', 'video generation', 'video diffusion model']}, 'abstract': {'value': \"Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \\\\url{https://cond-image-leak.github.io/}.\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ce3c145fb629337eae7208aec2f4d75453caa367.pdf'}, 'supplementary_material': {'value': '/attachment/cf426bc3c30072be76ef5de07b90f0c11e45c8f0.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024identifying,\\ntitle={Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model},\\nauthor={Min Zhao and Hongzhou Zhu and Chendong Xiang and Kaiwen Zheng and Chongxuan Li and Jun Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o9Lkiv1qpc}\\n}'}, 'paperhash': {'value': 'zhao|identifying_and_solving_conditional_image_leakage_in_imagetovideo_diffusion_model'}},forum = 'o9Lkiv1qpc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11041/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11041/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11041/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11041/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o8m4RM5mBk',number = 3549,cdate = 1715302482399,pdate = 1727287724098,odate = 1730873867492,mdate = 1730873867520,tcdate = 1715302482399,tmdate = 1730873867520,ddate = None,content = {'title': {'value': 'Attention Temperature Matters in ViT-Based Cross-Domain Few-Shot Learning'}, 'authors': {'value': ['Yixiong Zou', 'Ran Ma', 'Yuhua Li', 'Ruixuan Li']}, 'authorids': {'value': ['~Yixiong_Zou1', '~Ran_Ma1', '~Yuhua_Li2', '~Ruixuan_Li1']}, 'keywords': {'value': ['Cross-domain few-shot learning', 'Vision transformer', 'attention temperature']}, 'abstract': {'value': \"Cross-domain few-shot learning (CDFSL) is proposed to transfer knowledge from large-scale source-domain datasets to downstream target-domain datasets with only a few training samples. However, Vision Transformer (ViT), as a strong backbone network to achieve many top performances, is still under-explored in the CDFSL task in its transferability against large domain gaps. In this paper, we find an interesting phenomenon of ViT in the CDFSL task: by simply multiplying a temperature (even as small as 0) to the attention in ViT blocks, the target-domain performance consistently increases, even though the attention map is downgraded to a uniform map. In this paper, we delve into this phenomenon for an interpretation. Through experiments, we interpret this phenomenon as a remedy for the ineffective target-domain attention caused by the query-key attention mechanism under large domain gaps. Based on it, we further propose a simple but effective method for the CDFSL task to boost ViT's transferability by resisting the learning of query-key parameters and encouraging that of non-query-key ones. Experiments on four CDFSL datasets validate the rationale of our interpretation and method, showing we can consistently outperform state-of-the-art methods. Our codes are available at https://github.com/Zoilsen/Attn_Temp_CDFSL.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': \"We find a phenomenon for ViT-based CDFSL: multiplying a small temperature (even 0) to ViT's attention map can consistently improve performance. We delve into this phenomenon for an interpretation and propose an effective method for CDFSL.\"}, 'pdf': {'value': '/pdf/e5e1a8116920c85d89d3cc454c2cbf92f98caa8c.pdf'}, 'supplementary_material': {'value': '/attachment/29f4ec35094843b475df702d2304fe25208b5673.zip'}, '_bibtex': {'value': '@inproceedings{\\nzou2024attention,\\ntitle={Attention Temperature Matters in ViT-Based Cross-Domain Few-Shot Learning},\\nauthor={Yixiong Zou and Ran Ma and Yuhua Li and Ruixuan Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o8m4RM5mBk}\\n}'}, 'paperhash': {'value': 'zou|attention_temperature_matters_in_vitbased_crossdomain_fewshot_learning'}},forum = 'o8m4RM5mBk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3549/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3549/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3549/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3549/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'o863gX6DxA',number = 11741,cdate = 1715709711064,pdate = 1727287981492,odate = 1730873941839,mdate = 1730873941920,tcdate = 1715709711064,tmdate = 1730873941920,ddate = None,content = {'title': {'value': 'Code Repair with LLMs gives an Exploration-Exploitation Tradeoff'}, 'authors': {'value': ['Hao Tang', 'Keya Hu', 'Jin Peng Zhou', 'Si Cheng Zhong', 'Wei-Long Zheng', 'Xujie Si', 'Kevin Ellis']}, 'authorids': {'value': ['~Hao_Tang5', '~Keya_Hu1', '~Jin_Peng_Zhou1', '~Si_Cheng_Zhong1', '~Wei-Long_Zheng1', '~Xujie_Si1', '~Kevin_Ellis1']}, 'keywords': {'value': ['Program Synthesis', 'LLM']}, 'abstract': {'value': 'Iteratively improving and repairing source code with large language models (LLMs), known as refinement, has emerged as a popular way of generating programs that would be too complex to construct in one shot. Given a bank of test cases, together with a candidate program, an LLM can improve that program by being prompted with failed test cases. But it remains an open question how to best iteratively refine code, with prior work employing simple greedy or breadth-first strategies. We show here that refinement exposes an explore-exploit tradeoff: exploit by refining the program that passes the most test cases, or explore by refining a lesser considered program. We frame this as an arm-acquiring bandit problem, which we solve with Thompson Sampling. The resulting LLM-based program synthesis algorithm is broadly applicable: Across loop invariant synthesis, visual reasoning puzzles, and competition programming problems, we find that our new method can solve more problems using fewer language model calls.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f4dac508dca07ac2e37c96306bb1435333481698.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntang2024code,\\ntitle={Code Repair with {LLM}s gives an Exploration-Exploitation Tradeoff},\\nauthor={Hao Tang and Keya Hu and Jin Peng Zhou and Si Cheng Zhong and Wei-Long Zheng and Xujie Si and Kevin Ellis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o863gX6DxA}\\n}'}, 'paperhash': {'value': 'tang|code_repair_with_llms_gives_an_explorationexploitation_tradeoff'}},forum = 'o863gX6DxA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11741/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11741/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11741/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11741/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o7DOGbZeyP',number = 12990,cdate = 1715733026498,pdate = 1727288024753,odate = 1730873954059,mdate = 1730873954071,tcdate = 1715733026498,tmdate = 1730873954071,ddate = None,content = {'title': {'value': 'LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate'}, 'authors': {'value': ['Anthony Fuller', 'Daniel Kyrollos', 'Yousef Yassin', 'James R Green']}, 'authorids': {'value': ['~Anthony_Fuller1', '~Daniel_Kyrollos1', '~Yousef_Yassin1', '~James_R_Green1']}, 'keywords': {'value': ['vision transformers', 'position encoding', 'computer vision']}, 'TLDR': {'value': 'We introduce a position encoding method for plain ViTs, called LookHere, that improves performance at the training resolution and beyond.'}, 'abstract': {'value': 'High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning — ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.\\n\\nWe propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) — on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c456ddc1b0dbd841b80ff6464ae6d63c046b2815.pdf'}, 'supplementary_material': {'value': '/attachment/6e93b3c09d71bb24debf7a2a2ade81edd99a65fe.zip'}, '_bibtex': {'value': '@inproceedings{\\nfuller2024lookhere,\\ntitle={LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate},\\nauthor={Anthony Fuller and Daniel Kyrollos and Yousef Yassin and James R Green},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o7DOGbZeyP}\\n}'}, 'paperhash': {'value': 'fuller|lookhere_vision_transformers_with_directed_attention_generalize_and_extrapolate'}},forum = 'o7DOGbZeyP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12990/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12990/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12990/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12990/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o6Hk6vld20',number = 6322,cdate = 1715590216144,pdate = 1727287811075,odate = 1730873892327,mdate = 1736270423262,tcdate = 1715590216144,tmdate = 1736270423262,ddate = None,content = {'title': {'value': 'Constrained Sampling with Primal-Dual Langevin Monte Carlo'}, 'authors': {'value': ['Luiz F. O. Chamon', 'Mohammad Reza Karimi Jaghargh', 'Anna Korba']}, 'authorids': {'value': ['~Luiz_F._O._Chamon1', '~Mohammad_Reza_Karimi_Jaghargh1', '~Anna_Korba2']}, 'keywords': {'value': ['Sampling', 'optimization', 'constrained optimization', 'duality']}, 'TLDR': {'value': 'We consider the problem of sampling from a target probability distribution whose density is know up to a normalization constant while satisfying a set of statistical constraints specified by general nonlinear functions.'}, 'abstract': {'value': 'This work considers the problem of sampling from a probability distribution known up to a normalization constant while satisfying a set of statistical constraints specified by the expected values of general nonlinear functions. This problem finds applications in, e.g., Bayesian inference, where it can constrain moments to evaluate counterfactual scenarios or enforce desiderata such as prediction fairness. Methods developed to handle support constraints, such as those based on mirror maps, barriers, and penalties, are not suited for this task. This work therefore relies on gradient descent-ascent dynamics in Wasserstein space to put forward a discrete-time primal-dual Langevin Monte Carlo algorithm (PD-LMC) that simultaneously constrains the target distribution and samples from it. We analyze the convergence of PD-LMC under standard assumptions on the target distribution and constraints, namely (strong) convexity and log-Sobolev inequalities. To do so, we bring classical optimization arguments for saddle-point algorithms to the geometry of Wasserstein space. We illustrate the relevance and effectiveness of PD-LMC in several applications.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b822744943ad24bb25e1fd688ff9a62ba2c263b2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchamon2024constrained,\\ntitle={Constrained Sampling with Primal-Dual Langevin Monte Carlo},\\nauthor={Luiz F. O. Chamon and Mohammad Reza Karimi Jaghargh and Anna Korba},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o6Hk6vld20}\\n}'}, 'paperhash': {'value': 'chamon|constrained_sampling_with_primaldual_langevin_monte_carlo'}},forum = 'o6Hk6vld20',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6322/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6322/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6322/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6322/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o4coDIby7e',number = 10250,cdate = 1715692201764,pdate = 1727287933859,odate = 1730873927364,mdate = 1730873927384,tcdate = 1715692201764,tmdate = 1730873927384,ddate = None,content = {'title': {'value': 'Measuring Goal-Directedness'}, 'authors': {'value': ['Matt MacDermott', 'James Fox', 'Francesco Belardinelli', 'Tom Everitt']}, 'authorids': {'value': ['~Matt_MacDermott1', '~James_Fox2', '~Francesco_Belardinelli1', '~Tom_Everitt1']}, 'keywords': {'value': ['Causality', 'Graphical Models', 'Maximum Causal Entropy', 'Agency']}, 'TLDR': {'value': 'We propose a formal measure of goal-directedness in probabalistic graphical models, by adapting and generalising the maximum causal entropy framework.'}, 'abstract': {'value': 'We define maximum entropy goal-directedness (MEG), a formal measure of goal-\\ndirectedness in causal models and Markov decision processes, and give algorithms\\nfor computing it. Measuring goal-directedness is important, as it is a critical\\nelement of many concerns about harm from AI. It is also of philosophical interest,\\nas goal-directedness is a key aspect of agency. MEG is based on an adaptation of\\nthe maximum causal entropy framework used in inverse reinforcement learning. It\\ncan measure goal-directedness with respect to a known utility function, a hypothesis\\nclass of utility functions, or a set of random variables. We prove that MEG satisfies\\nseveral desiderata and demonstrate our algorithms with small-scale experiments.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a707011020a1a19716c8f5b5955810a063baced4.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmacdermott2024measuring,\\ntitle={Measuring Goal-Directedness},\\nauthor={Matt MacDermott and James Fox and Francesco Belardinelli and Tom Everitt},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o4coDIby7e}\\n}'}, 'paperhash': {'value': 'macdermott|measuring_goaldirectedness'}},forum = 'o4coDIby7e',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10250/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10250/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10250/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10250/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'o3i1JEfzKw',number = 20263,cdate = 1715795396190,pdate = 1727288229778,odate = 1730874000316,mdate = 1736713962446,tcdate = 1715795396190,tmdate = 1736713962446,ddate = None,content = {'title': {'value': 'Provable Partially Observable Reinforcement Learning with Privileged Information'}, 'authors': {'value': ['Yang Cai', 'Xiangyu Liu', 'Argyris Oikonomou', 'Kaiqing Zhang']}, 'authorids': {'value': ['~Yang_Cai1', '~Xiangyu_Liu4', '~Argyris_Oikonomou1', '~Kaiqing_Zhang3']}, 'keywords': {'value': ['reinforcement learning', 'pomdp', 'partial observability', 'computational', 'privileged information', 'expert distillation', 'teacher-student learning']}, 'TLDR': {'value': 'We study the provable efficiency of partially observable RL with privileged information to the latent state during training for both single-agent and multi-player settings.'}, 'abstract': {'value': 'Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain *privileged information* , e.g., the access to states from simulators, has been exploited in training and achieved prominent empirical successes. To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting, with both computation and sample efficiency analyses. Specifically, we first formalize the empirical paradigm of *expert distillation* (also known as  *teacher-student* learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the deterministic filter condition, under which expert distillation achieves sample and computational complexities that are *both* polynomial. Furthermore, we investigate another successful empirical paradigm of *asymmetric actor-critic*, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted optimistic asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, where one key component is a new provable oracle for learning belief states that preserve *filter stability* under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms with the feature of centralized-training-with-decentralized-execution, a popular framework in empirical MARL, with polynomial sample and (quasi-)polynomial computational complexity in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/03143b8a208debf6ba3bc9c75fd4b72224323628.pdf'}, 'supplementary_material': {'value': '/attachment/6cc718938c2bc8c0e1bacc884fc4063c546cde03.zip'}, '_bibtex': {'value': '@inproceedings{\\ncai2024provable,\\ntitle={Provable Partially Observable Reinforcement Learning with Privileged Information},\\nauthor={Yang Cai and Xiangyu Liu and Argyris Oikonomou and Kaiqing Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=o3i1JEfzKw}\\n}'}, 'paperhash': {'value': 'cai|provable_partially_observable_reinforcement_learning_with_privileged_information'}},forum = 'o3i1JEfzKw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20263/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20263/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20263/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20263/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nyp59a31Ju',number = 13018,cdate = 1715733585625,pdate = 1727288025775,odate = 1730873954378,mdate = 1730873954396,tcdate = 1715733585625,tmdate = 1730873954396,ddate = None,content = {'title': {'value': 'Is Value Learning Really the Main Bottleneck in Offline RL?'}, 'authors': {'value': ['Seohong Park', 'Kevin Frans', 'Sergey Levine', 'Aviral Kumar']}, 'authorids': {'value': ['~Seohong_Park1', '~Kevin_Frans1', '~Sergey_Levine1', '~Aviral_Kumar2']}, 'keywords': {'value': ['reinforcement learning', 'offline reinforcement learning']}, 'abstract': {'value': 'While imitation learning requires access to high-quality data, offline reinforcement learning (RL) should, in principle, perform similarly or better with substantially lower data quality by using a value function. However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL. Motivated by this observation, we aim to understand the bottlenecks in current offline RL algorithms. While poor performance of offline RL is typically attributed to an imperfect value function, we ask: *is the main bottleneck of offline RL indeed in learning the value function, or something else?* To answer this question, we perform a systematic empirical study of (1) value learning, (2) policy extraction, and (3) policy generalization in offline RL problems, analyzing how these components affect performance. We make two surprising observations. First, we find that the choice of a policy extraction algorithm significantly affects the performance and scalability of offline RL, often more so than the value learning objective. For instance, we show that common value-weighted behavioral cloning objectives (e.g., AWR) do not fully leverage the learned value function, and switching to behavior-constrained policy gradient objectives (e.g., DDPG+BC) often leads to substantial improvements in performance and scalability. Second, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states. We then show that the use of suboptimal but high-coverage data or test-time policy training techniques can address this generalization issue in practice. Specifically, we propose two simple test-time policy improvement methods and show that these methods lead to better performance.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d91da7edfb55832deba6ddea4345f1b3e29cb1b5.pdf'}, 'supplementary_material': {'value': '/attachment/86596bf90003909755701437b2fe331bf6466609.zip'}, '_bibtex': {'value': '@inproceedings{\\npark2024is,\\ntitle={Is Value Learning Really the Main Bottleneck in Offline {RL}?},\\nauthor={Seohong Park and Kevin Frans and Sergey Levine and Aviral Kumar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nyp59a31Ju}\\n}'}, 'paperhash': {'value': 'park|is_value_learning_really_the_main_bottleneck_in_offline_rl'}},forum = 'nyp59a31Ju',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13018/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13018/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13018/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13018/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nxumYwxJPB',number = 15956,cdate = 1715765231235,pdate = 1727288112362,odate = 1730873976126,mdate = 1730873976146,tcdate = 1715765231235,tmdate = 1730873976146,ddate = None,content = {'title': {'value': 'If You Want to Be Robust, Be Wary of Initialization'}, 'authors': {'value': ['Sofiane ENNADIR', 'Johannes F. Lutzeyer', 'Michalis Vazirgiannis', 'El houcine Bergou']}, 'authorids': {'value': ['~Sofiane_ENNADIR1', '~Johannes_F._Lutzeyer1', '~Michalis_Vazirgiannis1', '~El_houcine_Bergou1']}, 'keywords': {'value': ['Adversarial Robustness', 'Graph Neural Networks']}, 'TLDR': {'value': 'We theoretically study the direct relationship between initial weights, number of training epochs and the model’s adversarial vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms.'}, 'abstract': {'value': \"Graph Neural Networks (GNNs) have demonstrated remarkable performance across a spectrum of graph-related tasks, however concerns persist regarding their vulnerability to adversarial perturbations. While prevailing defense strategies focus primarily on pre-processing techniques and adaptive message-passing schemes, this study delves into an under-explored dimension: the impact of weight initialization and associated hyper-parameters, such as training epochs, on a model’s robustness.\\nWe introduce a theoretical framework bridging the connection between initialization strategies and a network's resilience to adversarial perturbations. Our analysis reveals a direct relationship between initial weights, number of training epochs and the model’s vulnerability, offering new insights into adversarial robustness beyond conventional defense mechanisms. While our primary focus is on GNNs, we extend our theoretical framework, providing a general upper-bound applicable to Deep Neural Networks.\\nExtensive experiments, spanning diverse models and real-world datasets subjected to various adversarial attacks, validate our findings. We illustrate that selecting appropriate initialization not only ensures performance on clean datasets but also enhances model robustness against adversarial perturbations, with observed gaps of up to 50\\\\% compared to alternative initialization approaches.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/83f110ffeb7712a25461de7489de5525dd1cf25b.pdf'}, 'supplementary_material': {'value': '/attachment/63ca99e1eaca651f9ceb0c2bc40bc716fbbc49da.zip'}, '_bibtex': {'value': '@inproceedings{\\nennadir2024if,\\ntitle={If You Want to Be Robust, Be Wary of Initialization},\\nauthor={Sofiane ENNADIR and Johannes F. Lutzeyer and Michalis Vazirgiannis and El houcine Bergou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nxumYwxJPB}\\n}'}, 'paperhash': {'value': 'ennadir|if_you_want_to_be_robust_be_wary_of_initialization'}},forum = 'nxumYwxJPB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15956/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15956/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15956/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15956/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC0 1.0'),\n",
       " Note(id = 'nxL7eazKBI',number = 10793,cdate = 1715697770058,pdate = 1727287951474,odate = 1730873932161,mdate = 1736837303185,tcdate = 1715697770058,tmdate = 1736837303185,ddate = None,content = {'title': {'value': 'Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks'}, 'authors': {'value': ['Jiacong Hu', 'Jing Gao', 'Jingwen Ye', 'Yang Gao', 'Xingen Wang', 'Zunlei Feng', 'Mingli Song']}, 'authorids': {'value': ['~Jiacong_Hu1', '~Jing_Gao4', '~Jingwen_Ye1', '~Yang_Gao21', '~Xingen_Wang1', '~Zunlei_Feng1', '~Mingli_Song1']}, 'keywords': {'value': ['Disassembling Models', 'Assembling Models', 'Model Reuse', 'Model Interpretability']}, 'abstract': {'value': 'With the rapid development of deep learning, the increasing complexity and scale of parameters make training a new model increasingly resource-intensive. In this paper, we start from the classic convolutional neural network (CNN) and explore a paradigm that does not require training to obtain new models. Similar to the birth of CNN inspired by receptive fields in the biological visual system, we draw inspiration from the information subsystem pathways in the biological visual system and propose Model Disassembling and Assembling (MDA). During model disassembling, we introduce the concept of relative contribution and propose a component locating technique to extract task-aware components from trained CNN classifiers. For model assembling, we present the alignment padding strategy and parameter scaling strategy to construct a new model tailored for a specific task, utilizing the disassembled task-aware components.\\nThe entire process is akin to playing with LEGO bricks, enabling arbitrary assembly of new models, and providing a novel perspective for model creation and reuse. Extensive experiments showcase that task-aware components disassembled from CNN classifiers or new models assembled using these components closely match or even surpass the performance of the baseline,\\ndemonstrating its promising results for model reuse. Furthermore, MDA exhibits diverse potential applications, with comprehensive experiments exploring model decision route analysis, model compression, knowledge distillation, and more.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/36e0af4cb05df214ff3e1501cc548981685419e6.pdf'}, 'supplementary_material': {'value': '/attachment/ca84f5e5a96d15e1f630ef96580b6e4fa212d745.zip'}, '_bibtex': {'value': '@inproceedings{\\nhu2024model,\\ntitle={Model {LEGO}: Creating Models Like Disassembling and Assembling Building Blocks},\\nauthor={Jiacong Hu and Jing Gao and Jingwen Ye and Yang Gao and Xingen Wang and Zunlei Feng and Mingli Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nxL7eazKBI}\\n}'}, 'paperhash': {'value': 'hu|model_lego_creating_models_like_disassembling_and_assembling_building_blocks'}},forum = 'nxL7eazKBI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10793/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10793/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10793/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10793/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nw9JmfL99s',number = 18952,cdate = 1715788338241,pdate = 1727288198046,odate = 1730873992833,mdate = 1730873992850,tcdate = 1715788338241,tmdate = 1730873992850,ddate = None,content = {'title': {'value': 'Nonlinear dynamics of localization in neural receptive fields'}, 'authors': {'value': ['Leon Lufkin', 'Andrew M Saxe', 'Erin Grant']}, 'authorids': {'value': ['~Leon_Lufkin1', '~Andrew_M_Saxe1', '~Erin_Grant1']}, 'keywords': {'value': ['localization', 'receptive fields', 'learning dynamics', 'emergence']}, 'abstract': {'value': 'Localized receptive fields—neurons that are selective for certain contiguous spatiotemporal features of their input—populate early sensory regions of the mammalian brain. Unsupervised learning algorithms that optimize explicit sparsity or independence criteria replicate features of these localized receptive fields, but fail to explain directly how localization arises through learning without efficient coding, as occurs in early layers of deep neural networks and might occur in early sensory regions of biological systems. We consider an alternative model in which localized receptive fields emerge without explicit top-down efficiency constraints—a feed-forward neural network trained on a data model inspired by the structure of natural images. Previous work identified the importance of non-Gaussian statistics to localization in this setting but left open questions about the mechanisms driving dynamical emergence. We address these questions by deriving the effective learning dynamics for a single nonlinear neuron, making precise how higher-order statistical properties of the input data drive emergent localization, and we demonstrate that the predictions of these effective dynamics extend to the many-neuron setting. Our analysis provides an alternative explanation for the ubiquity of localization as resulting from the nonlinear dynamics of learning in neural circuits'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We analyze the dynamics of emergent localization in receptive fields of a nonlinear neural network, which enables us to bind emergence to specific higher-order statistical properties of the input data.'}, 'pdf': {'value': '/pdf/639227bcf1a258fd70ce091042536afc3780a946.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlufkin2024nonlinear,\\ntitle={Nonlinear dynamics of localization in neural receptive fields},\\nauthor={Leon Lufkin and Andrew M Saxe and Erin Grant},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nw9JmfL99s}\\n}'}, 'paperhash': {'value': 'lufkin|nonlinear_dynamics_of_localization_in_neural_receptive_fields'}},forum = 'nw9JmfL99s',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18952/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18952/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18952/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18952/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nw8cXoNvep',number = 11061,cdate = 1715700553358,pdate = 1727287959754,odate = 1730873934877,mdate = 1734913351673,tcdate = 1715700553358,tmdate = 1734913351673,ddate = None,content = {'title': {'value': '3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction'}, 'authors': {'value': ['Jongmin Lee', 'Minsu Cho']}, 'authorids': {'value': ['~Jongmin_Lee2', '~Minsu_Cho1']}, 'keywords': {'value': ['SO(3) pose estimation', '3D rotation representation', 'SO(3)-equivariance', '3D equivariant networks', 'spherical harmonics', 'Wigner-D Matrix', 'spherical CNNs', 'Wigner-D coefficients prediction', 'uncertainty modeling', 'data sampling efficiency']}, 'TLDR': {'value': 'We address single-image 3D pose estimation by predicting Wigner-D coefficients in the frequency domain using SO(3)-equivariant networks, to improve pose estimation accuracy and data sampling efficiency.'}, 'abstract': {'value': 'Determining the 3D orientations of an object in an image, known as single-image pose estimation, is a crucial task in 3D vision applications. Existing methods typically learn 3D rotations parametrized in the spatial domain using Euler angles or quaternions, but these representations often introduce discontinuities and singularities. SO(3)-equivariant networks enable the structured capture of pose patterns with data-efficient learning, but the  parametrizations in spatial domain are incompatible with their architecture, particularly spherical CNNs, which operate in the frequency domain to enhance computational efficiency. To overcome these issues, we propose a frequency-domain approach that directly predicts Wigner-D coefficients for 3D rotation regression, aligning with the operations of spherical CNNs. Our SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial parameterizations, ensuring consistent pose estimation under arbitrary rotations. Trained with a frequency-domain regression loss, our method achieves state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, with significant improvements in accuracy, robustness, and data efficiency.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2c775e5dff2bb94590c91f17229f634168ee6ad9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024d,\\ntitle={3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction},\\nauthor={Jongmin Lee and Minsu Cho},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nw8cXoNvep}\\n}'}, 'paperhash': {'value': 'lee|3d_equivariant_pose_regression_via_direct_wignerd_harmonics_prediction'}},forum = 'nw8cXoNvep',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11061/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11061/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11061/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11061/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nw6ANsC66G',number = 15345,cdate = 1715759338772,pdate = 1727288095329,odate = 1730873972350,mdate = 1730873972369,tcdate = 1715759338772,tmdate = 1730873972369,ddate = None,content = {'title': {'value': 'Probabilistic Federated Prompt-Tuning with Non-IID and Imbalanced Data'}, 'authors': {'value': ['Pei-Yau Weng', 'Minh Hoang', 'Lam M. Nguyen', 'My T. Thai', 'Tsui-Wei Weng', 'Trong Nghia Hoang']}, 'authorids': {'value': ['~Pei-Yau_Weng1', '~Minh_Hoang1', '~Lam_M._Nguyen1', '~My_T._Thai2', '~Tsui-Wei_Weng1', '~Trong_Nghia_Hoang1']}, 'keywords': {'value': ['Probabilistic Learning']}, 'TLDR': {'value': 'We propose a new probabilistic prompt aggregation method for fine-tuning on decentralized and imbalance data settings.'}, 'abstract': {'value': \"Fine-tuning pre-trained models is a popular approach in machine learning for solving complex tasks with moderate data. However, fine-tuning the entire pre-trained model is ineffective in federated data scenarios where local data distributions are diversely skewed. To address this, we explore integrating federated learning with a more effective prompt-tuning method, optimizing for a small set of input prefixes to reprogram the pre-trained model's behavior. Our approach transforms federated learning into a distributed set modeling task, aggregating diverse sets of prompts to globally fine-tune the pre-trained model. We benchmark various baselines based on direct adaptations of existing federated model aggregation techniques and introduce a new probabilistic prompt aggregation method that substantially outperforms these baselines. Our reported results on a variety of computer vision datasets confirm that the proposed method is most effective to combat extreme data heterogeneity in federated learning.\"}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cf8d9f973759b6ffaf9bf1d1f48a45a54d4d766b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nweng2024probabilistic,\\ntitle={Probabilistic Federated Prompt-Tuning with Non-{IID} and Imbalanced Data},\\nauthor={Pei-Yau Weng and Minh Hoang and Lam M. Nguyen and My T. Thai and Tsui-Wei Weng and Trong Nghia Hoang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nw6ANsC66G}\\n}'}, 'paperhash': {'value': 'weng|probabilistic_federated_prompttuning_with_noniid_and_imbalanced_data'}},forum = 'nw6ANsC66G',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15345/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15345/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15345/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15345/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nw4TWuEPGx',number = 21525,cdate = 1715802337817,pdate = 1727288257339,odate = 1730874006115,mdate = 1737017020901,tcdate = 1715802337817,tmdate = 1737017020901,ddate = None,content = {'title': {'value': 'Discovering plasticity rules that organize and maintain neural circuits'}, 'authors': {'value': ['David G Bell', 'Alison Duffy', 'Adrienne Fairhall']}, 'authorids': {'value': ['~David_G_Bell1', '~Alison_Duffy1', '~Adrienne_Fairhall1']}, 'keywords': {'value': ['biologically plausible learning rules plasticity self-organization RNNs homeostasis meta-learning']}, 'TLDR': {'value': 'A supervised approach yields biologically plausible learning rules that self-organize and maintain robust representations of time within RNNs.'}, 'abstract': {'value': \"Intrinsic dynamics within the brain can accelerate learning by providing a prior scaffolding for dynamics aligned with task objectives. Such intrinsic dynamics would ideally self-organize and self-sustain in the face of biological noise including synaptic turnover and cell death. An example of such dynamics is the formation of sequences, a ubiquitous motif in neural activity. The sequence-generating circuit in zebra finch HVC provides a reliable timing scaffold for motor output in song and demonstrates a remarkable capacity for unsupervised recovery following perturbation. Inspired by HVC, we seek a local plasticity rule capable of organizing and maintaining sequence-generating dynamics despite continual network perturbations. We adopt a meta-learning approach introduced by Confavreux et al, which parameterizes a learning rule using basis functions constructed from pre- and postsynaptic activity and synapse size, with tunable time constants. Candidate rules are simulated within initially random networks, and their fitness is evaluated according to a loss function that measures the fidelity with which the resulting dynamics encode time. We use this approach to introduce biological noise, forcing meta-learning to find robust solutions. We first show that, in the absence of perturbations, meta-learning identifies a temporally asymmetric generalization of Oja's rule that reliably organizes sparse sequential activity. When synaptic turnover is introduced, the learned rule incorporates a form of homeostasis, better maintaining robust sequential dynamics relative to other previously proposed rules. Additionally, inspired by recent findings demonstrating that the strength of projections from inhibitory interneurons in HVC also dynamically responds to perturbations, we explore the role of inhibitory plasticity in sequence-generating circuits. We find that learned plasticity adjusts both excitation and inhibition in response to manipulations, outperforming rules applied only to excitatory connections. We demonstrate how plasticity acting on both excitatory and inhibitory synapses can better shape excitatory cell dynamics to scaffold timing representations.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2c8b4c60fb82f62769e10bc0419f404b8627168d.pdf'}, 'supplementary_material': {'value': '/attachment/875501345ad2812b7c03756b233dad7aad0535d1.zip'}, '_bibtex': {'value': '@inproceedings{\\nbell2024discovering,\\ntitle={Discovering plasticity rules that organize and maintain neural circuits},\\nauthor={David G Bell and Alison Duffy and Adrienne Fairhall},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nw4TWuEPGx}\\n}'}, 'paperhash': {'value': 'bell|discovering_plasticity_rules_that_organize_and_maintain_neural_circuits'}},forum = 'nw4TWuEPGx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21525/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21525/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21525/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21525/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nvn80cscVm',number = 3360,cdate = 1715263388043,pdate = 1727287717247,odate = 1730873865353,mdate = 1730873865368,tcdate = 1715263388043,tmdate = 1730873865368,ddate = None,content = {'title': {'value': 'Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models'}, 'authors': {'value': ['Lai Wei', 'Zhiquan Tan', 'Chenghai Li', 'Jindong Wang', 'Weiran Huang']}, 'authorids': {'value': ['~Lai_Wei7', '~Zhiquan_Tan1', '~Chenghai_Li1', '~Jindong_Wang4', '~Weiran_Huang1']}, 'keywords': {'value': ['Large Language Model', 'Multi-modal Large Language Model', 'Evaluation Metric']}, 'TLDR': {'value': 'We propose a novel rank-based metric, Diff-eRank, to evaluate LLMs based on representations.'}, 'abstract': {'value': 'Large Language Models (LLMs) have transformed natural language processing and extended their powerful capabilities to multi-modal domains. As LLMs continue to advance, it is crucial to develop diverse and appropriate metrics for their evaluation. In this paper, we introduce a novel rank-based metric, Diff-eRank, grounded in information theory and geometry principles. Diff-eRank assesses LLMs by analyzing their hidden representations, providing a quantitative measure of how efficiently they eliminate redundant information during training. We demonstrate the applicability of Diff-eRank in both single-modal (e.g., language) and multi-modal settings. For language models, our results show that Diff-eRank increases with model size and correlates well with conventional metrics such as loss and accuracy. In the multi-modal context, we propose an alignment evaluation method based on the eRank, and verify that contemporary multi-modal LLMs exhibit strong alignment performance based on our method. Our code is publicly available at https://github.com/waltonfuture/Diff-eRank.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/794e4505432b75f10b25351e3f544c7b0ccda026.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwei2024differank,\\ntitle={Diff-eRank: A Novel Rank-Based Metric for Evaluating Large Language Models},\\nauthor={Lai Wei and Zhiquan Tan and Chenghai Li and Jindong Wang and Weiran Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nvn80cscVm}\\n}'}, 'paperhash': {'value': 'wei|differank_a_novel_rankbased_metric_for_evaluating_large_language_models'}},forum = 'nvn80cscVm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3360/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3360/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3360/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3360/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nvYDPF4LJK',number = 2792,cdate = 1715163144427,pdate = 1727287700559,odate = 1730873861113,mdate = 1730873861130,tcdate = 1715163144427,tmdate = 1730873861130,ddate = None,content = {'title': {'value': 'VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks'}, 'authors': {'value': ['Jiannan Wu', 'Muyan Zhong', 'Sen Xing', 'Zeqiang Lai', 'Zhaoyang Liu', 'Zhe Chen', 'Wenhai Wang', 'Xizhou Zhu', 'Lewei Lu', 'Tong Lu', 'Ping Luo', 'Yu Qiao', 'Jifeng Dai']}, 'authorids': {'value': ['~Jiannan_Wu2', '~Muyan_Zhong1', '~Sen_Xing1', '~Zeqiang_Lai1', '~Zhaoyang_Liu1', '~Zhe_Chen10', '~Wenhai_Wang2', '~Xizhou_Zhu1', '~Lewei_Lu1', '~Tong_Lu1', '~Ping_Luo2', '~Yu_Qiao1', '~Jifeng_Dai1']}, 'keywords': {'value': ['Mutlimodal Large Language Model', 'Vision Generalist Model']}, 'TLDR': {'value': 'We proposed  VisionLLM v2, an End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks.'}, 'abstract': {'value': \"We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope. It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing. To this end, we propose a new information transmission mechanism termed ``super link'', as a medium to connect MLLM with task-specific decoders. It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios. In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks. In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models. We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/88c6ba80e503fc8179139756925d2386d233098b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwu2024visionllm,\\ntitle={Vision{LLM} v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks},\\nauthor={Jiannan Wu and Muyan Zhong and Sen Xing and Zeqiang Lai and Zhaoyang Liu and Zhe Chen and Wenhai Wang and Xizhou Zhu and Lewei Lu and Tong Lu and Ping Luo and Yu Qiao and Jifeng Dai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nvYDPF4LJK}\\n}'}, 'paperhash': {'value': 'wu|visionllm_v2_an_endtoend_generalist_multimodal_large_language_model_for_hundreds_of_visionlanguage_tasks'}},forum = 'nvYDPF4LJK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2792/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2792/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2792/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2792/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nv7ox1vd3q',number = 18036,cdate = 1715783355072,pdate = 1727288168015,odate = 1730873987525,mdate = 1730873987542,tcdate = 1715783355072,tmdate = 1730873987542,ddate = None,content = {'title': {'value': 'Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks'}, 'authors': {'value': ['Chiraag Kaushik', 'Justin Romberg', 'Vidya Muthukumar']}, 'authorids': {'value': ['~Chiraag_Kaushik1', '~Justin_Romberg1', '~Vidya_Muthukumar3']}, 'keywords': {'value': ['high-dimensional asymptotics', 'linear diagonal neural networks', 'feature learning', 'iteratively reweighted least-squares', 'sparse recovery']}, 'abstract': {'value': 'The classical iteratively reweighted least-squares (IRLS) algorithm aims to recover an unknown signal from linear measurements by performing a sequence of weighted least squares problems, where the weights are recursively updated at each step. Varieties of this algorithm have been shown to achieve favorable empirical performance and theoretical guarantees for sparse recovery and $\\\\ell_p$-norm minimization. Recently, some preliminary connections have also been made between IRLS and certain types of non-convex linear neural network architectures that are observed to exploit low-dimensional structure in high-dimensional linear models. In this work, we provide a unified asymptotic analysis for a family of algorithms that encompasses IRLS, the recently proposed lin-RFM algorithm (which was motivated by feature learning in neural networks), and the alternating minimization algorithm on linear diagonal neural networks. Our analysis operates in a \"batched\" setting with i.i.d. Gaussian covariates and shows that, with appropriately chosen reweighting policy, the algorithm can achieve favorable performance in only a handful of iterations. We also extend our results to the case of group-sparse recovery and show that leveraging this structure in the reweighting scheme provably improves test error compared to coordinate-wise reweighting.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We provide a precise asymptotic characterization of the iterates for a family of reweighted least-squares algorithms that learn linear diagonal neural networks.'}, 'pdf': {'value': '/pdf/81edec86112a0dbe03299c1717e0838d2d7c9dac.pdf'}, 'supplementary_material': {'value': '/attachment/165ecd042baf16c3554576f948660f1b2ffb2720.zip'}, '_bibtex': {'value': '@inproceedings{\\nkaushik2024precise,\\ntitle={Precise asymptotics of reweighted least-squares algorithms for linear diagonal networks},\\nauthor={Chiraag Kaushik and Justin Romberg and Vidya Muthukumar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nv7ox1vd3q}\\n}'}, 'paperhash': {'value': 'kaushik|precise_asymptotics_of_reweighted_leastsquares_algorithms_for_linear_diagonal_networks'}},forum = 'nv7ox1vd3q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18036/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18036/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18036/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18036/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nv2Qt5cj1a',number = 7824,cdate = 1715640794480,pdate = 1727287859991,odate = 1730873906976,mdate = 1730873906994,tcdate = 1715640794480,tmdate = 1730873906994,ddate = None,content = {'title': {'value': 'Membership Inference Attacks against Large Vision-Language Models'}, 'authors': {'value': ['Zhan Li', 'Yongtao Wu', 'Yihang Chen', 'Francesco Tonin', 'Elias Abad Rocamora', 'Volkan Cevher']}, 'authorids': {'value': ['~Zhan_Li7', '~Yongtao_Wu1', '~Yihang_Chen1', '~Francesco_Tonin1', '~Elias_Abad_Rocamora1', '~Volkan_Cevher1']}, 'keywords': {'value': ['Membership inference attack; Large Vision-Language Models']}, 'abstract': {'value': 'Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce the first membership inference attack (MIA)  benchmark for VLLMs and propose a novel MIA pipeline with a new MIA metric.'}, 'pdf': {'value': '/pdf/3454120d38fdf1e3d5b8fef5891e49ff7e98a260.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024membership,\\ntitle={Membership Inference Attacks against Large Vision-Language Models},\\nauthor={Zhan Li and Yongtao Wu and Yihang Chen and Francesco Tonin and Elias Abad Rocamora and Volkan Cevher},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nv2Qt5cj1a}\\n}'}, 'paperhash': {'value': 'li|membership_inference_attacks_against_large_visionlanguage_models'}},forum = 'nv2Qt5cj1a',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7824/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7824/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7824/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7824/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nuZv2iTlvn',number = 1259,cdate = 1714502998419,pdate = 1727287655970,odate = 1730873846866,mdate = 1730873846884,tcdate = 1714502998419,tmdate = 1730873846884,ddate = None,content = {'title': {'value': 'Non-Euclidean Mixture Model for Social Network Embedding'}, 'authors': {'value': ['Roshni Iyer', 'YEWEN WANG', 'Wei Wang', 'Yizhou Sun']}, 'authorids': {'value': ['~Roshni_Iyer1', '~YEWEN_WANG1', '~Wei_Wang13', '~Yizhou_Sun1']}, 'keywords': {'value': ['network science', 'social influence', 'representation learning']}, 'abstract': {'value': 'It is largely agreed that social network links are formed due to either homophily or social influence. Inspired by this, we aim at understanding the generation of links via providing a novel embedding-based graph formation model. Different from existing graph representation learning, where link generation probabilities are defined as a simple function of the corresponding node embeddings, we model the link generation as a mixture model of the two factors. In addition, we model the homophily factor in spherical space and the influence factor in hyperbolic space to accommodate the fact that (1) homophily results in cycles and (2) influence results in hierarchies in networks. We also design a special projection to align these two spaces. We call this model Non-Euclidean Mixture Model, i.e., NMM. We further integrate NMM with our non-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN. NMM-GNN learns embeddings through a unified framework which uses non-Euclidean GNN encoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novel space unification loss component to unify distinct non-Euclidean geometric spaces. Experiments on public datasets show NMM-GNN significantly outperforms state-of-the-art baselines on social network generation and classification tasks, demonstrating its ability to better explain how the social network is formed.'}, 'primary_area': {'value': 'machine_learning_for_social_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d5d69c7e63ec70791e10b1f795151b2e7b0934c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\niyer2024noneuclidean,\\ntitle={Non-Euclidean Mixture Model for Social Network Embedding},\\nauthor={Roshni Iyer and YEWEN WANG and Wei Wang and Yizhou Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nuZv2iTlvn}\\n}'}, 'paperhash': {'value': 'iyer|noneuclidean_mixture_model_for_social_network_embedding'}},forum = 'nuZv2iTlvn',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1259/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1259/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1259/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1259/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ntlFREw59A',number = 21087,cdate = 1715799985012,pdate = 1727288248353,odate = 1730874004600,mdate = 1730874004619,tcdate = 1715799985012,tmdate = 1730874004619,ddate = None,content = {'title': {'value': 'ActAnywhere: Subject-Aware Video Background Generation'}, 'authors': {'value': ['Boxiao Pan', 'Zhan Xu', 'Chun-Hao Paul Huang', 'Krishna Kumar Singh', 'Yang Zhou', 'Leonidas Guibas', 'Jimei Yang']}, 'authorids': {'value': ['~Boxiao_Pan1', '~Zhan_Xu1', '~Chun-Hao_Paul_Huang2', '~Krishna_Kumar_Singh4', '~Yang_Zhou10', '~Leonidas_Guibas1', '~Jimei_Yang1']}, 'keywords': {'value': ['Video Background Generation', 'Video Generation', 'Video Synthesis', 'Video Editing']}, 'abstract': {'value': 'We study a novel problem to automatically generate video background that tailors to foreground subject motion. It is an important problem for the movie industry and visual effects community, which traditionally requires tedious manual efforts to solve. To this end, we propose ActAnywhere, a video diffusion model that takes as input a sequence of foreground subject segmentation and an image of a novel background and generates a video of the subject interacting in this background. We train our model on a large-scale dataset of 2.4M videos of human-scene interactions. Through extensive evaluation, we show that our model produces videos with realistic foreground-background interaction while strictly following the guidance of the condition image. Our model generalizes to diverse scenarios including non-human subjects, gaming and animation clips, as well as videos with multiple moving subjects. Both quantitative and qualitative comparisons demonstrate that our model significantly outperforms existing methods, which fail to accomplish the studied task. Please visit our project webpage at https://actanywhere.github.io.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5bb1cb449e096181fbbdf1dc3cac36beee1a8cbe.pdf'}, 'supplementary_material': {'value': '/attachment/25ef7faf3ede09b16187f03b68bf377e404cb2fd.zip'}, '_bibtex': {'value': '@inproceedings{\\npan2024actanywhere,\\ntitle={ActAnywhere: Subject-Aware Video Background Generation},\\nauthor={Boxiao Pan and Zhan Xu and Chun-Hao Paul Huang and Krishna Kumar Singh and Yang Zhou and Leonidas Guibas and Jimei Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ntlFREw59A}\\n}'}, 'paperhash': {'value': 'pan|actanywhere_subjectaware_video_background_generation'}},forum = 'ntlFREw59A',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21087/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21087/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21087/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21087/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ntV5xZfzEk',number = 15181,cdate = 1715757964255,pdate = 1727288090494,odate = 1730873971222,mdate = 1736894614473,tcdate = 1715757964255,tmdate = 1736894614473,ddate = None,content = {'title': {'value': 'Constrained Binary Decision Making'}, 'authors': {'value': ['Daniel Průša', 'Vojtech Franc']}, 'authorids': {'value': ['~Daniel_Průša1', '~Vojtech_Franc1']}, 'keywords': {'value': ['binary statistical decision making', 'constrained optimization', 'Neyman-Pearson problem', 'selective classification']}, 'abstract': {'value': 'Binary statistical decision making involves choosing between two states based on statistical evidence. The optimal decision strategy is typically formulated through a constrained optimization problem, where both the objective and constraints are expressed as integrals involving two Lebesgue measurable functions, one of which represents the strategy being optimized. In this work, we present a comprehensive formulation of the binary decision making problem and provide a detailed characterization of the optimal solution. Our framework encompasses a wide range of well-known and recently proposed decision making problems as specific cases. We demonstrate how our generic approach can be used to derive the optimal decision strategies for these diverse instances. Our results offer a robust mathematical tool that simplifies the process of solving both existing and novel formulations of binary decision making problems which are in the core of many Machine Learning algorithms.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7c628335098616b1f2693a33d12ec5cc88ea5e35.pdf'}, '_bibtex': {'value': '@inproceedings{\\npr{\\\\r{u}}{\\\\v{s}}a2024constrained,\\ntitle={Constrained Binary Decision Making},\\nauthor={Daniel Pr{\\\\r{u}}{\\\\v{s}}a and Vojtech Franc},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ntV5xZfzEk}\\n}'}, 'paperhash': {'value': 'pra|constrained_binary_decision_making'}},forum = 'ntV5xZfzEk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15181/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15181/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15181/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15181/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ntF7D8tAlQ',number = 12563,cdate = 1715724200816,pdate = 1727288010428,odate = 1730873950013,mdate = 1736701158909,tcdate = 1715724200816,tmdate = 1736701158909,ddate = None,content = {'title': {'value': 'Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression'}, 'authors': {'value': ['Kai Tan', 'Pierre C Bellec']}, 'authorids': {'value': ['~Kai_Tan1', '~Pierre_C_Bellec1']}, 'keywords': {'value': ['Robust regression', 'generalization error', 'stochastic gradient descent', 'early stopping', \"Stein's formula\"]}, 'abstract': {'value': 'This paper studies the generalization performance of iterates obtained by Gradient Descent (GD), Stochastic Gradient Descent (SGD) and their proximal variants in high-dimensional robust regression problems. The number of features is comparable to the sample size and errors may be heavy-tailed. We introduce estimators that precisely track the generalization error of the iterates along the trajectory of the iterative algorithm. These estimators are provably consistent under suitable conditions. The results are illustrated through several examples, including Huber regression, pseudo-Huber regression, and their penalized variants with non-smooth regularizer. We provide explicit generalization error estimates for iterates generated from GD and SGD, or from proximal SGD in the presence of a non-smooth regularizer. The proposed risk estimates serve as effective proxies for the actual generalization error, allowing us to determine the optimal stopping iteration that minimizes the generalization error. Extensive simulations confirm the effectiveness of the proposed generalization error estimates.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/34445a6dbcca7d59b437582e07c2c2b3e4cd90cd.pdf'}, 'supplementary_material': {'value': '/attachment/b58033d62b582794f61261ec29940f7048af8a57.zip'}, '_bibtex': {'value': '@inproceedings{\\ntan2024estimating,\\ntitle={Estimating Generalization Performance Along the Trajectory of Proximal {SGD} in Robust Regression},\\nauthor={Kai Tan and Pierre C Bellec},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ntF7D8tAlQ}\\n}'}, 'TLDR': {'value': 'This paper provides generalization error estimates for each iteration of the SGD and proximal SGD algorithms in the context of robust regression.'}, 'paperhash': {'value': 'tan|estimating_generalization_performance_along_the_trajectory_of_proximal_sgd_in_robust_regression'}},forum = 'ntF7D8tAlQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12563/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12563/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12563/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12563/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nrgyOGU7ZP',number = 7740,cdate = 1715636142886,pdate = 1727287857057,odate = 1730873905847,mdate = 1730873905868,tcdate = 1715636142886,tmdate = 1730873905868,ddate = None,content = {'title': {'value': 'SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform'}, 'authors': {'value': ['Aditya Desai', 'Kimia Saedi', 'Apoorv Walia', 'Jihyeong Lee', 'Keren Zhou', 'Anshumali Shrivastava']}, 'authorids': {'value': ['~Aditya_Desai1', '~Kimia_Saedi1', '~Apoorv_Walia1', '~Jihyeong_Lee1', '~Keren_Zhou2', '~Anshumali_Shrivastava1']}, 'keywords': {'value': ['fast linear transform', 'parameter sharing', 'latency improvement', 'deployment']}, 'TLDR': {'value': 'Linear transform with latency benefits ,better parameter-quality tradeoff'}, 'abstract': {'value': 'Tensor multiplication with learned weight matrices is the fundamental building block in deep learning models. These matrices can often be sparsified, decomposed, quantized, or subjected to random parameter sharing without losing accuracy, suggesting the possibility of more efficient transforms. Although many variants of weight matrices exist, unstructured ones are incompatible with modern hardware, slowing inference and training. On the other hand, structured variants often limit expressivity or fail to deliver the promised latency benefits. We present Sketch Structured Transform (SS1), an expressive and GPU-friendly operator that accelerates inference. SS1 leverages parameter sharing in a random yet structured manner to reduce computation while retraining the rich expressive nature of parameter sharing. We confirm empirically that SS1 offers better quality-efficiency tradeoffs than competing variants. Interestingly SS1 can be combined with Quantization to achieve gains unattainable by either method alone, a finding we justify via theoretical analysis. The analysis may be of independent interest.\\nMoreover, existing pre-trained models can be projected onto SS1 and finetuned for efficient deployment. Surprisingly, these projected models can perform reasonably well even without finetuning. Our experiments highlight various applications of the SS1:\\n(a) Training GPT2 and DLRM models from scratch for faster inference. (b) Finetuning projected BERT models for 1.31× faster inference while maintaining GLUE scores. (c) Proof of concept with Llama-3-8b, showing 1.11× faster wall clock inference using projected SS1 layers without finetuning. We open source our code :https://github.com/apd10/Sketch-Structured-Linear/'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/992b8fece2614bb855bbb75824128b346d888593.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndesai2024ss,\\ntitle={{SS}1: Accelerating Inference with Fast and Expressive Sketch Structured Transform},\\nauthor={Aditya Desai and Kimia Saedi and Apoorv Walia and Jihyeong Lee and Keren Zhou and Anshumali Shrivastava},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nrgyOGU7ZP}\\n}'}, 'paperhash': {'value': 'desai|ss1_accelerating_inference_with_fast_and_expressive_sketch_structured_transform'}},forum = 'nrgyOGU7ZP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7740/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7740/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7740/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7740/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nqWaya7hiX',number = 16850,cdate = 1715774964675,pdate = 1727288138963,odate = 1730873981920,mdate = 1736995608488,tcdate = 1715774964675,tmdate = 1736995608488,ddate = None,content = {'title': {'value': 'Wings: Learning Multimodal LLMs without Text-only Forgetting'}, 'authors': {'value': ['Yi-Kai Zhang', 'Shiyin Lu', 'Yang Li', 'YanQing Ma', 'Qing-Guo Chen', 'Zhao Xu', 'Weihua Luo', 'Kaifu Zhang', 'De-Chuan Zhan', 'Han-Jia Ye']}, 'authorids': {'value': ['~Yi-Kai_Zhang2', '~Shiyin_Lu1', '~Yang_Li104', '~YanQing_Ma1', '~Qing-Guo_Chen1', '~Zhao_Xu7', '~Weihua_Luo2', '~Kaifu_Zhang2', '~De-Chuan_Zhan1', '~Han-Jia_Ye1']}, 'keywords': {'value': ['Visual and Textual Learners', 'Large Language Models', 'Multimodal Large Language Models']}, 'TLDR': {'value': 'A new Multimodal LLM: constructing visual and textual \"wings\" (i.e., learners) to extend visual comprehension without sacrificing text-only instruction capabilities.'}, 'abstract': {'value': 'Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, during the continued training, the MLLM catastrophically forgets the text-only instructions that the initial LLM masters. In this paper, we present Wings, a novel MLLM that excels in both text-only and multimodal instructions. By examining attention across layers of MLLM, we find that *text-only forgetting* is related to the attention shifts from pre-image to post-image text. From that, we construct an additional Low-Rank Residual Attention (LoRRA) block that acts as the \"modality learner\" to expand the learnable space and compensate for the attention shift. The complementary learners, like \"wings\" on either side, are connected in parallel to each layer\\'s attention block. The LoRRA mirrors the structure of attention but utilizes low-rank connections to ensure efficiency. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Later, textual learners are integrated with token-wise routing, blending the outputs of both modality learners collaboratively. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. Wings with *compensation of learners* addresses text-only forgetting during visual modality expansion in general MLLMs.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3d3430662e44e4f961eba4b6584b9d542a908621.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024wings,\\ntitle={Wings: Learning Multimodal {LLM}s without Text-only Forgetting},\\nauthor={Yi-Kai Zhang and Shiyin Lu and Yang Li and YanQing Ma and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and De-Chuan Zhan and Han-Jia Ye},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nqWaya7hiX}\\n}'}, 'paperhash': {'value': 'zhang|wings_learning_multimodal_llms_without_textonly_forgetting'}},forum = 'nqWaya7hiX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16850/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16850/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16850/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16850/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'npoHt6WV1F',number = 1735,cdate = 1714770273266,pdate = 1727287669881,odate = 1730873851494,mdate = 1730873851506,tcdate = 1714770273266,tmdate = 1730873851506,ddate = None,content = {'title': {'value': 'NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes'}, 'authors': {'value': ['Hao-Lun Sun', 'Lei Hsiung', 'Nandhini Chandramoorthy', 'Pin-Yu Chen', 'Tsung-Yi Ho']}, 'authorids': {'value': ['~Hao-Lun_Sun1', '~Lei_Hsiung1', '~Nandhini_Chandramoorthy1', '~Pin-Yu_Chen1', '~Tsung-Yi_Ho2']}, 'keywords': {'value': ['machine learning', 'energy efficient inference', 'bit error resilience']}, 'abstract': {'value': 'Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains problematically high. An effective strategy for reducing such consumption is supply-voltage reduction, but if done too aggressively, it can lead to accuracy degradation. This is due to random bit-flips in static random access memory (SRAM), where model parameters are stored. To address this challenge, we have developed NeuralFuse, a novel add-on module that handles the energy-accuracy tradeoff in low-voltage regimes by learning input transformations and using them to generate error-resistant data representations, thereby protecting DNN accuracy in both nominal and low-voltage scenarios. As well as being easy to implement, NeuralFuse can be readily applied to DNNs with limited access, such cloud-based APIs that are accessed remotely or non-configurable hardware. Our experimental results demonstrate that, at a 1% bit-error rate, NeuralFuse can reduce SRAM access energy by up to 24% while recovering accuracy by up to 57%. To the best of our knowledge, this is the first approach to addressing low-voltage-induced bit errors that requires no model retraining.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/91d39ad2f125f9e4d02522c589a6199f92aa6e6a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024neuralfuse,\\ntitle={NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes},\\nauthor={Hao-Lun Sun and Lei Hsiung and Nandhini Chandramoorthy and Pin-Yu Chen and Tsung-Yi Ho},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=npoHt6WV1F}\\n}'}, 'TLDR': {'value': 'NeuralFuse provides model-independent protection for AI accelerators built on a chip, allowing them to maintain stable performance when suffering low-voltage-induced bit errors.'}, 'paperhash': {'value': 'sun|neuralfuse_learning_to_recover_the_accuracy_of_accesslimited_neural_network_inference_in_lowvoltage_regimes'}},forum = 'npoHt6WV1F',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1735/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1735/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1735/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1735/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'npJQ6qS4bg',number = 19054,cdate = 1715788916026,pdate = 1727288200615,odate = 1730873993596,mdate = 1730873993613,tcdate = 1715788916026,tmdate = 1730873993613,ddate = None,content = {'title': {'value': 'Understanding and Minimising Outlier Features in Transformer Training'}, 'authors': {'value': ['Bobby He', 'Lorenzo Noci', 'Daniele Paliotta', 'Imanol Schlag', 'Thomas Hofmann']}, 'authorids': {'value': ['~Bobby_He1', '~Lorenzo_Noci1', '~Daniele_Paliotta1', '~Imanol_Schlag3', '~Thomas_Hofmann1']}, 'keywords': {'value': ['Outlier Features', 'Training Dynamics', 'Normalisation Layers', 'Signal Propagation', 'NN Optimisers', 'NN Architectures', 'Transformers', 'Understanding Deep Learning']}, 'TLDR': {'value': 'We study why Outlier Features occur during standard NN training dynamics, and which algorithmic choices one can make to minimise them.'}, 'abstract': {'value': \"Outlier Features (OFs) are neurons whose activation magnitudes significantly exceed the average over a neural network's (NN) width. They are well known to emerge during standard transformer training and have the undesirable effect of hindering quantisation in afflicted models. Despite their practical importance, little is known behind why OFs emerge during training, nor how one can minimise them.\\n\\nOur work focuses on the above questions, first identifying several quantitative metrics, such as the kurtosis over neuron activation norms, to measure OFs. With these metrics, we study how architectural and optimisation choices influence OFs, and provide practical insights to minimise OFs during training. As highlights, we introduce a novel unnormalised transformer block, the Outlier Protected block, and present a previously unknown benefit of non-diagonal preconditioning optimisers, finding both approaches to significantly reduce OFs and improve quantisation without compromising convergence speed, at scales of up to 7B parameters. Notably, our combination of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 weight-and-activation int8 perplexity (from 14.71 in standard precision), compared to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of Pre-Norm model and Adam, when quantising OPT-125m models post-training.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1a1a6d4bbd44ce3da3b7fbb33b801d55f1582c16.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhe2024understanding,\\ntitle={Understanding and Minimising Outlier Features in Transformer Training},\\nauthor={Bobby He and Lorenzo Noci and Daniele Paliotta and Imanol Schlag and Thomas Hofmann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=npJQ6qS4bg}\\n}'}, 'paperhash': {'value': 'he|understanding_and_minimising_outlier_features_in_transformer_training'}},forum = 'npJQ6qS4bg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19054/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19054/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19054/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19054/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nmUkwoOHFO',number = 15318,cdate = 1715759065935,pdate = 1727288094563,odate = 1730873972083,mdate = 1730873972101,tcdate = 1715759065935,tmdate = 1730873972101,ddate = None,content = {'title': {'value': 'The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models'}, 'authors': {'value': ['Diego Doimo', 'Alessandro Pietro Serra', 'Alessio ansuini', 'Alberto Cazzaniga']}, 'authorids': {'value': ['~Diego_Doimo1', '~Alessandro_Pietro_Serra2', '~Alessio_ansuini1', '~Alberto_Cazzaniga1']}, 'keywords': {'value': ['LLMs', 'geometry hidden representations', 'clustering']}, 'abstract': {'value': 'In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. \\nHowever, little is known about whether they induce similar representations inside LLMs.  We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while less-defined peaks characterize the landscape of ICL representations. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2923a46750903dc227873ec05914e4dfd961cf6c.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndoimo2024the,\\ntitle={The Representation Landscape of Few-Shot Learning and Fine-Tuning in Large Language Models},\\nauthor={Diego Doimo and Alessandro Pietro Serra and Alessio ansuini and Alberto Cazzaniga},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nmUkwoOHFO}\\n}'}, 'paperhash': {'value': 'doimo|the_representation_landscape_of_fewshot_learning_and_finetuning_in_large_language_models'}},forum = 'nmUkwoOHFO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15318/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15318/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15318/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15318/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nkzSE5KkCA',number = 6915,cdate = 1715607810280,pdate = 1727287828763,odate = 1730873897588,mdate = 1735188073954,tcdate = 1715607810280,tmdate = 1735188073954,ddate = None,content = {'title': {'value': 'Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning'}, 'authors': {'value': ['Penghui Ruan', 'Pichao WANG', 'Divya Saxena', 'Jiannong Cao', 'Yuhui Shi']}, 'authorids': {'value': ['~Penghui_Ruan1', '~Pichao_WANG3', '~Divya_Saxena1', '~Jiannong_Cao1', '~Yuhui_Shi2']}, 'keywords': {'value': ['Text-to-Video Generation', 'Diffusion Models']}, 'abstract': {'value': \"Despite advancements in Text-to-Video (T2V) generation, producing videos with realistic motion remains challenging. Current models often yield static or minimally dynamic outputs, failing to capture complex motions described by text. This issue stems from the internal biases in text encoding which overlooks motions, and inadequate conditioning mechanisms in T2V generation models. To address this, we propose a novel framework called DEcomposed MOtion (DEMO), which enhances motion synthesis in T2V generation by decomposing both text encoding and conditioning into content and motion components. Our method includes a content encoder for static elements and a motion encoder for temporal dynamics, alongside separate content and motion conditioning mechanisms. Crucially, we introduce text-motion and video-motion supervision to improve the model's understanding and generation of motion. Evaluations on benchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMO's superior ability to produce videos with enhanced motion dynamics while maintaining high visual quality. Our approach significantly advances T2V generation by integrating comprehensive motion understanding directly from textual descriptions. Project page: https://PR-Ryan.github.io/DEMO-project/\"}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a261ab5eac4415e02421fe3169a362369c977e34.pdf'}, 'supplementary_material': {'value': '/attachment/2a3a287f34d2907f429a408dd04b398960286edd.zip'}, '_bibtex': {'value': '@inproceedings{\\nruan2024enhancing,\\ntitle={Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning},\\nauthor={Penghui Ruan and Pichao WANG and Divya Saxena and Jiannong Cao and Yuhui Shi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nkzSE5KkCA}\\n}'}, 'paperhash': {'value': 'ruan|enhancing_motion_in_texttovideo_generation_with_decomposed_encoding_and_conditioning'}},forum = 'nkzSE5KkCA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6915/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6915/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6915/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6915/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nkwPiBSw1f',number = 10443,cdate = 1715694141902,pdate = 1727287940998,odate = 1730873929216,mdate = 1730873929233,tcdate = 1715694141902,tmdate = 1730873929233,ddate = None,content = {'title': {'value': 'Dual-Personalizing Adapter for Federated Foundation Models'}, 'authors': {'value': ['yiyuan yang', 'Guodong Long', 'Tao Shen', 'Jing Jiang', 'Michael Blumenstein']}, 'authorids': {'value': ['~yiyuan_yang2', '~Guodong_Long2', '~Tao_Shen1', '~Jing_Jiang6', '~Michael_Blumenstein2']}, 'keywords': {'value': ['Federated Learning', 'Foundation Models', 'Personalization', 'Test-Time Distribution Shifts']}, 'abstract': {'value': 'Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning diverse instruction data. Notably, federated foundation models (FedFM) emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to FedFM for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications, and conventional methods for test-time distribution shifts in personalized FL are less effective for FedFM due to their failure to adapt to complex distribution shift scenarios and the requirement to train all parameters. To bridge this gap, we refine the setting in FedFM, termed test-time personalization, which aims to learn personalized federated foundation models on clients while effectively handling test-time distribution shifts simultaneously. To address challenges in this setting, we explore a simple yet effective solution, a Federated Dual-Personalizing Adapter (FedDPA) architecture. By co-working with a foundation model, a global adapter and a local adapter jointly tackle the test-time distribution shifts and client-specific personalization. Additionally, we introduce an instance-wise dynamic weighting mechanism that dynamically integrates the global and local adapters for each test instance during inference, facilitating effective test-time personalization. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a6b36839b4ee3d9c5837662362dac36c10c4e38c.pdf'}, 'supplementary_material': {'value': '/attachment/a60742d6c6c8c0198a2d04997a248a0565303210.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024dualpersonalizing,\\ntitle={Dual-Personalizing Adapter for Federated Foundation Models},\\nauthor={yiyuan yang and Guodong Long and Tao Shen and Jing Jiang and Michael Blumenstein},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nkwPiBSw1f}\\n}'}, 'paperhash': {'value': 'yang|dualpersonalizing_adapter_for_federated_foundation_models'}},forum = 'nkwPiBSw1f',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10443/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10443/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10443/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10443/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nkHEl4n0JU',number = 1024,cdate = 1714316508638,pdate = 1727287649872,odate = 1730873844689,mdate = 1737117680496,tcdate = 1714316508638,tmdate = 1737117680496,ddate = None,content = {'title': {'value': 'Visual Fourier Prompt Tuning'}, 'authors': {'value': ['Runjia Zeng', 'Cheng Han', 'Qifan Wang', 'Chunshu Wu', 'Tong Geng', 'Lifu Huang', 'Ying Nian Wu', 'Dongfang Liu']}, 'authorids': {'value': ['~Runjia_Zeng1', '~Cheng_Han1', '~Qifan_Wang2', '~Chunshu_Wu1', '~Tong_Geng1', '~Lifu_Huang1', '~Ying_Nian_Wu1', '~Dongfang_Liu1']}, 'keywords': {'value': ['Parameter-efficient finetuning']}, 'abstract': {'value': 'With the scale of vision Transformer-based models continuing to grow, finetuning these large-scale pretrained models for new tasks has become increasingly parameter-intensive. Visual prompt tuning is introduced as a parameter-efficient finetuning (PEFT) method to this trend. Despite its successes, a notable research challenge persists within almost all PEFT approaches: significant performance degradation is observed when there is a substantial disparity between the datasets applied in pretraining and finetuning phases. To address this challenge, we draw inspiration from human visual cognition, and propose the Visual Fourier Prompt Tuning (VFPT) method as a general and effective solution for adapting large-scale transformer-based models. Our approach innovatively incorporates the Fast Fourier Transform into prompt embeddings and harmoniously considers both spatial and frequency domain information. Apart from its inherent simplicity and intuitiveness, VFPT exhibits superior performance across all datasets, offering a general solution to dataset challenges, irrespective of data disparities. Empirical results demonstrate that our approach outperforms current state-of-the-art baselines on two benchmarks, with low parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notable performance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Our code is avaliable at https://github.com/runtsang/VFPT.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Visual Fourier Prompt Tuning (VFPT) is a simple yet effective parameter-efficient finetuning approach for large-scale vision Transformer models, addressing the challenge of dataset disparity between pretraining and finetuning phases.'}, 'pdf': {'value': '/pdf/56e207c3fd3853354150e3d7f9f4c135c44488d9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzeng2024visual,\\ntitle={Visual Fourier Prompt Tuning},\\nauthor={Runjia Zeng and Cheng Han and Qifan Wang and Chunshu Wu and Tong Geng and Lifu Huang and Ying Nian Wu and Dongfang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nkHEl4n0JU}\\n}'}, 'paperhash': {'value': 'zeng|visual_fourier_prompt_tuning'}},forum = 'nkHEl4n0JU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1024/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1024/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1024/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1024/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'njwYBFau8E',number = 12273,cdate = 1715719501593,pdate = 1727288000901,odate = 1730873947641,mdate = 1730873947660,tcdate = 1715719501593,tmdate = 1730873947660,ddate = None,content = {'title': {'value': 'DistrictNet: Decision-aware learning for geographical districting'}, 'authors': {'value': ['Cheikh Ahmed', 'Alexandre Forel', 'Axel Parmentier', 'Thibaut Vidal']}, 'authorids': {'value': ['~Cheikh_Ahmed1', '~Alexandre_Forel1', '~Axel_Parmentier1', '~Thibaut_Vidal1']}, 'keywords': {'value': ['routing', 'combinatorial optimization', 'decision-focused learning']}, 'TLDR': {'value': 'We solve real-world districting problems in a few minutes using a structured learning pipeline.'}, 'abstract': {'value': 'Districting is a complex combinatorial problem that consists in partitioning a geographical area into small districts. In logistics, it is a major strategic decision determining operating costs for several years. Solving districting problems using traditional methods is intractable even for small geographical areas and existing heuristics often provide sub-optimal results. We present a structured learning approach to find high-quality solutions to real-world districting problems in a few minutes. It is based on integrating a combinatorial optimization layer, the capacitated minimum spanning tree problem, into a graph neural network architecture. To train this pipeline in a decision-aware fashion, we show how to construct target solutions embedded in a suitable space and learn from target solutions. Experiments show that our approach outperforms existing methods as it can significantly reduce costs on real-world cities.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f64d0fcbdecbca899d8871dd921130e3f6b558b9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nahmed2024districtnet,\\ntitle={DistrictNet: Decision-aware learning for geographical districting},\\nauthor={Cheikh Ahmed and Alexandre Forel and Axel Parmentier and Thibaut Vidal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=njwYBFau8E}\\n}'}, 'paperhash': {'value': 'ahmed|districtnet_decisionaware_learning_for_geographical_districting'}},forum = 'njwYBFau8E',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12273/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12273/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12273/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12273/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'njvPjG0BfK',number = 18092,cdate = 1715783720826,pdate = 1727288169654,odate = 1730873987799,mdate = 1730873987817,tcdate = 1715783720826,tmdate = 1730873987817,ddate = None,content = {'title': {'value': 'HardCore Generation: Generating Hard UNSAT Problems for Data Augmentation'}, 'authors': {'value': ['Joseph Cotnareanu', 'Zhanguang Zhang', 'Hui-Ling Zhen', 'Yingxue Zhang', 'Mark Coates']}, 'authorids': {'value': ['~Joseph_Cotnareanu1', '~Zhanguang_Zhang1', '~Hui-Ling_Zhen1', '~Yingxue_Zhang1', '~Mark_Coates1']}, 'keywords': {'value': ['Graph Learning', 'Boolean Satisfiability', 'Circuit Design']}, 'abstract': {'value': \"Efficiently determining the satisfiability of a boolean equation --- known as the SAT problem for brevity --- is crucial in various industrial problems.  Recently, the advent of deep learning methods has introduced significant potential for enhancing SAT solving. However, a major barrier to the advancement of this field has been the scarcity of large, realistic datasets.  The majority of current public datasets are either randomly generated or extremely limited, containing only a few examples from unrelated problem families. These datasets are inadequate for meaningful training of deep learning methods.  In light of this, researchers have started exploring generative techniques to create data that more accurately reflect SAT problems encountered in practical situations. These methods have so far suffered from either the inability to produce challenging SAT problems or time-scalability obstacles.  In this paper we address both by identifying and manipulating the key contributors to a problem's ``hardness'', known as cores. Although some previous work has addressed cores, the time costs are unacceptably high due to the expense of traditional heuristic core detection techniques. We introduce a fast core detection procedure that uses a graph neural network. Our empirical results demonstrate that we can efficiently generate problems that remain hard to solve and retain key attributes of the original example problems. We show via experiment that the generated synthetic SAT problems can be used in a data augmentation setting to provide improved prediction of solver runtimes.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We leverage intrinsic properties of SAT problems and GNNs to efficiently generate new SAT problems for data augmentation in Deep Learning settings.'}, 'pdf': {'value': '/pdf/11d85eae348bd6102c120bbcd9e9f0b24e68bfd5.pdf'}, 'supplementary_material': {'value': '/attachment/42770601d4aa0bb3ff5c559ef76ca491c0e16d78.zip'}, '_bibtex': {'value': '@inproceedings{\\ncotnareanu2024hardcore,\\ntitle={HardCore Generation: Generating Hard {UNSAT} Problems for Data Augmentation},\\nauthor={Joseph Cotnareanu and Zhanguang Zhang and Hui-Ling Zhen and Yingxue Zhang and Mark Coates},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=njvPjG0BfK}\\n}'}, 'paperhash': {'value': 'cotnareanu|hardcore_generation_generating_hard_unsat_problems_for_data_augmentation'}},forum = 'njvPjG0BfK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18092/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18092/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18092/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18092/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'niG3Yyb6oA',number = 4248,cdate = 1715393647989,pdate = 1727287745105,odate = 1730873874159,mdate = 1730873874170,tcdate = 1715393647989,tmdate = 1730873874170,ddate = None,content = {'title': {'value': 'A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks'}, 'authors': {'value': ['Xiaolei Liu', 'Shaoshuai Li', 'Kaixin Gao', 'Binfeng Wang']}, 'authorids': {'value': ['~Xiaolei_Liu3', '~Shaoshuai_Li1', '~Kaixin_Gao1', '~Binfeng_Wang1']}, 'keywords': {'value': ['Natural gradient method', 'second-order optimization', 'deep neural networks', 'Kronecker factorization']}, 'abstract': {'value': 'Second-order optimization algorithms, such as the Newton method and the natural gradient descent (NGD) method exhibit excellent convergence properties for training deep neural networks, but the high computational cost limits its practical application. In this paper, we focus on the NGD method and propose a novel layer-wise natural gradient descent (LNGD) method to further reduce computational costs and accelerate the training process. Specifically, based on the block diagonal approximation of the Fisher information matrix, we first propose the layer-wise sample method to compute each block matrix without performing a complete back-propagation. Then, each block matrix is approximated as a Kronecker product of two smaller matrices, one of which is a diagonal matrix, while keeping the traces equal before and after approximation. By these two steps, we provide a new approximation for the Fisher information matrix, which can effectively reduce the computational cost while preserving the main information of each block matrix. Moreover, we propose a new adaptive layer-wise learning rate to further accelerate training. Based on these new approaches, we propose the LNGD optimizer. The global convergence analysis of LNGD is established under some assumptions. Experiments on image classification and machine translation tasks show that our method is quite competitive compared to the state-of-the-art methods.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4e9a309d32add1feb680de166d83ef130bbc0ada.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024a,\\ntitle={A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks},\\nauthor={Xiaolei Liu and Shaoshuai Li and Kaixin Gao and Binfeng Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=niG3Yyb6oA}\\n}'}, 'paperhash': {'value': 'liu|a_layerwise_natural_gradient_optimizer_for_training_deep_neural_networks'}},forum = 'niG3Yyb6oA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4248/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4248/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4248/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4248/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ni3Ud2BV3G',number = 17721,cdate = 1715781569192,pdate = 1727288160310,odate = 1730873985964,mdate = 1737117478489,tcdate = 1715781569192,tmdate = 1737117478489,ddate = None,content = {'title': {'value': 'On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory'}, 'authors': {'value': ['Guhan Chen', 'Yicheng Li', 'Qian Lin']}, 'authorids': {'value': ['~Guhan_Chen1', '~Yicheng_Li2', '~Qian_Lin2']}, 'keywords': {'value': ['neural tangent kernel', 'random initialization', 'non-parametric regression', 'reproducing kernel Hilbert space']}, 'TLDR': {'value': 'This paper explores the impact of random initialization in Neural Tangent Kernel (NTK) theories and argues that NTK theories fail to explain the superior performance of neural networks.'}, 'abstract': {'value': \"This paper aims to discuss the impact of random initialization of neural networks in the neural tangent kernel (NTK) theory, which is ignored by most recent works in the NTK theory. It is well known that as the network's width tends to infinity, the neural network with random initialization converges to a Gaussian process \\\\(f^{\\\\mathrm{GP}}\\\\), which takes values in \\\\(L^{2}(\\\\mathcal{X})\\\\), where \\\\(\\\\mathcal{X}\\\\) is the domain of the data. In contrast, to adopt the traditional theory of kernel regression, most recent works introduced a special mirrored architecture and a mirrored (random) initialization to ensure the network's output is identically zero at initialization. Therefore, it remains a question whether the conventional setting and mirrored initialization would make wide neural networks exhibit different generalization capabilities. In this paper, we first show that the training dynamics of the gradient flow of neural networks with random initialization converge uniformly to that of the corresponding NTK regression with random initialization \\\\(f^{\\\\mathrm{GP}}\\\\). We then show that \\\\(\\\\mathbf{P}(f^{\\\\mathrm{GP}} \\\\in [\\\\mathcal{H}^{\\\\mathrm{NT}}]^{s}) = 1\\\\) for any \\\\(s < \\\\frac{3}{d+1}\\\\) and \\\\(\\\\mathbf{P}(f^{\\\\mathrm{GP}} \\\\in [\\\\mathcal{H}^{\\\\mathrm{NT}}]^{s}) = 0\\\\) for any \\\\(s \\\\geq \\\\frac{3}{d+1}\\\\), where \\\\([\\\\mathcal{H}^{\\\\mathrm{NT}}]^{s}\\\\) is the real interpolation space of the RKHS \\\\(\\\\mathcal{H}^{\\\\mathrm{NT}}\\\\) associated with the NTK. Consequently, the generalization error of the wide neural network trained by gradient descent is \\\\(\\\\Omega(n^{-\\\\frac{3}{d+3}})\\\\), and it still suffers from the curse of dimensionality. Thus, the NTK theory may not explain the superior performance of neural networks.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ecf67e2d3f381961f33b25949a8729b4efa5bdfa.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024on,\\ntitle={On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory},\\nauthor={Guhan Chen and Yicheng Li and Qian Lin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ni3Ud2BV3G}\\n}'}, 'paperhash': {'value': 'chen|on_the_impacts_of_the_random_initialization_in_the_neural_tangent_kernel_theory'}},forum = 'ni3Ud2BV3G',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17721/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17721/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17721/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17721/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nge5deRsEH',number = 17351,cdate = 1715779003549,pdate = 1727288150905,odate = 1730873984033,mdate = 1730873984045,tcdate = 1715779003549,tmdate = 1730873984045,ddate = None,content = {'title': {'value': 'On the Power of Decision Trees in Auto-Regressive Language Modeling'}, 'authors': {'value': ['Yulu Gan', 'Tomer Galanti', 'Tomaso A Poggio', 'eran malach']}, 'authorids': {'value': ['~Yulu_Gan1', '~Tomer_Galanti1', '~Tomaso_A_Poggio2', '~eran_malach1']}, 'keywords': {'value': ['Decision Trees', 'Auto-Regressive', 'Language Models']}, 'abstract': {'value': 'Originally proposed for handling time series data, Auto-regressive Decision Trees (ARDTs) have not yet been explored for language modeling. This paper delves into both the theoretical and practical applications of ARDTs in this new context. We theoretically demonstrate that ARDTs can compute complex functions, such as simulating automata, Turing machines, and sparse circuits, by leveraging \"chain-of-thought\" computations. Our analysis provides bounds on the size, depth, and computational efficiency of ARDTs, highlighting their surprising computational power. Empirically, we train ARDTs on simple language generation tasks, showing that they can learn to generate  coherent and grammatically correct text on par with a smaller Transformer model. Additionally, we show that ARDTs can be used on top of transformer representations to solve complex reasoning tasks. This research reveals the unique computational abilities of ARDTs, aiming to broaden the architectural diversity in language model development.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper explores Auto-regressive Decision Trees for language modeling through theoretical and empirical analysis, showing their ability for complex computations and coherent text generation.'}, 'pdf': {'value': '/pdf/eafa21f91a3d55737ee7ae5c15bc6ae47db8da06.pdf'}, 'supplementary_material': {'value': '/attachment/3b3b786b76e2b209d6e1b9be46f058ba61a5897c.zip'}, '_bibtex': {'value': '@inproceedings{\\ngan2024on,\\ntitle={On the Power of Decision Trees in Auto-Regressive Language Modeling},\\nauthor={Yulu Gan and Tomer Galanti and Tomaso A Poggio and eran malach},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nge5deRsEH}\\n}'}, 'paperhash': {'value': 'gan|on_the_power_of_decision_trees_in_autoregressive_language_modeling'}},forum = 'nge5deRsEH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17351/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17351/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17351/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17351/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nfq3GKfb4h',number = 18816,cdate = 1715787520166,pdate = 1727288194117,odate = 1730873992010,mdate = 1736958655891,tcdate = 1715787520166,tmdate = 1736958655891,ddate = None,content = {'title': {'value': 'Preference Learning of Latent Decision Utilities with a Human-like Model of Preferential Choice'}, 'authors': {'value': ['Sebastiaan De Peuter', 'Shibei Zhu', 'Yujia Guo', 'Andrew Howes', 'Samuel Kaski']}, 'authorids': {'value': ['~Sebastiaan_De_Peuter1', '~Shibei_Zhu1', '~Yujia_Guo1', '~Andrew_Howes1', '~Samuel_Kaski1']}, 'keywords': {'value': ['preference learning', 'human-in-the-loop', 'AI-assistance for decision making', 'user modeling', 'cogntitive science', 'retrosynthesis planning']}, 'TLDR': {'value': 'We improve learning of latent utilities from preferences for decision tasks, by using a cognitive model of preferential choice which models various context effects.'}, 'abstract': {'value': 'Preference learning methods make use of models of human choice in order to infer the latent utilities that underlie human behavior. However, accurate modeling of human choice behavior is challenging due to a range of context effects that arise from how humans contrast and evaluate options. Cognitive science has proposed several models that capture these intricacies but, due to their intractable nature, work on preference learning has, in practice, had to rely on tractable but simplified variants of the well-known Bradley-Terry model. In this paper, we take one state-of-the-art intractable cognitive model and propose a tractable surrogate that is suitable for deployment in preference learning.  We then introduce a mechanism for fitting the surrogate to human data and extend it to account for data that cannot be explained by the original cognitive model. We demonstrate on large-scale human data that this model produces significantly better inferences on static and actively elicited data than existing Bradley-Terry variants. We further show in simulation that when using this model for preference learning, we can significantly improve utility in a range of real-world tasks.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b926d9b1a4aa4e70b4aea3ac828ed06e41e596b5.pdf'}, '_bibtex': {'value': '@inproceedings{\\npeuter2024preference,\\ntitle={Preference Learning of Latent Decision Utilities with a Human-like Model of Preferential Choice},\\nauthor={Sebastiaan De Peuter and Shibei Zhu and Yujia Guo and Andrew Howes and Samuel Kaski},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nfq3GKfb4h}\\n}'}, 'paperhash': {'value': 'peuter|preference_learning_of_latent_decision_utilities_with_a_humanlike_model_of_preferential_choice'}},forum = 'nfq3GKfb4h',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18816/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18816/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18816/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18816/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nfK0ZXFFSn',number = 4205,cdate = 1715379740050,pdate = 1727287743771,odate = 1730873873726,mdate = 1730873873746,tcdate = 1715379740050,tmdate = 1730873873746,ddate = None,content = {'title': {'value': 'HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection'}, 'authors': {'value': ['Xuefeng Du', 'Chaowei Xiao', 'Yixuan Li']}, 'authorids': {'value': ['~Xuefeng_Du1', '~Chaowei_Xiao2', '~Yixuan_Li1']}, 'keywords': {'value': ['hallucination detection', 'LLM safety']}, 'abstract': {'value': 'The surge in applications of large language models (LLMs) has prompted concerns about the generation of misleading or fabricated information, known as hallucinations. Therefore, detecting hallucinations has become critical to maintaining trust in LLM-generated content. A primary challenge in learning a truthfulness classifier is the lack of a large amount of labeled truthful and hallucinated data. To address the challenge, we introduce HaloScope, a novel learning framework that leverages the unlabeled LLM generations in the wild for hallucination detection. Such unlabeled data arises freely upon deploying LLMs in the open world, and consists of both truthful and hallucinated information. To harness the unlabeled data, we present an automated scoring function for distinguishing between truthful and untruthful generations within unlabeled mixture data, thereby enabling the training of a binary classifier on top. Importantly, our framework does not require extra data collection and human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiments show that HaloScope can achieve superior hallucination detection performance, outperforming the competitive rivals by a significant margin.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2aa5c5a4f6fe82c75a1685bb7ec91893ade0f68b.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndu2024haloscope,\\ntitle={HaloScope: Harnessing Unlabeled {LLM} Generations for Hallucination Detection},\\nauthor={Xuefeng Du and Chaowei Xiao and Yixuan Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nfK0ZXFFSn}\\n}'}, 'paperhash': {'value': 'du|haloscope_harnessing_unlabeled_llm_generations_for_hallucination_detection'}},forum = 'nfK0ZXFFSn',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4205/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4205/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4205/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4205/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ndoeHX1Acq',number = 4366,cdate = 1715408324524,pdate = 1727287748553,odate = 1730873875314,mdate = 1730873875333,tcdate = 1715408324524,tmdate = 1730873875333,ddate = None,content = {'title': {'value': 'One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection'}, 'authors': {'value': ['Zhenyu Wang', 'Ya-Li Li', 'Hengshuang Zhao', 'Shengjin Wang']}, 'authorids': {'value': ['~Zhenyu_Wang3', '~Ya-Li_Li1', '~Hengshuang_Zhao2', '~Shengjin_Wang1']}, 'keywords': {'value': ['3D object detection']}, 'abstract': {'value': 'The current trend in computer vision is to utilize one universal model to address all various tasks. Achieving such a universal model inevitably requires incorporating multi-domain data for joint training to learn across multiple problem scenarios. In point cloud based 3D object detection, however, such multi-domain joint training is highly challenging, because large domain gaps among point clouds from different datasets lead to the severe domain-interference problem. In this paper, we propose OneDet3D, a universal one-for-all model that addresses 3D detection across different domains, including diverse indoor and outdoor scenes, within the same framework and only one set of parameters. We propose the domain-aware partitioning in scatter and context, guided by a routing mechanism, to address the data interference issue, and further incorporate the text modality for a language-guided classification to unify the multi-dataset label spaces and mitigate the category interference issue. The fully sparse structure and anchor-free head further accommodate point clouds with significant scale disparities. Extensive experiments demonstrate the strong universal ability of OneDet3D to utilize only one trained model for addressing almost all 3D object detection tasks (Fig. 1). We will open-source the code for future research and applications.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/08fc8dc3f9dfe59b9f9cc69d4cdc28f677604b19.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024one,\\ntitle={One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection},\\nauthor={Zhenyu Wang and Ya-Li Li and Hengshuang Zhao and Shengjin Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ndoeHX1Acq}\\n}'}, 'paperhash': {'value': 'wang|one_for_all_multidomain_joint_training_for_point_cloud_based_3d_object_detection'}},forum = 'ndoeHX1Acq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4366/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4366/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4366/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4366/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nd8Q4a8aWl',number = 11166,cdate = 1715701728930,pdate = 1727287962662,odate = 1730873935506,mdate = 1730873935519,tcdate = 1715701728930,tmdate = 1730873935519,ddate = None,content = {'title': {'value': 'A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models'}, 'authors': {'value': ['Hamidreza Kamkari', 'Brendan Leigh Ross', 'Rasa Hosseinzadeh', 'Jesse C. Cresswell', 'Gabriel Loaiza-Ganem']}, 'authorids': {'value': ['~Hamidreza_Kamkari1', '~Brendan_Leigh_Ross1', '~Rasa_Hosseinzadeh2', '~Jesse_C._Cresswell1', '~Gabriel_Loaiza-Ganem1']}, 'keywords': {'value': ['diffusion models', 'deep generative modelling', 'manifold hypothesis', 'intrinsic dimension']}, 'TLDR': {'value': 'We provide an efficient local intrinsic dimension estimator using diffusion models, outperforming traditional estimators. It aligns closely with qualitative complexity in images and scales to stable diffusion.'}, 'abstract': {'value': 'High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models: diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide an LID estimator which addresses the aforementioned deficiencies. Our estimator, called FLIPD, is easy to implement and compatible with all popular DMs. Applying FLIPD to synthetic LID estimation benchmarks, we find that DMs implemented as fully-connected networks are highly effective LID estimators that outperform existing baselines. We also apply FLIPD to natural images where the true LID is unknown. Despite being sensitive to the choice of network architecture, FLIPD estimates remain a useful measure of relative complexity; compared to competing estimators, FLIPD exhibits a consistently higher correlation with image PNG compression rate and better aligns with qualitative assessments of complexity. Notably, FLIPD is orders of magnitude faster than other LID estimators, and the first to be tractable at the scale of Stable Diffusion.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2923abb88a8b518f873227827f652a291f04f8ac.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkamkari2024a,\\ntitle={A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models},\\nauthor={Hamidreza Kamkari and Brendan Leigh Ross and Rasa Hosseinzadeh and Jesse C. Cresswell and Gabriel Loaiza-Ganem},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nd8Q4a8aWl}\\n}'}, 'paperhash': {'value': 'kamkari|a_geometric_view_of_data_complexity_efficient_local_intrinsic_dimension_estimation_with_diffusion_models'}},forum = 'nd8Q4a8aWl',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11166/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11166/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11166/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11166/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ncqauwSyl5',number = 10508,cdate = 1715694847463,pdate = 1727287943069,odate = 1730873929825,mdate = 1730873929841,tcdate = 1715694847463,tmdate = 1730873929841,ddate = None,content = {'title': {'value': 'Neural P$^3$M: A Long-Range Interaction Modeling Enhancer for Geometric GNNs'}, 'authors': {'value': ['Yusong Wang', 'Chaoran Cheng', 'Shaoning Li', 'Yuxuan Ren', 'Bin Shao', 'Ge Liu', 'Pheng-Ann Heng', 'Nanning Zheng']}, 'authorids': {'value': ['~Yusong_Wang1', '~Chaoran_Cheng2', '~Shaoning_Li2', '~Yuxuan_Ren2', '~Bin_Shao1', '~Ge_Liu2', '~Pheng-Ann_Heng1', '~Nanning_Zheng1']}, 'keywords': {'value': ['Molecule geometry modeling', 'Geometric GNNs', 'Long-range interactions']}, 'abstract': {'value': 'Geometric graph neural networks (GNNs) have emerged as powerful tools for modeling molecular geometry. However, they encounter limitations in effectively capturing long-range interactions in large molecular systems. To address this challenge, we introduce **Neural P$^3$M**, a versatile enhancer of geometric GNNs to expand the scope of their capabilities by incorporating mesh points alongside atoms and reimaging traditional mathematical operations in a trainable manner. Neural P$^3$M exhibits flexibility across a wide range of molecular systems and demonstrates remarkable accuracy in predicting energies and forces, outperforming on benchmarks such as the MD22 dataset. \\nIt also achieves an average improvement of 22% on the OE62 dataset while integrating with various architectures. Codes are available at https://github.com/OnlyLoveKFC/Neural_P3M.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f41e4a83de59e078f549c988ebd0fda9f5e00c2b.pdf'}, 'TLDR': {'value': 'We introduce Neural P$^3$M, which enhances Geometric GNNs by integrating meshes with atoms and reimaging traditional mathematical operations in a trainable manner.'}, '_bibtex': {'value': '@inproceedings{\\nwang2024neural,\\ntitle={Neural P\\\\${\\\\textasciicircum}3\\\\$M: A Long-Range Interaction Modeling Enhancer for Geometric {GNN}s},\\nauthor={Yusong Wang and Chaoran Cheng and Shaoning Li and Yuxuan Ren and Bin Shao and Ge Liu and Pheng-Ann Heng and Nanning Zheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ncqauwSyl5}\\n}'}, 'paperhash': {'value': 'wang|neural_p^3m_a_longrange_interaction_modeling_enhancer_for_geometric_gnns'}},forum = 'ncqauwSyl5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10508/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10508/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10508/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10508/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ncYGjx2vnE',number = 14135,cdate = 1715746410382,pdate = 1727288061051,odate = 1730873963565,mdate = 1730873963583,tcdate = 1715746410382,tmdate = 1730873963583,ddate = None,content = {'title': {'value': 'Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models'}, 'authors': {'value': ['Ali Behrouz', 'Michele Santacatterina', 'Ramin Zabih']}, 'authorids': {'value': ['~Ali_Behrouz1', '~Michele_Santacatterina2', '~Ramin_Zabih1']}, 'keywords': {'value': ['Multivariate Time Series', 'Time Series Modeling', 'Time Series Forecasting', 'Time Series Classification']}, 'abstract': {'value': 'Modeling multivariate time series is a well-established problem with a wide range of applications from healthcare to financial markets. It, however, is challenging as it requires methods to (1) have high expressive power of representing complicated dependencies along the time axis to capture both long-term progression and seasonal patterns, (2) capture the inter-variate dependencies when it is informative, (3) dynamically model the dependencies of variate and time dimensions, and (4) have efficient training and inference for very long sequences. Traditional State Space Models (SSMs) are classical approaches for univariate time series modeling due to their simplicity and expressive power to represent linear dependencies. They, however, have fundamentally limited expressive power to capture non-linear dependencies, are slow in practice, and fail to model the inter-variate information flow. Despite recent attempts to improve the expressive power of SSMs by using deep structured SSMs, the existing methods are either limited to univariate time series, fail to model complex patterns (e.g., seasonal patterns), fail to dynamically model the dependencies of variate and time dimensions, and/or are input-independent.  We present Chimera, an expressive variation of the 2-dimensional SSMs with careful design of parameters to maintain high expressive power while keeping the training complexity linear. Using two SSM heads with different discretization processes and input-dependent parameters, Chimera is provably able to learn long-term progression, seasonal patterns, and desirable dynamic autoregressive processes. To improve the efficiency of complex 2D recurrence, we present a fast training using a new 2-dimensional parallel selective scan. Our experimental evaluation shows the superior performance of Chimera on extensive and diverse benchmarks, including ECG and speech time series classification, long-term and short-term time series forecasting, and time series anomaly detection.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/293e7ef70612d586ad3576a085191e54b2c0eb16.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbehrouz2024chimera,\\ntitle={Chimera: Effectively Modeling Multivariate Time Series with 2-Dimensional State Space Models},\\nauthor={Ali Behrouz and Michele Santacatterina and Ramin Zabih},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ncYGjx2vnE}\\n}'}, 'paperhash': {'value': 'behrouz|chimera_effectively_modeling_multivariate_time_series_with_2dimensional_state_space_models'}},forum = 'ncYGjx2vnE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14135/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14135/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14135/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14135/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nbqvjkOs6S',number = 2945,cdate = 1715189607585,pdate = 1727287705935,odate = 1730873862123,mdate = 1730873862143,tcdate = 1715189607585,tmdate = 1730873862143,ddate = None,content = {'title': {'value': 'Gradient-free Decoder Inversion in Latent Diffusion Models'}, 'authors': {'value': ['Seongmin Hong', 'Suh Yoon Jeon', 'Kyeonghyun Lee', 'Ernest K. Ryu', 'Se Young Chun']}, 'authorids': {'value': ['~Seongmin_Hong1', '~Suh_Yoon_Jeon1', '~Kyeonghyun_Lee1', '~Ernest_K._Ryu1', '~Se_Young_Chun2']}, 'keywords': {'value': ['Latent diffusion model', 'Inversion', 'Gradient-free inversion', 'Resource-efficient inversion']}, 'TLDR': {'value': 'We propose an efficient gradient-free decoder inversion for LDMs for ensuring invertible latent diffusion model, which significantly reduced runtime and memory usage compared to gradient-based methods in various recent LDMs.'}, 'abstract': {'value': 'In latent diffusion models (LDMs), denoising diffusion process efficiently takes place on latent space whose dimension is lower than that of pixel space. Decoder is typically used to transform the representation in latent space to that in pixel space. While a decoder is assumed to have an encoder as an accurate inverse, exact encoder-decoder pair rarely exists in practice even though applications often require precise inversion of decoder. In other words, encoder is not the left-inverse but the right-inverse of the decoder; decoder inversion seeks the left-inverse. Prior works for decoder inversion in LDMs employed gradient descent inspired by inversions of generative adversarial networks. However, gradient-based methods require larger GPU memory and longer computation time for larger latent space. For example, recent video LDMs can generate more than 16 frames, but GPUs with 24 GB memory can only perform gradient-based decoder inversion for 4 frames. Here, we propose an efficient gradient-free decoder inversion for LDMs, which can be applied to diverse latent models. Theoretical convergence property of our proposed inversion has been investigated not only for the forward step method, but also for the inertial Krasnoselskii-Mann (KM) iterations under mild assumption on cocoercivity that is satisfied by recent LDMs. Our proposed gradient-free method with Adam optimizer and learning rate scheduling significantly reduced computation time and memory usage over prior gradient-based methods and enabled efficient computation in applications such as noise-space watermarking and background-preserving image editing while achieving comparable error levels.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/673b4bd9c2c5df584e9ce2632909dde3257b93a8.pdf'}, 'supplementary_material': {'value': '/attachment/30e9826032abfaa538b85ba902a9ca61eba6ef58.zip'}, '_bibtex': {'value': '@inproceedings{\\nhong2024gradientfree,\\ntitle={Gradient-free Decoder Inversion in Latent Diffusion Models},\\nauthor={Seongmin Hong and Suh Yoon Jeon and Kyeonghyun Lee and Ernest K. Ryu and Se Young Chun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nbqvjkOs6S}\\n}'}, 'paperhash': {'value': 'hong|gradientfree_decoder_inversion_in_latent_diffusion_models'}},forum = 'nbqvjkOs6S',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2945/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2945/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2945/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2945/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nZB1FpXUU6',number = 4408,cdate = 1715411363409,pdate = 1727287749894,odate = 1730873875637,mdate = 1730873875756,tcdate = 1715411363409,tmdate = 1730873875756,ddate = None,content = {'title': {'value': 'Implicit Curriculum in Procgen Made Explicit'}, 'authors': {'value': ['Zhenxiong Tan', 'Kaixin Wang', 'Xinchao Wang']}, 'authorids': {'value': ['~Zhenxiong_Tan1', '~Kaixin_Wang1', '~Xinchao_Wang1']}, 'keywords': {'value': ['Reinforcement Learning', 'Curriculum Learning', 'Procedurally Generated Environment']}, 'abstract': {'value': \"Procedurally generated environments such as Procgen Benchmark provide a testbed for evaluating the agent's ability to robustly learn a relevant skill, by situating the agent in ever-changing levels. The diverse levels associated with varying contexts are naturally connected to curriculum learning. Existing works mainly focus on arranging the levels to explicitly form a curriculum. In this work, we take a close look at the learning process itself under the multi-level training in Procgen. Interestingly, the learning process exhibits a gradual shift from easy contexts to hard contexts, suggesting an implicit curriculum in multi-level training. Our analysis is made possible through C-Procgen, a benchmark we build upon Procgen that enables explicit control of the contexts. We believe our findings will foster a deeper understanding of learning in diverse contexts, and our benchmark will benefit future research in curriculum reinforcement learning.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5f720379b9da89e568586d306f92a07c4beea5fd.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntan2024implicit,\\ntitle={Implicit Curriculum in Procgen Made Explicit},\\nauthor={Zhenxiong Tan and Kaixin Wang and Xinchao Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nZB1FpXUU6}\\n}'}, 'TLDR': {'value': 'We find an implicit curriculum in Procgen and introduce C-Procgen for controlled context learning research.'}, 'paperhash': {'value': 'tan|implicit_curriculum_in_procgen_made_explicit'}},forum = 'nZB1FpXUU6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4408/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4408/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4408/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4408/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nY7fGtsspU',number = 6518,cdate = 1715596869540,pdate = 1727287817294,odate = 1730873894208,mdate = 1730873894227,tcdate = 1715596869540,tmdate = 1730873894227,ddate = None,content = {'title': {'value': 'Graph Neural Networks Do Not Always Oversmooth'}, 'authors': {'value': ['Bastian Epping', 'Alexandre René', 'Moritz Helias', 'Michael T Schaub']}, 'authorids': {'value': ['~Bastian_Epping1', '~Alexandre_René1', '~Moritz_Helias1', '~Michael_T_Schaub1']}, 'keywords': {'value': ['graph neural networks', 'oversmoothing', 'Gaussian processes', 'chaos']}, 'TLDR': {'value': 'We adapt the chaos analysis from deep feedforward neural networks to graph neural networks and reveal a parameter regime in which graph neural networks do not oversmooth.'}, 'abstract': {'value': 'Graph neural networks (GNNs) have emerged as powerful tools for processing relational data in applications. However, GNNs suffer from the problem of oversmoothing, the property that features of all nodes exponentially converge to the same vector over layers, prohibiting the design of deep GNNs. In this work we study oversmoothing in graph convolutional networks (GCNs) by using their Gaussian process (GP) equivalence in the limit of infinitely many hidden features. By generalizing methods from conventional deep neural networks (DNNs), we can describe the distribution of features at the output layer of deep GCNs in terms of a GP: as expected, we find that typical parameter choices from the literature lead to oversmoothing. The theory, however, allows us to identify a new, non-oversmoothing phase: if the initial weights of the network have sufficiently large variance, GCNs do not oversmooth, and node features remain informative even at large depth. We demonstrate the validity of this prediction in finite-size GCNs by training a linear classifier on their output. Moreover, using the linearization of the GCN GP, we generalize the concept of propagation depth of information from DNNs to GCNs. This propagation depth diverges at the transition between the oversmoothing and non-oversmoothing phase. We test the predictions of our approach and find good agreement with finite-size GCNs. Initializing GCNs near the transition to the non-oversmoothing phase, we obtain networks which are both deep and expressive.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/da4dc1c0c104b2df3eea20a889b63bf4b54d2e59.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nepping2024graph,\\ntitle={Graph Neural Networks Do Not Always Oversmooth},\\nauthor={Bastian Epping and Alexandre Ren{\\\\'e} and Moritz Helias and Michael T Schaub},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nY7fGtsspU}\\n}\"}, 'paperhash': {'value': 'epping|graph_neural_networks_do_not_always_oversmooth'}},forum = 'nY7fGtsspU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6518/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6518/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6518/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6518/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nY0BrZdqLt',number = 20713,cdate = 1715797990836,pdate = 1727288239996,odate = 1730874002874,mdate = 1735397935152,tcdate = 1715797990836,tmdate = 1735397935152,ddate = None,content = {'title': {'value': 'Time-Reversal Provides Unsupervised Feedback to LLMs'}, 'authors': {'value': ['Yerram Varun', 'Rahul Madhavan', 'Sravanti Addepalli', 'Arun Suggala', 'Karthikeyan Shanmugam', 'Prateek Jain']}, 'authorids': {'value': ['~Yerram_Varun1', '~Rahul_Madhavan1', '~Sravanti_Addepalli1', '~Arun_Suggala1', '~Karthikeyan_Shanmugam1', '~Prateek_Jain1']}, 'keywords': {'value': ['LLMs', 'Reranking', 'reverse LLMs', 'reverse scoring', 'defenses', 'generative models', 'sequence reversal']}, 'TLDR': {'value': 'Reverse Scoring and generation provides unsupervised feedback to LLMs'}, 'abstract': {'value': 'Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\\\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/08ee7a3ea3b3fd8e7bf896a4e8fac2fc695aab87.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvarun2024timereversal,\\ntitle={Time-Reversal Provides Unsupervised Feedback to {LLM}s},\\nauthor={Yerram Varun and Rahul Madhavan and Sravanti Addepalli and Arun Suggala and Karthikeyan Shanmugam and Prateek Jain},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nY0BrZdqLt}\\n}'}, 'paperhash': {'value': 'varun|timereversal_provides_unsupervised_feedback_to_llms'}},forum = 'nY0BrZdqLt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20713/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20713/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20713/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20713/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nXYedmTf1T',number = 4986,cdate = 1715492408832,pdate = 1727287766811,odate = 1730873880865,mdate = 1730873880883,tcdate = 1715492408832,tmdate = 1730873880883,ddate = None,content = {'title': {'value': 'Calibrated Self-Rewarding Vision Language Models'}, 'authors': {'value': ['Yiyang Zhou', 'Zhiyuan Fan', 'Dongjie Cheng', 'Sihan Yang', 'Zhaorun Chen', 'Chenhang Cui', 'Xiyao Wang', 'Yun Li', 'Linjun Zhang', 'Huaxiu Yao']}, 'authorids': {'value': ['~Yiyang_Zhou1', '~Zhiyuan_Fan2', '~Dongjie_Cheng2', '~Sihan_Yang1', '~Zhaorun_Chen1', '~Chenhang_Cui1', '~Xiyao_Wang1', '~Yun_Li7', '~Linjun_Zhang1', '~Huaxiu_Yao1']}, 'keywords': {'value': ['Calibrated self-rewarding', 'large Vision-language models', 'Modality alignment']}, 'abstract': {'value': \"Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across twelve benchmarks and tasks, achieving substantial improvements over existing methods by 7.62\\\\%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Improving modality alignment in large vision-language models with calibrated self-rewarding'}, 'pdf': {'value': '/pdf/f89939751678e3e19c85ea240d620c05f898576b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024calibrated,\\ntitle={Calibrated Self-Rewarding Vision Language Models},\\nauthor={Yiyang Zhou and Zhiyuan Fan and Dongjie Cheng and Sihan Yang and Zhaorun Chen and Chenhang Cui and Xiyao Wang and Yun Li and Linjun Zhang and Huaxiu Yao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nXYedmTf1T}\\n}'}, 'paperhash': {'value': 'zhou|calibrated_selfrewarding_vision_language_models'}},forum = 'nXYedmTf1T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4986/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4986/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4986/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4986/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nXXwYsARXB',number = 2029,cdate = 1714955034100,pdate = 1727287678903,odate = 1730873854330,mdate = 1730873854364,tcdate = 1714955034100,tmdate = 1730873854364,ddate = None,content = {'title': {'value': 'A hierarchical decomposition for explaining ML performance discrepancies'}, 'authors': {'value': ['Harvineet Singh', 'Fan Xia', 'Adarsh Subbaswamy', 'Alexej Gossmann', 'Jean Feng']}, 'authorids': {'value': ['~Harvineet_Singh1', '~Fan_Xia2', '~Adarsh_Subbaswamy1', '~Alexej_Gossmann1', '~Jean_Feng1']}, 'keywords': {'value': ['explainability', 'distribution shift', 'double machine learning']}, 'TLDR': {'value': 'we propose a method to explain performance difference of a model between populations by highlighting influential variables'}, 'abstract': {'value': \"Machine learning (ML) algorithms can often differ in performance across domains. Understanding why their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Aggregate decompositions express the total performance gap as the gap due to a shift in the feature distribution $p(X)$ plus the gap due to a shift in the outcome's conditional  distribution $p(Y|X)$. While this coarse explanation is helpful for guiding root cause analyses, it provides limited details and can only suggest coarse fixes involving all variables in an ML system. Detailed decompositions quantify the importance of each variable to each term in the aggregate decomposition, which can provide a deeper understanding and suggest more targeted interventions. Although parametric methods exist for conducting a full hierarchical decomposition of an algorithm's performance gap at the aggregate and detailed levels, current nonparametric methods only cover parts of the hierarchy; many also require knowledge of the entire causal graph. We introduce a nonparametric hierarchical framework for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. Furthermore, we derive debiased, computationally-efficient estimators and statistical inference procedures to construct confidence intervals for the explanations.\"}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/189c9c99946e6807779fec397a2e58a7b52303f0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsingh2024a,\\ntitle={A hierarchical decomposition for explaining {ML} performance discrepancies},\\nauthor={Harvineet Singh and Fan Xia and Adarsh Subbaswamy and Alexej Gossmann and Jean Feng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nXXwYsARXB}\\n}'}, 'paperhash': {'value': 'singh|a_hierarchical_decomposition_for_explaining_ml_performance_discrepancies'}},forum = 'nXXwYsARXB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2029/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2029/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2029/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2029/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nWMqQHzI3W',number = 13969,cdate = 1715744669022,pdate = 1727288056205,odate = 1730873962499,mdate = 1730873962531,tcdate = 1715744669022,tmdate = 1730873962531,ddate = None,content = {'title': {'value': 'SEEV: Synthesis with Efficient Exact Verification for ReLU Neural Barrier Functions'}, 'authors': {'value': ['Hongchao Zhang', 'Zhizhen Qin', 'Sicun Gao', 'Andrew Clark']}, 'authorids': {'value': ['~Hongchao_Zhang2', '~Zhizhen_Qin1', '~Sicun_Gao1', '~Andrew_Clark1']}, 'keywords': {'value': ['Safe Control', 'Barrier Functions', 'Control Barrier Functions', 'Neural Networks']}, 'abstract': {'value': 'Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8c8be656daa65c9db0d7eaaf0f5e2cbcf3137202.pdf'}, 'supplementary_material': {'value': '/attachment/30b91db354495f76f642059c3cd273ea85e52ea7.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024seev,\\ntitle={{SEEV}: Synthesis with Efficient Exact Verification for Re{LU} Neural Barrier Functions},\\nauthor={Hongchao Zhang and Zhizhen Qin and Sicun Gao and Andrew Clark},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nWMqQHzI3W}\\n}'}, 'TLDR': {'value': 'This paper proposes a ReLU NCBF synthesis framework with efficient exact verification for robotic safety. Key insights: ReLU NCBF can be verified in linear pieces, boundary pieces are safety-critical, and limiting them improves efficiency.'}, 'paperhash': {'value': 'zhang|seev_synthesis_with_efficient_exact_verification_for_relu_neural_barrier_functions'}},forum = 'nWMqQHzI3W',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13969/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13969/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13969/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13969/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'nU4lvlMwrt',number = 3118,cdate = 1715229942064,pdate = 1727287710805,odate = 1730873863655,mdate = 1730873863675,tcdate = 1715229942064,tmdate = 1730873863675,ddate = None,content = {'title': {'value': 'Toward Real Ultra Image Segmentation: Leveraging Surrounding Context to Cultivate General Segmentation Model'}, 'authors': {'value': ['Sai Wang', 'Yutian Lin', 'Yu Wu', 'Bo Du']}, 'authorids': {'value': ['~Sai_Wang1', '~Yutian_Lin2', '~Yu_Wu3', '~Bo_Du3']}, 'keywords': {'value': ['Ultra Image Segmentation']}, 'abstract': {'value': 'Existing ultra image segmentation methods suffer from two major challenges, namely the scalability issue (i.e. they lack the stability and generality of standard segmentation models, as they are tailored to specific datasets), and the architectural issue (i.e. they are incompatible with real-world ultra image scenes, as they compromise between image size and computing resources).\\nTo tackle these issues, we revisit the classic sliding inference framework, upon which we propose a Surrounding Guided Segmentation  framework  (SGNet) for ultra image segmentation. \\nThe SGNet leverages a larger area around each image patch to refine the general segmentation results of local patches.\\nSpecifically, we propose a surrounding context integration module to absorb surrounding context information and extract specific features that are beneficial to local patches. Note that, SGNet can be seamlessly integrated to any general segmentation model.\\nExtensive experiments on five datasets demonstrate that SGNet achieves competitive performance and consistent improvements across a variety of general segmentation models, surpassing the traditional ultra image segmentation methods by a large margin.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0eceffa7f6da09e11f0e7396015fca9f4bbc9d09.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024toward,\\ntitle={Toward Real Ultra Image Segmentation: Leveraging Surrounding Context to Cultivate General Segmentation Model},\\nauthor={Sai Wang and Yutian Lin and Yu Wu and Bo Du},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nU4lvlMwrt}\\n}'}, 'paperhash': {'value': 'wang|toward_real_ultra_image_segmentation_leveraging_surrounding_context_to_cultivate_general_segmentation_model'}},forum = 'nU4lvlMwrt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3118/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3118/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3118/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3118/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nTJeOXlWyV',number = 12890,cdate = 1715730865018,pdate = 1727288021155,odate = 1730873953283,mdate = 1735202338082,tcdate = 1715730865018,tmdate = 1735202338082,ddate = None,content = {'title': {'value': 'RTify: Aligning Deep Neural Networks with Human Behavioral Decisions'}, 'authors': {'value': ['Yu-Ang Cheng', 'Ivan F Rodriguez Rodriguez', 'Sixuan Chen', 'Kohitij Kar', 'Takeo Watanabe', 'Thomas Serre']}, 'authorids': {'value': ['~Yu-Ang_Cheng1', '~Ivan_F_Rodriguez_Rodriguez1', '~Sixuan_Chen2', '~Kohitij_Kar2', '~Takeo_Watanabe1', '~Thomas_Serre1']}, 'keywords': {'value': ['Alignment; Recurrent neural networks; Reaction times; Visual decision making; Speed-accuracy tradeoff']}, 'TLDR': {'value': 'We present a novel differentiable framework to effectively align current vision models with human reaction times and behavioral choices'}, 'abstract': {'value': \"Current neural network models of primate vision focus on replicating overall levels of behavioral accuracy, often neglecting perceptual decisions' rich, dynamic nature. Here, we introduce a novel computational framework to model the dynamics of human behavioral choices by learning to align the temporal dynamics of a recurrent neural network (RNN) to human reaction times (RTs). We describe an approximation that allows us to constrain the number of time steps an RNN takes to solve a task with human RTs. The approach is extensively evaluated against various psychophysics experiments. We also show that the approximation can be used to optimize an ``ideal-observer'' RNN model to achieve an optimal tradeoff between speed and accuracy without human data. The resulting model is found to account well for human RT data. Finally, we use the approximation to train a deep learning implementation of the popular Wong-Wang decision-making model. The model is integrated with a convolutional neural network (CNN) model of visual processing and evaluated using both artificial and natural image stimuli. Overall, we present a novel framework that helps align current vision models with human behavior, bringing us closer to an integrated model of human vision.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6db9515e0b18b080a2e152c67101dadf6a756c37.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024rtify,\\ntitle={{RT}ify: Aligning Deep Neural Networks with Human Behavioral Decisions},\\nauthor={Yu-Ang Cheng and Ivan F Rodriguez Rodriguez and Sixuan Chen and Kohitij Kar and Takeo Watanabe and Thomas Serre},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nTJeOXlWyV}\\n}'}, 'paperhash': {'value': 'cheng|rtify_aligning_deep_neural_networks_with_human_behavioral_decisions'}},forum = 'nTJeOXlWyV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12890/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12890/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12890/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission12890/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nRp0XhTf61',number = 3325,cdate = 1715260387148,pdate = 1727287716584,odate = 1730873865216,mdate = 1730873865234,tcdate = 1715260387148,tmdate = 1730873865234,ddate = None,content = {'title': {'value': 'InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD'}, 'authors': {'value': ['Xiaoyi Dong', 'Pan Zhang', 'Yuhang Zang', 'Yuhang Cao', 'Bin Wang', 'Linke Ouyang', 'Songyang Zhang', 'Haodong Duan', 'Wenwei Zhang', 'Yining Li', 'Hang Yan', 'Yang Gao', 'Zhe Chen', 'xinyue zhang', 'Wei Li', 'Li Jingwen', 'Wenhai Wang', 'Kai Chen', 'Conghui He', 'Xingcheng ZHANG', 'Jifeng Dai', 'Yu Qiao', 'Dahua Lin', 'Jiaqi Wang']}, 'authorids': {'value': ['~Xiaoyi_Dong1', '~Pan_Zhang1', '~Yuhang_Zang1', '~Yuhang_Cao3', '~Bin_Wang21', '~Linke_Ouyang1', '~Songyang_Zhang1', '~Haodong_Duan1', '~Wenwei_Zhang1', '~Yining_Li1', '~Hang_Yan2', '~Yang_Gao7', '~Zhe_Chen10', '~xinyue_zhang8', '~Wei_Li86', '~Li_Jingwen1', '~Wenhai_Wang2', '~Kai_Chen4', '~Conghui_He2', '~Xingcheng_ZHANG2', '~Jifeng_Dai1', '~Yu_Qiao1', '~Dahua_Lin1', '~Jiaqi_Wang1']}, 'keywords': {'value': ['Large Vision Language Model (LVLM)']}, 'abstract': {'value': 'The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 $\\\\times$ 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 × 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 $\\\\times$ 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 × 1600) and beyond.'}, 'pdf': {'value': '/pdf/1155d659fc23c66face1a69fc64ca5781c59f825.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024internlmxcomposerkhd,\\ntitle={Intern{LM}-{XC}omposer2-4{KHD}: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K {HD}},\\nauthor={Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li and Hang Yan and Yang Gao and Zhe Chen and xinyue zhang and Wei Li and Li Jingwen and Wenhai Wang and Kai Chen and Conghui He and Xingcheng ZHANG and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nRp0XhTf61}\\n}'}, 'paperhash': {'value': 'dong|internlmxcomposer24khd_a_pioneering_large_visionlanguage_model_handling_resolutions_from_336_pixels_to_4k_hd'}},forum = 'nRp0XhTf61',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3325/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3325/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3325/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3325/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nRdST1qifJ',number = 16656,cdate = 1715773008594,pdate = 1727288132610,odate = 1730873980516,mdate = 1730873980534,tcdate = 1715773008594,tmdate = 1730873980534,ddate = None,content = {'title': {'value': 'Fight Back Against Jailbreaking via Prompt Adversarial Tuning'}, 'authors': {'value': ['Yichuan Mo', 'Yuji Wang', 'Zeming Wei', 'Yisen Wang']}, 'authorids': {'value': ['~Yichuan_Mo1', '~Yuji_Wang3', '~Zeming_Wei1', '~Yisen_Wang1']}, 'keywords': {'value': ['Large Language Model', 'Jailbreak Defense', 'Prompt Tuning']}, 'TLDR': {'value': 'We propose an approach, termed as Prompt Adversarial Tuning (PAT), to defend  the jailbreak attacks for LLMs.'}, 'abstract': {'value': \"While Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to jailbreaking attacks. Several primary defense strategies have been proposed to protect LLMs from producing harmful information, mostly focusing on model fine-tuning or heuristical defense designs. However, how to achieve intrinsic robustness through prompt optimization remains an open problem. In this paper, motivated by adversarial training paradigms for achieving reliable robustness, we propose an approach named **Prompt Adversarial Tuning (PAT)** that trains a prompt control attached to the user prompt as a guard prefix. To achieve our defense goal whilst maintaining natural performance, we optimize the control prompt with both adversarial and benign prompts. Comprehensive experiments show that our method is effective against both grey-box and black-box attacks, reducing the success rate of advanced attacks to nearly 0, while maintaining the model's utility on the benign task and incurring only negligible computational overhead, charting a new perspective for future explorations in LLM security. Our code is available at https://github.com/PKU-ML/PAT.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9c78dbb165f7e9b3965a7b882b3616b0733b6b9f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmo2024fight,\\ntitle={Fight Back Against Jailbreaking via Prompt Adversarial Tuning},\\nauthor={Yichuan Mo and Yuji Wang and Zeming Wei and Yisen Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nRdST1qifJ}\\n}'}, 'paperhash': {'value': 'mo|fight_back_against_jailbreaking_via_prompt_adversarial_tuning'}},forum = 'nRdST1qifJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16656/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16656/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16656/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16656/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nRRJsDahEg',number = 13792,cdate = 1715743054500,pdate = 1727288050280,odate = 1730873960515,mdate = 1730873960536,tcdate = 1715743054500,tmdate = 1730873960536,ddate = None,content = {'title': {'value': 'Towards a \"Universal Translator\" for Neural Dynamics at Single-Cell, Single-Spike Resolution'}, 'authors': {'value': ['Yizi Zhang', 'Yanchen Wang', 'Donato M. Jiménez-Benetó', 'Zixuan Wang', 'Mehdi Azabou', 'Blake Aaron Richards', 'Renee Tung', 'Olivier Winter', 'International Brain Laboratory', 'Eva L Dyer', 'Liam Paninski', 'Cole Lincoln Hurwitz']}, 'authorids': {'value': ['~Yizi_Zhang1', '~Yanchen_Wang1', '~Donato_M._Jiménez-Benetó1', '~Zixuan_Wang13', '~Mehdi_Azabou2', '~Blake_Aaron_Richards1', '~Renee_Tung1', '~Olivier_Winter1', '~International_Brain_Laboratory1', '~Eva_L_Dyer1', '~Liam_Paninski1', '~Cole_Lincoln_Hurwitz1']}, 'keywords': {'value': ['neural dynamics', 'transformer', 'self-supervised learning', 'pretraining', 'multi-task learning', 'masked modeling', 'brain-computer interfaces']}, 'abstract': {'value': 'Neuroscience research has made immense progress over the last decade, but our understanding of the brain remains fragmented and piecemeal: the dream of probing an arbitrary brain region and automatically reading out the information encoded in its neural activity remains out of reach. In this work, we build towards a first foundation model for neural spiking data that can solve a diverse set of tasks across multiple brain areas. We introduce a novel self-supervised modeling approach for population activity in which the model alternates between masking out and reconstructing neural activity across different time steps, neurons, and brain regions. To evaluate our approach, we design unsupervised and supervised prediction tasks using the International Brain Laboratory repeated site dataset, which is comprised of Neuropixels recordings targeting the same brain locations across 48 animals and experimental sessions. The prediction tasks include single-neuron and region-level activity prediction, forward prediction, and behavior decoding. We demonstrate that our multi-task-masking (MtM) approach significantly improves the performance of current state-of-the-art population models and enables multi-task learning. We also show that by training on multiple animals, we can improve the generalization ability of the model to unseen animals, paving the way for a foundation model of the brain at single-cell, single-spike resolution.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e7664eef58345c56b265804ba3f72932b5f88c14.pdf'}, 'supplementary_material': {'value': '/attachment/56cd6abe93c1d3c010184b45cf190771c8989c61.zip'}, '_bibtex': {'value': \"@inproceedings{\\nzhang2024towards,\\ntitle={Towards a ''Universal Translator'' for Neural Dynamics at Single-Cell, Single-Spike Resolution},\\nauthor={Yizi Zhang and Yanchen Wang and Donato M. Jim{\\\\'e}nez-Benet{\\\\'o} and Zixuan Wang and Mehdi Azabou and Blake Aaron Richards and Renee Tung and Olivier Winter and International Brain Laboratory and Eva L Dyer and Liam Paninski and Cole Lincoln Hurwitz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nRRJsDahEg}\\n}\"}, 'paperhash': {'value': 'zhang|towards_a_universal_translator_for_neural_dynamics_at_singlecell_singlespike_resolution'}},forum = 'nRRJsDahEg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13792/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13792/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13792/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission13792/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nQl8EjyMzh',number = 18281,cdate = 1715784669360,pdate = 1727288176533,odate = 1730873988986,mdate = 1730873988996,tcdate = 1715784669360,tmdate = 1730873988996,ddate = None,content = {'title': {'value': 'On conditional diffusion models for PDE simulations'}, 'authors': {'value': ['Aliaksandra Shysheya', 'Cristiana Diaconu', 'Federico Bergamin', 'Paris Perdikaris', 'José Miguel Hernández-Lobato', 'Richard E. Turner', 'Emile Mathieu']}, 'authorids': {'value': ['~Aliaksandra_Shysheya1', '~Cristiana_Diaconu1', '~Federico_Bergamin1', '~Paris_Perdikaris1', '~José_Miguel_Hernández-Lobato1', '~Richard_E_Turner1', '~Emile_Mathieu1']}, 'keywords': {'value': ['neural PDE solver', 'PDE', 'partial differential equation', 'forecasting', 'data-assimilation', 'diffusion', 'denoising', 'autoregressive', 'neural surrogate', 'reconstruction guidance', 'conditional score']}, 'TLDR': {'value': 'This paper provides a comprehensive analysis and extension of the current state of score-based diffusion models trained on short segments from PDE trajectories, and evaluated on forecasting and data assimilation tasks.'}, 'abstract': {'value': 'Modelling partial differential equations (PDEs) is of crucial importance in science and engineering, and it includes tasks ranging from forecasting to inverse problems, such as data assimilation. However, most previous numerical and machine learning approaches that target forecasting cannot be applied out-of-the-box for data assimilation. Recently, diffusion models have emerged as a powerful tool for conditional generation, being able to flexibly incorporate observations without retraining. In this work, we perform a comparative study of score-based diffusion models for forecasting and assimilation of sparse observations.  In particular, we focus on diffusion models that are either trained in a conditional manner, or conditioned after unconditional training. We address the shortcomings of existing models by proposing 1) an autoregressive sampling approach, that significantly improves performance in forecasting, 2) a new training strategy for conditional score-based models that achieves stable performance over a range of history lengths, and 3) a hybrid model which employs flexible pre-training conditioning on initial conditions and flexible post-training conditioning to handle data assimilation. We empirically show that these modifications are crucial for successfully tackling the combination of forecasting and data assimilation, a task commonly encountered in real-world scenarios.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1a5c15950100868995c51eeb90a09652172208dc.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nshysheya2024on,\\ntitle={On conditional diffusion models for {PDE} simulations},\\nauthor={Aliaksandra Shysheya and Cristiana Diaconu and Federico Bergamin and Paris Perdikaris and Jos{\\\\'e} Miguel Hern{\\\\'a}ndez-Lobato and Richard E. Turner and Emile Mathieu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nQl8EjyMzh}\\n}\"}, 'paperhash': {'value': 'shysheya|on_conditional_diffusion_models_for_pde_simulations'}},forum = 'nQl8EjyMzh',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18281/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18281/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18281/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18281/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nN6NSd1Qds',number = 16395,cdate = 1715769926420,pdate = 1727288125586,odate = 1730873979156,mdate = 1736769207688,tcdate = 1715769926420,tmdate = 1736769207688,ddate = None,content = {'title': {'value': 'UGC: Universal Graph Coarsening'}, 'authors': {'value': ['Mohit Kataria', 'Sandeep Kumar', 'Jayadeva Jayadeva']}, 'authorids': {'value': ['~Mohit_Kataria1', '~Sandeep_Kumar8', '~Jayadeva_Jayadeva1']}, 'keywords': {'value': ['Graph Coarsening', 'Graph Neural Networks', 'Locality sensitive hashing', 'Heterophilic Graph', 'Scaling Graph Learning']}, 'TLDR': {'value': 'UGC is a graph coarsening framework which is extremely fast, has lower eigen-error, and yields superior performance on downstream processing tasks.'}, 'abstract': {'value': \"In the era of big data, graphs have emerged as a natural representation of intricate relationships. However, graph sizes often become unwieldy, leading to storage, computation, and analysis challenges. A crucial demand arises for methods that can effectively downsize large graphs while retaining vital insights. Graph coarsening seeks to simplify large graphs while maintaining the basic statistics of the graphs, such as spectral properties and $\\\\epsilon$-similarity in the coarsened graph. This ensures that downstream processes are more efficient and effective. Most published methods are suitable for homophilic datasets, limiting their universal use. We propose **U**niversal **G**raph **C**oarsening (UGC), a framework equally suitable for homophilic and heterophilic datasets. UGC integrates node attributes and adjacency information, leveraging the dataset's heterophily factor. Results on benchmark datasets demonstrate that UGC preserves spectral similarity while coarsening. In comparison to existing methods, UGC is 4x to 15x faster, has lower eigen-error, and yields superior performance on downstream processing tasks even at 70% coarsening ratios.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7835e160109c7dd1023e90ff5f2eb18f64b1e6cb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkataria2024ugc,\\ntitle={{UGC}: Universal Graph Coarsening},\\nauthor={Mohit Kataria and Sandeep Kumar and Jayadeva Jayadeva},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nN6NSd1Qds}\\n}'}, 'paperhash': {'value': 'kataria|ugc_universal_graph_coarsening'}},forum = 'nN6NSd1Qds',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16395/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16395/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16395/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16395/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nLSLbJgL7f',number = 1764,cdate = 1714796236102,pdate = 1727287670727,odate = 1730873851753,mdate = 1731620988090,tcdate = 1714796236102,tmdate = 1731620988090,ddate = None,content = {'title': {'value': 'To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation'}, 'authors': {'value': ['Chenxi Zhao', 'Jinglei Shi', 'Liqiang Nie', 'Jufeng Yang']}, 'authorids': {'value': ['~Chenxi_Zhao3', '~Jinglei_Shi1', '~Liqiang_Nie2', '~Jufeng_Yang1']}, 'keywords': {'value': ['visual emotion recognition', 'evaluation measure']}, 'TLDR': {'value': 'New metrics inspired by cognitive nuroscience are proposed to evaluate'}, 'abstract': {'value': \"Accuracy is a commonly adopted performance metric in various classification tasks, which measures the proportion of correctly classified samples among all samples. It assumes equal importance for all classes, hence equal severity for misclassifications. However, in the task of emotional classification, due to the psychological similarities between emotions, misclassifying a certain emotion into one class may be more severe than another, e.g., misclassifying 'excitement' as 'anger' apparently is more severe than as 'awe'. Albeit high meaningful for many applications, metrics capable of measuring these cases of misclassifications in visual emotion recognition tasks have yet to be explored. In this paper, based on Mikel's emotion wheel from psychology, we propose a novel approach for evaluating the performance in visual emotion recognition, which takes into account the distance on the emotion wheel between different emotions to mimic the psychological nuances of emotions. Experimental results in semi-supervised learning on emotion recognition and user study have shown that our proposed metrics is more effective than the accuracy to assess the performance and conforms to the cognitive laws of human emotions. The code is available at https://github.com/ZhaoChenxi-nku/ECC.\"}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/f025452dacf49776acfd5917e7c0423d8e07c6e6.zip'}, 'pdf': {'value': '/pdf/ffcf8658f89e6056d714d63343cbe2be39e55b3e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024to,\\ntitle={To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation},\\nauthor={Chenxi Zhao and Jinglei Shi and Liqiang Nie and Jufeng Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nLSLbJgL7f}\\n}'}, 'paperhash': {'value': 'zhao|to_err_like_human_affective_biasinspired_measures_for_visual_emotion_recognition_evaluation'}},forum = 'nLSLbJgL7f',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1764/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1764/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1764/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1764/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nLQeE8QGGe',number = 12313,cdate = 1715720130970,pdate = 1727288002243,odate = 1730873947956,mdate = 1735945757667,tcdate = 1715720130970,tmdate = 1735945757667,ddate = None,content = {'title': {'value': 'Active learning of neural population dynamics using two-photon holographic optogenetics'}, 'authors': {'value': ['Andrew Wagenmaker', 'Lu Mi', 'Marton Rozsa', 'Matthew Storm Bull', 'Karel Svoboda', 'Kayvon Daie', 'Matthew D. Golub', 'Kevin Jamieson']}, 'authorids': {'value': ['~Andrew_Wagenmaker1', '~Lu_Mi1', '~Marton_Rozsa1', '~Matthew_Storm_Bull1', '~Karel_Svoboda1', '~Kayvon_Daie1', '~Matthew_D._Golub2', '~Kevin_Jamieson1']}, 'keywords': {'value': ['active learning', 'experiment design', 'neural system identification', 'neural behavior']}, 'TLDR': {'value': 'We develop active learning methods to guide two-photon photostimulation for the purpose of reducing the amount of data needed to estimate an accurate mode of the neural population dynamics.'}, 'abstract': {'value': 'Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain.  In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f65e32e082781fef08ab450af2a506bb55487173.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwagenmaker2024active,\\ntitle={Active design of two-photon holographic stimulation for identifying neural population dynamics},\\nauthor={Andrew Wagenmaker and Lu Mi and Marton Rozsa and Matthew Storm Bull and Karel Svoboda and Kayvon Daie and Matthew D. Golub and Kevin Jamieson},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nLQeE8QGGe}\\n}'}, 'paperhash': {'value': 'wagenmaker|active_learning_of_neural_population_dynamics_using_twophoton_holographic_optogenetics'}},forum = 'nLQeE8QGGe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12313/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12313/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12313/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12313/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nK6OnCpd3n',number = 18525,cdate = 1715785955943,pdate = 1727288186508,odate = 1730873990550,mdate = 1730873990569,tcdate = 1715785955943,tmdate = 1730873990569,ddate = None,content = {'title': {'value': 'Text-Aware Diffusion for Policy Learning'}, 'authors': {'value': ['Calvin Luo', 'Mandy He', 'Zilai Zeng', 'Chen Sun']}, 'authorids': {'value': ['~Calvin_Luo2', '~Mandy_He1', '~Zilai_Zeng1', '~Chen_Sun1']}, 'keywords': {'value': ['diffusion models', 'reinforcement learning']}, 'abstract': {'value': 'Training an agent to achieve particular goals or perform desired behaviors is often accomplished through reinforcement learning, especially in the absence of expert demonstrations.  However, supporting novel goals or behaviors through reinforcement learning requires the ad-hoc design of appropriate reward functions, which quickly becomes intractable. To address this challenge, we propose Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a pretrained, frozen text-conditioned diffusion model to compute dense zero-shot reward signals for text-aligned policy learning.  We hypothesize that large-scale pretrained generative models encode rich priors that can supervise a policy to behave not only in a text-aligned manner, but also in alignment with a notion of naturalness summarized from internet-scale training data.  In our experiments, we demonstrate that TADPoLe is able to learn policies for novel goal-achievement and continuous locomotion behaviors specified by natural language, in both Humanoid and Dog environments. The behaviors are learned zero-shot without ground-truth rewards or expert demonstrations, and are qualitatively more natural according to human evaluation. We further show that TADPoLe performs competitively when applied to robotic manipulation tasks in the Meta-World environment, without having access to any in-domain demonstrations.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7969af175f3dac2cdf426c6629415a5f6951c777.pdf'}, 'supplementary_material': {'value': '/attachment/2716387f28f6530aaef3405e7cf114b39fc09cd6.zip'}, 'TLDR': {'value': 'Pre-trained, frozen diffusion models generate dense zero-shot reward signals for text-conditioned policy learning.'}, '_bibtex': {'value': '@inproceedings{\\nluo2024textaware,\\ntitle={Text-Aware Diffusion for Policy Learning},\\nauthor={Calvin Luo and Mandy He and Zilai Zeng and Chen Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nK6OnCpd3n}\\n}'}, 'paperhash': {'value': 'luo|textaware_diffusion_for_policy_learning'}},forum = 'nK6OnCpd3n',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18525/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18525/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18525/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18525/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nJvkQSu9Z5',number = 20724,cdate = 1715798030714,pdate = 1727288240280,odate = 1730874003004,mdate = 1730874003022,tcdate = 1715798030714,tmdate = 1730874003022,ddate = None,content = {'title': {'value': 'Shared Autonomy with IDA: Interventional Diffusion Assistance'}, 'authors': {'value': ['Brandon J McMahan', 'Zhenghao Peng', 'Bolei Zhou', 'Jonathan Kao']}, 'authorids': {'value': ['~Brandon_J_McMahan1', '~Zhenghao_Peng1', '~Bolei_Zhou5', '~Jonathan_Kao1']}, 'keywords': {'value': ['Shared Autonomy', 'Diffusion Models', 'copilots', 'intervention reinforcement learning', 'reinforcement learning', 'lunar lander', 'Mujoco']}, 'TLDR': {'value': 'We develop an intervention function that dynamically shares control between a human operator and assistive AI agent by choosing the control that maximizes expected future returns.'}, 'abstract': {'value': 'The rapid development of artificial intelligence (AI) has unearthed the potential to assist humans in controlling advanced technologies. Shared autonomy (SA) facilitates control by combining inputs from a human pilot and an AI copilot. In prior SA studies, the copilot is constantly active in determining the action played at each time step. This limits human autonomy that may have deleterious effects on performance. In general, the amount of helpful copilot assistance varies greatly depending on the task dynamics. We therefore hypothesized that human autonomy and SA performance improves through dynamic and selective copilot intervention. To address this, we develop a goal-agnostic intervention assistance (IA) that dynamically shares control by having the copilot intervene only when the expected value of the copilot’s action exceeds that of the human’s action. We implement IA with a diffusion copilot (termed IDA) trained on expert demonstrations with goal masking. We prove that IDA performance is lower bounded by human performance, so that IDA does not negatively impact human control. In experiments with simulated human pilots, we show that IDA achieves higher performance than both pilot-only and traditional SA control in variants of the Reacher environment and Lunar Lander. We then demonstrate with human-in the-loop experiments that IDA achieves better control in Lunar Lander and that human participants experience greater autonomy and prefer IDA over pilot-only and traditional SA control. We attribute the success of IDA to preserving human autonomy while simultaneously offering assistance to prevent the human from entering universally bad states.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f15789848abf5e6972bd32102c5f084f4e28b8e6.pdf'}, 'supplementary_material': {'value': '/attachment/c3f5bcf3c8ecbe9a038aa7366d4ff206b3a3faef.zip'}, '_bibtex': {'value': '@inproceedings{\\nmcmahan2024shared,\\ntitle={Shared Autonomy with {IDA}: Interventional Diffusion Assistance},\\nauthor={Brandon J McMahan and Zhenghao Peng and Bolei Zhou and Jonathan Kao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nJvkQSu9Z5}\\n}'}, 'paperhash': {'value': 'mcmahan|shared_autonomy_with_ida_interventional_diffusion_assistance'}},forum = 'nJvkQSu9Z5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20724/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20724/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20724/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20724/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nJKfNiEBvq',number = 6627,cdate = 1715600317466,pdate = 1727287821190,odate = 1730873894929,mdate = 1737001860896,tcdate = 1715600317466,tmdate = 1737001860896,ddate = None,content = {'title': {'value': 'Learning the Latent Causal Structure for Modeling Label Noise'}, 'authors': {'value': ['Yexiong Lin', 'Yu Yao', 'Tongliang Liu']}, 'authorids': {'value': ['~Yexiong_Lin1', '~Yu_Yao3', '~Tongliang_Liu1']}, 'keywords': {'value': ['Label noise', 'transition matrices']}, 'abstract': {'value': 'In label-noise learning, the noise transition matrix reveals how an instance transitions from its clean label to its noisy label. Accurately estimating an instance\\'s noise transition matrix is crucial for estimating its clean label. However, when only a noisy dataset is available, noise transition matrices can be estimated only for some \"special\" instances. To leverage these estimated transition matrices to help estimate the transition matrices of other instances, it is essential to explore relations between the matrices of these \"special\" instances and those of others. Existing studies typically build the relation by explicitly defining the similarity between the estimated noise transition matrices of \"special\" instances and those of other instances. However, these similarity-based assumptions are hard to validate and may not align with real-world data. If these assumptions fail, both noise transition matrices and clean labels cannot be accurately estimated. In this paper, we found that by learning the latent causal structure governing the generating process of noisy data, we can estimate noise transition matrices without the need for similarity-based assumptions. Unlike previous generative label-noise learning methods, we consider causal relations between latent causal variables and model them with a learnable graphical model. Utilizing only noisy data, our method can effectively learn the latent causal structure. Experimental results on various noisy datasets demonstrate that our method achieves state-of-the-art performance in estimating noise transition matrices, which leads to improved classification accuracy. The code is available at: https://github.com/tmllab/2024_NeurIPS_CSGN.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/004832b2f309fcf60e7b1f59c6b4b9019dd86d51.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024learning,\\ntitle={Learning the Latent Causal Structure for Modeling Label Noise},\\nauthor={Yexiong Lin and Yu Yao and Tongliang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nJKfNiEBvq}\\n}'}, 'paperhash': {'value': 'lin|learning_the_latent_causal_structure_for_modeling_label_noise'}},forum = 'nJKfNiEBvq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6627/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6627/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6627/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6627/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'nIeufGuQ9x',number = 6624,cdate = 1715600274211,pdate = 1727287821101,odate = 1730873894888,mdate = 1730873894907,tcdate = 1715600274211,tmdate = 1730873894907,ddate = None,content = {'title': {'value': 'DiffSF: Diffusion Models for Scene Flow Estimation'}, 'authors': {'value': ['Yushan Zhang', 'Bastian Wandt', 'Maria Magnusson', 'Michael Felsberg']}, 'authorids': {'value': ['~Yushan_Zhang1', '~Bastian_Wandt2', '~Maria_Magnusson1', '~Michael_Felsberg2']}, 'keywords': {'value': ['Scene Flow Estimation', 'Denoising Diffusion Models', 'Uncertainty']}, 'abstract': {'value': 'Scene flow estimation is an essential ingredient for a variety of real-world applications, especially for autonomous agents, such as self-driving cars and robots. While recent scene flow estimation approaches achieve reasonable accuracy, their applicability to real-world systems additionally benefits from a reliability measure. Aiming at improving accuracy while additionally providing an estimate for uncertainty, we propose DiffSF that combines transformer-based scene flow estimation with denoising diffusion models. In the diffusion process, the ground truth scene flow vector field is gradually perturbed by adding Gaussian noise. In the reverse process, starting from randomly sampled Gaussian noise, the scene flow vector field prediction is recovered by conditioning on a source and a target point cloud. We show that the diffusion process greatly increases the robustness of predictions compared to prior approaches resulting in state-of-the-art performance on standard scene flow estimation benchmarks. Moreover, by sampling multiple times with different initial states, the denoising process predicts multiple hypotheses, which enables measuring the output uncertainty, allowing our approach to detect a majority of the inaccurate predictions. The code is available at https://github.com/ZhangYushan3/DiffSF.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/469a83f616f15f8f0d5c0e23e222481187df6199.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024diffsf,\\ntitle={Diff{SF}: Diffusion Models for Scene Flow Estimation},\\nauthor={Yushan Zhang and Bastian Wandt and Maria Magnusson and Michael Felsberg},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nIeufGuQ9x}\\n}'}, 'paperhash': {'value': 'zhang|diffsf_diffusion_models_for_scene_flow_estimation'}},forum = 'nIeufGuQ9x',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6624/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6624/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6624/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6624/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'nF34qXcY0b',number = 21147,cdate = 1715800290132,pdate = 1727288249215,odate = 1730874004917,mdate = 1736901547013,tcdate = 1715800290132,tmdate = 1736901547013,ddate = None,content = {'title': {'value': 'Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees'}, 'authors': {'value': ['Negin Musavi', 'Ziyao Guo', 'Geir Dullerud', 'Yingying Li']}, 'authorids': {'value': ['~Negin_Musavi1', '~Ziyao_Guo2', '~Geir_Dullerud1', '~Yingying_Li3']}, 'keywords': {'value': ['set-membership', 'least-squares', 'nonlinear systems', 'non-asymptotic guarantees']}, 'abstract': {'value': 'This paper focuses on the system identification of an important class of nonlinear systems: nonlinear systems that are linearly parameterized, which enjoy wide applications in robotics and other mechanical systems. We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters. We provide non-asymptotic convergence rates for LSE and SME under  i.i.d. control inputs and control policies with i.i.d. random perturbations, both of which are considered as non-active-exploration inputs. Compared with the counter-example based on piecewise-affine systems in the literature, the success of non-active exploration in our setting relies on a key assumption about the system dynamics: we require the system functions to be real-analytic. Our results, together with the piecewise-affine counter-example, reveal the importance of differentiability in nonlinear system identification through non-active exploration. Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/93c6f92221521c38dae4a17d2c9fb3a257c92ed1.pdf'}, 'supplementary_material': {'value': '/attachment/ae7d9dab406eeaf52c397476e92c7c51911b0bc1.zip'}, 'TLDR': {'value': 'This paper generalizes the system estimation conditions for nonlinear control systems under i.i.d. inputs and provides non-asymptotic analysis.'}, '_bibtex': {'value': '@inproceedings{\\nmusavi2024identification,\\ntitle={Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees},\\nauthor={Negin Musavi and Ziyao Guo and Geir Dullerud and Yingying Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nF34qXcY0b}\\n}'}, 'paperhash': {'value': 'musavi|identification_of_analytic_nonlinear_dynamical_systems_with_nonasymptotic_guarantees'}},forum = 'nF34qXcY0b',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21147/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21147/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21147/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21147/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nExI4FuKWD',number = 13709,cdate = 1715742332521,pdate = 1727288047631,odate = 1730873959874,mdate = 1730873959891,tcdate = 1715742332521,tmdate = 1730873959891,ddate = None,content = {'title': {'value': 'FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding'}, 'authors': {'value': ['Dong Jing', 'Xiaolong He', 'Yutian Luo', 'Nanyi Fei', 'Guoxing Yang', 'Wei Wei', 'Huiwen Zhao', 'Zhiwu Lu']}, 'authorids': {'value': ['~Dong_Jing1', '~Xiaolong_He1', '~Yutian_Luo1', '~Nanyi_Fei1', '~Guoxing_Yang3', '~Wei_Wei28', '~Huiwen_Zhao1', '~Zhiwu_Lu1']}, 'keywords': {'value': ['CLIP', 'Fine-grained Understanding']}, 'abstract': {'value': 'Contrastive Language-Image Pre-training (CLIP) achieves impressive performance on tasks like image classification and image-text retrieval by learning on large-scale image-text datasets. However, CLIP struggles with dense prediction tasks due to the poor grasp of the fine-grained details. Although existing works pay attention to this issue, they achieve limited improvements and usually sacrifice the important visual-semantic consistency. To overcome these limitations, we propose FineCLIP, which keeps the global contrastive learning to preserve the visual-semantic consistency and further enhances the fine-grained understanding through two innovations: 1) A real-time self-distillation scheme that facilitates the transfer of representation capability from global to local features. 2) A semantically-rich regional contrastive learning paradigm with generated region-text pairs, boosting the local representation capabilities with abundant fine-grained knowledge. \\nBoth cooperate to fully leverage diverse semantics and multi-grained complementary information.\\nTo validate the superiority of our FineCLIP and the rationality of each design, we conduct extensive experiments on challenging dense prediction and image-level tasks. \\nAll the observations demonstrate the effectiveness of FineCLIP.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e77b9bf69974b22ae77ee4209dc907d97148cbdd.pdf'}, 'supplementary_material': {'value': '/attachment/65ec4e49350e13daa3d5c3d5f3858057985de330.zip'}, '_bibtex': {'value': '@inproceedings{\\njing2024fineclip,\\ntitle={Fine{CLIP}: Self-distilled Region-based {CLIP} for Better Fine-grained Understanding},\\nauthor={Dong Jing and Xiaolong He and Yutian Luo and Nanyi Fei and Guoxing Yang and Wei Wei and Huiwen Zhao and Zhiwu Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nExI4FuKWD}\\n}'}, 'paperhash': {'value': 'jing|fineclip_selfdistilled_regionbased_clip_for_better_finegrained_understanding'}},forum = 'nExI4FuKWD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13709/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13709/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13709/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13709/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nEqU0iCa0s',number = 1051,cdate = 1714360572462,pdate = 1727287650646,odate = 1730873844990,mdate = 1730873845002,tcdate = 1714360572462,tmdate = 1730873845002,ddate = None,content = {'title': {'value': 'Self-Distilled Depth Refinement with Noisy Poisson Fusion'}, 'authors': {'value': ['Jiaqi Li', 'Yiran Wang', 'Jinghong Zheng', 'Zihao Huang', 'Ke Xian', 'Zhiguo Cao', 'Jianming Zhang']}, 'authorids': {'value': ['~Jiaqi_Li7', '~Yiran_Wang3', '~Jinghong_Zheng1', '~Zihao_Huang2', '~Ke_Xian2', '~Zhiguo_Cao1', '~Jianming_Zhang1']}, 'keywords': {'value': ['Depth refinement', 'Noisy Poisson fusion', 'Self-distilled training']}, 'abstract': {'value': 'Depth refinement aims to infer high-resolution depth with fine-grained edges and details, refining low-resolution results of depth estimation models. The prevailing methods adopt tile-based manners by merging numerous patches, which lacks efficiency and produces inconsistency. Besides, prior arts suffer from fuzzy depth boundaries and limited generalizability. Analyzing the fundamental reasons for these limitations, we model depth refinement as a noisy Poisson fusion problem with local inconsistency and edge deformation noises. We propose the Self-distilled Depth Refinement (SDDR) framework to enforce robustness against the noises, which mainly consists of depth edge representation and edge-based guidance. With noisy depth predictions as input, SDDR generates low-noise depth edge representations as pseudo-labels by coarse-to-fine self-distillation. Edge-based guidance with edge-guided gradient loss and edge-based fusion loss serves as the optimization objective equivalent to Poisson fusion. When depth maps are better refined, the labels also become more noise-free. Our model can acquire strong robustness to the noises, achieving significant improvements in accuracy, edge quality, efficiency, and generalizability on five different benchmarks. Moreover, directly training another model with edge labels produced by SDDR brings improvements, suggesting that our method could help with training robust refinement models in future works.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/652424a5ba1133321596e1672a29b0226cb640f8.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024selfdistilled,\\ntitle={Self-Distilled Depth Refinement with Noisy Poisson Fusion},\\nauthor={Jiaqi Li and Yiran Wang and Jinghong Zheng and Zihao Huang and Ke Xian and Zhiguo Cao and Jianming Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nEqU0iCa0s}\\n}'}, 'TLDR': {'value': 'SDDR proposes to model the depth refinement task by noisy Poisson fusion and train the refinement network in a self-distillation paradigm, achieving both high accuracy and efficiency.'}, 'paperhash': {'value': 'li|selfdistilled_depth_refinement_with_noisy_poisson_fusion'}},forum = 'nEqU0iCa0s',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1051/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1051/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1051/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1051/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nBrnfYeKf9',number = 9207,cdate = 1715676377462,pdate = 1727287903789,odate = 1730873918738,mdate = 1730873918758,tcdate = 1715676377462,tmdate = 1730873918758,ddate = None,content = {'title': {'value': 'Rad-NeRF: Ray-decoupled Training of Neural Radiance Field'}, 'authors': {'value': ['Lidong Guo', 'Xuefei Ning', 'Yonggan Fu', 'Tianchen Zhao', 'Zhuoliang Kang', 'Jincheng Yu', 'Yingyan Celine Lin', 'Yu Wang']}, 'authorids': {'value': ['~Lidong_Guo1', '~Xuefei_Ning1', '~Yonggan_Fu1', '~Tianchen_Zhao2', '~Zhuoliang_Kang3', '~Jincheng_Yu2', '~Yingyan_Celine_Lin1', '~Yu_Wang3']}, 'keywords': {'value': ['Neural rendering field', 'Mutual learning', 'Novel view synthesis', 'Soft gate module', 'Complex scenes with occlusions']}, 'abstract': {'value': 'Although the neural radiance field (NeRF) exhibits high-fidelity visualization on the rendering task, it still suffers from rendering defects, especially in complex scenes. In this paper, we delve into the reason for the unsatisfactory performance and conjecture that it comes from interference in the training process. Due to occlusions in complex scenes, a 3D point may be invisible to some rays. On such a point, training with those rays that do not contain valid information about the point might interfere with the NeRF training. Based on the above intuition, we decouple the training process of NeRF in the ray dimension softly and propose a Ray-decoupled Training Framework for neural rendering (Rad-NeRF). Specifically, we construct an ensemble of sub-NeRFs and train a soft gate module to assign the gating scores to these sub-NeRFs based on specific rays. The gate module is jointly optimized with the sub-NeRF ensemble to learn the preference of sub-NeRFs for different rays automatically. Furthermore, we introduce depth-based mutual learning to enhance the rendering consistency among multiple sub-NeRFs and mitigate the depth ambiguity. Experiments on five datasets demonstrate that Rad-NeRF can enhance the rendering performance across a wide range of scene types compared with existing single-NeRF and multi-NeRF methods. With only 0.2% extra parameters, Rad-NeRF improves rendering performance by up to 1.5dB. Code is available at https://github.com/thu-nics/Rad-NeRF.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/156d15e45afe9ab5b87a685535ea6e3d6c612495.pdf'}, 'supplementary_material': {'value': '/attachment/3c53f1f3f2d2618e4488169a0b5b36d4e0c5c47b.zip'}, '_bibtex': {'value': '@inproceedings{\\nguo2024radnerf,\\ntitle={Rad-Ne{RF}: Ray-decoupled Training of Neural Radiance Field},\\nauthor={Lidong Guo and Xuefei Ning and Yonggan Fu and Tianchen Zhao and Zhuoliang Kang and Jincheng Yu and Yingyan Celine Lin and Yu Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nBrnfYeKf9}\\n}'}, 'paperhash': {'value': 'guo|radnerf_raydecoupled_training_of_neural_radiance_field'}},forum = 'nBrnfYeKf9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9207/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9207/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9207/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9207/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nBjmMF2IZU',number = 2395,cdate = 1715065052675,pdate = 1727287689331,odate = 1730873857698,mdate = 1730873857718,tcdate = 1715065052675,tmdate = 1730873857718,ddate = None,content = {'title': {'value': 'Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning'}, 'authors': {'value': ['Yuexiang Zhai', 'Hao Bai', 'Zipeng Lin', 'Jiayi Pan', 'Shengbang Tong', 'Yifei Zhou', 'Alane Suhr', 'Saining Xie', 'Yann LeCun', 'Yi Ma', 'Sergey Levine']}, 'authorids': {'value': ['~Yuexiang_Zhai1', '~Hao_Bai1', '~Zipeng_Lin1', '~Jiayi_Pan1', '~Shengbang_Tong1', '~Yifei_Zhou1', '~Alane_Suhr1', '~Saining_Xie2', '~Yann_LeCun1', '~Yi_Ma4', '~Sergey_Levine1']}, 'keywords': {'value': ['large vision language model', 'reinforcement learning']}, 'TLDR': {'value': 'We directly use reinforcement learning to fine-tune large vision-language model using task-specific reward function'}, 'abstract': {'value': 'Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/84c596658e84793b911234c08d43b5038aa98129.pdf'}, 'supplementary_material': {'value': '/attachment/3c38e47903a6d4e66fc398bb8297e61c56b73238.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhai2024finetuning,\\ntitle={Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning},\\nauthor={Yuexiang Zhai and Hao Bai and Zipeng Lin and Jiayi Pan and Shengbang Tong and Yifei Zhou and Alane Suhr and Saining Xie and Yann LeCun and Yi Ma and Sergey Levine},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nBjmMF2IZU}\\n}'}, 'paperhash': {'value': 'zhai|finetuning_large_visionlanguage_models_as_decisionmaking_agents_via_reinforcement_learning'}},forum = 'nBjmMF2IZU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2395/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2395/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2395/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2395/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nBhfIcDnRP',number = 2588,cdate = 1715108497765,pdate = 1727287694663,odate = 1730873859297,mdate = 1736995588136,tcdate = 1715108497765,tmdate = 1736995588136,ddate = None,content = {'title': {'value': 'Efficient Graph Matching for Correlated Stochastic Block Models'}, 'authors': {'value': ['Shuwen Chai', 'Miklos Z. Racz']}, 'authorids': {'value': ['~Shuwen_Chai1', '~Miklos_Z._Racz1']}, 'keywords': {'value': ['Graph matching', 'correlated random graphs', 'stochastic block model', 'community recovery', 'subgraph counting']}, 'TLDR': {'value': 'We give the first efficient algorithm for graph matching for correlated stochastic block models with two balanced communities.'}, 'abstract': {'value': \"We study learning problems on correlated stochastic block models with two balanced communities. Our main result gives the first efficient algorithm for graph matching in this setting. In the most interesting regime where the average degree is logarithmic in the number of vertices, this algorithm correctly matches all but a vanishing fraction of vertices with high probability, whenever the edge correlation parameter $s$ satisfies $s^2 > \\\\alpha \\\\approx 0.338$, where $\\\\alpha$ is Otter's tree-counting constant. Moreover, we extend this to an efficient algorithm for exact graph matching whenever this is information-theoretically possible, positively resolving an open problem of Rácz and Sridhar (NeurIPS 2021). Our algorithm generalizes the recent breakthrough work of Mao, Wu, Xu, and Yu (STOC 2023), which is based on centered subgraph counts of a large family of trees termed chandeliers. A major technical challenge that we overcome is dealing with the additional estimation errors that are necessarily present due to the fact that, in relevant parameter regimes, the latent community partition cannot be exactly recovered from a single graph. As an application of our results, we give an efficient algorithm for exact community recovery using multiple correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1184b0e42393069c86765b9ce4e15d9e1b6ba56c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchai2024efficient,\\ntitle={Efficient Graph Matching for Correlated Stochastic Block Models},\\nauthor={Shuwen Chai and Miklos Z. Racz},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nBhfIcDnRP}\\n}'}, 'paperhash': {'value': 'chai|efficient_graph_matching_for_correlated_stochastic_block_models'}},forum = 'nBhfIcDnRP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2588/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2588/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2588/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2588/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nBQHTBVnfr',number = 17809,cdate = 1715781998649,pdate = 1727288162398,odate = 1730873986403,mdate = 1730873986425,tcdate = 1715781998649,tmdate = 1730873986425,ddate = None,content = {'title': {'value': 'Geometric Analysis of Nonlinear Manifold Clustering'}, 'authors': {'value': ['Nimita Shinde', 'Tianjiao Ding', 'Daniel Robinson', 'Rene Vidal']}, 'authorids': {'value': ['~Nimita_Shinde1', '~Tianjiao_Ding1', '~Daniel_Robinson2', '~Rene_Vidal1']}, 'keywords': {'value': ['Manifold clustering', 'large scale clustering']}, 'TLDR': {'value': 'We provide a model for manifold clustering with theoretical guarantees'}, 'abstract': {'value': 'Manifold clustering is an important problem in motion and video segmentation, natural image clustering, and other applications where high-dimensional data lie on multiple, low-dimensional, nonlinear manifolds. While current state-of-the-art methods on large-scale datasets such as CIFAR provide good empirical performance, they do not have any proof of theoretical correctness. In this work, we propose a method that clusters data belonging to a union of nonlinear manifolds. Furthermore, for a given input data sample $y$ belonging to the $l$th manifold $\\\\mathcal{M}_l$, we provide geometric conditions that guarantee a manifold-preserving representation of $y$ can be recovered from the solution to the proposed model. The geometric conditions require that (i) $\\\\mathcal{M}_l$ is well-sampled in the neighborhood of $y$, with the sampling density given as a function of the curvature, and (ii) $\\\\mathcal{M}_l$ is sufficiently separated from the other manifolds. In addition to providing proof of correctness in this setting, a numerical comparison with state-of-the-art methods on CIFAR datasets shows that our method performs competitively although marginally worse than methods without'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/832da95c752d83d50879b1a914a93fc1e5c54fce.pdf'}, 'supplementary_material': {'value': '/attachment/94c24b54fa0a75f41c505f0dfdb8fa9fd669eb49.zip'}, '_bibtex': {'value': '@inproceedings{\\nshinde2024geometric,\\ntitle={Geometric Analysis of Nonlinear Manifold Clustering},\\nauthor={Nimita Shinde and Tianjiao Ding and Daniel Robinson and Rene Vidal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nBQHTBVnfr}\\n}'}, 'paperhash': {'value': 'shinde|geometric_analysis_of_nonlinear_manifold_clustering'}},forum = 'nBQHTBVnfr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17809/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17809/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17809/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17809/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nBOdYBptWW',number = 1141,cdate = 1714429511971,pdate = 1727287653126,odate = 1730873845833,mdate = 1735968109625,tcdate = 1714429511971,tmdate = 1735968109625,ddate = None,content = {'title': {'value': 'UniTS: A Unified Multi-Task Time Series Model'}, 'authors': {'value': ['Shanghua Gao', 'Teddy Koker', 'Owen Queen', 'Thomas Hartvigsen', 'Theodoros Tsiligkaridis', 'Marinka Zitnik']}, 'authorids': {'value': ['~Shanghua_Gao1', '~Teddy_Koker1', '~Owen_Queen1', '~Thomas_Hartvigsen1', '~Theodoros_Tsiligkaridis1', '~Marinka_Zitnik1']}, 'keywords': {'value': ['Time Series Forecasting', 'Time Series Classification', 'Time Series Imputation', 'Time Series Anomaly Detection', 'Prompt Learning', 'Pretraining', 'Few-Shot Learning', 'Unified Model', 'Multi-task', 'Task Tokenization']}, 'abstract': {'value': 'Although pre-trained transformers and reprogrammed text-based LLMs have shown strong performance on time series tasks, the best-performing architectures vary widely across tasks, with most models narrowly focused on specific areas, such as time series forecasting. Unifying predictive and generative time series tasks within a single model remains challenging. We introduce UniTS, a unified multi-task time series model that utilizes task tokenization to integrate predictive and generative tasks into a single framework. UniTS employs a modified transformer block to capture universal time series representations, enabling transferability from a heterogeneous, multi-domain pre-training dataset—characterized by diverse dynamic patterns, sampling rates, and temporal scales—to a wide range of downstream datasets with varied task specifications and data domains. Tested on 38 datasets across human activity sensors, healthcare, engineering, and finance, UniTS achieves superior performance compared to 12 forecasting models, 20 classification models, 18 anomaly detection models, and 16 imputation models, including adapted text-based LLMs. UniTS also demonstrates strong few-shot and prompt capabilities when applied to new domains and tasks. In single-task settings, UniTS outperforms competitive task-specialized time series models. Code and datasets are available at https://github.com/mims-harvard/UniTS.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'UniTS is a unified multi-task time series model that can process predictive and generative tasks across time series domains.'}, 'pdf': {'value': '/pdf/76cc318f954f6e20e4694bd3cfc093467379d42d.pdf'}, '_bibtex': {'value': '@inproceedings{\\ngao2024units,\\ntitle={Uni{TS}: A Unified Multi-Task Time Series Model},\\nauthor={Shanghua Gao and Teddy Koker and Owen Queen and Thomas Hartvigsen and Theodoros Tsiligkaridis and Marinka Zitnik},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nBOdYBptWW}\\n}'}, 'paperhash': {'value': 'gao|units_a_unified_multitask_time_series_model'}},forum = 'nBOdYBptWW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1141/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1141/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1141/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1141/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nAnEStxyfy',number = 7466,cdate = 1715623319081,pdate = 1727287847965,odate = 1730873903367,mdate = 1730873903386,tcdate = 1715623319081,tmdate = 1730873903386,ddate = None,content = {'title': {'value': 'Generating Highly Designable Proteins with Geometric Algebra Flow Matching'}, 'authors': {'value': ['Simon Wagner', 'Leif Seute', 'Vsevolod Viliuga', 'Nicolas Wolf', 'Frauke Gräter', 'Jan Stuehmer']}, 'authorids': {'value': ['~Simon_Wagner1', '~Leif_Seute1', '~Vsevolod_Viliuga1', '~Nicolas_Wolf1', '~Frauke_Gräter1', '~Jan_Stuehmer1']}, 'keywords': {'value': ['Proteins; Flow Matching; Geometric Algebra; Generative models; Equivariant models; Protein design; AlphaFold; Local frames']}, 'TLDR': {'value': 'Geometric Algebra based architecture for protein structure that achieves high designability and structural diversity as flow matching model for protein generation.'}, 'abstract': {'value': 'We introduce a generative model for protein backbone design utilizing geometric products and higher order message passing. In particular, we propose Clifford Frame Attention (CFA), an extension of the invariant point attention (IPA) architecture from AlphaFold2, in which the backbone residue frames and geometric features are represented in the projective geometric algebra. This enables to construct geometrically expressive messages between residues, including higher order terms, using the bilinear operations of the algebra. We evaluate our architecture by incorporating it into the framework of FrameFlow, a state-of-the-art flow matching model for protein backbone generation. The proposed model achieves high designability, diversity and novelty, while also sampling protein backbones that follow the statistical distribution of secondary structure elements found in naturally occurring proteins, a property so far only insufficiently achieved by many state-of-the-art generative models.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5cb59c78931cfaf13f75476f9422bcff4723d2eb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwagner2024generating,\\ntitle={Generating Highly Designable Proteins with Geometric Algebra Flow Matching},\\nauthor={Simon Wagner and Leif Seute and Vsevolod Viliuga and Nicolas Wolf and Frauke Gr{\\\\\"a}ter and Jan Stuehmer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nAnEStxyfy}\\n}'}, 'paperhash': {'value': 'wagner|generating_highly_designable_proteins_with_geometric_algebra_flow_matching'}},forum = 'nAnEStxyfy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7466/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7466/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7466/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7466/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'nAIhvNy15T',number = 6836,cdate = 1715605853321,pdate = 1727287826934,odate = 1730873896753,mdate = 1730873896765,tcdate = 1715605853321,tmdate = 1730873896765,ddate = None,content = {'title': {'value': 'Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models'}, 'authors': {'value': ['Tuomas Kynkäänniemi', 'Miika Aittala', 'Tero Karras', 'Samuli Laine', 'Timo Aila', 'Jaakko Lehtinen']}, 'authorids': {'value': ['~Tuomas_Kynkäänniemi1', '~Miika_Aittala2', '~Tero_Karras1', '~Samuli_Laine1', '~Timo_Aila1', '~Jaakko_Lehtinen1']}, 'keywords': {'value': ['generative models', 'diffusion models', 'classifier-free guidance']}, 'TLDR': {'value': 'We improve quantitative and qualitative results of image-generating diffusion models by applying classifier-free guidance in a limited interval.'}, 'abstract': {'value': 'Guidance is a crucial technique for extracting the best performance out of image-generating diffusion models. Traditionally, a constant guidance weight has been applied throughout the sampling chain of an image. We show that guidance is clearly harmful toward the beginning of the chain (high noise levels), largely unnecessary toward the end (low noise levels), and only beneficial in the middle. We thus restrict it to a specific range of noise levels, improving both the inference speed and result quality. This limited guidance interval improves the record FID in ImageNet-512 significantly, from 1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial across different sampler parameters, network architectures, and datasets, including the large-scale setting of Stable Diffusion XL. We thus suggest exposing the guidance interval as a hyperparameter in all diffusion models that use guidance.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0e4e2aaba5bb103cca5994d4a6f202229790e0f6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkynk{\\\\\"a}{\\\\\"a}nniemi2024applying,\\ntitle={Applying Guidance in a Limited Interval Improves Sample and Distribution Quality in Diffusion Models},\\nauthor={Tuomas Kynk{\\\\\"a}{\\\\\"a}nniemi and Miika Aittala and Tero Karras and Samuli Laine and Timo Aila and Jaakko Lehtinen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nAIhvNy15T}\\n}'}, 'paperhash': {'value': 'kynkäänniemi|applying_guidance_in_a_limited_interval_improves_sample_and_distribution_quality_in_diffusion_models'}},forum = 'nAIhvNy15T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6836/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6836/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6836/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6836/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'nA4Q983a1v',number = 5217,cdate = 1715515275639,pdate = 1727287777113,odate = 1730873882934,mdate = 1730873882947,tcdate = 1715515275639,tmdate = 1730873882947,ddate = None,content = {'title': {'value': 'Recurrent Reinforcement Learning with Memoroids'}, 'authors': {'value': ['Steven Morad', 'Chris Lu', 'Ryan Kortvelesy', 'Stephan Liwicki', 'Jakob Nicolaus Foerster', 'Amanda Prorok']}, 'authorids': {'value': ['~Steven_Morad1', '~Chris_Lu1', '~Ryan_Kortvelesy1', '~Stephan_Liwicki3', '~Jakob_Nicolaus_Foerster1', '~Amanda_Prorok1']}, 'keywords': {'value': ['POMDP', 'reinforcement learning', 'memory models', 'recurrent neural network']}, 'TLDR': {'value': 'We propose a new memory model framework and batching method to improve time, space, and sample efficiency in POMDPs'}, 'abstract': {'value': 'Memory models such as Recurrent Neural Networks (RNNs) and Transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models called Linear Recurrent Models. We discover that the recurrent update of these models resembles a monoid, leading us to reformulate existing models using a novel monoid-based framework that we call memoroids. We revisit the traditional approach to batching in recurrent reinforcement learning, highlighting theoretical and empirical deficiencies. We leverage memoroids to propose a batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in reinforcement learning.'}, 'pdf': {'value': '/pdf/35e29d8eea598ce21a9717776bdf8cdf9000f2bb.pdf'}, 'supplementary_material': {'value': '/attachment/c1bbdc4e84c73bf23d26b078588310eddfa63161.zip'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nmorad2024recurrent,\\ntitle={Recurrent Reinforcement Learning with Memoroids},\\nauthor={Steven Morad and Chris Lu and Ryan Kortvelesy and Stephan Liwicki and Jakob Nicolaus Foerster and Amanda Prorok},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=nA4Q983a1v}\\n}'}, 'paperhash': {'value': 'morad|recurrent_reinforcement_learning_with_memoroids'}},forum = 'nA4Q983a1v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5217/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5217/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5217/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5217/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n9xVaQMJNK',number = 1695,cdate = 1714749841623,pdate = 1727287668603,odate = 1730873851083,mdate = 1730873851097,tcdate = 1714749841623,tmdate = 1730873851097,ddate = None,content = {'title': {'value': 'Few-Shot Adversarial Prompt Learning on Vision-Language Models'}, 'authors': {'value': ['Yiwei Zhou', 'Xiaobo Xia', 'Zhiwei Lin', 'Bo Han', 'Tongliang Liu']}, 'authorids': {'value': ['~Yiwei_Zhou2', '~Xiaobo_Xia1', '~Zhiwei_Lin3', '~Bo_Han1', '~Tongliang_Liu1']}, 'keywords': {'value': ['Adversarial Robustness', 'Prompt Learning', 'Vision-Language Models']}, 'abstract': {'value': 'The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1\\\\% training data. Code is available at: https://github.com/lionel-w2/FAP.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/982358a60e8e49c5548679470c88d0032f141142.pdf'}, 'supplementary_material': {'value': '/attachment/8e0e03e69725e20fbb54ff650ededbc7423c6235.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024fewshot,\\ntitle={Few-Shot Adversarial Prompt Learning on Vision-Language Models},\\nauthor={Yiwei Zhou and Xiaobo Xia and Zhiwei Lin and Bo Han and Tongliang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n9xVaQMJNK}\\n}'}, 'paperhash': {'value': 'zhou|fewshot_adversarial_prompt_learning_on_visionlanguage_models'}},forum = 'n9xVaQMJNK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1695/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1695/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1695/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1695/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n60xBFZWrk',number = 6401,cdate = 1715592428407,pdate = 1727287813284,odate = 1730873893255,mdate = 1730873893273,tcdate = 1715592428407,tmdate = 1730873893273,ddate = None,content = {'title': {'value': 'Hyperbolic Embeddings of Supervised Models'}, 'authors': {'value': ['Richard Nock', 'Ehsan Amid', 'Frank Nielsen', 'Alexander Soen', 'Manfred K Warmuth']}, 'authorids': {'value': ['~Richard_Nock1', '~Ehsan_Amid1', '~Frank_Nielsen1', '~Alexander_Soen1', '~Manfred_K_Warmuth1']}, 'keywords': {'value': ['Hyperbolic geometry', 'supervised model embedding', 'decision trees', 'boosting']}, 'TLDR': {'value': 'A full-fledged solution to embed supervised *models* in hyperbolic geometry, and more'}, 'abstract': {'value': \"Models of hyperbolic geometry have been successfully used in ML for two main tasks: embedding *models* in unsupervised learning (*e.g.* hierarchies) and embedding *data*. \\nTo our knowledge, there are no approaches that provide embeddings for supervised models; even when hyperbolic geometry provides convenient properties for expressing popular hypothesis classes, such as decision trees (and ensembles).\\nIn this paper, we propose a full-fledged solution to the problem in three independent contributions. The first linking the theory of losses for class probability estimation to hyperbolic embeddings in Poincar\\\\'e disk model. The second resolving an issue for a clean, unambiguous embedding of (ensembles of) decision trees in this model. The third showing how to smoothly tweak the Poincar\\\\'e hyperbolic distance to improve its encoding and visualization properties near the border of the disk, a crucial region for our application, while keeping hyperbolicity.\\nThis last step has substantial independent interest as it is grounded in a generalization of Leibniz-Newton's fundamental Theorem of calculus.\"}, 'pdf': {'value': '/pdf/462449e98af15b416d5418ce67b4921a32541c11.pdf'}, 'supplementary_material': {'value': '/attachment/029dfc73d375c21608ec7602bc715ab553f0f50b.zip'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nnock2024hyperbolic,\\ntitle={Hyperbolic Embeddings of Supervised Models},\\nauthor={Richard Nock and Ehsan Amid and Frank Nielsen and Alexander Soen and Manfred K Warmuth},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n60xBFZWrk}\\n}'}, 'paperhash': {'value': 'nock|hyperbolic_embeddings_of_supervised_models'}},forum = 'n60xBFZWrk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6401/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6401/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6401/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6401/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'n5lLSskwtu',number = 12013,cdate = 1715714819537,pdate = 1727287991567,odate = 1730873944419,mdate = 1736911758434,tcdate = 1715714819537,tmdate = 1736911758434,ddate = None,content = {'title': {'value': 'Evidential Mixture Machines: Deciphering Multi-Label Correlations for Active Learning Sensitivity'}, 'authors': {'value': ['Dayou Yu', 'Minghao Li', 'Weishi Shi', 'Qi Yu']}, 'authorids': {'value': ['~Dayou_Yu1', '~Minghao_Li12', '~Weishi_Shi2', '~Qi_Yu1']}, 'keywords': {'value': ['Active learning', 'multi-label classification']}, 'abstract': {'value': \"Multi-label active learning is a crucial yet challenging area in contemporary machine learning, often complicated by a large and sparse label space. This challenge is further exacerbated in active learning scenarios where labeling resources are constrained. Drawing inspiration from existing mixture of Bernoulli models, which efficiently compress the label space into a more manageable weight coefficient space by learning correlated Bernoulli components, we propose a novel model called Evidential Mixture Machines (EMM). Our model leverages mixture components derived from unsupervised learning in the label space and improves prediction accuracy by predicting weight coefficients following the evidential learning paradigm. These coefficients are aggregated as proxy pseudo counts to enhance component offset predictions. The evidential learning approach provides an uncertainty-aware connection between input features and the predicted coefficients and components. Additionally, our method combines evidential uncertainty with predicted label embedding covariances for active sample selection, creating a richer, multi-source uncertainty metric beyond traditional uncertainty scores. Experiments on synthetic datasets show the effectiveness of evidential uncertainty prediction and EMM's capability to capture label correlations through predicted components. Further testing on real-world datasets demonstrates improved performance compared to existing multi-label active learning methods.\"}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/951c4ba4cdcd1d3b044b89eca3c505a7af66aabd.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyu2024evidential,\\ntitle={Evidential Mixture Machines: Deciphering Multi-Label Correlations for Active Learning Sensitivity},\\nauthor={Dayou Yu and Minghao Li and Weishi Shi and Qi Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n5lLSskwtu}\\n}'}, 'paperhash': {'value': 'yu|evidential_mixture_machines_deciphering_multilabel_correlations_for_active_learning_sensitivity'}},forum = 'n5lLSskwtu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12013/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n5R6TvBVcX',number = 12439,cdate = 1715722078935,pdate = 1727288006589,odate = 1730873949121,mdate = 1730873949133,tcdate = 1715722078935,tmdate = 1730873949133,ddate = None,content = {'title': {'value': 'WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models'}, 'authors': {'value': ['Liwei Jiang', 'Kavel Rao', 'Seungju Han', 'Allyson Ettinger', 'Faeze Brahman', 'Sachin Kumar', 'Niloofar Mireshghallah', 'Ximing Lu', 'Maarten Sap', 'Yejin Choi', 'Nouha Dziri']}, 'authorids': {'value': ['~Liwei_Jiang2', '~Kavel_Rao1', '~Seungju_Han2', '~Allyson_Ettinger1', '~Faeze_Brahman1', '~Sachin_Kumar1', '~Niloofar_Mireshghallah1', '~Ximing_Lu1', '~Maarten_Sap1', '~Yejin_Choi1', '~Nouha_Dziri2']}, 'keywords': {'value': ['Red-teaming', 'AI Safety', 'Safety Training', 'LLM Defense', 'Safety Training Data', 'Adversarial Training', 'Adversarial Attacks', 'Jailbreak']}, 'abstract': {'value': 'We introduce WildTeaming, an automatic red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks.\\nCompared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system.  WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in more diverse and successful adversarial attacks compared to state-of-the-art jailbreaking methods. \\n\\nWhile there exist many datasets for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (both vanilla and adversarial) and 2) benign queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce an efficient jailbreaking method along with a large-scale safety dataset (WildJailbreak). Our experiments show that training on WildJailbreak leads to substantial safety improvements while minimally affecting general capabilities.'}, 'pdf': {'value': '/pdf/5c0e189c5b92a109f691a752108334b171f24840.pdf'}, '_bibtex': {'value': '@inproceedings{\\njiang2024wildteaming,\\ntitle={WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models},\\nauthor={Liwei Jiang and Kavel Rao and Seungju Han and Allyson Ettinger and Faeze Brahman and Sachin Kumar and Niloofar Mireshghallah and Ximing Lu and Maarten Sap and Yejin Choi and Nouha Dziri},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n5R6TvBVcX}\\n}'}, 'paperhash': {'value': 'jiang|wildteaming_at_scale_from_inthewild_jailbreaks_to_adversarially_safer_language_models'}},forum = 'n5R6TvBVcX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12439/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12439/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12439/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12439/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n2dvAKKQoM',number = 8471,cdate = 1715661495760,pdate = 1727287880641,odate = 1730873912621,mdate = 1730873912638,tcdate = 1715661495760,tmdate = 1730873912638,ddate = None,content = {'title': {'value': 'Task-oriented Time Series Imputation Evaluation via Generalized Representers'}, 'authors': {'value': ['Zhixian Wang', 'Linxiao Yang', 'Liang Sun', 'Qingsong Wen', 'Yi Wang']}, 'authorids': {'value': ['~Zhixian_Wang2', '~Linxiao_Yang1', '~Liang_Sun2', '~Qingsong_Wen2', '~Yi_Wang43']}, 'keywords': {'value': ['time series imputation', 'downstream task oriented imputation evaluation']}, 'abstract': {'value': 'Time series analysis is widely used in many fields such as power energy, economics, and transportation, including different tasks such as forecasting, anomaly detection, classification, etc. Missing values are widely observed in these tasks, and often leading to unpredictable negative effects on existing methods, hindering their further application. In response to this situation, existing time series imputation methods mainly focus on restoring sequences based on their data characteristics, while ignoring the performance of the restored sequences in downstream tasks. Considering different requirements of downstream tasks (e.g., forecasting), this paper proposes an efficient downstream task-oriented time series imputation evaluation approach. By combining time series imputation with neural network models used for downstream tasks, the gain of different imputation strategies on downstream tasks is estimated without retraining, and the most favorable imputation value for downstream tasks is given by combining different imputation strategies according to the estimated gain.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ad41397f6b878c343bd484ca0cb2057edfba1ab7.pdf'}, 'supplementary_material': {'value': '/attachment/c68fae5da8e6fa8be6a7a3d1aad1ec1a4d5424f5.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024taskoriented,\\ntitle={Task-oriented Time Series Imputation Evaluation via Generalized Representers},\\nauthor={Zhixian Wang and Linxiao Yang and Liang Sun and Qingsong Wen and Yi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n2dvAKKQoM}\\n}'}, 'paperhash': {'value': 'wang|taskoriented_time_series_imputation_evaluation_via_generalized_representers'}},forum = 'n2dvAKKQoM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8471/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8471/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8471/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8471/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n0arS0DDot',number = 10929,cdate = 1715699164461,pdate = 1727287955318,odate = 1730873933392,mdate = 1730873933406,tcdate = 1715699164461,tmdate = 1730873933406,ddate = None,content = {'title': {'value': 'BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference'}, 'authors': {'value': ['Changwoo Lee', 'Soo Min Kwon', 'Qing Qu', 'Hun-Seok Kim']}, 'authorids': {'value': ['~Changwoo_Lee2', '~Soo_Min_Kwon1', '~Qing_Qu2', '~Hun-Seok_Kim1']}, 'keywords': {'value': ['Efficiency', 'Compression', 'Low Rank', 'Pruning', 'Matrix Factorization', 'Structured Matrix', 'Acceleration', 'Optimization', 'Transformer', 'Large Language Model', 'Diffusion Model', 'Vision Model', 'Preconditioned Gradient Descent']}, 'TLDR': {'value': 'We improve DNN inference efficiency by learning low-dimensional weight structures through BLAST.'}, 'abstract': {'value': 'Large-scale foundation models have demonstrated exceptional performance in language and vision tasks. However, the numerous dense matrix-vector operations involved in these large networks pose significant computational challenges during inference. To address these challenges, we introduce the Block-Level Adaptive STructured (BLAST) matrix, designed to learn and leverage efficient structures prevalent in the weight matrices of linear layers within deep learning models. Compared to existing structured matrices, the BLAST matrix offers substantial flexibility, as it can represent various types of structures that are either learned from data or computed from pre-existing weight matrices. We demonstrate the efficiency of using the BLAST matrix for compressing both language and vision tasks, showing that (i) for medium-sized models such as ViT and GPT-2, training with BLAST weights boosts performance while reducing complexity by 70\\\\% and 40\\\\%, respectively; and (ii) for large foundation models such as Llama-7B and DiT-XL, the BLAST matrix achieves a 2x compression while exhibiting the lowest performance degradation among all tested structured matrices. Our code is available at https://github.com/changwoolee/BLAST.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/245959aba0605ea588b564289cc90eb61c7e3dd9.pdf'}, 'supplementary_material': {'value': '/attachment/32ffae24f835ceb9bc516fe294dc933ca5ede97a.zip'}, '_bibtex': {'value': '@inproceedings{\\nlee2024blast,\\ntitle={{BLAST}: Block-Level Adaptive Structured Matrices for Efficient Deep Neural Network Inference},\\nauthor={Changwoo Lee and Soo Min Kwon and Qing Qu and Hun-Seok Kim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n0arS0DDot}\\n}'}, 'paperhash': {'value': 'lee|blast_blocklevel_adaptive_structured_matrices_for_efficient_deep_neural_network_inference'}},forum = 'n0arS0DDot',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10929/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10929/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10929/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10929/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'n01yLUy7Mj',number = 3485,cdate = 1715290029985,pdate = 1727287722332,odate = 1730873866867,mdate = 1736501776684,tcdate = 1715290029985,tmdate = 1736501776684,ddate = None,content = {'title': {'value': \"Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge\"}, 'authors': {'value': ['Fawaz Sammani', 'Nikos Deligiannis']}, 'authorids': {'value': ['~Fawaz_Sammani1', '~Nikos_Deligiannis1']}, 'keywords': {'value': ['CLIP', 'vision-language', 'explainability', 'interpretability', 'mutual knowledge']}, 'abstract': {'value': 'Contrastive Language-Image Pretraining (CLIP) performs zero-shot image classification by mapping images and textual class representation into a shared embedding space, then retrieving the class closest to the image. This work provides a new approach for interpreting CLIP models for image classification from the lens of mutual knowledge between the two modalities. Specifically, we ask: what concepts do both vision and language CLIP encoders learn in common that influence the joint embedding space, causing points to be closer or further apart? We answer this question via an approach of textual concept-based explanations, showing their effectiveness, and perform an analysis encompassing a pool of 13 CLIP models varying in architecture, size and pretraining datasets. We explore those different aspects in relation to mutual knowledge, and analyze zero-shot predictions. Our approach demonstrates an effective and human-friendly way of understanding zero-shot classification decisions with CLIP.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5072f6b69ed42aad55e801718e130d64d252585e.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nsammani2024interpreting,\\ntitle={Interpreting and Analysing {CLIP}'s Zero-Shot Image Classification via Mutual Knowledge},\\nauthor={Fawaz Sammani and Nikos Deligiannis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=n01yLUy7Mj}\\n}\"}, 'paperhash': {'value': 'sammani|interpreting_and_analysing_clips_zeroshot_image_classification_via_mutual_knowledge'}},forum = 'n01yLUy7Mj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3485/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3485/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3485/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3485/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mxMvWwyBWe',number = 16814,cdate = 1715774612890,pdate = 1727288137630,odate = 1730873981525,mdate = 1737828157062,tcdate = 1715774612890,tmdate = 1737828157062,ddate = None,content = {'title': {'value': 'Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions'}, 'authors': {'value': ['Vinamra Benara', 'Chandan Singh', 'John Xavier Morris', 'Richard Antonello', 'Ion Stoica', 'Alexander Huth', 'Jianfeng Gao']}, 'authorids': {'value': ['~Vinamra_Benara1', '~Chandan_Singh1', '~John_Xavier_Morris1', '~Richard_Antonello1', '~Ion_Stoica1', '~Alexander_Huth1', '~Jianfeng_Gao1']}, 'keywords': {'value': ['fMRI', 'Encoding models', 'Neuroscience', 'Language neuroscience', 'Interpretability', 'Large language models', 'Explainability', 'Brain mapping']}, 'abstract': {'value': 'Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.\\n\\nWe use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8904189315579c1ea025bc4e2def975c66d0b873.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nbenara2024crafting,\\ntitle={Crafting Interpretable Embeddings for Language Neuroscience by Asking {LLM}s Questions},\\nauthor={Vinamra Benara and Chandan Singh and John Xavier Morris and Richard Antonello and Ion Stoica and Alexander Huth and Jianfeng Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mxMvWwyBWe}\\n}'}, 'paperhash': {'value': 'benara|crafting_interpretable_embeddings_for_language_neuroscience_by_asking_llms_questions'}},forum = 'mxMvWwyBWe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16814/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16814/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16814/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16814/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mwN1bbD5DQ',number = 14748,cdate = 1715753260315,pdate = 1727288078518,odate = 1730873968193,mdate = 1730873968211,tcdate = 1715753260315,tmdate = 1730873968211,ddate = None,content = {'title': {'value': 'Learning De-Biased Representations for Remote-Sensing Imagery'}, 'authors': {'value': ['Zichen Tian', 'Zhaozheng Chen', 'Qianru Sun']}, 'authorids': {'value': ['~Zichen_Tian1', '~Zhaozheng_Chen1', '~Qianru_Sun2']}, 'keywords': {'value': ['Adaptation', 'Long-tailed learning', 'Remote Sensing']}, 'TLDR': {'value': 'We propose debLoRA to adapt foundation models to data-scarce remote sensing domains with long-tailed distributions, by efficiently and unsupervisedly augmenting minor class features using major class features to mitigate representation bias.'}, 'abstract': {'value': 'Remote sensing (RS) imagery, which requires specialized satellites to collect and is difficult to annotate, suffers from data scarcity and class imbalance in certain spectrums. Due to their data scarcity, training large-scale RS models from scratch is unrealistic, and the alternative is to transfer pre-trained models by fine-tuning or a more data-efficient method LoRA. Due to class imbalance, transferred models exhibit strong bias, where features of the major class dominate over those of the minor class. In this paper, we propose debLoRA, a generic training approach that works with any LoRA variants to yield debiased features. It is an unsupervised learning approach that can diversify minor class features based on the shared attributes with major classes, where the attributes are obtained by a simple step of clustering. To evaluate it, we conduct extensive experiments in two transfer learning scenarios in the RS domain: from natural to optical RS images, and from optical RS to multi-spectrum RS images. We perform object classification and oriented object detection tasks on the optical RS dataset DOTA and the SAR dataset FUSRS. Results show that our debLoRA consistently surpasses prior arts across these RS adaptation settings, yielding up to 3.3 and 4.7 percentage points gains on the tail classes for natural $\\\\to$ optical RS and optical RS $\\\\to$ multi-spectrum RS adaptations, respectively, while preserving the performance on head classes, substantiating its efficacy and adaptability'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8bdd977870febf3be6a6b37a71c1e0754df0aff0.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntian2024learning,\\ntitle={Learning De-Biased Representations for Remote-Sensing Imagery},\\nauthor={Zichen Tian and Zhaozheng Chen and Qianru Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mwN1bbD5DQ}\\n}'}, 'paperhash': {'value': 'tian|learning_debiased_representations_for_remotesensing_imagery'}},forum = 'mwN1bbD5DQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14748/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14748/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14748/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14748/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'muYhNDlxWc',number = 13494,cdate = 1715740174230,pdate = 1727288040991,odate = 1730873958248,mdate = 1736759417236,tcdate = 1715740174230,tmdate = 1736759417236,ddate = None,content = {'title': {'value': 'MGF: Mixed Gaussian Flow for Diverse Trajectory Prediction'}, 'authors': {'value': ['Jiahe Chen', 'Jinkun Cao', 'Dahua Lin', 'Kris M. Kitani', 'Jiangmiao Pang']}, 'authorids': {'value': ['~Jiahe_Chen1', '~Jinkun_Cao1', '~Dahua_Lin1', '~Kris_M._Kitani1', '~Jiangmiao_Pang1']}, 'keywords': {'value': ['trajectory prediction', 'trajectory forecasting']}, 'abstract': {'value': 'To predict future trajectories, the normalizing flow with a standard Gaussian prior suffers from weak diversity. \\nThe ineffectiveness comes from the conflict between the fact of asymmetric and multi-modal distribution of likely outcomes and symmetric and single-modal original distribution and supervision losses.\\nInstead, we propose constructing a mixed Gaussian prior for a normalizing flow model for trajectory prediction.\\nThe prior is constructed by analyzing the trajectory patterns in the training samples without requiring extra annotations while showing better expressiveness and being multi-modal and asymmetric.\\nBesides diversity, it also provides better controllability for probabilistic trajectory generation.\\nWe name our method Mixed Gaussian Flow (MGF). It achieves state-of-the-art performance in the evaluation of both trajectory alignment and diversity on the popular UCY/ETH and SDD datasets. Code is available at https://github.com/mulplue/MGF.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a11044897deff6568a01c74aefb77e5928555379.pdf'}, 'supplementary_material': {'value': '/attachment/7491e7fad72cd61338a3859a4fbe9edce575d7a0.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024mgf,\\ntitle={{MGF}: Mixed Gaussian Flow for Diverse Trajectory Prediction},\\nauthor={Jiahe Chen and Jinkun Cao and Dahua Lin and Kris M. Kitani and Jiangmiao Pang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=muYhNDlxWc}\\n}'}, 'paperhash': {'value': 'chen|mgf_mixed_gaussian_flow_for_diverse_trajectory_prediction'}},forum = 'muYhNDlxWc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13494/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13494/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13494/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13494/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mtyy3Myyhz',number = 6348,cdate = 1715590637667,pdate = 1727287811904,odate = 1730873892714,mdate = 1730873892733,tcdate = 1715590637667,tmdate = 1730873892733,ddate = None,content = {'title': {'value': 'S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning'}, 'authors': {'value': ['Weihao Lin', 'Shengji Tang', 'Chong Yu', 'Peng Ye', 'Tao Chen']}, 'authorids': {'value': ['~Weihao_Lin1', '~Shengji_Tang1', '~Chong_Yu1', '~Peng_Ye4', '~Tao_Chen6']}, 'keywords': {'value': ['model pruning', 'knowledge distillation', 'model compression']}, 'TLDR': {'value': 'A novel differentiable mask pruning method  to alleviate discretization gap'}, 'abstract': {'value': 'Recently, differentiable mask pruning methods optimize the continuous relaxation architecture (soft network) as the proxy of the pruned discrete network (hard network) for superior sub-architecture search. However, due to the agnostic impact of the discretization process, the hard network struggles with the equivalent representational capacity as the soft network, namely discretization gap, which severely spoils the pruning performance. In this paper, we first investigate the discretization gap and propose a novel structural differentiable mask pruning framework named S2HPruner to bridge the discretization gap in a one-stage manner. In the training procedure, SH2Pruner forwards both the soft network and its corresponding hard network, then distills the hard network under the supervision of the soft network. To optimize the mask and prevent performance degradation, we propose a decoupled bidirectional knowledge distillation. It blocks the weight updating from the hard to the soft network while maintaining the gradient corresponding to the mask. Compared with existing pruning arts, S2HPruner achieves surpassing pruning performance without fine-tuning on comprehensive benchmarks, including CIFAR-100, Tiny ImageNet, and ImageNet with a variety of network architectures. Besides, investigation and analysis experiments explain the effectiveness of S2HPruner. Codes will be released soon.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/109cc2aaecb1682c381e6f746e4bcf164254f641.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024shpruner,\\ntitle={S2{HP}runer: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning},\\nauthor={Weihao Lin and Shengji Tang and Chong Yu and Peng Ye and Tao Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mtyy3Myyhz}\\n}'}, 'paperhash': {'value': 'lin|s2hpruner_softtohard_distillation_bridges_the_discretization_gap_in_pruning'}},forum = 'mtyy3Myyhz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6348/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6348/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6348/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6348/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mtOPyMkSRk',number = 11583,cdate = 1715706950986,pdate = 1727287975649,odate = 1730873940004,mdate = 1734598254590,tcdate = 1715706950986,tmdate = 1734598254590,ddate = None,content = {'title': {'value': 'A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation'}, 'authors': {'value': ['Jer Pelhan', 'Alan Lukezic', 'Vitjan Zavrtanik', 'Matej Kristan']}, 'authorids': {'value': ['~Jer_Pelhan2', '~Alan_Lukezic1', '~Vitjan_Zavrtanik1', '~Matej_Kristan1']}, 'keywords': {'value': ['low-shot counting', 'few-shot counting', 'object counting;small object detection; small object coutning;small objects;detection;counting;loss function']}, 'abstract': {'value': 'Low-shot object counters estimate the number of objects in an image using few or no annotated exemplars. Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation.\\nDue to potentially diverse object appearances, the existing approaches often lead to overgeneralization and false positive detections.\\nFurthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each object center. This loss is sensitive to annotation error, hyperparameters and does not directly optimize the detection task, leading to suboptimal counts.\\nWe introduce GeCo, a novel low-shot counter that achieves accurate object detection, segmentation, and count estimation in a unified architecture.\\nGeCo robustly generalizes the prototypes across objects appearances through a novel dense object query formulation. \\nIn addition, a novel counting loss is proposed, that directly optimizes the detection task and avoids the issues of the standard surrogate loss. \\nGeCo surpasses the leading few-shot detection-based counters by $\\\\sim$25\\\\% in the total count MAE, achieves superior detection accuracy and sets a new solid state-of-the-art result across all low-shot counting setups. \\nThe code will be available on GitHub.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d0b2cb2b23e87543d69c0f54f25700b7c5a26296.pdf'}, '_bibtex': {'value': '@inproceedings{\\npelhan2024a,\\ntitle={A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation},\\nauthor={Jer Pelhan and Alan Lukezic and Vitjan Zavrtanik and Matej Kristan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mtOPyMkSRk}\\n}'}, 'paperhash': {'value': 'pelhan|a_novel_unified_architecture_for_lowshot_counting_by_detection_and_segmentation'}},forum = 'mtOPyMkSRk',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11583/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11583/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11583/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11583/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mtBmKqyqGS',number = 2513,cdate = 1715086986658,pdate = 1727287692239,odate = 1730873858662,mdate = 1737005434141,tcdate = 1715086986658,tmdate = 1737005434141,ddate = None,content = {'title': {'value': 'Phased Consistency Models'}, 'authors': {'value': ['Fu-Yun Wang', 'Zhaoyang Huang', 'Alexander William Bergman', 'Dazhong Shen', 'Peng Gao', 'Michael Lingelbach', 'Keqiang Sun', 'Weikang Bian', 'Guanglu Song', 'Yu Liu', 'Xiaogang Wang', 'Hongsheng Li']}, 'authorids': {'value': ['~Fu-Yun_Wang1', '~Zhaoyang_Huang2', '~Alexander_William_Bergman1', '~Dazhong_Shen1', '~Peng_Gao3', '~Michael_Lingelbach1', '~Keqiang_Sun1', '~Weikang_Bian2', '~Guanglu_Song2', '~Yu_Liu2', '~Xiaogang_Wang2', '~Hongsheng_Li3']}, 'keywords': {'value': ['Consistency Models', 'Diffusion Models', 'Distillation']}, 'abstract': {'value': 'Consistency Models (CMs) have made significant progress in accelerating the generation of diffusion models. However, their application to high-resolution, text-conditioned image generation in the latent space remains unsatisfactory. In this paper, we identify three key flaws in the current design of Latent Consistency Models~(LCMs). We investigate the reasons behind these limitations and propose Phased Consistency Models (PCMs), which generalize the design space and address the identified limitations. Our evaluations demonstrate that PCMs outperform LCMs across 1--16 step generation settings. While PCMs are specifically designed for multi-step refinement, they achieve comparable 1-step generation results to previously state-of-the-art specifically designed 1-step methods. Furthermore, we show the methodology of PCMs is versatile and applicable to video generation, enabling us to train the state-of-the-art few-step text-to-video generator. Our code is available at https://github.com/G-U-N/Phased-Consistency-Model.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Consistency model analysis and training design for advanced performance on text-to-image generation and text-to-video generation'}, 'pdf': {'value': '/pdf/a2dac5afba654b2980f63a0302470f633c4a8d8d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024phased,\\ntitle={Phased Consistency Models},\\nauthor={Fu-Yun Wang and Zhaoyang Huang and Alexander William Bergman and Dazhong Shen and Peng Gao and Michael Lingelbach and Keqiang Sun and Weikang Bian and Guanglu Song and Yu Liu and Xiaogang Wang and Hongsheng Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mtBmKqyqGS}\\n}'}, 'paperhash': {'value': 'wang|phased_consistency_models'}},forum = 'mtBmKqyqGS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2513/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2513/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2513/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2513/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mpDbWjLzfT',number = 12987,cdate = 1715732917411,pdate = 1727288024560,odate = 1730873954031,mdate = 1730873954043,tcdate = 1715732917411,tmdate = 1730873954043,ddate = None,content = {'title': {'value': 'CONTRAST: Continual Multi-source Adaptation to Dynamic Distributions'}, 'authors': {'value': ['Sk Miraj Ahmed', 'Fahim Faisal Niloy', 'Xiangyu Chang', 'Dripta S. Raychaudhuri', 'Samet Oymak', 'Amit Roy-Chowdhury']}, 'authorids': {'value': ['~Sk_Miraj_Ahmed1', '~Fahim_Faisal_Niloy1', '~Xiangyu_Chang2', '~Dripta_S._Raychaudhuri2', '~Samet_Oymak2', '~Amit_Roy-Chowdhury2']}, 'keywords': {'value': ['Multi-source Adaptation', 'Online learning', 'Test Time Adaptation']}, 'abstract': {'value': 'Adapting to dynamic data distributions is a practical yet challenging task. One effective strategy is to use a model ensemble, which leverages the diverse expertise of different models to transfer knowledge to evolving data distributions. However, this approach faces difficulties when the dynamic test distribution is available only in small batches and without access to the original source data. To address the challenge of adapting to dynamic distributions in such practical settings, we propose continual multi-source adaptation to dynamic distributions (CONTRAST), a novel method that optimally combines multiple source models to adapt to the dynamic test data. CONTRAST has two distinguishing features. First, it efficiently computes the optimal combination weights to combine the source models to adapt to the test data distribution continuously as a function of time. Second, it identifies which of the source model parameters to update so that only the model which is most correlated to the target data is adapted, leaving the less correlated ones untouched; this mitigates the issue of ``forgetting\" the source model parameters by focusing only on the source model that exhibits the strongest correlation with the test batch distribution. Through theoretical analysis we show that the proposed method is able to optimally combine the source models and prioritize updates to the model least prone to forgetting. Experimental analysis on diverse datasets demonstrates that the combination of multiple source models does at least as well as the best source (with hindsight knowledge), and performance does not degrade as the test data distribution changes over time (robust to forgetting).'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e782de99a42bd7cbb46ed030b5bdbf45b070ae1f.pdf'}, 'TLDR': {'value': 'We propose the first work to consider dynamically evolving multi-source adaptation at test time.'}, '_bibtex': {'value': '@inproceedings{\\nahmed2024contrast,\\ntitle={{CONTRAST}: Continual Multi-source Adaptation to Dynamic Distributions},\\nauthor={Sk Miraj Ahmed and Fahim Faisal Niloy and Xiangyu Chang and Dripta S. Raychaudhuri and Samet Oymak and Amit Roy-Chowdhury},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mpDbWjLzfT}\\n}'}, 'paperhash': {'value': 'ahmed|contrast_continual_multisource_adaptation_to_dynamic_distributions'}},forum = 'mpDbWjLzfT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12987/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12987/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12987/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12987/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'mp8u2Pcmqz',number = 5272,cdate = 1715519863927,pdate = 1727287778573,odate = 1730873883433,mdate = 1736871808817,tcdate = 1715519863927,tmdate = 1736871808817,ddate = None,content = {'title': {'value': 'DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs'}, 'authors': {'value': ['Haokun Lin', 'Haobo Xu', 'Yichen Wu', 'Jingzhi Cui', 'Yingtao Zhang', 'Linzhan Mou', 'Linqi Song', 'Zhenan Sun', 'Ying Wei']}, 'authorids': {'value': ['~Haokun_Lin4', '~Haobo_Xu2', '~Yichen_Wu2', '~Jingzhi_Cui1', '~Yingtao_Zhang3', '~Linzhan_Mou1', '~Linqi_Song1', '~Zhenan_Sun1', '~Ying_Wei1']}, 'keywords': {'value': ['Model compression', 'Post-training Quantization', 'PTQ of Large Language Models']}, 'abstract': {'value': 'Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address Normal Outliers, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing Massive Outliers that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f75c5ce9e83cea209b9b5807072a3537bac72822.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlin2024duquant,\\ntitle={DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized {LLM}s},\\nauthor={Haokun Lin and Haobo Xu and Yichen Wu and Jingzhi Cui and Yingtao Zhang and Linzhan Mou and Linqi Song and Zhenan Sun and Ying Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mp8u2Pcmqz}\\n}'}, 'TLDR': {'value': 'We identify massive outliers in the down-projection layer of the FFN module and introduce DuQuant, which uses rotation and permutation transformations to effectively mitigate both massive and normal outliers.'}, 'paperhash': {'value': 'lin|duquant_distributing_outliers_via_dual_transformation_makes_stronger_quantized_llms'}},forum = 'mp8u2Pcmqz',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5272/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5272/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5272/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5272/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mp6OWpDIJC',number = 8226,cdate = 1715655872781,pdate = 1727287873080,odate = 1730873910359,mdate = 1730873910376,tcdate = 1715655872781,tmdate = 1730873910376,ddate = None,content = {'title': {'value': 'Autonomous Agents for Collaborative Task under Information Asymmetry'}, 'authors': {'value': ['Wei Liu', 'Chenxi Wang', 'YiFei Wang', 'Zihao Xie', 'Rennai Qiu', 'Yufan Dang', 'Zhuoyun Du', 'Weize Chen', 'Cheng Yang', 'Chen Qian']}, 'authorids': {'value': ['~Wei_Liu40', '~Chenxi_Wang6', '~YiFei_Wang13', '~Zihao_Xie1', '~Rennai_Qiu1', '~Yufan_Dang1', '~Zhuoyun_Du1', '~Weize_Chen1', '~Cheng_Yang6', '~Chen_Qian8']}, 'keywords': {'value': ['autonomous agent', 'social network', 'large language model']}, 'abstract': {'value': \"Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.\"}, 'primary_area': {'value': 'machine_learning_for_social_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cbc02f9426edb1ceb1bfcc4517984b4916550c94.pdf'}, 'supplementary_material': {'value': '/attachment/6b708535a83900de21332a4c91d5966ab83e1ab4.zip'}, 'TLDR': {'value': 'This paper propose iAgents, a new LLM Multi-Agent framework where agents collaborate on behalf of human in the mirrored agent network and deal with information asymmetry problems.'}, '_bibtex': {'value': '@inproceedings{\\nliu2024autonomous,\\ntitle={Autonomous Agents for Collaborative Task under Information Asymmetry},\\nauthor={Wei Liu and Chenxi Wang and YiFei Wang and Zihao Xie and Rennai Qiu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mp6OWpDIJC}\\n}'}, 'paperhash': {'value': 'liu|autonomous_agents_for_collaborative_task_under_information_asymmetry'}},forum = 'mp6OWpDIJC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8226/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8226/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8226/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8226/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'motImXq3B1',number = 10420,cdate = 1715693918690,pdate = 1727287939976,odate = 1730873928860,mdate = 1737020160071,tcdate = 1715693918690,tmdate = 1737020160071,ddate = None,content = {'title': {'value': 'P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics'}, 'authors': {'value': ['Qi Wang', 'Pu Ren', 'Hao Zhou', 'Xin-Yang Liu', 'Zhiwen Deng', 'Yi Zhang', 'Ruizhi Chengze', 'Hongsheng Liu', 'Zidong Wang', 'Jian-Xun Wang', 'Ji-Rong Wen', 'Hao Sun', 'Yang Liu']}, 'authorids': {'value': ['~Qi_Wang30', '~Pu_Ren1', '~Hao_Zhou33', '~Xin-Yang_Liu1', '~Zhiwen_Deng1', '~Yi_Zhang92', '~Ruizhi_Chengze1', '~Hongsheng_Liu1', '~Zidong_Wang1', '~Jian-Xun_Wang1', '~Ji-Rong_Wen1', '~Hao_Sun4', '~Yang_Liu52']}, 'keywords': {'value': ['physics-informed learning', 'coarse model', 'spatiotemporal dynamics prediction']}, 'abstract': {'value': 'When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost. Recently, machine learning has been increasingly utilized to solve PDE problems, but they often encounter challenges related to interpretability, generalizability, and strong dependency on rich labeled data. Hence, we introduce a new PDE-Preserved Coarse Correction Network (P$^2$C$^2$Net) to efficiently solve spatiotemporal PDE problems on coarse mesh grids in small data regimes. The model consists of two synergistic modules: (1) a trainable PDE block that learns to update the coarse solution (i.e., the system state), based on a high-order numerical scheme with boundary condition encoding, and (2) a neural network block that consistently corrects the solution on the fly. In particular, we propose a learnable symmetric Conv filter, with weights shared over the entire model, to accurately estimate the spatial derivatives of PDE based on the neural-corrected system state. The resulting physics-encoded model is capable of handling limited training data (e.g., 3--5 trajectories) and accelerates the prediction of PDE solutions on coarse spatiotemporal grids while maintaining a high accuracy. P$^2$C$^2$Net achieves consistent state-of-the-art performance with over 50\\\\% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion processes and turbulent flows.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Introduced a PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics.'}, 'pdf': {'value': '/pdf/773e47a41e0bca5c6bc853b01def265140046231.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024pcnet,\\ntitle={P\\\\${\\\\textasciicircum}2\\\\$C\\\\${\\\\textasciicircum}2\\\\$Net: {PDE}-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics},\\nauthor={Qi Wang and Pu Ren and Hao Zhou and Xin-Yang Liu and Zhiwen Deng and Yi Zhang and Ruizhi Chengze and Hongsheng Liu and Zidong Wang and Jian-Xun Wang and Ji-Rong Wen and Hao Sun and Yang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=motImXq3B1}\\n}'}, 'paperhash': {'value': 'wang|p^2c^2net_pdepreserved_coarse_correction_network_for_efficient_prediction_of_spatiotemporal_dynamics'}},forum = 'motImXq3B1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10420/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10420/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10420/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10420/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mmSFfib6pI',number = 8285,cdate = 1715657007766,pdate = 1727287875204,odate = 1730873910922,mdate = 1730873910941,tcdate = 1715657007766,tmdate = 1730873910941,ddate = None,content = {'title': {'value': 'Validating Climate Models with Spherical Convolutional Wasserstein Distance'}, 'authors': {'value': ['Robert C. Garrett', 'Trevor Harris', 'Zhuo Wang', 'Bo Li']}, 'authorids': {'value': ['~Robert_C._Garrett1', '~Trevor_Harris1', '~Zhuo_Wang6', '~Bo_Li8']}, 'keywords': {'value': ['Climate Models', 'Wasserstein Distance', 'Convolution', 'Functional Data']}, 'TLDR': {'value': 'We create a Wasserstein distance variant based on spherical convolutions of functional data and apply the method to climate model validation.'}, 'abstract': {'value': 'The validation of global climate models is crucial to ensure the accuracy and efficacy of model output. We introduce the spherical convolutional Wasserstein distance to more comprehensively measure differences between climate models and reanalysis data. This new similarity measure accounts for spatial variability using convolutional projections and quantifies local differences in the distribution of climate variables. We apply this method to evaluate the historical model outputs of the Coupled Model Intercomparison Project (CMIP) members by comparing them to observational and reanalysis data products. Additionally, we investigate the progression from CMIP phase 5 to phase 6 and find modest improvements in the phase 6 models regarding their ability to produce realistic climatologies.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d2936d5d2e539246fd12de9624014d27f805dbec.pdf'}, 'supplementary_material': {'value': '/attachment/1680c763a1dd3abf00496cf670963e7fa4f690b6.zip'}, '_bibtex': {'value': '@inproceedings{\\ngarrett2024validating,\\ntitle={Validating Climate Models with Spherical Convolutional Wasserstein Distance},\\nauthor={Robert C. Garrett and Trevor Harris and Zhuo Wang and Bo Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mmSFfib6pI}\\n}'}, 'paperhash': {'value': 'garrett|validating_climate_models_with_spherical_convolutional_wasserstein_distance'}},forum = 'mmSFfib6pI',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8285/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8285/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8285/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8285/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mlmTxJwVsb',number = 8883,cdate = 1715671043224,pdate = 1727287894059,odate = 1730873916091,mdate = 1730873916115,tcdate = 1715671043224,tmdate = 1730873916115,ddate = None,content = {'title': {'value': 'DMNet: Self-comparison Driven Model for Subject-independent Seizure Detection'}, 'authors': {'value': ['Shihao Tu', 'Linfeng Cao', 'Daoze Zhang', 'Junru Chen', 'Lvbin Ma', 'Yin Zhang', 'Yang Yang']}, 'authorids': {'value': ['~Shihao_Tu4', '~Linfeng_Cao1', '~Daoze_Zhang1', '~Junru_Chen1', '~Lvbin_Ma1', '~Yin_Zhang1', '~Yang_Yang35']}, 'keywords': {'value': ['seizure detection', 'domain generalization']}, 'abstract': {'value': 'Automated seizure detection (ASD) using intracranial electroencephalography (iEEG) is critical for effective epilepsy treatment. However, the significant domain shift of iEEG signals across subjects poses a major challenge, limiting their applicability in real-world clinical scenarios. In this paper, we address this issue by analyzing the primary cause behind the failure of existing iEEG models for subject-independent seizure detection, and identify a critical universal seizure pattern: seizure events consistently exhibit higher average amplitude compared to adjacent normal events. To mitigate the domain shifts and preserve the universal seizure patterns, we propose a novel self-comparison mechanism. This mechanism effectively aligns iEEG signals across subjects and time intervals. Building upon these findings, we propose Difference Matrix-based Neural Network (DMNet), a subject-independent seizure detection model, which leverages self-comparison based on two constructed (contextual, channel-level) references to mitigate shifts of iEEG, and utilize a simple yet effective difference matrix to encode the universal seizure patterns. Extensive experiments show that DMNet significantly outperforms previous SOTAs while maintaining high efficiency on a real-world clinical dataset collected by us and two public datasets for subject-independent seizure detection. Moreover, the visualization results demonstrate that the generated difference matrix can effectively capture the seizure activity changes during the seizure evolution process. Additionally, we deploy our method in an online diagnosis system to illustrate its effectiveness in real clinical applications.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/72bd52ddff251b9f910257ff0b20a7f4bb236452.pdf'}, 'supplementary_material': {'value': '/attachment/c4ae1d4d6b58f349801ececb9bdcbb47c72d2d80.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ntu2024dmnet,\\ntitle={{DMN}et: Self-comparison Driven Model for Subject-independent Seizure Detection},\\nauthor={Shihao Tu and Linfeng Cao and Daoze Zhang and Junru Chen and Lvbin Ma and Yin Zhang and Yang Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mlmTxJwVsb}\\n}'}, 'TLDR': {'value': 'DMNet is a self-comparison driven model designed for subject-independent seizure detection.'}, 'paperhash': {'value': 'tu|dmnet_selfcomparison_driven_model_for_subjectindependent_seizure_detection'}},forum = 'mlmTxJwVsb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8883/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8883/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8883/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission8883/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mlm3nUwOeQ',number = 12046,cdate = 1715715346117,pdate = 1727287992711,odate = 1730873944904,mdate = 1730873944929,tcdate = 1715715346117,tmdate = 1730873944929,ddate = None,content = {'title': {'value': 'Tight Rates for Bandit Control Beyond Quadratics'}, 'authors': {'value': ['Y. Jennifer Sun', 'Zhou Lu']}, 'authorids': {'value': ['~Y._Jennifer_Sun1', '~Zhou_Lu1']}, 'keywords': {'value': ['Bandit Problem', 'Online Control']}, 'abstract': {'value': 'Unlike classical control theory, such as Linear Quadratic Control (LQC), real-world control problems are highly complex. These problems often involve adversarial perturbations, bandit feedback models, and non-quadratic, adversarially chosen cost functions. A fundamental yet unresolved question is whether optimal regret can be achieved for these general control problems. The standard approach to addressing this problem involves a reduction to bandit convex optimization with memory. In the bandit setting, constructing a gradient estimator with low variance is challenging due to the memory structure and non-quadratic loss functions.\\n\\nIn this paper, we provide an affirmative answer to this question. Our main contribution is an algorithm that achieves an $\\\\tilde{O}(\\\\sqrt{T})$ optimal regret for bandit non-stochastic control with strongly-convex and smooth cost functions in the presence of adversarial perturbations, improving the previously known $\\\\tilde{O}(T^{2/3})$ regret bound from \\\\citep{cassel2020bandit}. Our algorithm overcomes the memory issue by reducing the problem to Bandit Convex Optimization (BCO) without memory and addresses general strongly-convex costs using recent advancements in BCO from \\\\citep{suggala2024second}. Along the way, we develop an improved algorithm for BCO with memory, which may be of independent interest.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/21571625278a93f35acb6aaca70e4b0bb0577e37.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024tight,\\ntitle={Tight Rates for Bandit Control Beyond Quadratics},\\nauthor={Y. Jennifer Sun and Zhou Lu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mlm3nUwOeQ}\\n}'}, 'paperhash': {'value': 'sun|tight_rates_for_bandit_control_beyond_quadratics'}},forum = 'mlm3nUwOeQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12046/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12046/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12046/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12046/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mljDUaQpln',number = 2092,cdate = 1714971601008,pdate = 1727287680669,odate = 1730873855043,mdate = 1734949581280,tcdate = 1714971601008,tmdate = 1734949581280,ddate = None,content = {'title': {'value': 'Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus'}, 'authors': {'value': ['Terufumi Morishita', 'Gaku Morio', 'Atsuki Yamaguchi', 'Yasuhiro Sogawa']}, 'authorids': {'value': ['~Terufumi_Morishita1', '~Gaku_Morio1', '~Atsuki_Yamaguchi1', '~Yasuhiro_Sogawa1']}, 'keywords': {'value': ['large language model', 'artificial intelligence', 'reasoning', 'logical reasoning', 'math', 'coding', 'synthetic corpus']}, 'TLDR': {'value': \"We enhanced LLM's reasoning capabilities by principled synthetic corpus.\"}, 'abstract': {'value': \"Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning.\\nTo address this, we propose $\\\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples.\\nWe first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights.\\nThen, based on these principles, we construct a synthetic corpus named $\\\\textbf{Formal} \\\\ \\\\textbf{Logic} \\\\ \\\\textbf{\\\\textit{D}eduction} \\\\ \\\\textbf{\\\\textit{D}iverse}$ (FLD$ _{\\\\times2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors.\\nFinally, we empirically show that ALT on FLD$ _{\\\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B.\\nImprovements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/246a03e416453fbc32408c9fdfd22b45f6bbfff0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmorishita2024enhancing,\\ntitle={Enhancing Reasoning Capabilities of {LLM}s via Principled Synthetic Logic Corpus},\\nauthor={Terufumi Morishita and Gaku Morio and Atsuki Yamaguchi and Yasuhiro Sogawa},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mljDUaQpln}\\n}'}, 'paperhash': {'value': 'morishita|enhancing_reasoning_capabilities_of_llms_via_principled_synthetic_logic_corpus'}},forum = 'mljDUaQpln',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2092/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2092/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2092/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2092/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ml01XyP698',number = 1314,cdate = 1714558412140,pdate = 1727287657449,odate = 1730873847469,mdate = 1730873847491,tcdate = 1714558412140,tmdate = 1730873847491,ddate = None,content = {'title': {'value': 'FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes'}, 'authors': {'value': ['Yunsong Wang', 'Tianxin Huang', 'Hanlin Chen', 'Gim Hee Lee']}, 'authorids': {'value': ['~Yunsong_Wang1', '~Tianxin_Huang1', '~Hanlin_Chen2', '~Gim_Hee_Lee1']}, 'keywords': {'value': ['3D Gaussian Splatting', 'Generalization', '3D from multi-view sensors', 'Novel View Synthesis', '3D Computer Vision']}, 'TLDR': {'value': 'We propose a generalizable 3D Gaussian Splatting framework to progressively fuse multi-view pixel-aligned 3D Gaussians for large-scale scene photorealistic reconstruction.'}, 'abstract': {'value': 'Empowering 3D Gaussian Splatting with generalization ability is appealing. However, existing generalizable 3D Gaussian Splatting methods are largely confined to narrow-range interpolation between stereo images due to their heavy backbones, thus lacking the ability to accurately localize 3D Gaussian and support free-view synthesis across wide view range. In this paper, we present a novel framework FreeSplat that is capable of reconstructing geometrically consistent 3D scenes from long sequence input towards free-view synthesis.Specifically, we firstly introduce Low-cost Cross-View Aggregation achieved by constructing adaptive cost volumes among nearby views and aggregating features using a multi-scale structure. Subsequently, we present the Pixel-wise Triplet Fusion to eliminate redundancy of 3D Gaussians in overlapping view regions and to aggregate features observed across multiple views. Additionally, we propose a simple but effective free-view training strategy that ensures robust view synthesis across broader view range regardless of the number of views. Our empirical results demonstrate state-of-the-art novel view synthesis peformances in both novel view rendered color maps quality and depth maps accuracy across different numbers of input views. We also show that FreeSplat performs inference more efficiently and can effectively reduce redundant Gaussians, offering the possibility of feed-forward large scene reconstruction without depth priors. Our code will be made open-source upon paper acceptance.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a257236fcea44b03493a459dc55a90ebcbfe1903.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwang2024freesplat,\\ntitle={FreeSplat: Generalizable 3D Gaussian Splatting Towards Free View Synthesis of Indoor Scenes},\\nauthor={Yunsong Wang and Tianxin Huang and Hanlin Chen and Gim Hee Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ml01XyP698}\\n}'}, 'paperhash': {'value': 'wang|freesplat_generalizable_3d_gaussian_splatting_towards_free_view_synthesis_of_indoor_scenes'}},forum = 'ml01XyP698',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1314/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1314/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1314/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission1314/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mkzpN2T87C',number = 7300,cdate = 1715617393063,pdate = 1727287842336,odate = 1730873901521,mdate = 1730873901532,tcdate = 1715617393063,tmdate = 1730873901532,ddate = None,content = {'title': {'value': 'Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search'}, 'authors': {'value': ['Qiujiang Jin', 'Ruichen Jiang', 'Aryan Mokhtari']}, 'authorids': {'value': ['~Qiujiang_Jin1', '~Ruichen_Jiang1', '~Aryan_Mokhtari3']}, 'keywords': {'value': ['Quasi-Newton method', 'BFGS Algorithm', 'Global convergence analysis', 'Non-asymptotic convergence analysis', 'Inexact line search']}, 'abstract': {'value': \"In this paper, we present the first explicit and non-asymptotic global convergence rates of the BFGS method when implemented with an inexact line search scheme satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global linear convergence rate of $(1 - \\\\frac{1}{\\\\kappa})^t$ for $\\\\mu$-strongly convex functions with $L$-Lipschitz gradients, where $\\\\kappa = \\\\frac{L}{\\\\mu}$ represents the condition number. Additionally, if the objective function's Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search achieves a linear convergence rate that depends solely on the line search parameters, independent of the condition number. We also establish a global superlinear convergence rate of $\\\\mathcal{O}((\\\\frac{1}{t})^t)$. These global bounds are all valid for any starting point $x_0$ and any symmetric positive definite initial Hessian approximation matrix $B_0$, though the choice of $B_0$ impacts the number of iterations needed to achieve these rates. By synthesizing these results, we outline the first global complexity characterization of BFGS with the Armijo-Wolfe line search. Additionally, we clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe conditions and characterize its overall complexity.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/381eca59c41cc65a2d39cfb89c6a5c1a62451b77.pdf'}, '_bibtex': {'value': '@inproceedings{\\njin2024nonasymptotic,\\ntitle={Non-asymptotic Global Convergence Analysis of {BFGS} with the Armijo-Wolfe Line Search},\\nauthor={Qiujiang Jin and Ruichen Jiang and Aryan Mokhtari},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mkzpN2T87C}\\n}'}, 'paperhash': {'value': 'jin|nonasymptotic_global_convergence_analysis_of_bfgs_with_the_armijowolfe_line_search'}},forum = 'mkzpN2T87C',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7300/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7300/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7300/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7300/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mkw6x0OExg',number = 20308,cdate = 1715795627204,pdate = 1727288230944,odate = 1730874000656,mdate = 1737128930797,tcdate = 1715795627204,tmdate = 1737128930797,ddate = None,content = {'title': {'value': 'Explanations that reveal all through the deﬁnition of encoding'}, 'authors': {'value': ['Aahlad Manas Puli', 'Nhi Nguyen', 'Rajesh Ranganath']}, 'authorids': {'value': ['~Aahlad_Manas_Puli1', '~Nhi_Nguyen1', '~Rajesh_Ranganath2']}, 'keywords': {'value': ['feature attributions', 'model explanations', 'evaluating explanations', 'encoding the prediction', 'interpretability']}, 'TLDR': {'value': 'We formalize the definition of encoding in explanation methods and provide two methods to detect encoding.'}, 'abstract': {'value': 'Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a deﬁnition of encoding that identiﬁes this extra predictive power via conditional dependence and show that the deﬁnition ﬁts existing examples of encoding. This deﬁnition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a “what you see is what you get” property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to show that despite prompting an LLM to produce non-encoding explanations for a sentiment analysis task, the LLM-generated explanations encode.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/010d475110d04ae68686d4efff7d0b96a43730f5.pdf'}, '_bibtex': {'value': '@inproceedings{\\npuli2024explanations,\\ntitle={Explanations that reveal all through the definition of encoding},\\nauthor={Aahlad Manas Puli and Nhi Nguyen and Rajesh Ranganath},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mkw6x0OExg}\\n}'}, 'paperhash': {'value': 'puli|explanations_that_reveal_all_through_the_denition_of_encoding'}},forum = 'mkw6x0OExg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20308/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20308/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20308/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20308/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'mjGy8g3pgi',number = 1139,cdate = 1714424664901,pdate = 1727287653086,odate = 1730873845750,mdate = 1731951947448,tcdate = 1714424664901,tmdate = 1731951947448,ddate = None,content = {'title': {'value': \"Yo'LLaVA: Your Personalized Language and Vision Assistant\"}, 'authors': {'value': ['Thao Nguyen', 'Haotian Liu', 'Yuheng Li', 'Mu Cai', 'Utkarsh Ojha', 'Yong Jae Lee']}, 'authorids': {'value': ['~Thao_Nguyen4', '~Haotian_Liu1', '~Yuheng_Li1', '~Mu_Cai1', '~Utkarsh_Ojha1', '~Yong_Jae_Lee2']}, 'keywords': {'value': ['personalization', 'multimodal models']}, 'abstract': {'value': 'Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering).\\nWhile broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user\\'s pet dog).\\n\\nHuman reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, \"What should I buy for *my dog*\\'s birthday?\"; as opposed to a generic inquiry about \"What should I buy for *a dog*\\'s birthday?\".\\nSimilarly, when looking at a friend\\'s image, the interest lies in seeing their activities (e.g., \"*my friend* is holding a cat\"), rather than merely observing generic human actions (e.g., \"*a man* is holding a cat\").\\n\\nIn this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo\\'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject.  Our qualitative and quantitative analyses reveal that Yo\\'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA).'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': \"We introduce Yo'LLaVA -- a framework to embed personalized subjects (e.g., your pet) into a comprehensible prompt for Large Multimodal Models (e.g., LLaVA)\"}, 'pdf': {'value': '/pdf/27db4874c16bb351aed45cbd5c641e028cc244b5.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nnguyen2024yollava,\\ntitle={Yo'{LL}a{VA}: Your Personalized Language and Vision Assistant},\\nauthor={Thao Nguyen and Haotian Liu and Yuheng Li and Mu Cai and Utkarsh Ojha and Yong Jae Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mjGy8g3pgi}\\n}\"}, 'paperhash': {'value': 'nguyen|yollava_your_personalized_language_and_vision_assistant'}},forum = 'mjGy8g3pgi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1139/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1139/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1139/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1139/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mirkQqx6po',number = 15642,cdate = 1715762271578,pdate = 1727288103093,odate = 1730873973968,mdate = 1730873973988,tcdate = 1715762271578,tmdate = 1730873973988,ddate = None,content = {'title': {'value': 'Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems'}, 'authors': {'value': ['Vincent Cohen-Addad', \"Tommaso d'Orsi\", 'Anupam Gupta', 'Euiwoong Lee', 'Debmalya Panigrahi']}, 'authorids': {'value': ['~Vincent_Cohen-Addad1', \"~Tommaso_d'Orsi1\", '~Anupam_Gupta2', '~Euiwoong_Lee2', '~Debmalya_Panigrahi1']}, 'keywords': {'value': ['learning-augmented algorithm', 'approximation algorithm', 'maximization', 'CSPs', 'learning with advice']}, 'abstract': {'value': 'In recent years, there has been a surge of interest in the use of machine-learned predictions to bypass worst-case lower bounds for classical problems in combinatorial optimization. So far, the focus has mostly been on online algorithms, where information-theoretic barriers are overcome using predictions about the unknown future. In this paper, we consider the complementary question of using learned information to overcome computational barriers in the form of approximation hardness of polynomial-time algorithms for NP-hard (offline) problems. We show that noisy predictions about the optimal solution can be used to break classical hardness results for maximization problems such as the max-cut problem and more generally, maximization versions of constraint satisfaction problems (CSPs).'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b4d11784a114bcaba7a10b109e103a3add6b35b3.pdf'}, '_bibtex': {'value': \"@inproceedings{\\ncohen-addad2024learningaugmented,\\ntitle={Learning-Augmented Approximation Algorithms for Maximum Cut and Related Problems},\\nauthor={Vincent Cohen-Addad and Tommaso d'Orsi and Anupam Gupta and Euiwoong Lee and Debmalya Panigrahi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mirkQqx6po}\\n}\"}, 'TLDR': {'value': 'This paper presents learning-augmented algorithms for maximum cut and other max-CSPs.'}, 'paperhash': {'value': 'cohenaddad|learningaugmented_approximation_algorithms_for_maximum_cut_and_related_problems'}},forum = 'mirkQqx6po',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15642/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15642/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15642/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15642/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'miO8odRzto',number = 14488,cdate = 1715750523514,pdate = 1727288071016,odate = 1730873966200,mdate = 1736819066632,tcdate = 1715750523514,tmdate = 1736819066632,ddate = None,content = {'title': {'value': 'Online Relational Inference for Evolving Multi-agent Interacting Systems'}, 'authors': {'value': ['Beomseok Kang', 'Priyabrata Saha', 'Sudarshan Sharma', 'Biswadeep Chakraborty', 'Saibal Mukhopadhyay']}, 'authorids': {'value': ['~Beomseok_Kang1', '~Priyabrata_Saha1', '~Sudarshan_Sharma1', '~Biswadeep_Chakraborty1', '~Saibal_Mukhopadhyay2']}, 'keywords': {'value': ['Neural Relational Inference', 'Online Learning', 'Multi-agent System']}, 'abstract': {'value': 'We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4fb239ecfecd14053ab49d36e096b20fb212be52.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkang2024online,\\ntitle={Online Relational Inference for Evolving Multi-agent Interacting Systems},\\nauthor={Beomseok Kang and Priyabrata Saha and Sudarshan Sharma and Biswadeep Chakraborty and Saibal Mukhopadhyay},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=miO8odRzto}\\n}'}, 'paperhash': {'value': 'kang|online_relational_inference_for_evolving_multiagent_interacting_systems'}},forum = 'miO8odRzto',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14488/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14488/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14488/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14488/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mhhlZeAr67',number = 17382,cdate = 1715779224112,pdate = 1727288151898,odate = 1730873984240,mdate = 1734690715036,tcdate = 1715779224112,tmdate = 1734690715036,ddate = None,content = {'title': {'value': 'Reciprocal Learning'}, 'authors': {'value': ['Julian Rodemann', 'Christoph Jansen', 'Georg Schollmeyer']}, 'authorids': {'value': ['~Julian_Rodemann1', '~Christoph_Jansen1', '~Georg_Schollmeyer1']}, 'keywords': {'value': ['Convergence', 'Decision Theory', 'Bandits', 'Active Learning', 'Self-Training', 'Semi-Supervised Learning', 'Online Learning']}, 'abstract': {'value': 'We demonstrate that numerous machine learning algorithms are specific instances of one single paradigm: reciprocal learning. These instances range from active learning over multi-armed bandits to self-training. We show that all these algorithms not only learn parameters from data but also vice versa: They iteratively alter training data in a way that depends on the current model fit. We introduce reciprocal learning as a generalization of these algorithms using the language of decision theory. This allows us to study under what conditions they converge. The key is to guarantee that reciprocal learning contracts such that the Banach fixed-point theorem applies. In this way, we find that reciprocal learning converges at linear rates to an approximately optimal model under some assumptions on the loss function, if their predictions are probabilistic and the sample adaption is both non-greedy and either randomized or regularized. We interpret these findings and provide corollaries that relate them to active learning, self-training, and bandits.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e9cbc54ce2010cf533aae8b07ae85dd69abab94b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nrodemann2024reciprocal,\\ntitle={Reciprocal Learning},\\nauthor={Julian Rodemann and Christoph Jansen and Georg Schollmeyer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mhhlZeAr67}\\n}'}, 'TLDR': {'value': 'We generalize active learning, self-training, multi-armed bandits, superset learning, and Bayesian optimization to reciprocal learning and give sufficient conditions for convergence.'}, 'paperhash': {'value': 'rodemann|reciprocal_learning'}},forum = 'mhhlZeAr67',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17382/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17382/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17382/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17382/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mfvKEdJ4zW',number = 19181,cdate = 1715789616956,pdate = 1727288203909,odate = 1730873994331,mdate = 1731950106273,tcdate = 1715789616956,tmdate = 1731950106273,ddate = None,content = {'title': {'value': 'Latent Functional Maps: a spectral framework for representation alignment'}, 'authors': {'value': ['Marco Fumero', 'Marco Pegoraro', 'Valentino Maiorca', 'Francesco Locatello', 'Emanuele Rodolà']}, 'authorids': {'value': ['~Marco_Fumero1', '~Marco_Pegoraro1', '~Valentino_Maiorca1', '~Francesco_Locatello1', '~Emanuele_Rodolà1']}, 'keywords': {'value': ['Representation alignment; Spectral methods;']}, 'abstract': {'value': 'Neural models learn data representations that lie on low-dimensional manifolds, yet modeling the relation between these representational spaces is an ongoing challenge.\\nBy integrating spectral geometry principles into neural modeling, we show that this problem can be better addressed in the functional domain, mitigating complexity, while enhancing interpretability and performances on downstream tasks. \\nTo this end, we introduce a multi-purpose framework to the representation learning community, which allows to: (i) compare different spaces in an interpretable way and measure their intrinsic similarity; (ii) find correspondences between them, both in unsupervised and weakly supervised settings, and (iii) to effectively transfer representations between distinct spaces.\\nWe validate our framework on various applications, ranging from stitching to retrieval tasks, and on multiple modalities, demonstrating that Latent Functional Maps can serve as a swiss-army knife for representation alignment.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/694399fae6b83572f24b2ece1dcef5ecf14e6b69.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfumero2024latent,\\ntitle={Latent Functional Maps: a spectral framework for representation alignment},\\nauthor={Marco Fumero and Marco Pegoraro and Valentino Maiorca and Francesco Locatello and Emanuele Rodol{\\\\`a}},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mfvKEdJ4zW}\\n}'}, 'paperhash': {'value': 'fumero|latent_functional_maps_a_spectral_framework_for_representation_alignment'}},forum = 'mfvKEdJ4zW',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19181/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19181/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19181/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19181/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mfTvNzhsht',number = 11734,cdate = 1715709585849,pdate = 1727287981150,odate = 1730873941780,mdate = 1730873941800,tcdate = 1715709585849,tmdate = 1730873941800,ddate = None,content = {'title': {'value': 'Dueling over Dessert, Mastering the Art of Repeated Cake Cutting'}, 'authors': {'value': ['Simina Branzei', 'MohammadTaghi Hajiaghayi', 'Reed Phillips', 'Suho Shin', 'Kun Wang']}, 'authorids': {'value': ['~Simina_Branzei1', '~MohammadTaghi_Hajiaghayi1', '~Reed_Phillips1', '~Suho_Shin1', '~Kun_Wang9']}, 'keywords': {'value': ['fair division', 'online learning', 'fictitious play', 'repeated games']}, 'abstract': {'value': \"We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice  cuts the cake at a point of her choice, while  Bob  chooses the left piece or the right piece, leaving the remainder for Alice. \\nWe consider two versions: sequential, where Bob observes Alice's cut point  before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler.\\n \\nWe observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.\\n\\nWe analyze the limits of how much a player can exploit the other one and show that fair utility profiles are in fact achievable. Specifically, the players can enforce  the equitable utility profile  of $(1/2, 1/2)$  in the limit  on every trajectory of play, by keeping the other player's utility to approximately $1/2$ on average while guaranteeing they themselves get at least approximately $1/2$ on average. We show this theorem using a connection with Blackwell approachability.\\n\\nFinally, we analyze a natural dynamic known as fictitious play, where players best respond to the empirical distribution of the other player. We show that\\nfictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a rate of $O(1/\\\\sqrt{T})$.\"}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7f68a3d996ea18de7f300e14ce2c1a0e9687e069.pdf'}, 'supplementary_material': {'value': '/attachment/aba829d784d7537a79fbcb6cdb7b0a004f3bc092.zip'}, '_bibtex': {'value': '@inproceedings{\\nbranzei2024dueling,\\ntitle={Dueling over Dessert, Mastering the Art of Repeated Cake Cutting},\\nauthor={Simina Branzei and MohammadTaghi Hajiaghayi and Reed Phillips and Suho Shin and Kun Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mfTvNzhsht}\\n}'}, 'TLDR': {'value': 'We analyze repeated cake cutting between two players'}, 'paperhash': {'value': 'branzei|dueling_over_dessert_mastering_the_art_of_repeated_cake_cutting'}},forum = 'mfTvNzhsht',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11734/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11734/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11734/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission11734/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'merJ77Jipt',number = 11994,cdate = 1715714534201,pdate = 1727287990820,odate = 1730873944304,mdate = 1730873944323,tcdate = 1715714534201,tmdate = 1730873944323,ddate = None,content = {'title': {'value': 'DiffPO: A causal diffusion model for learning distributions of potential outcomes'}, 'authors': {'value': ['Yuchen Ma', 'Valentyn Melnychuk', 'Jonas Schweisthal', 'Stefan Feuerriegel']}, 'authorids': {'value': ['~Yuchen_Ma3', '~Valentyn_Melnychuk1', '~Jonas_Schweisthal1', '~Stefan_Feuerriegel1']}, 'keywords': {'value': ['Causal inference', 'Treatment effect estimation', 'Diffusion models', 'Machine Learning for healthcare', 'CATE']}, 'abstract': {'value': 'Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3ab42b4019b91a4da0db8fd15de4fc431fae84d8.pdf'}, '_bibtex': {'value': '@inproceedings{\\nma2024diffpo,\\ntitle={Diff{PO}: A causal diffusion model for learning distributions of potential outcomes},\\nauthor={Yuchen Ma and Valentyn Melnychuk and Jonas Schweisthal and Stefan Feuerriegel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=merJ77Jipt}\\n}'}, 'paperhash': {'value': 'ma|diffpo_a_causal_diffusion_model_for_learning_distributions_of_potential_outcomes'}},forum = 'merJ77Jipt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11994/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11994/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11994/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11994/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'me1MpmENpw',number = 21316,cdate = 1715801204298,pdate = 1727288252748,odate = 1730874005410,mdate = 1730874005422,tcdate = 1715801204298,tmdate = 1730874005422,ddate = None,content = {'title': {'value': 'Semantics and Spatiality of Emergent Communication'}, 'authors': {'value': ['Rotem Ben Zion', 'Boaz Carmeli', 'Orr Paradise', 'Yonatan Belinkov']}, 'authorids': {'value': ['~Rotem_Ben_Zion1', '~Boaz_Carmeli1', '~Orr_Paradise1', '~Yonatan_Belinkov1']}, 'keywords': {'value': ['emergent communication', \"Lewis' games\"]}, 'abstract': {'value': 'When artificial agents are jointly trained to perform collaborative tasks using a communication channel, they develop opaque goal-oriented communication protocols. Good task performance is often considered sufficient evidence that meaningful communication is taking place, but existing empirical results show that communication strategies induced by common objectives can be counterintuitive whilst solving the task nearly perfectly. In this work, we identify a goal-agnostic prerequisite to meaningful communication, which we term semantic consistency, based on the idea that messages should have similar meanings across instances. We provide a formal definition for this idea, and use it to compare the two most common objectives in the field of emergent communication: discrimination and reconstruction. We prove, under mild assumptions, that semantically inconsistent communication protocols can be optimal solutions to the discrimination task, but not to reconstruction. We further show that the reconstruction objective encourages a stricter property, spatial meaningfulness, which also accounts for the distance between messages. Experiments with emergent communication games validate our theoretical results. These findings demonstrate an inherent advantage of distance-based communication goals, and contextualize previous empirical discoveries.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/409ab38f12a2bf75f079992ad0e99e129151a201.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzion2024semantics,\\ntitle={Semantics and Spatiality of Emergent Communication},\\nauthor={Rotem Ben Zion and Boaz Carmeli and Orr Paradise and Yonatan Belinkov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=me1MpmENpw}\\n}'}, 'paperhash': {'value': 'zion|semantics_and_spatiality_of_emergent_communication'}},forum = 'me1MpmENpw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21316/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21316/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21316/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21316/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mdWz5koY5p',number = 5667,cdate = 1715563454863,pdate = 1727287791358,odate = 1730873887017,mdate = 1730873887037,tcdate = 1715563454863,tmdate = 1730873887037,ddate = None,content = {'title': {'value': 'RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space'}, 'authors': {'value': ['Jingdi Chen', 'Hanhan Zhou', 'Yongsheng Mei', 'Carlee Joe-Wong', 'Gina Adam', 'Nathaniel D. Bastian', 'Tian Lan']}, 'authorids': {'value': ['~Jingdi_Chen1', '~Hanhan_Zhou1', '~Yongsheng_Mei1', '~Carlee_Joe-Wong1', '~Gina_Adam1', '~Nathaniel_D._Bastian1', '~Tian_Lan4']}, 'keywords': {'value': ['Multi-agent Reinforcement Learning', 'Decision Tree', 'Explainable AI']}, 'abstract': {'value': 'Deep Reinforcement Learning (DRL) algorithms have achieved great success in solving many challenging tasks while their black-box nature hinders interpretability and real-world applicability, making it difficult for human experts to interpret and understand DRL policies. \\nExisting works on interpretable reinforcement learning have shown promise in extracting decision tree (DT) based policies from DRL policies with most focus on the single-agent settings while prior attempts to introduce DT policies in multi-agent scenarios mainly focus on heuristic designs which do not provide any quantitative guarantees on the expected return.\\nIn this paper, we establish an upper bound on the return gap between the oracle expert policy and an optimal decision tree policy. This enables us to recast the DT extraction problem into a novel non-euclidean clustering problem over the local observation and action values space of each agent, with action values as cluster labels and the upper bound on the return gap as clustering loss.\\nBoth the algorithm and the upper bound are extended to multi-agent decentralized DT extractions by an iteratively-grow-DT procedure guided by an action-value function conditioned on the current DTs of other agents. Further, we propose the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which is a surprisingly simple design and is integrated with reinforcement learning through the utilization of a novel Regularized Information Maximization loss. Evaluations on tasks like D4RL show that RGMDT significantly outperforms heuristic DT-based baselines and can achieve nearly optimal returns under given DT complexity constraints (e.g., maximum number of DT nodes).'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/41ac0f4467bcfc63a3fb68d7ce0f9cbeeaf0bbd9.pdf'}, 'supplementary_material': {'value': '/attachment/b2028ea02551eca88ca0f4350cbaeea4901349b4.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024rgmdt,\\ntitle={{RGMDT}: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space},\\nauthor={Jingdi Chen and Hanhan Zhou and Yongsheng Mei and Carlee Joe-Wong and Gina Adam and Nathaniel D. Bastian and Tian Lan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mdWz5koY5p}\\n}'}, 'paperhash': {'value': 'chen|rgmdt_returngapminimizing_decision_tree_extraction_in_noneuclidean_metric_space'}},forum = 'mdWz5koY5p',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5667/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5667/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5667/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission5667/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mcY221BgKi',number = 4569,cdate = 1715428612259,pdate = 1727287754337,odate = 1730873877161,mdate = 1734699794039,tcdate = 1715428612259,tmdate = 1734699794039,ddate = None,content = {'title': {'value': 'Learning Cooperative Trajectory Representations for Motion Forecasting'}, 'authors': {'value': ['Hongzhi Ruan', 'Haibao Yu', 'Wenxian Yang', 'Siqi Fan', 'Zaiqing Nie']}, 'authorids': {'value': ['~Hongzhi_Ruan1', '~Haibao_Yu2', '~Wenxian_Yang1', '~Siqi_Fan2', '~Zaiqing_Nie2']}, 'keywords': {'value': ['Cooperative Autonomous Driving', 'Motion Forecasting']}, 'abstract': {'value': 'Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.\\nExisting research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. \\nIn this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. \\nSpecifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. \\nV2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.\\nTo further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.\\nExperimental results on both V2X-Seq and V2X-Traj show the advantage of our method. \\nWe hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.\\nFind the project at https://github.com/AIR-THU/V2X-Graph.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/262f0f4a1a5579270c8d2e1921cd2c8b943c1a59.pdf'}, 'supplementary_material': {'value': '/attachment/36df1818fba82926f5a9bc27ec8ca3641e9a8de3.zip'}, '_bibtex': {'value': '@inproceedings{\\nruan2024learning,\\ntitle={Learning Cooperative Trajectory Representations for Motion Forecasting},\\nauthor={Hongzhi Ruan and Haibao Yu and Wenxian Yang and Siqi Fan and Zaiqing Nie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mcY221BgKi}\\n}'}, 'paperhash': {'value': 'ruan|learning_cooperative_trajectory_representations_for_motion_forecasting'}},forum = 'mcY221BgKi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4569/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4569/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4569/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4569/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'manHbkpIW6',number = 14033,cdate = 1715745337750,pdate = 1727288058313,odate = 1730873963049,mdate = 1730873963068,tcdate = 1715745337750,tmdate = 1730873963068,ddate = None,content = {'title': {'value': 'Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge'}, 'authors': {'value': ['Fang Dong', 'Mengyi Chen', 'Jixian Zhou', 'Yubin Shi', 'Yixuan Chen', 'Mingzhi Dong', 'Yujiang Wang', 'Dongsheng Li', 'Xiaochen Yang', 'Rui Zhu', 'Robert P. Dick', 'Qin Lv', 'Fan Yang', 'Tun Lu', 'Ning Gu', 'Li Shang']}, 'authorids': {'value': ['~Fang_Dong2', '~Mengyi_Chen2', '~Jixian_Zhou1', '~Yubin_Shi1', '~Yixuan_Chen1', '~Mingzhi_Dong1', '~Yujiang_Wang1', '~Dongsheng_Li2', '~Xiaochen_Yang2', '~Rui_Zhu8', '~Robert_P._Dick1', '~Qin_Lv1', '~Fan_Yang31', '~Tun_Lu1', '~Ning_Gu2', '~Li_Shang3']}, 'keywords': {'value': ['language model', 'long-tail', 'clustering']}, 'TLDR': {'value': 'The long-tail data in language model suffer from its gradient inconsistency with overall data, causing model struggle to capture domain knowledge during pretrain. We use a clustering-based sparse expert network, yields better performance.'}, 'abstract': {'value': \"Language models (LMs) only pretrained on a general and massive corpus usually cannot attain satisfying performance on domain-specific downstream tasks, and hence, applying domain-specific pretraining to LMs is a common and indispensable practice.\\nHowever, domain-specific pretraining can be costly and time-consuming, hindering LMs' deployment in real-world applications.\\nIn this work, we consider the incapability to memorize domain-specific knowledge embedded in the general corpus with rare occurrences and long-tail distributions as the leading cause for pretrained LMs' inferior downstream performance. \\nAnalysis of Neural Tangent Kernels (NTKs) reveals that those long-tail data are commonly overlooked in the model's gradient updates and, consequently, are not effectively memorized, leading to poor domain-specific downstream performance.\\nBased on the intuition that data with similar semantic meaning are closer in the embedding space, we devise a Cluster-guided Sparse Expert (CSE) layer to actively learn long-tail domain knowledge typically neglected in previous pretrained LMs.\\nDuring pretraining, a CSE layer efficiently clusters domain knowledge together and assigns long-tail knowledge to designate extra experts. CSE is also a lightweight structure that only needs to be incorporated in several deep layers.\\nWith our training strategy, we found that during pretraining, data of long-tail knowledge gradually formulate isolated, outlier clusters in an LM's representation spaces, especially in deeper layers. Our experimental results show that only pretraining CSE-based LMs is enough to achieve superior performance than regularly pretrained-finetuned LMs on various downstream tasks, implying the prospects of domain-specific-pretraining-free language models.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b28c3a4f4f5da3bb75eb2cc6852c1eb990371e11.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024once,\\ntitle={Once Read is Enough: Domain-specific Pretraining-free Language Models with Cluster-guided Sparse Experts for Long-tail Domain Knowledge},\\nauthor={Fang Dong and Mengyi Chen and Jixian Zhou and Yubin Shi and Yixuan Chen and Mingzhi Dong and Yujiang Wang and Dongsheng Li and Xiaochen Yang and Rui Zhu and Robert P. Dick and Qin Lv and Fan Yang and Tun Lu and Ning Gu and Li Shang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=manHbkpIW6}\\n}'}, 'paperhash': {'value': 'dong|once_read_is_enough_domainspecific_pretrainingfree_language_models_with_clusterguided_sparse_experts_for_longtail_domain_knowledge'}},forum = 'manHbkpIW6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14033/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14033/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14033/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14033/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mZwilh3hd2',number = 4969,cdate = 1715490853821,pdate = 1727287766327,odate = 1730873880705,mdate = 1730873880723,tcdate = 1715490853821,tmdate = 1730873880723,ddate = None,content = {'title': {'value': 'Polynomial-Time Computation of Exact $\\\\Phi$-Equilibria in Polyhedral Games'}, 'authors': {'value': ['Gabriele Farina', 'Charilaos Pipis']}, 'authorids': {'value': ['~Gabriele_Farina1', '~Charilaos_Pipis1']}, 'keywords': {'value': ['equilibrium computation', 'exact equilibria', 'extensive-form games', 'polyhedral games', 'ellipsoid', 'phi-equilibria']}, 'abstract': {'value': \"It is a well-known fact that correlated equilibria can be computed in polynomial time in a large class of concisely represented games using the celebrated Ellipsoid Against Hope algorithm \\\\citep{Papadimitriou2008:Computing, Jiang2015:Polynomial}. However, the landscape of efficiently computable equilibria in sequential (extensive-form) games remains unknown. The Ellipsoid Against Hope does not apply directly to these games, because they do not have the required ``polynomial type'' property. Despite this barrier, \\\\citet{Huang2008:Computing} altered the algorithm to compute exact extensive-form correlated equilibria.\\n\\nIn this paper, we generalize the Ellipsoid Against Hope and develop a simple algorithmic framework for efficiently computing saddle-points in bilinear zero-sum games, even when one of the dimensions is exponentially large. Moreover, the framework only requires a ``good-enough-response'' oracle, which is a weakened notion of a best-response oracle.\\n\\nUsing this machinery, we develop a general algorithmic framework for computing exact linear $\\\\Phi$-equilibria in any polyhedral game (under mild assumptions), including correlated equilibria in normal-form games, and extensive-form correlated equilibria in extensive-form games. This enables us to give the first polynomial-time algorithm for computing exact linear-deviation correlated equilibria in extensive-form games, thus resolving an open question by \\\\citet{Farina2023:Polynomial}. Furthermore, even for the cases for which a polynomial time algorithm for exact equilibria was already known, our framework provides a conceptually simpler solution.\"}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6a58ad8a87873801cdd3838c48d1d9959c0a228b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfarina2024polynomialtime,\\ntitle={Polynomial-Time Computation of Exact \\\\${\\\\textbackslash}Phi\\\\$-Equilibria in Polyhedral Games},\\nauthor={Gabriele Farina and Charilaos Pipis},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mZwilh3hd2}\\n}'}, 'paperhash': {'value': 'farina|polynomialtime_computation_of_exact_\\\\phiequilibria_in_polyhedral_games'}},forum = 'mZwilh3hd2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4969/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4969/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4969/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4969/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mZsvm58FPG',number = 3490,cdate = 1715290763110,pdate = 1727287722489,odate = 1730873866887,mdate = 1736817793284,tcdate = 1715290763110,tmdate = 1736817793284,ddate = None,content = {'title': {'value': 'ECMamba: Consolidating Selective State Space Model with Retinex Guidance for Efficient Multiple Exposure Correction'}, 'authors': {'value': ['Wei Dong', 'Han Zhou', 'Yulun Zhang', 'Xiaohong Liu', 'Jun Chen']}, 'authorids': {'value': ['~Wei_Dong9', '~Han_Zhou6', '~Yulun_Zhang1', '~Xiaohong_Liu2', '~Jun_Chen8']}, 'keywords': {'value': ['Retinex Theory', 'Mamba', 'Exposure Correction']}, 'abstract': {'value': 'Exposure Correction (EC) aims to recover proper exposure conditions for images captured under over-exposure or under-exposure scenarios. While existing deep learning models have shown promising results, few have fully embedded Retinex theory into their architecture, highlighting a gap in current methodologies. Additionally, the balance between high performance and efficiency remains an under-explored problem for exposure correction task. Inspired by Mamba which demonstrates powerful and highly efficient sequence modeling, we introduce a novel framework based on \\\\textbf{Mamba} for \\\\textbf{E}xposure \\\\textbf{C}orrection (\\\\textbf{ECMamba}) with dual pathways, each dedicated to the restoration of reflectance and illumination map, respectively. Specifically, we firstly derive the Retinex theory and we train a Retinex estimator capable of mapping inputs into two intermediary spaces, each approximating the target reflectance and illumination map, respectively. This setup facilitates the refined restoration process of the subsequent \\\\textbf{E}xposure \\\\textbf{C}orrection \\\\textbf{M}amba \\\\textbf{M}odule (\\\\textbf{ECMM}). Moreover, we develop a novel \\\\textbf{2D S}elective \\\\textbf{S}tate-space layer guided by \\\\textbf{Retinex} information (\\\\textbf{Retinex-SS2D}) as the core operator of \\\\textbf{ECMM}. This architecture incorporates an innovative 2D scanning strategy based on deformable feature aggregation, thereby enhancing both efficiency and effectiveness. Extensive experiment results and comprehensive ablation studies demonstrate the outstanding performance and the importance of each component of our proposed ECMamba. Code is available at \\\\url{https://github.com/LowlevelAI/ECMamba}.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e0014fdf94cb33e9d519342ca6b311191c040a4a.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndong2024ecmamba,\\ntitle={{ECM}amba: Consolidating Selective State Space Model with Retinex Guidance for Efficient Multiple Exposure Correction},\\nauthor={Wei Dong and Han Zhou and Yulun Zhang and Xiaohong Liu and Jun Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mZsvm58FPG}\\n}'}, 'paperhash': {'value': 'dong|ecmamba_consolidating_selective_state_space_model_with_retinex_guidance_for_efficient_multiple_exposure_correction'}},forum = 'mZsvm58FPG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3490/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3490/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3490/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3490/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mZHbkbYWTp',number = 19003,cdate = 1715788672354,pdate = 1727288199134,odate = 1730873992966,mdate = 1730873992979,tcdate = 1715788672354,tmdate = 1730873992979,ddate = None,content = {'title': {'value': 'Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise'}, 'authors': {'value': ['Francesco Damiani', 'Akiyuki Anzai', 'Jan Drugowitsch', 'Gregory C DeAngelis', 'Rubén Moreno-Bote']}, 'authorids': {'value': ['~Francesco_Damiani1', '~Akiyuki_Anzai1', '~Jan_Drugowitsch1', '~Gregory_C_DeAngelis1', '~Rubén_Moreno-Bote1']}, 'keywords': {'value': ['control theory', 'stochastic optimal control', 'sensorimotor system', 'multiplicative and internal noise', 'motor control']}, 'TLDR': {'value': 'We propose a novel iterative algorithm to solve a stochastic optimal control problem under multiplicative and internal noise, outperforming state-of-the-art solutions in the presence of internal noise and before algorithmic convergence.'}, 'abstract': {'value': 'A pivotal brain computation relies on the ability to sustain perception-action loops. Stochastic optimal control theory offers a mathematical framework to explain these processes at the algorithmic level through optimality principles. However, incorporating a realistic noise model of the sensorimotor system — accounting for multiplicative noise in feedback and motor output, as well as internal noise in estimation — makes the problem challenging. Currently, the algorithm that is commonly used is the one proposed in the seminal study in (Todorov, 2005). After discovering some pitfalls in the original derivation, i.e., unbiased estimation does not hold, we improve the algorithm by proposing an efficient gradient descent-based optimization that minimizes the cost-to-go while only imposing linearity of the control law. The optimal solution is obtained by iteratively propagating in closed form the sufficient statistics to compute the expected cost and then minimizing this cost with respect to the filter and control gains. We demonstrate that this approach results in a significantly lower overall cost than current state-of-the-art solutions, particularly in the presence of internal noise, though the improvement is present in other circumstances as well, with theoretical explanations for this enhanced performance. Providing the optimal control law is key for inverse control inference, especially in explaining behavioral data under rationality assumptions.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a826f11781a5d55afb6dcd697be52fca839946f6.pdf'}, 'supplementary_material': {'value': '/attachment/574f9e5b0f869d73c1d6ceb26a93af71ae1d7863.zip'}, '_bibtex': {'value': \"@inproceedings{\\ndamiani2024stochastic,\\ntitle={Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise},\\nauthor={Francesco Damiani and Akiyuki Anzai and Jan Drugowitsch and Gregory C DeAngelis and Rub{\\\\'e}n Moreno-Bote},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mZHbkbYWTp}\\n}\"}, 'paperhash': {'value': 'damiani|stochastic_optimal_control_and_estimation_with_multiplicative_and_internal_noise'}},forum = 'mZHbkbYWTp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19003/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19003/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19003/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19003/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'mYEjc7qGRA',number = 5591,cdate = 1715552763496,pdate = 1727287788974,odate = 1730873886321,mdate = 1730873886346,tcdate = 1715552763496,tmdate = 1730873886346,ddate = None,content = {'title': {'value': 'Towards Robust Multimodal Sentiment Analysis with Incomplete Data'}, 'authors': {'value': ['Haoyu Zhang', 'Wenbin Wang', 'Tianshu Yu']}, 'authorids': {'value': ['~Haoyu_Zhang5', '~Wenbin_Wang2', '~Tianshu_Yu2']}, 'keywords': {'value': ['Multimodal Sentiment Analysis', 'Multimodal Learning', 'Robust MSA']}, 'abstract': {'value': \"The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (e.g., MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.\"}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9da4ae43fcdb223baae597dc851a5bdef7c54a42.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024towards,\\ntitle={Towards Robust Multimodal Sentiment Analysis with Incomplete Data},\\nauthor={Haoyu Zhang and Wenbin Wang and Tianshu Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mYEjc7qGRA}\\n}'}, 'paperhash': {'value': 'zhang|towards_robust_multimodal_sentiment_analysis_with_incomplete_data'}},forum = 'mYEjc7qGRA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5591/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5591/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5591/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5591/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mY0ZnS2s9u',number = 3496,cdate = 1715291412883,pdate = 1727287722687,odate = 1730873866961,mdate = 1730873867008,tcdate = 1715291412883,tmdate = 1730873867008,ddate = None,content = {'title': {'value': 'DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering'}, 'authors': {'value': ['Zhongpai Gao', 'Benjamin Planche', 'Meng Zheng', 'Xiao Chen', 'Terrence Chen', 'Ziyan Wu']}, 'authorids': {'value': ['~Zhongpai_Gao1', '~Benjamin_Planche1', '~Meng_Zheng1', '~Xiao_Chen12', '~Terrence_Chen4', '~Ziyan_Wu2']}, 'keywords': {'value': ['3D Gaussian splatting', 'image registration', 'pose estimation']}, 'abstract': {'value': 'Digitally reconstructed radiographs (DRRs) are simulated 2D X-ray images generated from 3D CT volumes, widely used in preoperative settings but limited in intraoperative applications due to computational bottlenecks. Physics-based Monte Carlo simulations provide accurate representations but are extremely computationally intensity. Analytical DRR renderers are much more efficient, but at the price of ignoring anisotropic X-ray image formation phenomena such as Compton scattering. We propose a novel approach that balances realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS). Our direction-disentangled 3DGS (DDGS) method decomposes the radiosity contribution into isotropic and direction-dependent components, able to approximate complex anisotropic interactions without complex runtime simulations. Additionally, we adapt the 3DGS initialization to account for tomography data properties, enhancing accuracy and efficiency. Our method outperforms state-of-the-art techniques in image accuracy and inference speed, demonstrating its potential for intraoperative applications and inverse problems like pose registration.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0c564dbb6edaaa1d191947e75b2e6803793b4e5d.pdf'}, 'TLDR': {'value': 'A novel approach that balances realistic physics-inspired X-ray simulation with efficient, differentiable DRR generation using 3D Gaussian splatting (3DGS)'}, '_bibtex': {'value': '@inproceedings{\\ngao2024ddgsct,\\ntitle={{DDGS}-{CT}: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering},\\nauthor={Zhongpai Gao and Benjamin Planche and Meng Zheng and Xiao Chen and Terrence Chen and Ziyan Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mY0ZnS2s9u}\\n}'}, 'paperhash': {'value': 'gao|ddgsct_directiondisentangled_gaussian_splatting_for_realistic_volume_rendering'}},forum = 'mY0ZnS2s9u',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3496/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3496/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3496/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3496/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mXpq6ut8J3',number = 5053,cdate = 1715499664054,pdate = 1727287771881,odate = 1730873881548,mdate = 1731566939428,tcdate = 1715499664054,tmdate = 1731566939428,ddate = None,content = {'title': {'value': 'SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering'}, 'authors': {'value': ['John Yang', 'Carlos E Jimenez', 'Alexander Wettig', 'Kilian Lieret', 'Shunyu Yao', 'Karthik R Narasimhan', 'Ofir Press']}, 'authorids': {'value': ['~John_Yang3', '~Carlos_E_Jimenez1', '~Alexander_Wettig1', '~Kilian_Lieret1', '~Shunyu_Yao1', '~Karthik_R_Narasimhan1', '~Ofir_Press1']}, 'keywords': {'value': ['Language models', 'Natural language processing', 'Software engineering']}, 'abstract': {'value': \"Language model agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that language model agents represent a new category of end users with their own needs and abilities, and would benefit from specially built interfaces to the software they use. We investigate how the role of interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates language model agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive language models. Finally, we provide insight on how the design of the agent-computer interface can impact agents' behavior and performance.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce the concept of agent-computer interfaces to enhance LM agent performance on software engineering tasks.'}, 'pdf': {'value': '/pdf/7b9425730150fb166d4e6c77995f67ea38638fca.pdf'}, 'supplementary_material': {'value': '/attachment/3dfcb29ed523e757fce7d25335349794e197bade.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024sweagent,\\ntitle={{SWE}-agent: Agent-Computer Interfaces Enable Automated Software Engineering},\\nauthor={John Yang and Carlos E Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik R Narasimhan and Ofir Press},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mXpq6ut8J3}\\n}'}, 'paperhash': {'value': 'yang|sweagent_agentcomputer_interfaces_enable_automated_software_engineering'}},forum = 'mXpq6ut8J3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5053/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5053/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5053/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5053/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mXlR1FLFDc',number = 21077,cdate = 1715799956064,pdate = 1727288248107,odate = 1730874004553,mdate = 1737011614981,tcdate = 1715799956064,tmdate = 1737011614981,ddate = None,content = {'title': {'value': 'A Compositional Atlas for Algebraic Circuits'}, 'authors': {'value': ['Benjie Wang', 'Denis Mauá', 'Guy Van den Broeck', 'YooJung Choi']}, 'authorids': {'value': ['~Benjie_Wang1', '~Denis_Mauá1', '~Guy_Van_den_Broeck1', '~YooJung_Choi1']}, 'keywords': {'value': ['semiring', 'probabilistic circuits', 'logic circuits', 'probabilistic inference', 'algebraic']}, 'abstract': {'value': 'Circuits based on sum-product structure have become a ubiquitous representation to compactly encode knowledge, from Boolean functions to probability distributions. By imposing constraints on the structure of such circuits, certain inference queries become tractable, such as model counting and most probable configuration. Recent works have explored analyzing probabilistic and causal inference queries\\nas compositions of basic operators to derive tractability conditions. In this paper, we take an algebraic perspective for compositional inference, and show that a large class of queries—including marginal MAP, probabilistic answer set programming inference, and causal backdoor adjustment—correspond to a combination of basic operators over semirings: aggregation, product, and elementwise mapping. Using this framework, we uncover simple and general sufficient conditions for tractable composition of these operators, in terms of circuit properties (e.g., marginal determinism, compatibility) and conditions on the elementwise mappings. Applying our analysis, we derive novel tractability conditions for many such compositional queries. Our results unify tractability conditions for existing problems on circuits, while providing a blueprint for analysing novel compositional inference queries.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a unifying framework for deriving algorithms and tractability conditions for complex compositional inference queries, such as marginal MAP, logic programming inference and causal inference.'}, 'pdf': {'value': '/pdf/bd4c61aeb4f7164f2050e25fbba45bc042146c20.pdf'}, '_bibtex': {'value': \"@inproceedings{\\nwang2024a,\\ntitle={A Compositional Atlas for Algebraic Circuits},\\nauthor={Benjie Wang and Denis Mau{\\\\'a} and Guy Van den Broeck and YooJung Choi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mXlR1FLFDc}\\n}\"}, 'paperhash': {'value': 'wang|a_compositional_atlas_for_algebraic_circuits'}},forum = 'mXlR1FLFDc',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21077/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21077/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21077/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21077/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mVfRrMfGdY',number = 16854,cdate = 1715774989423,pdate = 1727288138965,odate = 1730873981963,mdate = 1730873981992,tcdate = 1715774989423,tmdate = 1730873981992,ddate = None,content = {'title': {'value': 'Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification'}, 'authors': {'value': ['Jan Schuchardt', 'Mihail Stoian', 'Arthur Kosmala', 'Stephan Günnemann']}, 'authorids': {'value': ['~Jan_Schuchardt1', '~Mihail_Stoian1', '~Arthur_Kosmala1', '~Stephan_Günnemann1']}, 'keywords': {'value': ['Differential Privacy', 'Privacy-Preserving Machine Learning', 'Amplification', 'Subsampling', 'Accounting']}, 'TLDR': {'value': 'We propose a unified procedure for deriving amplification-by-subsampling guarantees and use it to analyze group privacy under subsampling.'}, 'abstract': {'value': \"Amplification by subsampling is one of the main primitives in machine learning with differential privacy (DP): Training a model on random batches instead of complete datasets results in stronger privacy. This is traditionally formalized via mechanism-agnostic subsampling guarantees that express the privacy parameters of a subsampled mechanism as a function of the original mechanism's privacy parameters. We propose the first general framework for deriving mechanism-specific guarantees, which leverage additional information beyond these parameters to more tightly characterize the subsampled mechanism's privacy. Such guarantees are of particular importance for privacy accounting, i.e., tracking privacy over multiple iterations. Overall, our framework based on conditional optimal transport lets us derive existing and novel guarantees for approximate DP, accounting with Renyi DP, and accounting with dominating pairs in a unified, principled manner. As an application, we analyze how subsampling affects the privacy of groups of multiple users. Our tight mechanism-specific bounds outperform tight mechanism-agnostic bounds and classic group privacy results.\"}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/cb943281d091aa59c4fcfbdf6cf59974ec38b6dc.pdf'}, '_bibtex': {'value': '@inproceedings{\\nschuchardt2024unified,\\ntitle={Unified Mechanism-Specific Amplification by Subsampling and Group Privacy Amplification},\\nauthor={Jan Schuchardt and Mihail Stoian and Arthur Kosmala and Stephan G{\\\\\"u}nnemann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mVfRrMfGdY}\\n}'}, 'paperhash': {'value': 'schuchardt|unified_mechanismspecific_amplification_by_subsampling_and_group_privacy_amplification'}},forum = 'mVfRrMfGdY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16854/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16854/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16854/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16854/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mTAbl8kUzq',number = 10817,cdate = 1715697953509,pdate = 1727287952041,odate = 1730873932443,mdate = 1730873932461,tcdate = 1715697953509,tmdate = 1730873932461,ddate = None,content = {'title': {'value': 'LiteVAE: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models'}, 'authors': {'value': ['Seyedmorteza Sadat', 'Jakob Buhmann', 'Derek Bradley', 'Otmar Hilliges', 'Romann M. Weber']}, 'authorids': {'value': ['~Seyedmorteza_Sadat1', '~Jakob_Buhmann1', '~Derek_Bradley1', '~Otmar_Hilliges1', '~Romann_M._Weber1']}, 'keywords': {'value': ['variational autoencoders', 'latent diffusion models']}, 'TLDR': {'value': 'We introduce an efficiency-adapted variational autoencoder design for latent diffusion models with improved scalability and computational efficiency.'}, 'abstract': {'value': 'Advances in latent diffusion models (LDMs) have revolutionized high-resolution image generation, but the design space of the autoencoder that is central to these systems remains underexplored. In this paper, we introduce LiteVAE, a new autoencoder design for LDMs, which leverages the 2D discrete wavelet transform to enhance scalability and computational efficiency over standard variational autoencoders (VAEs) with no sacrifice in output quality. We investigate the training methodologies and the decoder architecture of LiteVAE and propose several enhancements that improve the training dynamics and reconstruction quality. Our base LiteVAE model matches the quality of the established VAEs in current LDMs with a six-fold reduction in encoder parameters, leading to faster training and lower GPU memory requirements, while our larger model outperforms VAEs of comparable complexity across all evaluated metrics (rFID, LPIPS, PSNR, and SSIM).'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b1fce93bd97fc7aaa3b8834237bc72fd9abd3095.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsadat2024litevae,\\ntitle={Lite{VAE}: Lightweight and Efficient Variational Autoencoders for Latent Diffusion Models},\\nauthor={Seyedmorteza Sadat and Jakob Buhmann and Derek Bradley and Otmar Hilliges and Romann M. Weber},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mTAbl8kUzq}\\n}'}, 'paperhash': {'value': 'sadat|litevae_lightweight_and_efficient_variational_autoencoders_for_latent_diffusion_models'}},forum = 'mTAbl8kUzq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10817/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10817/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10817/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10817/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'mSaqxZVZW8',number = 1325,cdate = 1714568354478,pdate = 1727287657728,odate = 1730873847554,mdate = 1734573453510,tcdate = 1714568354478,tmdate = 1734573453510,ddate = None,content = {'title': {'value': 'SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling'}, 'authors': {'value': ['Dengwei Zhao', 'Shikui Tu', 'Lei Xu']}, 'authorids': {'value': ['~Dengwei_Zhao1', '~Shikui_Tu1', '~Lei_Xu7']}, 'keywords': {'value': ['search algorithm', 'reinforcement learning', 'exploration']}, 'TLDR': {'value': 'SeeA* is proposed to incorporate exploration into A* search by introducing an dynamic candidate set.'}, 'abstract': {'value': 'Monte-Carlo tree search (MCTS) and reinforcement learning contributed crucially to the success of AlphaGo and AlphaZero, and A$^*$ is a tree search algorithm among the most well-known ones in the classical AI literature. MCTS and  A$^*$ both perform heuristic search and are mutually beneficial. Efforts have been made to the renaissance of A$^*$ from three possible aspects, two of which have been confirmed by studies in recent years, while the third is about the OPEN list that consists of open nodes of A$^*$ search, but still lacks deep investigation. This paper aims at the third, i.e., developing the Sampling-exploration enhanced A$^*$ (SeeA$^*$) search by constructing a dynamic subset of OPEN through a selective sampling process, such that the node with the best heuristic value in this subset instead of in the OPEN is expanded. Nodes with the best heuristic values in OPEN are most probably picked into this subset, but sometimes may not be included, which enables SeeA$^*$ to explore other promising branches. Three sampling techniques are presented for comparative investigations. Moreover, under the assumption about the distribution of prediction errors, we have theoretically shown the superior efficiency of SeeA$^*$ over A$^*$ search, particularly when the accuracy of the guiding heuristic function is insufficient. Experimental results on retrosynthetic planning in organic chemistry, logic synthesis in integrated circuit design, and the classical Sokoban game empirically demonstrate the efficiency of SeeA$^*$, in comparison with the state-of-the-art heuristic search algorithms.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fa5dedfe169ea46edcf332d8d7d9b5256b506793.pdf'}, 'supplementary_material': {'value': '/attachment/c251dd9ab9e8bc48d54131bc46852e44a599927d.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024seea,\\ntitle={SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling},\\nauthor={Dengwei Zhao and Shikui Tu and Lei Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mSaqxZVZW8}\\n}'}, 'paperhash': {'value': 'zhao|seea_efficient_explorationenhanced_a_search_by_selective_sampling'}},forum = 'mSaqxZVZW8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1325/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1325/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1325/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1325/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mSHs6C7Nfa',number = 10536,cdate = 1715695045337,pdate = 1727287943963,odate = 1730873930195,mdate = 1730873930213,tcdate = 1715695045337,tmdate = 1730873930213,ddate = None,content = {'title': {'value': 'Improving the Training of Rectified Flows'}, 'authors': {'value': ['Sangyun Lee', 'Zinan Lin', 'Giulia Fanti']}, 'authorids': {'value': ['~Sangyun_Lee1', '~Zinan_Lin1', '~Giulia_Fanti1']}, 'keywords': {'value': ['generative modeling', 'rectified flow', 'diffusion model']}, 'abstract': {'value': 'Diffusion models have shown great promise for image and video generation, but sampling from state-of-the-art models requires expensive numerical integration of a generative ODE.\\n    One approach for tackling this problem is rectified flows, which iteratively learn smooth ODE paths that are less susceptible to truncation error.\\n    However, rectified flows still require a relatively large number of function evaluations (NFEs).\\n    In this work, we propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting.\\n    Our main insight is that under realistic settings, a single iteration of the Reflow algorithm for training rectified flows is sufficient to learn nearly straight trajectories; hence, the current practice of using multiple Reflow iterations is unnecessary.\\n    We thus propose techniques to improve one-round training of rectified flows, including a U-shaped timestep distribution and LPIPS-Huber premetric.\\n    With these techniques, we improve the FID of the previous 2-rectified flow by up to 75\\\\% in the 1 NFE setting on CIFAR-10.\\n    On ImageNet 64$\\\\times$64, our improved rectified flow outperforms the state-of-the-art distillation methods\\n    such as consistency distillation and progressive distillation in both one-step and two-step settings and rivals the performance of improved consistency training (iCT) in FID.\\n    Code is available at https://github.com/sangyun884/rfpp.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose improved techniques for training rectified flows, allowing them to compete with knowledge distillation methods even in the low NFE setting'}, 'pdf': {'value': '/pdf/876809b80692e3d3bb48e5861b766c0b86adece6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024improving,\\ntitle={Improving the Training of Rectified Flows},\\nauthor={Sangyun Lee and Zinan Lin and Giulia Fanti},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mSHs6C7Nfa}\\n}'}, 'paperhash': {'value': 'lee|improving_the_training_of_rectified_flows'}},forum = 'mSHs6C7Nfa',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10536/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10536/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10536/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10536/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'mRIQz8Zd6O',number = 1364,cdate = 1714613159177,pdate = 1727287658749,odate = 1730873847980,mdate = 1730873847998,tcdate = 1714613159177,tmdate = 1730873847998,ddate = None,content = {'title': {'value': 'AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents'}, 'authors': {'value': ['Yao Fu', 'Dong-Ki Kim', 'Jaekyeom Kim', 'Sungryull Sohn', 'Lajanugen Logeswaran', 'Kyunghoon Bae', 'Honglak Lee']}, 'authorids': {'value': ['~Yao_Fu5', '~Dong-Ki_Kim1', '~Jaekyeom_Kim1', '~Sungryull_Sohn1', '~Lajanugen_Logeswaran1', '~Kyunghoon_Bae2', '~Honglak_Lee2']}, 'keywords': {'value': ['large language model agents', 'sequential decision-making']}, 'abstract': {'value': \"Recent advances in large language models (LLMs) have empowered AI agents capable of performing various sequential decision-making tasks. However, effectively guiding LLMs to perform well in unfamiliar domains like web navigation, where they lack sufficient knowledge, has proven to be difficult with the demonstration-based in-context learning paradigm. In this paper, we introduce a novel framework, called AutoGuide, which addresses this limitation by automatically generating context-aware guidelines from offline experiences. Importantly, each context-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the context where it is applicable. As a result, our guidelines facilitate the provision of relevant knowledge for the agent's current decision-making process, overcoming the limitations of the conventional demonstration-based learning paradigm. Our evaluation demonstrates that AutoGuide significantly outperforms competitive baselines in complex benchmark domains, including real-world web navigation.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'AutoGuide extracts a comprehensive set of context-aware guidelines to improve the sequential decision-making ability of large language model agents.'}, 'pdf': {'value': '/pdf/72f0fa8a1ec129e7e890cda96edbcc8d2d98f171.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfu2024autoguide,\\ntitle={AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents},\\nauthor={Yao Fu and Dong-Ki Kim and Jaekyeom Kim and Sungryull Sohn and Lajanugen Logeswaran and Kyunghoon Bae and Honglak Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mRIQz8Zd6O}\\n}'}, 'paperhash': {'value': 'fu|autoguide_automated_generation_and_selection_of_contextaware_guidelines_for_large_language_model_agents'}},forum = 'mRIQz8Zd6O',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1364/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1364/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1364/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1364/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mP084aMFsd',number = 18531,cdate = 1715785969163,pdate = 1727288186711,odate = 1730873990667,mdate = 1730873990686,tcdate = 1715785969163,tmdate = 1730873990686,ddate = None,content = {'title': {'value': 'A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences'}, 'authors': {'value': ['Mingjia Li', 'Shuo Liu', 'Hong Qian', 'Aimin Zhou']}, 'authorids': {'value': ['~Mingjia_Li3', '~Shuo_Liu7', '~Hong_Qian1', '~Aimin_Zhou1']}, 'keywords': {'value': ['Telecommunication Network Fault Diagnosis', 'Topological Hawkes Processes', 'Causal structure learning', 'Event Sequences', 'Scalability']}, 'abstract': {'value': 'In modern telecommunication networks, faults manifest as alarms, generating thousands of events daily. Network operators need an efficient method to identify the root causes of these alarms to mitigate potential losses. This task is challenging due to the increasing scale of telecommunication networks and the interconnected nature of devices, where one fault can trigger a cascade of alarms across multiple devices within a topological network. Recent years have seen a growing focus on causal approaches to addressing this problem, emphasizing the importance of learning a Granger causal graph from topological event sequences. Such causal graphs delineate the relations among alarms and can significantly aid engineers in identifying and rectifying faults. However, existing methods either ignore the topological relationships among devices or suffer from relatively low scalability and efficiency, failing to deliver high-quality responses in a timely manner. To this end, this paper proposes $S^2GCSL$, a simple yet scalable Granger causal structural learning approach for topological event sequences. $S^2GCSL$ utilizes a linear kernel to model activation interactions among various event types within a topological network, and employs gradient descent to efficiently optimize the likelihood function. Notably, it can seamlessly incorporate expert knowledge as constraints within the optimization process, which enhances the interpretability of the outcomes. Extensive experimental results on both large-scale synthetic and real-world problems verify the scalability and efficacy of $S^2GCSL$.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5ffde646c6f78d98a34098818f079bb9ae62df7b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024a,\\ntitle={A Simple yet Scalable Granger Causal Structural Learning Approach for Topological Event Sequences},\\nauthor={Mingjia Li and Shuo Liu and Hong Qian and Aimin Zhou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mP084aMFsd}\\n}'}, 'paperhash': {'value': 'li|a_simple_yet_scalable_granger_causal_structural_learning_approach_for_topological_event_sequences'}},forum = 'mP084aMFsd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18531/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18531/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18531/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18531/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'mOK4yD8JFd',number = 631,cdate = 1714012111792,pdate = 1727287642168,odate = 1730873842460,mdate = 1734573138160,tcdate = 1714012111792,tmdate = 1734573138160,ddate = None,content = {'title': {'value': 'Quality-Improved and Property-Preserved Polarimetric Imaging via Complementarily Fusing'}, 'authors': {'value': ['Chu Zhou', 'Yixing Liu', 'Chao Xu', 'Boxin Shi']}, 'authorids': {'value': ['~Chu_Zhou1', '~Yixing_Liu7', '~Chao_Xu1', '~Boxin_Shi3']}, 'keywords': {'value': ['Polarimetric Imaging', 'Exposure fusion', 'Deep Learning']}, 'TLDR': {'value': 'We propose a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones.'}, 'abstract': {'value': 'Polarimetric imaging is a challenging problem in the field of polarization-based vision, since setting a short exposure time reduces the signal-to-noise ratio, making the degree of polarization (DoP) and the angle of polarization (AoP) severely degenerated, while if setting a relatively long exposure time, the DoP and AoP would tend to be over-smoothed due to the frequently-occurring motion blur. This work proposes a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones. By adopting a neural network-based three-phase fusing scheme with specially-designed modules tailored to each phase, our framework can not only improve the image quality but also preserve the polarization properties. Experimental results show that our framework achieves state-of-the-art performance.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ad7ff9defdfad11680f8cbd2354bff708adb0441.pdf'}, 'supplementary_material': {'value': '/attachment/f182b64be525a6ba17ceab047d6561795d8d167b.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024qualityimproved,\\ntitle={Quality-Improved and Property-Preserved Polarimetric Imaging via Complementarily Fusing},\\nauthor={Chu Zhou and Yixing Liu and Chao Xu and Boxin Shi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mOK4yD8JFd}\\n}'}, 'paperhash': {'value': 'zhou|qualityimproved_and_propertypreserved_polarimetric_imaging_via_complementarily_fusing'}},forum = 'mOK4yD8JFd',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission631/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission631/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission631/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'mJZH9w8qgu',number = 2883,cdate = 1715177123389,pdate = 1727287703763,odate = 1730873861712,mdate = 1736616410518,tcdate = 1715177123389,tmdate = 1736616410518,ddate = None,content = {'title': {'value': 'In-Trajectory Inverse Reinforcement Learning: Learn Incrementally Before an Ongoing Trajectory Terminates'}, 'authors': {'value': ['Shicheng Liu', 'Minghui Zhu']}, 'authorids': {'value': ['~Shicheng_Liu1', '~Minghui_Zhu1']}, 'keywords': {'value': ['inverse reinforcement learning']}, 'abstract': {'value': 'Inverse reinforcement learning (IRL) aims to learn a reward function and a corresponding policy that best fit the demonstrated trajectories of an expert. However, current IRL works cannot learn incrementally from an ongoing trajectory because they have to wait to collect at least one complete trajectory to learn. To bridge the gap, this paper considers the problem of learning a reward function and a corresponding policy while observing the initial state-action pair of an ongoing trajectory and keeping updating the learned reward and policy when new state-action pairs of the ongoing trajectory are observed. We formulate this problem as an online bi-level optimization problem where the upper level dynamically adjusts the learned reward according to the newly observed state-action pairs with the help of a meta-regularization term, and the lower level learns the corresponding policy. We propose a novel algorithm to solve this problem and guarantee that the algorithm achieves sub-linear local regret $O(\\\\sqrt{T}+\\\\log T+\\\\sqrt{T}\\\\log T)$. If the reward function is linear, we prove that the proposed algorithm achieves sub-linear regret $O(\\\\log T)$. Experiments are used to validate the proposed algorithm.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/c1baf38d10361bc13924a33244c763faf63aa595.zip'}, 'pdf': {'value': '/pdf/ea746bec3dcd6ab777a0d7beef69a24679f99785.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024intrajectory,\\ntitle={In-Trajectory Inverse Reinforcement Learning: Learn Incrementally From An Ongoing Trajectory},\\nauthor={Shicheng Liu and Minghui Zhu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mJZH9w8qgu}\\n}'}, 'paperhash': {'value': 'liu|intrajectory_inverse_reinforcement_learning_learn_incrementally_before_an_ongoing_trajectory_terminates'}},forum = 'mJZH9w8qgu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2883/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2883/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2883/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2883/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mHtOyh5taj',number = 937,cdate = 1714223984649,pdate = 1727287647943,odate = 1730873844210,mdate = 1730873844229,tcdate = 1714223984649,tmdate = 1730873844229,ddate = None,content = {'title': {'value': 'Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare'}, 'authors': {'value': ['Hanwei Zhu', 'Haoning Wu', 'Yixuan Li', 'Zicheng Zhang', 'Baoliang Chen', 'Lingyu Zhu', 'Yuming Fang', 'Guangtao Zhai', 'Weisi Lin', 'Shiqi Wang']}, 'authorids': {'value': ['~Hanwei_Zhu1', '~Haoning_Wu1', '~Yixuan_Li12', '~Zicheng_Zhang7', '~Baoliang_Chen2', '~Lingyu_Zhu3', '~Yuming_Fang1', '~Guangtao_Zhai1', '~Weisi_Lin1', '~Shiqi_Wang1']}, 'keywords': {'value': ['Image Quality Assessment', 'Paired Comparsion', 'Large Multimodal Model']}, 'abstract': {'value': 'While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce an all-around LMM-based NR-IQA model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparison outcomes into a continuous quality score. Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality scores for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4a7c252d88318fd8cb811b8d23cd9956a27dc305.pdf'}, 'supplementary_material': {'value': '/attachment/8cf5ec211f31b5c95ff2c2c734218c1922377f95.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024adaptive,\\ntitle={Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare},\\nauthor={Hanwei Zhu and Haoning Wu and Yixuan Li and Zicheng Zhang and Baoliang Chen and Lingyu Zhu and Yuming Fang and Guangtao Zhai and Weisi Lin and Shiqi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mHtOyh5taj}\\n}'}, 'paperhash': {'value': 'zhu|adaptive_image_quality_assessment_via_teaching_large_multimodal_model_to_compare'}},forum = 'mHtOyh5taj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission937/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission937/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission937/-/Revision', 'NeurIPS.cc/2024/Conference/Submission937/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mHVmsy9len',number = 12761,cdate = 1715727896806,pdate = 1727288016672,odate = 1730873951620,mdate = 1730873951632,tcdate = 1715727896806,tmdate = 1730873951632,ddate = None,content = {'title': {'value': 'Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension'}, 'authors': {'value': ['Kedar Karhadkar', 'Michael Murray', 'Guido Montufar']}, 'authorids': {'value': ['~Kedar_Karhadkar1', '~Michael_Murray3', '~Guido_Montufar1']}, 'keywords': {'value': ['neural tangent kernel', 'initialization', 'minimum eigenvalue', 'smallest eigenvalue', 'low-dimensional', 'hemisphere transform', 'spherical harmonics', 'separated']}, 'abstract': {'value': 'Bounds on the smallest eigenvalue of the neural tangent kernel (NTK) are a key ingredient in the analysis of neural network optimization and memorization. However, existing results require distributional assumptions on the data and are limited to a high-dimensional setting, where the input dimension $d_0$ scales at least logarithmically in the number of samples $n$. In this work we remove both of these requirements and instead provide bounds in terms of a measure of distance between data points: notably these bounds hold with high probability even when $d_0$ is held constant versus $n$. We prove our results through a novel application of the hemisphere transform.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We bound the smallest eigenvalue of the NTK without distributional assumptions on the data.'}, 'pdf': {'value': '/pdf/fc64dfe0d79cb125c2577c3c2488762284e984b7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkarhadkar2024bounds,\\ntitle={Bounds for the smallest eigenvalue of the {NTK} for arbitrary spherical data of arbitrary dimension},\\nauthor={Kedar Karhadkar and Michael Murray and Guido Montufar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mHVmsy9len}\\n}'}, 'paperhash': {'value': 'karhadkar|bounds_for_the_smallest_eigenvalue_of_the_ntk_for_arbitrary_spherical_data_of_arbitrary_dimension'}},forum = 'mHVmsy9len',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12761/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12761/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12761/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12761/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mH1xtt2bJE',number = 4716,cdate = 1715443040601,pdate = 1727287758742,odate = 1730873878598,mdate = 1730873878617,tcdate = 1715443040601,tmdate = 1730873878617,ddate = None,content = {'title': {'value': 'MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts'}, 'authors': {'value': ['RENCHUNZI XIE', 'Ambroise Odonnat', 'Vasilii Feofanov', 'Weijian Deng', 'Jianfeng Zhang', 'Bo An']}, 'authorids': {'value': ['~RENCHUNZI_XIE1', '~Ambroise_Odonnat1', '~Vasilii_Feofanov1', '~Weijian_Deng1', '~Jianfeng_Zhang2', '~Bo_An2']}, 'keywords': {'value': ['Unsupervised Learning', 'Distribution Shifts', 'Unsupervised Accuracy Estimation', 'Generalization', 'Deep Learning']}, 'TLDR': {'value': 'Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts'}, 'abstract': {'value': \"Leveraging the model’s outputs, specifically the logits, is a common approach to estimating the test accuracy of a pre-trained neural network on out-of-distribution (OOD) samples without requiring access to the corresponding ground-truth labels.\\nDespite their ease of implementation and computational efficiency, current logit-based methods are vulnerable to overconfidence issues, leading to prediction bias, especially under the natural shift. In this work, we first study the relationship between logits and generalization performance from the view of low-density separation assumption. Our findings motivate our proposed method \\\\method{} that \\\\textbf{(1)}~applies a data-dependent normalization on the logits to reduce prediction bias, and \\\\textbf{(2)} takes the $L_p$ norm of the matrix of normalized logits as the estimation score. Our theoretical analysis highlights the connection between the provided score and the model's uncertainty. \\nWe conduct an extensive empirical study on common unsupervised accuracy estimation benchmarks and demonstrate that \\\\method{} achieves state-of-the-art performance across various architectures in the presence of synthetic, natural, or subpopulation shifts. The code is available at https://github.com/Renchunzi-Xie/MaNo.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9631c6bbc3edf891cd13ddecc4321745d5758956.pdf'}, '_bibtex': {'value': '@inproceedings{\\nxie2024mano,\\ntitle={MaNo: Exploiting Matrix Norm for Unsupervised Accuracy Estimation Under Distribution Shifts},\\nauthor={RENCHUNZI XIE and Ambroise Odonnat and Vasilii Feofanov and Weijian Deng and Jianfeng Zhang and Bo An},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mH1xtt2bJE}\\n}'}, 'paperhash': {'value': 'xie|mano_exploiting_matrix_norm_for_unsupervised_accuracy_estimation_under_distribution_shifts'}},forum = 'mH1xtt2bJE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4716/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4716/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4716/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4716/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'mGz3Jux9wS',number = 14898,cdate = 1715755054752,pdate = 1727288082352,odate = 1730873969148,mdate = 1734667559964,tcdate = 1715755054752,tmdate = 1734667559964,ddate = None,content = {'title': {'value': 'Long-tailed Object Detection Pretraining: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction'}, 'authors': {'value': ['Chen-Long Duan', 'Yong Li', 'Xiu-Shen Wei', 'Lin Zhao']}, 'authorids': {'value': ['~Chen-Long_Duan1', '~Yong_Li13', '~Xiu-Shen_Wei1', '~Lin_Zhao14']}, 'keywords': {'value': ['Long-tailed object detection', 'pretraining']}, 'abstract': {'value': 'Pre-training plays a vital role in various vision tasks, such as object recognition and detection. Commonly used pre-training methods, which typically rely on randomized approaches like uniform or Gaussian distributions to initialize model parameters, often fall short when confronted with long-tailed distributions, especially in detection tasks. This is largely due to extreme data imbalance and the issue of simplicity bias. In this paper, we introduce a novel pre-training framework for object detection, called Dynamic Rebalancing Contrastive Learning with Dual Reconstruction (2DRCL). Our method builds on a Holistic-Local Contrastive Learning mechanism, which aligns pre-training with object detection by capturing both global contextual semantics and detailed local patterns. To tackle the imbalance inherent in long-tailed data, we design a dynamic rebalancing strategy that adjusts the sampling of underrepresented instances throughout the pre-training process, ensuring better representation of tail classes. Moreover, Dual Reconstruction addresses simplicity bias by enforcing a reconstruction task aligned with the self-consistency principle, specifically benefiting underrepresented tail classes. Experiments on COCO and LVIS v1.0 datasets demonstrate the effectiveness of our method, particularly in improving the mAP/AP scores for tail classes.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c7fd268804da2070567d713d2fd8604f13b995c6.pdf'}, '_bibtex': {'value': '@inproceedings{\\nduan2024longtailed,\\ntitle={Long-tailed Object Detection Pretraining: Dynamic Rebalancing Contrastive Learning with Dual Reconstruction},\\nauthor={Chen-Long Duan and Yong Li and Xiu-Shen Wei and Lin Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mGz3Jux9wS}\\n}'}, 'paperhash': {'value': 'duan|longtailed_object_detection_pretraining_dynamic_rebalancing_contrastive_learning_with_dual_reconstruction'}},forum = 'mGz3Jux9wS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14898/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14898/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14898/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14898/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mFrlCI8sov',number = 12228,cdate = 1715718740214,pdate = 1727287999336,odate = 1730873946973,mdate = 1735761155999,tcdate = 1715718740214,tmdate = 1735761155999,ddate = None,content = {'title': {'value': 'Interventional Causal Discovery in a Mixture of DAGs'}, 'authors': {'value': ['Burak Varıcı', 'Dmitriy A Katz', 'Dennis Wei', 'Prasanna Sattigeri', 'Ali Tajer']}, 'authorids': {'value': ['~Burak_Varıcı1', '~Dmitriy_A_Katz1', '~Dennis_Wei1', '~Prasanna_Sattigeri1', '~Ali_Tajer1']}, 'keywords': {'value': ['causal discovery', 'intervention design', 'mixture of DAGs', 'mixture models']}, 'abstract': {'value': 'Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) an inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as the *true* edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\\\\cal O}(n^2)$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the *cyclic complexity number* of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1b479d170e7f3c5cbb35a67cb00b9fc4c4843848.pdf'}, '_bibtex': {'value': '@inproceedings{\\nvar{\\\\i}c{\\\\i}2024interventional,\\ntitle={Interventional Causal Discovery in a Mixture of {DAG}s},\\nauthor={Burak Var{\\\\i}c{\\\\i} and Dmitriy A Katz and Dennis Wei and Prasanna Sattigeri and Ali Tajer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mFrlCI8sov}\\n}'}, 'paperhash': {'value': 'varc|interventional_causal_discovery_in_a_mixture_of_dags'}},forum = 'mFrlCI8sov',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12228/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12228/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12228/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12228/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mCWZj7pa0M',number = 11370,cdate = 1715704114661,pdate = 1727287969162,odate = 1730873938139,mdate = 1730873938162,tcdate = 1715704114661,tmdate = 1730873938162,ddate = None,content = {'title': {'value': 'Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals'}, 'authors': {'value': ['Christian Holberg', 'Cristopher Salvi']}, 'authorids': {'value': ['~Christian_Holberg1', '~Cristopher_Salvi1']}, 'keywords': {'value': ['stochastic differential equations', 'spiking neural networks', 'backpropagation', 'rough paths', 'signatures']}, 'abstract': {'value': \"We introduce a mathematically rigorous framework based on rough path theory to model stochastic spiking neural networks (SSNNs) as stochastic differential equations with event discontinuities (Event SDEs) and driven by càdlàg rough paths. Our formalism is general enough to allow for potential jumps to be present both in the solution trajectories as well as in the driving noise. We then identify a set of sufficient conditions ensuring the existence of pathwise gradients of solution trajectories and event times with respect to the network's parameters and show how these gradients satisfy a recursive relation. Furthermore, we introduce a general-purpose loss function defined by means of a new class of signature kernels indexed on càdlàg rough paths and use it to train SSNNs as generative models. We provide an end-to-end autodifferentiable solver for Event SDEs  and make its implementation available as part of the $\\\\texttt{diffrax}$ library. Our framework is, to our knowledge, the first enabling gradient-based training of SSNNs with noise affecting both the spike timing and the network's dynamics.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/12bfd1313c2ad9fae822910f9e4239ddc9fb8525.pdf'}, 'supplementary_material': {'value': '/attachment/9e08e5f309e846752db3ca8580f4ff48f9d37ef6.zip'}, '_bibtex': {'value': '@inproceedings{\\nholberg2024exact,\\ntitle={Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals},\\nauthor={Christian Holberg and Cristopher Salvi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mCWZj7pa0M}\\n}'}, 'paperhash': {'value': 'holberg|exact_gradients_for_stochastic_spiking_neural_networks_driven_by_rough_signals'}},forum = 'mCWZj7pa0M',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11370/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11370/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11370/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11370/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'mAdGQ1Hh3L',number = 9378,cdate = 1715678983553,pdate = 1727287908940,odate = 1730873920372,mdate = 1730873920390,tcdate = 1715678983553,tmdate = 1730873920390,ddate = None,content = {'title': {'value': 'START: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation'}, 'authors': {'value': ['Jintao Guo', 'Lei Qi', 'Yinghuan Shi', 'Yang Gao']}, 'authorids': {'value': ['~Jintao_Guo1', '~Lei_Qi1', '~Yinghuan_Shi3', '~Yang_Gao3']}, 'keywords': {'value': ['Domain generalization', 'state space models']}, 'abstract': {'value': 'Domain Generalization (DG) aims to enable models to generalize to unseen target domains by learning from multiple source domains. Existing DG methods primarily rely on convolutional neural networks (CNNs), which inherently learn texture biases due to their limited receptive fields, making them prone to overfitting source domains. While some works have introduced transformer-based methods (ViTs) for DG to leverage the global receptive field, these methods incur high computational costs due to the quadratic complexity of self-attention. Recently, advanced state space models (SSMs), represented by Mamba, have shown promising results in supervised learning tasks by achieving linear complexity in sequence length during training and fast RNN-like computation during inference. Inspired by this, we investigate the generalization ability of the Mamba model under domain shifts and find that input-dependent matrices within SSMs could accumulate and amplify domain-specific features, thus hindering model generalization. To address this issue, we propose a novel SSM-based architecture with saliency-based token-aware transformation (namely START), which achieves state-of-the-art (SOTA) performances and offers a competitive alternative to CNNs and ViTs. Our START can selectively perturb and suppress domain-specific features in salient tokens within the input-dependent matrices of SSMs, thus effectively reducing the discrepancy between different domains. Extensive experiments on five benchmarks demonstrate that START outperforms existing SOTA DG methods with efficient linear complexity. Our code is available at https://github.com/lingeringlight/START.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1e045e8d4c432c88ee7d528f95cd5406dfc62b43.pdf'}, 'supplementary_material': {'value': '/attachment/ce883f1aa347d3f4ebb234dc743b97b4275be824.zip'}, '_bibtex': {'value': '@inproceedings{\\nguo2024start,\\ntitle={{START}: A Generalized State Space Model with Saliency-Driven Token-Aware Transformation},\\nauthor={Jintao Guo and Lei Qi and Yinghuan Shi and Yang Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=mAdGQ1Hh3L}\\n}'}, 'paperhash': {'value': 'guo|start_a_generalized_state_space_model_with_saliencydriven_tokenaware_transformation'}},forum = 'mAdGQ1Hh3L',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9378/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9378/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9378/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9378/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm9WZrEXWl5',number = 3555,cdate = 1715303748361,pdate = 1727287724366,odate = 1730873867636,mdate = 1736811911853,tcdate = 1715303748361,tmdate = 1736811911853,ddate = None,content = {'title': {'value': 'Directional Smoothness and Gradient Methods: Convergence and Adaptivity'}, 'authors': {'value': ['Aaron Mishkin', 'Ahmed Khaled', 'Yuanhao Wang', 'Aaron Defazio', 'Robert M. Gower']}, 'authorids': {'value': ['~Aaron_Mishkin1', '~Ahmed_Khaled1', '~Yuanhao_Wang1', '~Aaron_Defazio1', '~Robert_M._Gower1']}, 'keywords': {'value': ['directional smoothness', 'gradient descent', 'exponential search', 'polyak stepsize', 'normalized gradient descent']}, 'TLDR': {'value': 'We derive new convergence rates for gradient descent which depend only on local properties of the objective using directional smoothness.'}, 'abstract': {'value': 'We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants.  Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective.  Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes.  For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness.  Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on $L$-smoothness.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/28b965d46dd03b06f87fa1e07106bba359ce0192.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmishkin2024directional,\\ntitle={Directional Smoothness and Gradient Methods: Convergence and Adaptivity},\\nauthor={Aaron Mishkin and Ahmed Khaled and Yuanhao Wang and Aaron Defazio and Robert M. Gower},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m9WZrEXWl5}\\n}'}, 'paperhash': {'value': 'mishkin|directional_smoothness_and_gradient_methods_convergence_and_adaptivity'}},forum = 'm9WZrEXWl5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3555/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3555/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3555/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3555/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'm906PS5G9x',number = 13862,cdate = 1715743684147,pdate = 1727288052846,odate = 1730873961288,mdate = 1736931849965,tcdate = 1715743684147,tmdate = 1736931849965,ddate = None,content = {'title': {'value': 'Bayesian Adaptive Calibration and Optimal Design'}, 'authors': {'value': ['Rafael Oliveira', 'Dino Sejdinovic', 'David Howard', 'Edwin V. Bonilla']}, 'authorids': {'value': ['~Rafael_Oliveira1', '~Dino_Sejdinovic1', '~David_Howard1', '~Edwin_V._Bonilla1']}, 'keywords': {'value': ['Gaussian processes', 'Bayesian inference', 'variational inference', 'experimental design', 'active learning', 'calibration']}, 'TLDR': {'value': 'We propose a method which jointly estimates posteriors and informative designs to calibrate computer simulations of physical processes.'}, 'abstract': {'value': 'The process of calibrating computer models of natural phenomena is essential for applications in the physical sciences, where plenty of domain knowledge can be embedded into simulations and then calibrated against real observations. Current machine learning approaches, however, mostly rely on rerunning simulations over a fixed set of designs available in the observed data, potentially neglecting informative correlations across the design space and requiring a large amount of simulations. Instead, we consider the calibration process from the perspective of Bayesian adaptive experimental design and propose a data-efficient algorithm to run maximally informative simulations within a batch-sequential process. At each round, the algorithm jointly estimates the parameters posterior distribution and optimal designs by maximising a variational lower bound of the expected information gain. The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and real data with the unknown calibration parameters. We show the benefits of our method when compared to related approaches across synthetic and real-data problems.'}, 'primary_area': {'value': 'active_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d3d7dd5ce31a82a73aa12cf2f27678e6ecfb51ae.pdf'}, '_bibtex': {'value': '@inproceedings{\\noliveira2024bayesian,\\ntitle={Bayesian Adaptive Calibration and Optimal Design},\\nauthor={Rafael Oliveira and Dino Sejdinovic and David Howard and Edwin V. Bonilla},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m906PS5G9x}\\n}'}, 'paperhash': {'value': 'oliveira|bayesian_adaptive_calibration_and_optimal_design'}},forum = 'm906PS5G9x',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13862/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13862/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13862/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13862/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm8MElyzuwp',number = 8952,cdate = 1715672191517,pdate = 1727287896275,odate = 1730873916531,mdate = 1735308228963,tcdate = 1715672191517,tmdate = 1735308228963,ddate = None,content = {'title': {'value': 'Fetch and Forge: Efficient Dataset Condensation for Object Detection'}, 'authors': {'value': ['Ding Qi', 'Jian Li', 'Jinlong Peng', 'Bo Zhao', 'Shuguang Dou', 'Jialin Li', 'Jiangning Zhang', 'Yabiao Wang', 'Chengjie Wang', 'Cairong Zhao']}, 'authorids': {'value': ['~Ding_Qi1', '~Jian_Li12', '~Jinlong_Peng1', '~Bo_Zhao4', '~Shuguang_Dou1', '~Jialin_Li3', '~Jiangning_Zhang1', '~Yabiao_Wang1', '~Chengjie_Wang1', '~Cairong_Zhao2']}, 'keywords': {'value': ['Dataset Condensation', 'Object Detection']}, 'abstract': {'value': 'Dataset condensation (DC) is an emerging technique capable of creating compact synthetic datasets from large originals while maintaining considerable performance. It is crucial for accelerating network training and reducing data storage requirements. \\nHowever, current research on DC mainly focuses on image classification, with less exploration of object detection.\\nThis is primarily due to two challenges: (i) the multitasking nature of object detection complicates the condensation process, and (ii) Object detection datasets are characterized by large-scale and high-resolution data, which are difficult for existing DC methods to handle.\\nAs a remedy, we propose DCOD, the first dataset condensation framework for object detection. It operates in two stages: Fetch and Forge, initially storing key localization and classification information into model parameters, and then reconstructing synthetic images via model inversion. \\nFor the complex of multiple objects in an image, we propose Foreground Background Decoupling to centrally update the foreground of multiple instances and Incremental PatchExpand to further enhance the diversity of foregrounds.\\nExtensive experiments on various detection datasets demonstrate the superiority of DCOD. Even at an extremely low compression rate of 1\\\\%, we achieve 46.4\\\\% and 24.7\\\\% $\\\\text{AP}_{50}$ on the VOC and COCO, respectively, significantly reducing detector training duration.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c12d4adfe1c74582a5747a9602fb1d1e0f9cf70c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nqi2024fetch,\\ntitle={Fetch and Forge: Efficient Dataset Condensation for Object Detection},\\nauthor={Ding Qi and Jian Li and Jinlong Peng and Bo Zhao and Shuguang Dou and Jialin Li and Jiangning Zhang and Yabiao Wang and Chengjie Wang and Cairong Zhao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m8MElyzuwp}\\n}'}, 'paperhash': {'value': 'qi|fetch_and_forge_efficient_dataset_condensation_for_object_detection'}},forum = 'm8MElyzuwp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8952/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8952/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8952/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8952/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm6pVpdIN0y',number = 21353,cdate = 1715801386050,pdate = 1727288253780,odate = 1730874005488,mdate = 1730874005520,tcdate = 1715801386050,tmdate = 1730874005520,ddate = None,content = {'title': {'value': 'Neglected Hessian component explains mysteries in sharpness regularization'}, 'authors': {'value': ['Yann Dauphin', 'Atish Agarwala', 'Hossein Mobahi']}, 'authorids': {'value': ['~Yann_Dauphin1', '~Atish_Agarwala1', '~Hossein_Mobahi2']}, 'keywords': {'value': ['sharpness', 'flatness', 'regularization']}, 'TLDR': {'value': 'Understanding the neglected indefinite part of the Hessian explains important phenomena in sharpness regularization'}, 'abstract': {'value': 'Recent work has shown that methods that regularize second order information like SAM can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We investigate this inconsistency and reveal its connection to the the structure of the Hessian of the loss. Specifically, its decomposition into the positive semi-definite Gauss-Newton matrix and an indefinite matrix, which we call the Nonlinear Modeling Error (NME) matrix. Previous studies have largely overlooked the significance of the NME in their analysis for various reasons. However, we provide empirical and theoretical evidence that the NME is important to the performance of gradient penalties and explains their sensitivity to activation functions. We also provide evidence that the difference in regularization performance between gradient penalties and weight noise can be explained by the NME. Our findings emphasize the necessity of considering the NME in both experimental design and theoretical analysis for sharpness regularization.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c1f6ee9f0e9ec1784d61f91dc4fd53ea887eb3b7.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndauphin2024neglected,\\ntitle={Neglected Hessian component explains mysteries in sharpness regularization},\\nauthor={Yann Dauphin and Atish Agarwala and Hossein Mobahi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m6pVpdIN0y}\\n}'}, 'paperhash': {'value': 'dauphin|neglected_hessian_component_explains_mysteries_in_sharpness_regularization'}},forum = 'm6pVpdIN0y',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21353/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21353/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21353/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21353/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'm5dyKArVn8',number = 21653,cdate = 1715802905572,pdate = 1727288259493,odate = 1730874006543,mdate = 1736963056918,tcdate = 1715802905572,tmdate = 1736963056918,ddate = None,content = {'title': {'value': 'How many classifiers do we need?'}, 'authors': {'value': ['Hyunsuk Kim', 'Liam Hodgkinson', 'Ryan Theisen', 'Michael W. Mahoney']}, 'authorids': {'value': ['~Hyunsuk_Kim1', '~Liam_Hodgkinson1', '~Ryan_Theisen1', '~Michael_W._Mahoney1']}, 'keywords': {'value': ['ensemble', 'model aggregation', 'machine learning', 'computer vision']}, 'abstract': {'value': 'As performance gains through scaling data and/or model size experience diminishing returns, it is becoming increasingly popular to turn to ensembling, where the predictions of multiple models are combined to improve accuracy. \\nIn this paper, we provide a detailed analysis of how the disagreement and the polarization (a notion we introduce and define in this paper) among classifiers relate to the performance gain achieved by aggregating individual classifiers, for majority vote strategies in classification tasks.\\nWe address these questions in the following ways. \\n(1) An upper bound for polarization is derived, and we propose what we call a neural polarization law: most interpolating neural network models are 4/3-polarized. Our empirical results not only support this conjecture but also show that polarization is nearly constant for a dataset, regardless of hyperparameters or architectures of classifiers. \\n(2) The error rate of the majority vote classifier is considered under restricted entropy conditions, and we present a tight upper bound that indicates that the disagreement is linearly correlated with the error rate, and that the slope is linear in the polarization.\\n(3) We prove results for the asymptotic behavior of the disagreement in terms of the number of classifiers, which we show can help in predicting the performance for a larger number of classifiers from that of a smaller number. \\nOur theoretical findings are supported by empirical results on several image classification tasks with various types of neural networks.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6c0cc9c39e8b9e81213e491a5cd72662641eba0d.pdf'}, 'TLDR': {'value': 'We develop bounds on the majority vote error that are tight enough to predict ensemble performance.'}, '_bibtex': {'value': '@inproceedings{\\nkim2024how,\\ntitle={How many classifiers do we need?},\\nauthor={Hyunsuk Kim and Liam Hodgkinson and Ryan Theisen and Michael W. Mahoney},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m5dyKArVn8}\\n}'}, 'paperhash': {'value': 'kim|how_many_classifiers_do_we_need'}},forum = 'm5dyKArVn8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission21653/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission21653/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission21653/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission21653/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm5CAnUui0Z',number = 14288,cdate = 1715748186133,pdate = 1727288064843,odate = 1730873964582,mdate = 1730873964599,tcdate = 1715748186133,tmdate = 1730873964599,ddate = None,content = {'title': {'value': 'Label Delay in Online Continual Learning'}, 'authors': {'value': ['Botos Csaba', 'Wenxuan Zhang', 'Matthias Müller', 'Ser-Nam Lim', 'Philip Torr', 'Adel Bibi']}, 'authorids': {'value': ['~Botos_Csaba1', '~Wenxuan_Zhang3', '~Matthias_Müller1', '~Ser-Nam_Lim3', '~Philip_Torr1', '~Adel_Bibi1']}, 'keywords': {'value': ['Online continual learning', 'delayed feedback', 'computational constraint', 'efficient learning']}, 'abstract': {'value': 'Online continual learning, the process of training models on streaming data, has gained increasing attention in recent years. However, a critical aspect often overlooked is the label delay, where new data may not be labeled due to slow and costly annotation processes. We introduce a new continual learning framework with explicit modeling of the label delay between data and label streams over time steps. In each step, the framework reveals both unlabeled data from the current time step t and labels delayed with d steps, from the time step t−d. In our extensive experiments amounting to 1060 GPU days, we show that merely augmenting the computational resources is insufficient to tackle this challenge. Our findings underline a notable performance decline when solely relying on labeled data when the label delay becomes significant. More surprisingly, when using state-of-the-art SSL and TTA techniques to utilize the newer, unlabeled data, they fail to surpass the performance of a naïve method that simply trains on the delayed supervised stream. To this end, we introduce a simple, efficient baseline that rehearses from the labeled memory samples that are most similar to the new unlabeled samples. This method bridges the accuracy gap caused by label delay without significantly increasing computational complexity. We show experimentally that our method is the least affected by the label delay factor and in some cases successfully recovers the accuracy of the non-delayed counterpart. We conduct various ablations and sensitivity experiments, demonstrating the effectiveness of our approach.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/170ed172ba6d096fafba4f36e54f20fec4c17dcf.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncsaba2024label,\\ntitle={Label Delay in Online Continual Learning},\\nauthor={Botos Csaba and Wenxuan Zhang and Matthias M{\\\\\"u}ller and Ser-Nam Lim and Philip Torr and Adel Bibi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m5CAnUui0Z}\\n}'}, 'paperhash': {'value': 'csaba|label_delay_in_online_continual_learning'}},forum = 'm5CAnUui0Z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14288/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14288/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14288/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14288/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm5106RRLgx',number = 13878,cdate = 1715743814078,pdate = 1727288053569,odate = 1730873961515,mdate = 1730873961589,tcdate = 1715743814078,tmdate = 1730873961589,ddate = None,content = {'title': {'value': 'Are More LLM Calls All You Need? Towards the Scaling Properties of Compound AI Systems'}, 'authors': {'value': ['Lingjiao Chen', 'Jared Quincy Davis', 'Boris Hanin', 'Peter Bailis', 'Ion Stoica', 'Matei Zaharia', 'James Zou']}, 'authorids': {'value': ['~Lingjiao_Chen1', '~Jared_Quincy_Davis2', '~Boris_Hanin1', '~Peter_Bailis2', '~Ion_Stoica1', '~Matei_Zaharia1', '~James_Zou1']}, 'keywords': {'value': ['Scaling Laws; Compound AI systems; language models']}, 'TLDR': {'value': 'We study the scaling properties of compound inference systems both theoretically and empirically'}, 'abstract': {'value': 'Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model (LM) calls and aggregate their responses. However, there is little understanding of how the number of LM calls -- e.g., when asking the LM to answer each question multiple times and taking a majority vote -- affects such a compound system\\'s performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of LM calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate LM responses via majority voting, optionally applying LM filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of LM calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more LM calls lead to higher performance on \"easy\" queries, but lower performance on \"hard\" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of LM calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of LM calls to make.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/933389eba2d451a433d83e7d55975efe12b0a17b.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024are,\\ntitle={Are More {LLM} Calls All You Need? Towards the Scaling Properties of Compound {AI} Systems},\\nauthor={Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m5106RRLgx}\\n}'}, 'paperhash': {'value': 'chen|are_more_llm_calls_all_you_need_towards_the_scaling_properties_of_compound_ai_systems'}},forum = 'm5106RRLgx',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13878/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13878/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13878/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13878/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'm4ZcDrVvid',number = 2969,cdate = 1715195546578,pdate = 1727287706298,odate = 1730873862277,mdate = 1737001833890,tcdate = 1715195546578,tmdate = 1737001833890,ddate = None,content = {'title': {'value': 'Practical Bayesian Algorithm Execution via Posterior Sampling'}, 'authors': {'value': ['Chu Xin Cheng', 'Raul Astudillo', 'Thomas Desautels', 'Yisong Yue']}, 'authorids': {'value': ['~Chu_Xin_Cheng1', '~Raul_Astudillo1', '~Thomas_Desautels1', '~Yisong_Yue1']}, 'keywords': {'value': ['Bayesian algorithm execution', 'Bayesian optimization', 'posterior sampling', 'probabilistic numerics']}, 'TLDR': {'value': 'We propose a posterior sampling-based algorithm to efficiently estimate a target set of input points defined in terms of a function with expensive evaluations.'}, 'abstract': {'value': 'We consider Bayesian algorithm execution (BAX), a framework for efficiently selecting evaluation points of an expensive function to infer a property of interest encoded as the output of a base algorithm. Since the base algorithm typically requires more evaluations than are feasible, it cannot be directly applied. Instead, BAX methods sequentially select evaluation points using a probabilistic numerical approach. Current BAX methods use expected information gain to guide this selection. However, this approach is computationally intensive.  Observing that, in many tasks, the property of interest corresponds to a target set of points defined by the function, we introduce PS-BAX, a simple, effective, and scalable BAX method based on posterior sampling. PS-BAX is applicable to a wide range of problems, including many optimization variants and level set estimation. Experiments across diverse tasks demonstrate that PS-BAX performs competitively with existing baselines while being significantly faster, simpler to implement, and easily parallelizable, setting a strong baseline for future research.  Additionally, we establish conditions under which PS-BAX is asymptotically convergent, offering new insights into posterior sampling as an algorithm design paradigm.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8495adc8d0607cd4e73e6f1573c654527144ee33.pdf'}, '_bibtex': {'value': '@inproceedings{\\ncheng2024practical,\\ntitle={Practical Bayesian Algorithm Execution via Posterior Sampling},\\nauthor={Chu Xin Cheng and Raul Astudillo and Thomas Desautels and Yisong Yue},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m4ZcDrVvid}\\n}'}, 'paperhash': {'value': 'cheng|practical_bayesian_algorithm_execution_via_posterior_sampling'}},forum = 'm4ZcDrVvid',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2969/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2969/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2969/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2969/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm4NI2yIwJA',number = 1358,cdate = 1714611960969,pdate = 1727287658621,odate = 1730873847861,mdate = 1730873847878,tcdate = 1714611960969,tmdate = 1730873847878,ddate = None,content = {'title': {'value': 'Deep Graph Mating'}, 'authors': {'value': ['Yongcheng Jing', 'Seok-Hee Hong', 'Dacheng Tao']}, 'authorids': {'value': ['~Yongcheng_Jing1', '~Seok-Hee_Hong1', '~Dacheng_Tao1']}, 'keywords': {'value': ['Efficient Learning', 'Graph Neural Networks']}, 'abstract': {'value': 'In this paper, we introduce the first learning-free model reuse task within the non-Euclidean domain, termed as Deep Graph Mating (Grama). We strive to create a child Graph Neural Network (GNN) that integrates knowledge from pre-trained parent models without requiring re-training, fine-tuning, or annotated labels. To this end, we begin by investigating the permutation invariance property of GNNs, which leads us to develop two vanilla approaches for Grama: Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), both employing topology-independent interpolation in the parameter space. However, neither approach has achieved the anticipated results. Through theoretical analysis of VPI and VAPI, we identify critical challenges unique to Grama, including increased sensitivity to parameter misalignment and further the inherent topology-dependent complexities. Motivated by these findings, we propose the Dual-Message Coordination and Calibration (DuMCC) methodology, comprising the Parent Message Coordination (PMC) scheme to optimise the permutation matrices for parameter interpolation by coordinating aggregated messages, and the Child Message Calibration (CMC) scheme to mitigate over-smoothing identified in PMC by calibrating the message statistics within child GNNs. Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4dea7fe0bc8803fadef62f09edc29b28b5793b47.pdf'}, 'supplementary_material': {'value': '/attachment/d30f83567540154fa9838461a3f3c2739e035cab.zip'}, '_bibtex': {'value': '@inproceedings{\\njing2024deep,\\ntitle={Deep Graph Mating},\\nauthor={Yongcheng Jing and Seok-Hee Hong and Dacheng Tao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m4NI2yIwJA}\\n}'}, 'paperhash': {'value': 'jing|deep_graph_mating'}},forum = 'm4NI2yIwJA',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1358/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1358/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1358/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1358/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm2DaXpCoIi',number = 15389,cdate = 1715759768453,pdate = 1727288096532,odate = 1730873972594,mdate = 1730873972614,tcdate = 1715759768453,tmdate = 1730873972614,ddate = None,content = {'title': {'value': 'Practical Shuffle Coding'}, 'authors': {'value': ['Julius Kunze', 'Daniel Severo', 'Jan-Willem van de Meent', 'James Townsend']}, 'authorids': {'value': ['~Julius_Kunze1', '~Daniel_Severo1', '~Jan-Willem_van_de_Meent1', '~James_Townsend1']}, 'keywords': {'value': ['graph compression', 'entropy coding', 'bits-back coding', 'lossless compression', 'generative models', 'information theory', 'probabilistic models', 'graph neural networks', 'multiset compression', 'asymmetric numeral systems', 'compression', 'entropy', 'shuffle coding']}, 'TLDR': {'value': 'We present a general method for practical lossless compression of unordered data structures that achieves state-of-the-art rates and speeds on large graphs.'}, 'abstract': {'value': \"We present a general method for lossless compression of unordered data structures, including multisets and graphs. It is a variant of shuffle coding that is many orders of magnitude faster than the original and enables 'one-shot' compression of single unordered objects. Our method achieves state-of-the-art compression rates on various large-scale network graphs at speeds of megabytes per second, efficiently handling even a multi-gigabyte plain graph with one billion edges. We release an implementation that can be easily adapted to different data types and statistical models.\"}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a9d9fdbbc18d0ae34f1bf23538ba122f745c0a62.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkunze2024practical,\\ntitle={Practical Shuffle Coding},\\nauthor={Julius Kunze and Daniel Severo and Jan-Willem van de Meent and James Townsend},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m2DaXpCoIi}\\n}'}, 'paperhash': {'value': 'kunze|practical_shuffle_coding'}},forum = 'm2DaXpCoIi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15389/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15389/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15389/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15389/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm296WJXyzQ',number = 17369,cdate = 1715779070933,pdate = 1727288151392,odate = 1730873984096,mdate = 1730873984114,tcdate = 1715779070933,tmdate = 1730873984114,ddate = None,content = {'title': {'value': 'Scanning Trojaned Models Using Out-of-Distribution Samples'}, 'authors': {'value': ['Hossein Mirzaei', 'Ali Ansari', 'Bahar Dibaei Nia', 'Mojtaba Nafez', 'Moein Madadi', 'Sepehr Rezaee', 'Zeinab Sadat Taghavi', 'Arad Maleki', 'Kian Shamsaie', 'Mahdi Hajialilue', 'Jafar Habibi', 'Mohammad Sabokrou', 'Mohammad Hossein Rohban']}, 'authorids': {'value': ['~Hossein_Mirzaei1', '~Ali_Ansari2', '~Bahar_Dibaei_Nia1', '~Mojtaba_Nafez1', '~Moein_Madadi1', '~Sepehr_Rezaee1', '~Zeinab_Sadat_Taghavi1', '~Arad_Maleki1', '~Kian_Shamsaie3', '~Mahdi_Hajialilue1', '~Jafar_Habibi1', '~Mohammad_Sabokrou1', '~Mohammad_Hossein_Rohban1']}, 'keywords': {'value': ['Trojan Scanning Method', 'Trojan Post-Training Defense', 'Backdoor Attacks', 'Out-of-Distribution Samples', 'Adversarially Perturbed Out-of-Distribution Samples']}, 'abstract': {'value': 'Scanning for trojan (backdoor) in deep neural networks is crucial due to their significant real-world applications. There has been an increasing focus on developing effective general trojan scanning methods across various trojan attacks. Despite advancements, there remains a shortage of methods that perform effectively without preconceived assumptions about the backdoor attack method. Additionally, we have observed that current methods struggle to identify classifiers trojaned using adversarial training. Motivated by these challenges, our study introduces a novel scanning method named TRODO (TROjan scanning by Detection of adversarial shifts in Out-of-distribution samples). TRODO leverages the concept of \"blind spots\"—regions where trojaned classifiers erroneously identify out-of-distribution (OOD) samples as in-distribution (ID). We scan for these blind spots by adversarially shifting OOD samples towards in-distribution. The increased likelihood of perturbed OOD samples being classified as ID serves as a signature for trojan detection. TRODO is both trojan and label mapping agnostic, effective even against adversarially trained trojaned classifiers. It is applicable even in scenarios where training data is absent, demonstrating high accuracy and adaptability across various scenarios and datasets, highlighting its potential as a robust trojan scanning strategy.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/9f6232153a8708654d58a0fadd512fd893b8f78e.zip'}, 'pdf': {'value': '/pdf/d0c4508c8f190e88268652a94367eba05f5afd98.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmirzaei2024scanning,\\ntitle={Scanning Trojaned Models Using Out-of-Distribution Samples},\\nauthor={Hossein Mirzaei and Ali Ansari and Bahar Dibaei Nia and Mojtaba Nafez and Moein Madadi and Sepehr Rezaee and Zeinab Sadat Taghavi and Arad Maleki and Kian Shamsaie and Mahdi Hajialilue and Jafar Habibi and Mohammad Sabokrou and Mohammad Hossein Rohban},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m296WJXyzQ}\\n}'}, 'TLDR': {'value': 'In this work, we designed a trojan scanning method which is robust in various aspects, including trojan attack type, label mapping, and adversarial robustness of the classifier.'}, 'paperhash': {'value': 'mirzaei|scanning_trojaned_models_using_outofdistribution_samples'}},forum = 'm296WJXyzQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17369/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17369/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17369/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17369/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm1a4CrRJR7',number = 16714,cdate = 1715773651723,pdate = 1727288134013,odate = 1730873980759,mdate = 1730873980776,tcdate = 1715773651723,tmdate = 1730873980776,ddate = None,content = {'title': {'value': 'Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure'}, 'authors': {'value': ['Jin Zhang', 'Ze Liu', 'Defu Lian', 'Enhong Chen']}, 'authorids': {'value': ['~Jin_Zhang18', '~Ze_Liu4', '~Defu_Lian1', '~Enhong_Chen1']}, 'keywords': {'value': ['Two-stage Recommender Systems', 'Recommender Systems', 'Generalization error bounds', 'Rademacher complexities', 'Tree-based Learning']}, 'abstract': {'value': 'Two-stage recommender systems play a crucial role in efficiently identifying relevant items and personalizing recommendations from a vast array of options. This paper, based on an error decomposition framework, analyzes the generalization error for two-stage recommender systems with a tree structure, which consist of an efficient tree-based retriever and a more precise yet time-consuming ranker. We use the Rademacher complexity to establish the generalization upper bound for various tree-based retrievers using beam search, as well as for different ranker models under a shifted training distribution. Both theoretical insights and practical experiments on real-world datasets indicate that increasing the branches in tree-based retrievers and harmonizing distributions across stages can enhance the generalization performance of two-stage recommender systems.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0573ad42adbbc93100e6c898b23c116d78de695b.pdf'}, 'supplementary_material': {'value': '/attachment/f4a1019fc100b199547ca32632e0acce9ea849da.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024generalization,\\ntitle={Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure},\\nauthor={Jin Zhang and Ze Liu and Defu Lian and Enhong Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m1a4CrRJR7}\\n}'}, 'paperhash': {'value': 'zhang|generalization_error_bounds_for_twostage_recommender_systems_with_tree_structure'}},forum = 'm1a4CrRJR7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16714/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16714/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16714/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16714/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm1PVjNHvtP',number = 11145,cdate = 1715701451914,pdate = 1727287962029,odate = 1730873935437,mdate = 1730873935455,tcdate = 1715701451914,tmdate = 1730873935455,ddate = None,content = {'title': {'value': 'GLinSAT: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent'}, 'authors': {'value': ['Hongtai Zeng', 'Chao Yang', 'Yanzhen Zhou', 'Cheng Yang', 'Qinglai Guo']}, 'authorids': {'value': ['~Hongtai_Zeng1', '~Chao_Yang7', '~Yanzhen_Zhou2', '~Cheng_Yang3', '~Qinglai_Guo1']}, 'keywords': {'value': ['Differentiable general linear satisfiability neural network layer', 'Constraint Satisfaction', 'Accelerated gradient descent']}, 'TLDR': {'value': 'We design GLinSAT based on accelerated gradient descent method, which is the first general linear satisfiability neural network layer where all the operations are differentiable and matrix-factorization-free.'}, 'abstract': {'value': 'Ensuring that the outputs of neural networks satisfy specific constraints is crucial for applying neural networks to real-life decision-making problems. In this paper, we consider making a batch of neural network outputs satisfy bounded and general linear constraints. We first reformulate the neural network output projection problem as an entropy-regularized linear programming problem. We show that such a problem can be equivalently transformed into an unconstrained convex optimization problem with Lipschitz continuous gradient according to the duality theorem. Then, based on an accelerated gradient descent algorithm with numerical performance enhancement, we present our architecture, GLinSAT, to solve the problem. To the best of our knowledge, this is the first general linear satisfiability layer in which all the operations are differentiable and matrix-factorization-free. Despite the fact that we can explicitly perform backpropagation based on automatic differentiation mechanism, we also provide an alternative approach in GLinSAT to calculate the derivatives based on implicit differentiation of the optimality condition. Experimental results on constrained traveling salesman problems, partial graph matching with outliers, predictive portfolio allocation and power system unit commitment demonstrate the advantages of GLinSAT over existing satisfiability layers. Our implementation is available at https://github.com/HunterTracer/GLinSAT.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/aeec6ec4aa9143f07f7c57ad8c1b6f5eaa7991c6.pdf'}, 'supplementary_material': {'value': '/attachment/b9330e3e9d961dfdf9fb80bfbc1e760d792f4d98.zip'}, '_bibtex': {'value': '@inproceedings{\\nzeng2024glinsat,\\ntitle={{GL}in{SAT}: The General Linear Satisfiability Neural Network Layer By Accelerated Gradient Descent},\\nauthor={Hongtai Zeng and Chao Yang and Yanzhen Zhou and Cheng Yang and Qinglai Guo},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m1PVjNHvtP}\\n}'}, 'paperhash': {'value': 'zeng|glinsat_the_general_linear_satisfiability_neural_network_layer_by_accelerated_gradient_descent'}},forum = 'm1PVjNHvtP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11145/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11145/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11145/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11145/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'm0jZUvlKl7',number = 8118,cdate = 1715653575446,pdate = 1727287869186,odate = 1730873909605,mdate = 1737123368505,tcdate = 1715653575446,tmdate = 1737123368505,ddate = None,content = {'title': {'value': 'AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties'}, 'authors': {'value': ['Xiayan Ji', 'Anton Xue', 'Eric Wong', 'Oleg Sokolsky', 'Insup Lee']}, 'authorids': {'value': ['~Xiayan_Ji1', '~Anton_Xue1', '~Eric_Wong1', '~Oleg_Sokolsky1', '~Insup_Lee1']}, 'keywords': {'value': ['anomaly detection', 'anomaly explanation', 'anomaly repair', 'diffusion model']}, 'abstract': {'value': 'Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability.\\nWe leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection.\\nGiven an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version $\\\\textit{should have looked like}$.\\nA key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations.\\nWe demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro.'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8090b931950e35a56d82920a9747c49bf62cdeda.pdf'}, 'supplementary_material': {'value': '/attachment/c1726cc57ec0c80dc0728f3ad080dc50940110d8.zip'}, '_bibtex': {'value': '@inproceedings{\\nji2024arpro,\\ntitle={{AR}-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties},\\nauthor={Xiayan Ji and Anton Xue and Eric Wong and Oleg Sokolsky and Insup Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m0jZUvlKl7}\\n}'}, 'paperhash': {'value': 'ji|arpro_counterfactual_explanations_for_anomaly_repair_with_formal_properties'}},forum = 'm0jZUvlKl7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8118/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8118/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8118/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8118/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'm0DS4OOmSY',number = 4258,cdate = 1715394240603,pdate = 1727287745525,odate = 1730873874200,mdate = 1730873874218,tcdate = 1715394240603,tmdate = 1730873874218,ddate = None,content = {'title': {'value': 'Should We Really Edit Language Models? On the Evaluation of Edited Language Models'}, 'authors': {'value': ['Qi Li', 'Xiang Liu', 'Zhenheng Tang', 'Peijie Dong', 'Zeyu Li', 'Xinglin Pan', 'Xiaowen Chu']}, 'authorids': {'value': ['~Qi_Li23', '~Xiang_Liu10', '~Zhenheng_Tang2', '~Peijie_Dong1', '~Zeyu_Li3', '~Xinglin_Pan1', '~Xiaowen_Chu2']}, 'keywords': {'value': ['Model Editing', 'Language Models', 'Language Model Evaluation']}, 'abstract': {'value': 'Model editing has become an increasingly popular alternative for efficiently updating knowledge within language models. \\nCurrent methods mainly focus on reliability, generalization, and locality,  with many methods excelling across these criteria. \\nSome recent works disclose the pitfalls of these editing methods such as knowledge distortion or conflict. However, the general abilities of post-edited language models remain unexplored. \\nIn this paper, we perform a comprehensive evaluation on various editing methods and different language models, and have following findings.\\n(1) Existing editing methods lead to inevitable performance deterioration on general benchmarks, indicating that existing editing methods maintain the general abilities of the model within only a few dozen edits.\\nWhen the number of edits is slightly large, the intrinsic knowledge structure of the model is disrupted or even completely damaged. \\n(2) Instruction-tuned models are more robust to editing, showing less performance drop on general knowledge after editing. \\n(3) Language model with large scale is more resistant to editing compared to small model.\\n(4) The safety of the edited model, is significantly weakened, even for those safety-aligned models.\\nOur findings indicate that current editing methods are only suitable for small-scale knowledge updates within language models, which motivates further research on more practical and reliable editing methods.'}, 'primary_area': {'value': 'evaluation'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c301e68e1654036a0ce0fcafa25aff07b827879a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024should,\\ntitle={Should We Really Edit Language Models? On the Evaluation of Edited Language Models},\\nauthor={Qi Li and Xiang Liu and Zhenheng Tang and Peijie Dong and Zeyu Li and Xinglin Pan and Xiaowen Chu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=m0DS4OOmSY}\\n}'}, 'paperhash': {'value': 'li|should_we_really_edit_language_models_on_the_evaluation_of_edited_language_models'}},forum = 'm0DS4OOmSY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4258/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4258/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4258/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4258/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lzfzjYuWgY',number = 15815,cdate = 1715763917595,pdate = 1727288107711,odate = 1730873974941,mdate = 1730873974958,tcdate = 1715763917595,tmdate = 1730873974958,ddate = None,content = {'title': {'value': 'Do LLMs Build World Representations? Probing Through the Lens of State Abstraction'}, 'authors': {'value': ['Zichao Li', 'Yanshuai Cao', 'Jackie CK Cheung']}, 'authorids': {'value': ['~Zichao_Li3', '~Yanshuai_Cao1', '~Jackie_CK_Cheung1']}, 'keywords': {'value': ['Large Language Models', 'World Models', 'World Representation', 'Probing', 'Reinforcement Learning', 'State Abstraction']}, 'TLDR': {'value': 'We introduce a new framework for probing world abstraction within LLM-built representations, and our experiments with a text-based planning task demonstrate LLMs prefer maintaining goal-oriented abstractions during decoding.'}, 'abstract': {'value': \"How do large language models (LLMs) encode the state of the world, including the status of entities and their relations, as described by a text? While existing work directly probes for a complete state of the world, our research explores whether and how LLMs abstract this world state in their internal representations. We propose a new framework for probing for world representations through the lens of state abstraction theory from reinforcement learning, which emphasizes different levels of abstraction, distinguishing between general abstractions that facilitate predicting future states and goal-oriented abstractions that guide the subsequent actions to accomplish tasks. To instantiate this framework, we design a text-based planning task, where an LLM acts as an agent in an environment and interacts with objects in containers to achieve a specified goal state. Our experiments reveal that fine-tuning as well as advanced pre-training strengthens LLM-built representations' tendency of maintaining goal-oriented abstractions during decoding, prioritizing task completion over recovery of the world's state and dynamics.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/50079f3eb0494986796a27a886c0bd9bec9158d3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024do,\\ntitle={Do {LLM}s Build World Representations? Probing Through the Lens of State Abstraction},\\nauthor={Zichao Li and Yanshuai Cao and Jackie CK Cheung},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lzfzjYuWgY}\\n}'}, 'paperhash': {'value': 'li|do_llms_build_world_representations_probing_through_the_lens_of_state_abstraction'}},forum = 'lzfzjYuWgY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15815/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15815/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15815/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15815/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lxuXvJSOcP',number = 6434,cdate = 1715593628129,pdate = 1727287814120,odate = 1730873893472,mdate = 1734548705071,tcdate = 1715593628129,tmdate = 1734548705071,ddate = None,content = {'title': {'value': 'Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection'}, 'authors': {'value': ['Gyusam Chang', 'Jiwon Lee', 'Donghyun Kim', 'Jinkyu Kim', 'Dongwook Lee', 'Daehyun Ji', 'Sujin Jang', 'Sangpil Kim']}, 'authorids': {'value': ['~Gyusam_Chang1', '~Jiwon_Lee1', '~Donghyun_Kim2', '~Jinkyu_Kim1', '~Dongwook_Lee4', '~Daehyun_Ji1', '~Sujin_Jang2', '~Sangpil_Kim4']}, 'keywords': {'value': ['Domain Generalization.+Domain Adaptation.+Multi-view 3D Object Detection.+Autonomous driving.+Domain Generalization.']}, 'TLDR': {'value': 'Label-Efficient Domain Adaptation for Multi-view 3D Object Detection'}, 'abstract': {'value': 'Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks.\\nHowever, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (i.e., direct transfer) due to the inevitable geometric misalignment between the source and target domains.\\nIn practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors.\\nIn this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks.\\nWe first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes.\\nThen, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (i.e., 1$\\\\%$ and 5$\\\\%)$, while preserving well-defined source knowledge for training efficiency.\\nOverall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations.\\nWe demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b5dc22bb89aacc415a9f812bb5b590d0629a8c41.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchang2024unified,\\ntitle={Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection},\\nauthor={Gyusam Chang and Jiwon Lee and Donghyun Kim and Jinkyu Kim and Dongwook Lee and Daehyun Ji and Sujin Jang and Sangpil Kim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lxuXvJSOcP}\\n}'}, 'paperhash': {'value': 'chang|unified_domain_generalization_and_adaptation_for_multiview_3d_object_detection'}},forum = 'lxuXvJSOcP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6434/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6434/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6434/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6434/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'lxhoVDf1Sw',number = 1551,cdate = 1714681927003,pdate = 1727287664329,odate = 1730873849695,mdate = 1730873849712,tcdate = 1714681927003,tmdate = 1730873849712,ddate = None,content = {'title': {'value': 'Predictive Attractor Models'}, 'authors': {'value': ['Ramy Mounir', 'Sudeep Sarkar']}, 'authorids': {'value': ['~Ramy_Mounir1', '~Sudeep_Sarkar1']}, 'keywords': {'value': ['Sequential Memory', 'Predictive Models', 'Fixed-point Attractors', 'Associative Memory Models', 'State Space Models', 'Biologically plausible', 'Hebbian Plasticity Rules', 'Local Computations', 'Hierarchical Temporal Memory', 'Continual Learning', 'Multiple Possibilities Generation', 'Noise Tolerance']}, 'TLDR': {'value': 'A biologically plausible sequential memory model that (1) represents and generates multiple possibilities by learning fixed-point attractors, (2) learns continually while avoiding catastrophic forgetting, and (3) is robust to noise.'}, 'abstract': {'value': 'Sequential memory, the ability to form and accurately recall a sequence of events or stimuli in the correct order, is a fundamental prerequisite for biological and artificial intelligence as it underpins numerous cognitive functions (e.g., language comprehension, planning, episodic memory formation, etc.) However, existing methods of sequential memory suffer from catastrophic forgetting, limited capacity, slow iterative learning procedures, low-order Markov memory, and, most importantly, the inability to represent and generate multiple valid future possibilities stemming from the same context. Inspired by biologically plausible neuroscience theories of cognition, we propose Predictive Attractor Models (PAM), a novel sequence memory architecture with desirable generative properties. PAM is a streaming model that learns a sequence in an online, continuous manner by observing each input only once. Additionally, we find that PAM avoids catastrophic forgetting by uniquely representing past context through lateral inhibition in cortical minicolumns, which prevents new memories from overwriting previously learned knowledge. PAM generates future predictions by sampling from a union set of predicted possibilities; this generative ability is realized through an attractor model trained alongside the predictor. We show that PAM is trained with local computations through Hebbian plasticity rules in a biologically plausible framework. Other desirable traits (e.g., noise tolerance, CPU-based learning, capacity scaling) are discussed throughout the paper. Our findings suggest that PAM represents a significant step forward in the pursuit of biologically plausible and computationally efficient sequential memory models, with broad implications for cognitive science and artificial intelligence research. Illustration videos and code are available on our project page: https://ramymounir.com/publications/pam.'}, 'pdf': {'value': '/pdf/63cfa59df4579eb9d3f27a90faed984b47da0466.pdf'}, 'supplementary_material': {'value': '/attachment/29633e37d3cd6750a5eab0009bc61db9bd218b2b.zip'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nmounir2024predictive,\\ntitle={Predictive Attractor Models},\\nauthor={Ramy Mounir and Sudeep Sarkar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lxhoVDf1Sw}\\n}'}, 'paperhash': {'value': 'mounir|predictive_attractor_models'}},forum = 'lxhoVDf1Sw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1551/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1551/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1551/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'lxSmLxlVks',number = 5448,cdate = 1715533648087,pdate = 1727287784112,odate = 1730873884690,mdate = 1730873884706,tcdate = 1715533648087,tmdate = 1730873884706,ddate = None,content = {'title': {'value': 'Search for Efficient Large Language Models'}, 'authors': {'value': ['Xuan Shen', 'Pu Zhao', 'Yifan Gong', 'Zhenglun Kong', 'Zheng Zhan', 'Yushu Wu', 'Ming Lin', 'Chao Wu', 'Xue Lin', 'Yanzhi Wang']}, 'authorids': {'value': ['~Xuan_Shen1', '~Pu_Zhao1', '~Yifan_Gong2', '~Zhenglun_Kong1', '~Zheng_Zhan3', '~Yushu_Wu1', '~Ming_Lin4', '~Chao_Wu5', '~Xue_Lin1', '~Yanzhi_Wang3']}, 'keywords': {'value': ['Architecture Search', 'Large Language Models']}, 'abstract': {'value': 'Large Language Models (LLMs) have long held sway in the realms of artificial intelligence research.\\nNumerous efficient techniques, including weight pruning, quantization, and distillation, have been embraced to compress LLMs, targeting memory reduction and inference acceleration, which underscore the redundancy  in LLMs.\\nHowever, most model compression techniques concentrate on weight optimization, overlooking the exploration of optimal architectures.\\nBesides, traditional architecture search methods, limited by the elevated complexity with extensive parameters, struggle to demonstrate their effectiveness on LLMs.\\nIn this paper, we propose a training-free architecture search framework to identify optimal subnets that preserve the fundamental strengths of the original LLMs while achieving inference acceleration.\\nFurthermore, after generating subnets that inherit specific weights from the original LLMs, we introduce a reformation algorithm that utilizes the omitted weights to rectify the inherited weights  with a small amount of calibration data.\\nCompared with SOTA training-free structured pruning works that can generate smaller networks, our method demonstrates superior performance across standard benchmarks.\\nFurthermore, our generated subnets can directly reduce the usage of GPU memory and achieve inference acceleration.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/52a4b2cabc9e2c0c70ccee7a15600851da69cfb9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshen2024search,\\ntitle={Search for Efficient Large Language Models},\\nauthor={Xuan Shen and Pu Zhao and Yifan Gong and Zhenglun Kong and Zheng Zhan and Yushu Wu and Ming Lin and Chao Wu and Xue Lin and Yanzhi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lxSmLxlVks}\\n}'}, 'paperhash': {'value': 'shen|search_for_efficient_large_language_models'}},forum = 'lxSmLxlVks',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5448/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5448/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5448/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5448/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'lwpfH9wVkO',number = 20923,cdate = 1715799122962,pdate = 1727288244753,odate = 1730874003797,mdate = 1730874003816,tcdate = 1715799122962,tmdate = 1730874003816,ddate = None,content = {'title': {'value': 'Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound'}, 'authors': {'value': ['Reuben Adams', 'John Shawe-Taylor', 'Benjamin Guedj']}, 'authorids': {'value': ['~Reuben_Adams1', '~John_Shawe-Taylor1', '~Benjamin_Guedj1']}, 'keywords': {'value': ['PAC-Bayes', 'Generalization', 'Statistical Learning Theory']}, 'TLDR': {'value': 'We prove a novel PAC-Bayes bound which provides rich information on the types of errors likely to be made at inference time.'}, 'abstract': {'value': 'Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis-classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of $M$ error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided weighting of the error types. In contrast our bound implicitly controls all uncountably many weightings simultaneously.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b2a01b03d14b41742206340ab313fea1facf52fc.pdf'}, '_bibtex': {'value': '@inproceedings{\\nadams2024controlling,\\ntitle={Controlling Multiple Errors Simultaneously with a {PAC}-Bayes Bound},\\nauthor={Reuben Adams and John Shawe-Taylor and Benjamin Guedj},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lwpfH9wVkO}\\n}'}, 'paperhash': {'value': 'adams|controlling_multiple_errors_simultaneously_with_a_pacbayes_bound'}},forum = 'lwpfH9wVkO',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20923/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20923/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20923/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20923/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lvibangnAs',number = 12141,cdate = 1715717140212,pdate = 1727287995958,odate = 1730873945804,mdate = 1730873945822,tcdate = 1715717140212,tmdate = 1730873945822,ddate = None,content = {'title': {'value': 'Unifying Generation and Prediction on Graphs with Latent Graph Diffusion'}, 'authors': {'value': ['Cai Zhou', 'Xiyuan Wang', 'Muhan Zhang']}, 'authorids': {'value': ['~Cai_Zhou2', '~Xiyuan_Wang1', '~Muhan_Zhang1']}, 'keywords': {'value': ['latent graph generation', 'theory of diffusion model', 'general purpose generative models']}, 'TLDR': {'value': 'We propose Latent Graph Diffusion, the first framework that enables solving graph learning tasks of all levels and all types (generation, regression and classification) with one diffusion model.'}, 'abstract': {'value': \"In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) using one formulation. We first formulate prediction tasks including regression and classification into a generic (conditional) generation framework, which enables diffusion models to perform deterministic tasks with provable guarantees. We then propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder and decoder, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Leveraging LGD and the ``all tasks as generation'' formulation, our framework is capable of solving graph tasks of various levels and types. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across a wide range of generation and regression tasks.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a4a43894359b12777ca40a61fe02fe738116f751.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024unifying,\\ntitle={Unifying Generation and Prediction on Graphs with Latent Graph Diffusion},\\nauthor={Cai Zhou and Xiyuan Wang and Muhan Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lvibangnAs}\\n}'}, 'paperhash': {'value': 'zhou|unifying_generation_and_prediction_on_graphs_with_latent_graph_diffusion'}},forum = 'lvibangnAs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12141/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12141/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12141/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12141/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lvcWA24dxB',number = 16937,cdate = 1715775631359,pdate = 1727288141298,odate = 1730873982405,mdate = 1730873982416,tcdate = 1715775631359,tmdate = 1730873982416,ddate = None,content = {'title': {'value': 'MotionCraft: Physics-Based Zero-Shot Video Generation'}, 'authors': {'value': ['Antonio Montanaro', 'Luca Savant Aira', 'Emanuele Aiello', 'Diego Valsesia', 'Enrico Magli']}, 'authorids': {'value': ['~Antonio_Montanaro1', '~Luca_Savant_Aira1', '~Emanuele_Aiello1', '~Diego_Valsesia1', '~Enrico_Magli1']}, 'keywords': {'value': ['Zero-shot video generation', 'diffusion model', 'physics-based video generation']}, 'abstract': {'value': 'Generating videos with realistic and physically plausible motion is one of the main recent challenges in computer vision. \\nWhile diffusion models are achieving compelling results in image generation, video diffusion models are limited by heavy training and huge models, resulting in videos that are still biased to the training dataset. In this work we propose MotionCraft, a new zero-shot video generator to craft physics-based and realistic videos. MotionCraft is able to warp the noise latent space of an image diffusion model, such as Stable Diffusion, by applying an optical flow derived from a physics simulation. We show that warping the noise latent space results in coherent application of the desired motion while allowing the model to generate missing elements consistent with the scene evolution, which would otherwise result in artefacts or missing content if the flow was applied in the pixel space.\\nWe compare our method with the state-of-the-art Text2Video-Zero reporting qualitative and quantitative improvements, demonstrating the effectiveness of our approach to generate videos with finely-prescribed complex motion dynamics.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/204eafaa11af77e60f19e38ab6fac1c10fb66f5d.pdf'}, 'supplementary_material': {'value': '/attachment/1c486da7f9ad3348d6c4d46255d7d6ac422054c7.zip'}, '_bibtex': {'value': '@inproceedings{\\nmontanaro2024motioncraft,\\ntitle={MotionCraft: Physics-Based Zero-Shot Video Generation},\\nauthor={Antonio Montanaro and Luca Savant Aira and Emanuele Aiello and Diego Valsesia and Enrico Magli},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lvcWA24dxB}\\n}'}, 'paperhash': {'value': 'montanaro|motioncraft_physicsbased_zeroshot_video_generation'}},forum = 'lvcWA24dxB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16937/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16937/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16937/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16937/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lvS2b8CjG5',number = 15971,cdate = 1715765347125,pdate = 1727288112772,odate = 1730873976197,mdate = 1730873976210,tcdate = 1715765347125,tmdate = 1730873976210,ddate = None,content = {'title': {'value': 'EEGPT: Pretrained Transformer for Universal and Reliable Representation of EEG Signals'}, 'authors': {'value': ['Guangyu Wang', 'Wenchao Liu', 'Yuhong He', 'Cong Xu', 'Lin Ma', 'Haifeng Li']}, 'authorids': {'value': ['~Guangyu_Wang2', '~Wenchao_Liu2', '~Yuhong_He1', '~Cong_Xu5', '~Lin_Ma11', '~Haifeng_Li2']}, 'keywords': {'value': ['Electroencephalography', 'self-supervised learning', 'representational learning', 'masked autoencoder', 'transformer', 'brain-computer interfaces']}, 'TLDR': {'value': 'EEGPT, a novel over 10-million-parameter pretrained transformer for universal EEG feature extraction,  achieving SOTA on various tasks through efficient, robust, and scalable representation learning.'}, 'abstract': {'value': \"Electroencephalography (EEG) is crucial for recording brain activity, \\n  with applications in medicine, neuroscience, and brain-computer interfaces (BCI). \\n  However, challenges such as low signal-to-noise ratio (SNR), high inter-subject variability, and channel mismatch complicate the extraction of robust, \\n  universal EEG representations. \\n  We propose EEGPT, a novel 10-million-parameter pretrained transformer model designed for universal EEG feature extraction. \\n  In EEGPT, a mask-based dual self-supervised learning method for efficient feature extraction is designed. \\n  Compared to other mask-based self-supervised learning methods, \\n  EEGPT introduces spatio-temporal representation alignment. \\n  This involves constructing a self-supervised task based on \\n  EEG representations that possess high SNR and rich semantic information, \\n  rather than on raw signals. \\n  Consequently, this approach mitigates the issue of poor feature quality typically \\n  extracted from low SNR signals.\\n  Additionally, EEGPT's hierarchical structure processes spatial and temporal information separately, \\n  reducing computational complexity while increasing flexibility and adaptability for BCI applications. \\n  By training on a large mixed multi-task EEG dataset, we fully exploit EEGPT's capabilities.\\n  The experiment validates the efficacy and scalability of EEGPT, \\n  achieving state-of-the-art performance on a range of downstream tasks with linear-probing.\\n  Our research advances EEG representation learning, offering innovative solutions for bio-signal processing and AI applications.\\n  The code for this paper is available at: https://github.com/BINE022/EEGPT\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/dd23c819644a6ef3db30b1b7ffa6ed31f9fe4fba.pdf'}, 'supplementary_material': {'value': '/attachment/920c72c1c6669ec8a289f1d986e76e08db113099.zip'}, '_bibtex': {'value': '@inproceedings{\\nwang2024eegpt,\\ntitle={{EEGPT}: Pretrained Transformer for Universal and Reliable Representation of {EEG} Signals},\\nauthor={Guangyu Wang and Wenchao Liu and Yuhong He and Cong Xu and Lin Ma and Haifeng Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lvS2b8CjG5}\\n}'}, 'paperhash': {'value': 'wang|eegpt_pretrained_transformer_for_universal_and_reliable_representation_of_eeg_signals'}},forum = 'lvS2b8CjG5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15971/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15971/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15971/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15971/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'luQiVmnviX',number = 12087,cdate = 1715716165937,pdate = 1727287993919,odate = 1730873945325,mdate = 1730873945343,tcdate = 1715716165937,tmdate = 1730873945343,ddate = None,content = {'title': {'value': 'UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation'}, 'authors': {'value': ['Hanzhang Zhou', 'Zijian Feng', 'Zixiao Zhu', 'Junlang Qian', 'Kezhi Mao']}, 'authorids': {'value': ['~Hanzhang_Zhou1', '~Zijian_Feng2', '~Zixiao_Zhu2', '~Junlang_Qian1', '~Kezhi_Mao1']}, 'keywords': {'value': ['LLM Bias', 'In-Context Learning', 'Attention and FFN Manipulation', 'Prompt Brittleness']}, 'abstract': {'value': \"Large language models (LLMs) have demonstrated impressive capabilities in various tasks using the in-context learning (ICL) paradigm. However, their effectiveness is often compromised by inherent bias, leading to prompt brittleness—sensitivity to design settings such as example selection, order, and prompt formatting. Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored. Our work delves into these mechanisms, particularly investigating how feedforward neural networks (FFNs) and attention heads result in the bias of LLMs. By Interpreting the contribution of individual FFN vectors and attention heads, we identify the biased LLM components that skew LLMs' prediction toward specific labels. To mitigate these biases, we introduce UniBias, an inference-only method that effectively identifies and eliminates biased FFN vectors and attention heads. Extensive experiments across 12 NLP datasets demonstrate that UniBias significantly enhances ICL performance and alleviates prompt brittleness of LLMs.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/358d90ff7d8d269005fc19a00baabfed20752d30.pdf'}, 'TLDR': {'value': 'Unveiling internal mechanisms that lead to LLM bias and mitigate such bias by manipulation of LLM internal structures.'}, '_bibtex': {'value': '@inproceedings{\\nzhou2024unibias,\\ntitle={UniBias: Unveiling and Mitigating {LLM} Bias through Internal Attention and {FFN} Manipulation},\\nauthor={Hanzhang Zhou and Zijian Feng and Zixiao Zhu and Junlang Qian and Kezhi Mao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=luQiVmnviX}\\n}'}, 'paperhash': {'value': 'zhou|unibias_unveiling_and_mitigating_llm_bias_through_internal_attention_and_ffn_manipulation'}},forum = 'luQiVmnviX',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12087/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12087/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12087/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12087/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ltnDg0EzF9',number = 17508,cdate = 1715779993430,pdate = 1727288155345,odate = 1730873984890,mdate = 1730873984903,tcdate = 1715779993430,tmdate = 1730873984903,ddate = None,content = {'title': {'value': 'Latent Intrinsics Emerge from Training to Relight'}, 'authors': {'value': ['Xiao Zhang', 'William Gao', 'Seemandhar Jain', 'Michael Maire', 'David Forsyth', 'Anand Bhattad']}, 'authorids': {'value': ['~Xiao_Zhang11', '~William_Gao1', '~Seemandhar_Jain1', '~Michael_Maire1', '~David_Forsyth1', '~Anand_Bhattad1']}, 'keywords': {'value': ['Emergent Albedos', 'Latent Intrinsics', 'Relighting', 'Unsupervised Learning', 'Intrinsic Images']}, 'abstract': {'value': 'Image relighting is the task of showing what a scene from a source image would look like if illuminated differently.  Inverse graphic schemes recover an explicit representation of geometry and a set of chosen intrinsics, then relight with some form of renderer.  But error control for inverse graphics is difficult, and inverse graphics methods can represent only the effects of the chosen intrinsics. This paper describes a relighting method that is entirely data-driven, where intrinsics and lighting are each represented as latent variables.  Our approach produces SOTA relightings of real scenes, as measured by standard metrics.  We show that albedo can be recovered from our latent intrinsics without using any example albedos, and that the albedos recovered are competitive with SOTA methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/29f443e642012f7d29c1f0d893b1100d764b548d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024latent,\\ntitle={Latent Intrinsics Emerge from Training to Relight},\\nauthor={Xiao Zhang and William Gao and Seemandhar Jain and Michael Maire and David Forsyth and Anand Bhattad},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ltnDg0EzF9}\\n}'}, 'TLDR': {'value': 'Free albedo extraction from latent features in a relighting-trained model, without seeing albedo like images'}, 'paperhash': {'value': 'zhang|latent_intrinsics_emerge_from_training_to_relight'}},forum = 'ltnDg0EzF9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17508/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17508/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17508/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17508/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lsd27JUJ8v',number = 14901,cdate = 1715755061999,pdate = 1727288082502,odate = 1730873969154,mdate = 1734627612704,tcdate = 1715755061999,tmdate = 1734627612704,ddate = None,content = {'title': {'value': 'Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration'}, 'authors': {'value': ['Hongming Zhang', 'Chenjun Xiao', 'Chao Gao', 'Han Wang', 'bo xu', 'Martin Müller']}, 'authorids': {'value': ['~Hongming_Zhang3', '~Chenjun_Xiao1', '~Chao_Gao1', '~Han_Wang8', '~bo_xu1', '~Martin_Müller2']}, 'keywords': {'value': ['Deep Reinforcement Learning; Empirical MDP Iteration;']}, 'TLDR': {'value': 'Improving online reinforcement learning by regularizing such algorithms with Empirical MDP ITeration (EMIT).'}, 'abstract': {'value': 'Reinforcement learning (RL) algorithms are typically based on optimizing a Markov Decision Process (MDP) using the optimal Bellman equation. Recent studies have revealed that focusing the optimization of Bellman equations solely on in-sample actions tends to result in more stable optimization, especially in the presence of function approximation. Upon on these findings, in this paper, we propose an Empirical MDP Iteration (EMIT) framework. EMIT constructs a sequence of empirical MDPs using data from the growing replay memory. For each of these empirical MDPs, it learns an estimated Q-function denoted as $\\\\widehat{Q}$. The key strength is that by restricting the Bellman update to in-sample bootstrapping, each empirical MDP converges to a unique optimal $\\\\widehat{Q}$ function. Furthermore, gradually expanding from the empirical MDPs to the original MDP induces a monotonic policy improvement. Instead of creating entirely new algorithms, we demonstrate that EMIT can be seamlessly integrated with existing online RL algorithms, effectively acting as a regularizer for contemporary Q-learning methods. We show this by implementing EMIT for two representative RL algorithms, DQN and TD3. Experimental results on Atari and MuJoCo benchmarks show that EMIT significantly reduces estimation errors and substantially improves the performance of both algorithms.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d5d1e05131fd8693629b3b9ba2421406fc72f7f0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024exploiting,\\ntitle={Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical {MDP} Iteration},\\nauthor={Hongming Zhang and Chenjun Xiao and Chao Gao and Han Wang and bo xu and Martin M{\\\\\"u}ller},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lsd27JUJ8v}\\n}'}, 'paperhash': {'value': 'zhang|exploiting_the_replay_memory_before_exploring_the_environment_enhancing_reinforcement_learning_through_empirical_mdp_iteration'}},forum = 'lsd27JUJ8v',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14901/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14901/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14901/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14901/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lrSrJZZCle',number = 14575,cdate = 1715751345215,pdate = 1727288073548,odate = 1730873966897,mdate = 1730873966908,tcdate = 1715751345215,tmdate = 1730873966908,ddate = None,content = {'title': {'value': 'CODA: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts'}, 'authors': {'value': ['Ziquan OU', 'Zijun Zhang']}, 'authorids': {'value': ['~Ziquan_OU1', '~Zijun_Zhang2']}, 'keywords': {'value': ['deep neural networks', 'feature engineering', 'model generalizability', 'class imbalance', 'spurious correlations']}, 'abstract': {'value': 'Data-driven models learned often struggle to generalize due to widespread subpopulation shifts, especially the presence of both spurious correlations and group imbalance (SC-GI). To learn models more powerful for defending against SC-GI, we propose a {\\\\bf Correlation-Oriented Disentanglement and Augmentation (CODA)} modeling scheme, which includes two unique developments: (1) correlation-oriented disentanglement and (2) strategic sample augmentation with reweighted consistency (RWC) loss. In (1), a bi-branch encoding process is developed to enable the disentangling of variant and invariant correlations by coordinating with a decoy classifier and the decoder reconstruction. In (2), a strategic sample augmentation based on disentangled latent features with RWC loss is designed to reinforce the training of a more generalizable model. The effectiveness of CODA is verified by benchmarking against a set of SOTA models in terms of worst-group accuracy and maximum group accuracy gap based on two famous datasets, ColoredMNIST and CelebA.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c4fccf2fda9cada4b67e0286b68de1a250a68c41.pdf'}, '_bibtex': {'value': '@inproceedings{\\nou2024coda,\\ntitle={{CODA}: A Correlation-Oriented Disentanglement and Augmentation Modeling Scheme for Better Resisting Subpopulation Shifts},\\nauthor={Ziquan OU and Zijun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lrSrJZZCle}\\n}'}, 'paperhash': {'value': 'ou|coda_a_correlationoriented_disentanglement_and_augmentation_modeling_scheme_for_better_resisting_subpopulation_shifts'}},forum = 'lrSrJZZCle',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14575/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14575/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14575/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14575/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'lpxdG0hk4H',number = 6305,cdate = 1715589679107,pdate = 1727287810423,odate = 1730873892008,mdate = 1736086053295,tcdate = 1715589679107,tmdate = 1736086053295,ddate = None,content = {'title': {'value': 'ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling'}, 'authors': {'value': ['Quanwei Yang', 'Jiazhi Guan', 'Kaisiyuan Wang', 'Lingyun Yu', 'Wenqing Chu', 'Hang Zhou', 'ZhiQiang Feng', 'Haocheng Feng', 'Errui Ding', 'Jingdong Wang', 'Hongtao Xie']}, 'authorids': {'value': ['~Quanwei_Yang1', '~Jiazhi_Guan1', '~Kaisiyuan_Wang2', '~Lingyun_Yu1', '~Wenqing_Chu1', '~Hang_Zhou4', '~ZhiQiang_Feng1', '~Haocheng_Feng1', '~Errui_Ding2', '~Jingdong_Wang1', '~Hongtao_Xie2']}, 'keywords': {'value': ['human video generation; diffusion model']}, 'abstract': {'value': 'Although significant progress has been made in human video generation, most previous studies focus on either human facial animation or full-body animation, which cannot be directly applied to produce realistic conversational human videos with frequent hand gestures and various facial movements simultaneously.\\nTo address these limitations, we propose a 2D human video generation framework, named ShowMaker, capable of generating high-fidelity half-body conversational videos via fine-grained diffusion modeling.\\nWe leverage dual-stream diffusion models as the backbone of our framework and carefully design two novel components for crucial local regions (i.e., hands and face) that can be easily integrated into our backbone.\\nSpecifically, to handle the challenging hand generation caused by sparse motion guidance, we propose a novel Key Point-based Fine-grained Hand Modeling module by amplifying positional information from raw hand key points and constructing a corresponding key point-based codebook. \\nMoreover, to restore richer facial details in generated results, we introduce a Face Recapture module, which extracts facial texture features and global identity features from the aligned human face and integrates them into the diffusion process for face enhancement. \\nExtensive quantitative and qualitative experiments demonstrate the superior visual quality and temporal consistency of our method.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/e67b0b5ed920a7549b42fbab3926f6da635b951b.zip'}, 'pdf': {'value': '/pdf/2aa2861673d4d0792b3cd9c6c544f355d479d936.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024showmaker,\\ntitle={ShowMaker: Creating High-Fidelity 2D Human Video via Fine-Grained Diffusion Modeling},\\nauthor={Quanwei Yang and Jiazhi Guan and Kaisiyuan Wang and Lingyun Yu and Wenqing Chu and Hang Zhou and ZhiQiang Feng and Haocheng Feng and Errui Ding and Jingdong Wang and Hongtao Xie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lpxdG0hk4H}\\n}'}, 'paperhash': {'value': 'yang|showmaker_creating_highfidelity_2d_human_video_via_finegrained_diffusion_modeling'}},forum = 'lpxdG0hk4H',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6305/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6305/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6305/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6305/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lpXDZKiAnt',number = 2807,cdate = 1715166025475,pdate = 1727287701108,odate = 1730873861238,mdate = 1734655467985,tcdate = 1715166025475,tmdate = 1734655467985,ddate = None,content = {'title': {'value': 'Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack'}, 'authors': {'value': ['Tiansheng Huang', 'Sihao Hu', 'Ling Liu']}, 'authorids': {'value': ['~Tiansheng_Huang1', '~Sihao_Hu1', '~Ling_Liu3']}, 'keywords': {'value': ['Larger language model', 'safety alignment', 'perturbation-aware alignment', 'harmful finetuning attack']}, 'TLDR': {'value': 'We propose Vaccine, a perturbation-aware alignment solution for large language model against harmful fine-tuning attack.'}, 'abstract': {'value': 'The new paradigm of fine-tuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the fine-tuning to produce an alignment-broken model. We conduct an empirical analysis and uncover\\na \\\\textit{harmful embedding drift} phenomenon, showing a probable \\ncause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users  fine-tuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from  un-sanitized user data in the fine-tuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompts. Our code is available at  https://github.com/git-disl/Vaccine.'}, 'pdf': {'value': '/pdf/69c7003f7279c297f9e4e46a7c73c9cf3d0f0c5b.pdf'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024vaccine,\\ntitle={Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack},\\nauthor={Tiansheng Huang and Sihao Hu and Ling Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lpXDZKiAnt}\\n}'}, 'paperhash': {'value': 'huang|vaccine_perturbationaware_alignment_for_large_language_models_against_harmful_finetuning_attack'}},forum = 'lpXDZKiAnt',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2807/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2807/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2807/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2807/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lpFDhC91Oj',number = 356,cdate = 1713875200046,pdate = 1727287635747,odate = 1730873840222,mdate = 1734593259927,tcdate = 1713875200046,tmdate = 1734593259927,ddate = None,content = {'title': {'value': 'Black-Box Forgetting'}, 'authors': {'value': ['Yusuke Kuwana', 'Yuta Goto', 'Takashi Shibata', 'Go Irie']}, 'authorids': {'value': ['~Yusuke_Kuwana1', '~Yuta_Goto1', '~Takashi_Shibata3', '~Go_Irie3']}, 'keywords': {'value': ['Black-Box Tuning', 'Vision-Language Models']}, 'abstract': {'value': \"Large-scale pre-trained models (PTMs) provide remarkable zero-shot classification capability covering a wide variety of object classes. However, practical applications do not always require the classification of all kinds of objects, and leaving the model capable of recognizing unnecessary classes not only degrades overall accuracy but also leads to operational disadvantages. To mitigate this issue, we explore the selective forgetting problem for PTMs, where the task is to make the model unable to recognize only the specified classes, while maintaining accuracy for the rest. All the existing methods assume ''white-box'' settings, where model information such as architectures, parameters, and gradients is available for training. However, PTMs are often ''black-box,'' where information on such models is unavailable for commercial reasons or social responsibilities. In this paper, we address a novel problem of selective forgetting for black-box models, named Black-Box Forgetting, and propose an approach to the problem. Given that information on the model is unavailable, we optimize the input prompt to decrease the accuracy of specified classes through derivative-free optimization. To avoid difficult high-dimensional optimization while ensuring high forgetting performance, we propose Latent Context Sharing, which introduces common low-dimensional latent components among multiple tokens for the prompt. Experiments on four standard benchmark datasets demonstrate the superiority of our method with reasonable baselines. The code is available at https://github.com/yusukekwn/Black-Box-Forgetting.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We introduce a novel problem of selective forgetting for black-box models and propose a novel method for this problem.'}, 'pdf': {'value': '/pdf/44c580114e459439b5c85268f8c524a7df6cb64d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkuwana2024blackbox,\\ntitle={Black-Box Forgetting},\\nauthor={Yusuke Kuwana and Yuta Goto and Takashi Shibata and Go Irie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lpFDhC91Oj}\\n}'}, 'paperhash': {'value': 'kuwana|blackbox_forgetting'}},forum = 'lpFDhC91Oj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission356/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission356/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission356/-/Revision', 'NeurIPS.cc/2024/Conference/Submission356/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'loQCk0qruU',number = 12043,cdate = 1715715323508,pdate = 1727287992600,odate = 1730873944871,mdate = 1730873944889,tcdate = 1715715323508,tmdate = 1730873944889,ddate = None,content = {'title': {'value': 'Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models'}, 'authors': {'value': ['Deep Shankar Pandey', 'Spandan Pyakurel', 'Qi Yu']}, 'authorids': {'value': ['~Deep_Shankar_Pandey1', '~Spandan_Pyakurel1', '~Qi_Yu1']}, 'keywords': {'value': ['Model calibration', 'uncertainty quantification', 'few-shot adaptation', 'Parameter-Efficient Fine-Tuning']}, 'abstract': {'value': 'Large transformer-based foundation models have been commonly used as pre-trained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance. Parameter efficient fine-tuning ($\\\\texttt{PEFT}$) provides promising generalization performance in adaptation while incurring minimum computational overhead. However, adaptation of these foundation models through $\\\\texttt{PEFT}$ leads to accurate but severely underconfident models, especially in few-shot learning settings. Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains. To fill out this critical gap, we develop a novel lightweight {Bayesian Parameter Efficient Fine-Tuning} (referred to as $\\\\texttt{Bayesian-PEFT}$) framework for large transformer-based foundation models. The framework integrates state-of-the-art $\\\\texttt{PEFT}$ techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components.\\nOur thorough theoretical analysis justifies that the  Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple $\\\\texttt{PEFT}$ techniques demonstrate the outstanding prediction and calibration performance by $\\\\texttt{Bayesian-PEFT}$.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0f211c89a99253cf0f4d97fe37005d821b00cfa5.pdf'}, 'supplementary_material': {'value': '/attachment/37d7a1c90c82bcb36e1ff2d5520c8f53e1bb92a9.zip'}, '_bibtex': {'value': '@inproceedings{\\npandey2024be,\\ntitle={Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models},\\nauthor={Deep Shankar Pandey and Spandan Pyakurel and Qi Yu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=loQCk0qruU}\\n}'}, 'paperhash': {'value': 'pandey|be_confident_in_what_you_know_bayesian_parameter_efficient_finetuning_of_vision_foundation_models'}},forum = 'loQCk0qruU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12043/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12043/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12043/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12043/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'loMa99A4p8',number = 13843,cdate = 1715743485264,pdate = 1727288052175,odate = 1730873960962,mdate = 1730873960985,tcdate = 1715743485264,tmdate = 1730873960985,ddate = None,content = {'title': {'value': 'Diffusion Models With Learned Adaptive Noise'}, 'authors': {'value': ['Subham Sekhar Sahoo', 'Aaron Gokaslan', 'Christopher De Sa', 'Volodymyr Kuleshov']}, 'authorids': {'value': ['~Subham_Sekhar_Sahoo1', '~Aaron_Gokaslan1', '~Christopher_De_Sa2', '~Volodymyr_Kuleshov1']}, 'keywords': {'value': ['Generative Models', 'Diffusion Models', 'Image Diffusion']}, 'TLDR': {'value': 'A learned noise schedule can improve the log-likelihood of diffusion models.'}, 'abstract': {'value': 'Diffusion models have gained traction as powerful algorithms for synthesizing high-quality images. Central to these algorithms is the diffusion process, a set of equations which maps data to noise \\nin a way that can significantly affect performance. \\nIn this paper, we explore whether the diffusion\\nprocess can be learned from data.\\nOur work is grounded in Bayesian inference and seeks to improve log-likelihood estimation by casting the learned diffusion process as an approximate variational posterior that yields a tighter lower bound (ELBO) on the likelihood.\\nA widely held assumption is that the ELBO is invariant to the noise process: our work dispels this assumption and proposes multivariate learned adaptive noise (MuLAN), a learned diffusion process that applies noise at different rates across an image. Our method consists of three components: a multivariate noise schedule, adaptive input-conditional diffusion, and auxiliary variables; these components ensure that the ELBO is no longer invariant to the choice of the noise schedule as in previous works.  Empirically, MuLAN sets a new **state-of-the-art** in density estimation on CIFAR-10 and ImageNet while matching the performance of previous state-of-the-art models with **50%** fewer steps.  We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/MuLAN'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/dde73f70550a3352ef5e2ea68d6a2803649c09eb.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsahoo2024diffusion,\\ntitle={Diffusion Models With Learned Adaptive Noise},\\nauthor={Subham Sekhar Sahoo and Aaron Gokaslan and Christopher De Sa and Volodymyr Kuleshov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=loMa99A4p8}\\n}'}, 'paperhash': {'value': 'sahoo|diffusion_models_with_learned_adaptive_noise'}},forum = 'loMa99A4p8',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13843/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13843/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13843/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13843/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lmsCSDymEP',number = 2946,cdate = 1715189969984,pdate = 1727287705983,odate = 1730873862159,mdate = 1730873862177,tcdate = 1715189969984,tmdate = 1730873862177,ddate = None,content = {'title': {'value': 'Human-Object Interaction Detection Collaborated with Large Relation-driven Diffusion Models'}, 'authors': {'value': ['Liulei Li', 'Wenguan Wang', 'Yi Yang']}, 'authorids': {'value': ['~Liulei_Li1', '~Wenguan_Wang4', '~Yi_Yang22']}, 'keywords': {'value': ['HOI detection', 'relation understanding', 'diffusion models']}, 'abstract': {'value': 'Prevalent human-object interaction (HOI) detection approaches typically leverage large-scale visual-linguistic models to help recognize events involving humans and objects. Though promising, models trained via contrastive learning on text-image pairs often neglect mid/low-level visual cues and struggle at compositional reasoning. In response, we introduce DIFFUSIONHOI, a new HOI detector shedding light on text-to-image diffusion models. Unlike the aforementioned models, diffusion models excel in discerning mid/low-level visual concepts as generative models, and possess strong compositionality to handle novel concepts expressed in text inputs. Considering diffusion models usually emphasize instance objects, we first devise an inversion-based strategy to learn the expression of relation patterns between humans and objects in embedding space. These learned relation embeddings then serve as textual prompts, to steer diffusion models generate images that depict specific interactions, and extract HOI-relevant cues from images without heavy finetuning. Benefited from above, DIFFUSIONHOI achieves SOTA performance on three datasets under both regular and zero-shot setups.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d968c578f9956e1c66a75ead0ee823a71f966e3f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024humanobject,\\ntitle={Human-Object Interaction Detection Collaborated with Large Relation-driven Diffusion Models},\\nauthor={Liulei Li and Wenguan Wang and Yi Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lmsCSDymEP}\\n}'}, 'paperhash': {'value': 'li|humanobject_interaction_detection_collaborated_with_large_relationdriven_diffusion_models'}},forum = 'lmsCSDymEP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2946/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2946/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2946/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2946/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'llTroju97T',number = 4304,cdate = 1715399254288,pdate = 1727287747097,odate = 1730873874859,mdate = 1730873874893,tcdate = 1715399254288,tmdate = 1730873874893,ddate = None,content = {'title': {'value': 'Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models'}, 'authors': {'value': ['Shengchao Chen', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']}, 'authorids': {'value': ['~Shengchao_Chen1', '~Guodong_Long2', '~Jing_Jiang6', '~Chengqi_Zhang1']}, 'keywords': {'value': ['Meteorological Variable Modeling', 'Federared Learning', 'On-device Intelligence', 'Foundation Model']}, 'TLDR': {'value': 'Taming pre-trained language models as foundation models for personalized on-device meteorological variables modeling.'}, 'abstract': {'value': 'This paper demonstrates that pre-trained language models (PLMs) are strong foundation models for on-device meteorological variable modeling. We present LM-Weather, a generic approach to taming PLMs, that have learned massive sequential knowledge from the universe of natural language databases, to acquire an immediate capability to obtain highly customized models for heterogeneous meteorological data on devices while keeping high efficiency. Concretely, we introduce a lightweight personalized adapter into PLMs and endows it with weather pattern awareness. During communication between clients and the server, low-rank-based transmission is performed to effectively fuse the global knowledge among devices while maintaining high communication efficiency and ensuring privacy. Experiments on real-wold dataset show that LM-Weather outperforms the state-of-the-art results by a large margin across various tasks (e.g., forecasting and imputation at different scales). We provide extensive and in-depth analyses experiments, which verify that LM-Weather can (1) indeed leverage sequential knowledge from natural language to accurately handle meteorological sequence, (2) allows each devices obtain highly customized models under significant heterogeneity, and (3) generalize under data-limited and out-of-distribution (OOD) scenarios.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f2a6e5afdf4428ea2b7277c8b93ae10ce241b3ca.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024personalized,\\ntitle={Personalized Adapter for Large Meteorology Model on Devices: Towards Weather Foundation Models},\\nauthor={Shengchao Chen and Guodong Long and Jing Jiang and Chengqi Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=llTroju97T}\\n}'}, 'paperhash': {'value': 'chen|personalized_adapter_for_large_meteorology_model_on_devices_towards_weather_foundation_models'}},forum = 'llTroju97T',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4304/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4304/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4304/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4304/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lkx3OpcqSZ',number = 13321,cdate = 1715738344909,pdate = 1727288035758,odate = 1730873957180,mdate = 1737135348641,tcdate = 1715738344909,tmdate = 1737135348641,ddate = None,content = {'title': {'value': 'Compressing Large Language Models using Low Rank and Low Precision Decomposition'}, 'authors': {'value': ['Rajarshi Saha', 'Naomi Sagan', 'Varun Srivastava', 'Andrea Goldsmith', 'Mert Pilanci']}, 'authorids': {'value': ['~Rajarshi_Saha1', '~Naomi_Sagan1', '~Varun_Srivastava1', '~Andrea_Goldsmith1', '~Mert_Pilanci3']}, 'keywords': {'value': ['Large Language Models (LLMs)', 'Model Compression', 'Post-training Quantization', 'Low-Rank Decomposition', 'Low-Precision Formats', 'Quantization Error Analysis', 'Rank-Constrained Regression', 'Randomized Linear Algebra', 'Sketching']}, 'TLDR': {'value': 'We propose a post-training compression algorithm for Large Language Models (LLMs), that harnesses the inherent low-rank structure of LLM weight matrices, that effectively combines low-rank and low-precision matrix decompositions,'}, 'abstract': {'value': 'The prohibitive sizes of Large Language Models (LLMs) today make it difficult to deploy them on memory-constrained edge devices. This work introduces $\\\\rm CALDERA$ -- a new post-training LLM compression algorithm that harnesses the inherent low-rank structure of a weight matrix $\\\\mathbf{W}$ by approximating it via a low-rank, low-precision decomposition as $\\\\mathbf{W} \\\\approx \\\\mathbf{Q} + \\\\mathbf{L}\\\\mathbf{R}$. Here, $\\\\mathbf{L}$ and $\\\\mathbf{R}$ are low rank factors, and the entries of $\\\\mathbf{Q}$, $\\\\mathbf{L}$ and $\\\\mathbf{R}$ are quantized. The model is compressed by substituting each layer with its $\\\\mathbf{Q} + \\\\mathbf{L}\\\\mathbf{R}$ decomposition, and the zero-shot performance of the compressed model is evaluated. Additionally, $\\\\mathbf{L}$ and $\\\\mathbf{R}$ are readily amenable to low-rank adaptation, consequently enhancing the zero-shot performance. $\\\\rm CALDERA$ obtains this decomposition by formulating it as an optimization problem $\\\\min_{\\\\mathbf{Q},\\\\mathbf{L},\\\\mathbf{R}}\\\\lVert(\\\\mathbf{Q} + \\\\mathbf{L}\\\\mathbf{R} - \\\\mathbf{W})\\\\mathbf{X}^\\\\top\\\\rVert_{\\\\rm F}^2$, where $\\\\mathbf{X}$ is the calibration data, and $\\\\mathbf{Q}, \\\\mathbf{L}, \\\\mathbf{R}$ are constrained to be representable using low-precision formats. Theoretical upper bounds on the approximation error of $\\\\rm CALDERA$ are established using a rank-constrained regression framework, and the tradeoff between compression ratio and model performance is studied by analyzing the impact of target rank and quantization bit budget. Results illustrate that compressing LlaMa-$2$ $7$B/$13$B/$70$B and LlaMa-$3$ $8$B models obtained using $\\\\rm CALDERA$ outperforms existing post-training LLM compression techniques in the regime of less than $2.5$ bits per parameter.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f8a58d3fc9e09b52eb5177783fd20c8a1b80a83a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsaha2024compressing,\\ntitle={Compressing Large Language Models using Low Rank and Low Precision Decomposition},\\nauthor={Rajarshi Saha and Naomi Sagan and Varun Srivastava and Andrea Goldsmith and Mert Pilanci},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lkx3OpcqSZ}\\n}'}, 'paperhash': {'value': 'saha|compressing_large_language_models_using_low_rank_and_low_precision_decomposition'}},forum = 'lkx3OpcqSZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13321/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13321/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13321/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13321/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'liHe9iumIi',number = 12276,cdate = 1715719581092,pdate = 1727288001014,odate = 1730873947696,mdate = 1730873947715,tcdate = 1715719581092,tmdate = 1730873947715,ddate = None,content = {'title': {'value': 'FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage Training'}, 'authors': {'value': ['Ruihong Yin', 'Vladimir Yugay', 'Yue Li', 'Sezer Karaoglu', 'Theo Gevers']}, 'authorids': {'value': ['~Ruihong_Yin1', '~Vladimir_Yugay1', '~Yue_Li12', '~Sezer_Karaoglu1', '~Theo_Gevers1']}, 'keywords': {'value': ['Few-shot view synthesis', 'gaussian splatting']}, 'TLDR': {'value': 'A novel method with multi-stage training scheme, novel view consistency constraints, and local regularization losses for few-shot view synthesis'}, 'abstract': {'value': 'The field of novel view synthesis from images has seen rapid advancements with the introduction of Neural Radiance Fields (NeRF) and more recently with 3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its efficiency and ability to render novel views accurately. While Gaussian Splatting performs well when a sufficient amount of training images are available, its unstructured explicit representation tends to overfit in scenarios with sparse input images, resulting in poor rendering performance. To address this, we present a 3D Gaussian-based novel view synthesis method using sparse input images that can accurately render the scene from the viewpoints not covered by the training images. We propose a multi-stage training scheme with matching-based consistency constraints imposed on the novel views without relying on pre-trained depth estimation or diffusion models. This is achieved by using the matches of the available training images to supervise the generation of the novel views sampled between the training frames with color, geometry, and semantic losses. In addition, we introduce a locality preserving regularization for 3D Gaussians which removes rendering artifacts by preserving the local color structure of the scene. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance of our method in few-shot novel view synthesis compared to existing state-of-the-art methods.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ed5cc740cd652472f5409c5773b77bab5a4ff3c2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyin2024fewviewgs,\\ntitle={FewView{GS}: Gaussian Splatting with Few View Matching and Multi-stage Training},\\nauthor={Ruihong Yin and Vladimir Yugay and Yue Li and Sezer Karaoglu and Theo Gevers},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=liHe9iumIi}\\n}'}, 'paperhash': {'value': 'yin|fewviewgs_gaussian_splatting_with_few_view_matching_and_multistage_training'}},forum = 'liHe9iumIi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12276/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12276/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12276/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12276/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lhlIUxD5eE',number = 303,cdate = 1713862927556,pdate = 1727287634891,odate = 1730873839895,mdate = 1730873839914,tcdate = 1713862927556,tmdate = 1730873839914,ddate = None,content = {'title': {'value': 'Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow'}, 'authors': {'value': ['Chen-Hao Chao', 'Chien Feng', 'Wei-Fang Sun', 'Cheng-Kuang Lee', 'Simon See', 'Chun-Yi Lee']}, 'authorids': {'value': ['~Chen-Hao_Chao2', '~Chien_Feng2', '~Wei-Fang_Sun1', '~Cheng-Kuang_Lee1', '~Simon_See1', '~Chun-Yi_Lee1']}, 'keywords': {'value': ['Reinforcement Learning', 'Maximum Entropy Reinforcement Learning', 'Normalizing Flows', 'Energy-Based Models', 'Energy-Based Normalizing Flow']}, 'abstract': {'value': 'Existing Maximum-Entropy (MaxEnt) Reinforcement Learning (RL) methods for continuous action spaces are typically formulated based on actor-critic frameworks and optimized through alternating steps of policy evaluation and policy improvement. In the policy evaluation steps, the critic is updated to capture the soft Q-function. In the policy improvement steps, the actor is adjusted in accordance with the updated soft Q-function. In this paper, we introduce a new MaxEnt RL framework modeled using Energy-Based Normalizing Flows (EBFlow). This framework integrates the policy evaluation steps and the policy improvement steps, resulting in a single objective training process. Our method enables the calculation of the soft value function used in the policy evaluation target without Monte Carlo approximation. Moreover, this design supports the modeling of multi-modal action distributions while facilitating efficient action sampling. To evaluate the performance of our method, we conducted experiments on the MuJoCo benchmark suite and a number of high-dimensional robotic tasks simulated by Omniverse Isaac Gym. The evaluation results demonstrate that our method achieves superior performance compared to widely-adopted representative baselines.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ec522f54f9fae11a7e5da3c9da4418013805b5c3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchao2024maximum,\\ntitle={Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow},\\nauthor={Chen-Hao Chao and Chien Feng and Wei-Fang Sun and Cheng-Kuang Lee and Simon See and Chun-Yi Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lhlIUxD5eE}\\n}'}, 'paperhash': {'value': 'chao|maximum_entropy_reinforcement_learning_via_energybased_normalizing_flow'}},forum = 'lhlIUxD5eE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission303/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission303/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission303/-/Revision', 'NeurIPS.cc/2024/Conference/Submission303/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lgtsXxk4dF',number = 12294,cdate = 1715719898444,pdate = 1727288001632,odate = 1730873947878,mdate = 1730873947896,tcdate = 1715719898444,tmdate = 1730873947896,ddate = None,content = {'title': {'value': 'Clustering with Non-adaptive Subset Queries'}, 'authors': {'value': ['Hadley Black', 'Euiwoong Lee', 'Arya Mazumdar', 'Barna Saha']}, 'authorids': {'value': ['~Hadley_Black1', '~Euiwoong_Lee2', '~Arya_Mazumdar1', '~Barna_Saha3']}, 'keywords': {'value': ['Clustering', 'query algorithms']}, 'abstract': {'value': 'Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade. Given a query $S \\\\subset U$, $|S|=2$, the oracle returns \"yes\" if the points are in the same cluster and \"no\" otherwise. We study a natural generalization of this problem to subset queries for $|S|>2$, where the oracle returns the number of clusters intersecting $S$. Our aim is to determine the minimum number of queries needed for exactly recovering an arbitrary $k$-clustering. We focus on non-adaptive schemes, where all the queries are asked in one round, thus allowing for the querying process to be parallelized, which is a highly desirable property. \\n\\nFor adaptive algorithms with pair-wise queries, the complexity is known to be $\\\\Theta(nk)$, where $k$ is the number of clusters. \\nIn contrast, non-adaptive pair-wise query algorithms are extremely limited: even for $k=3$, such algorithms require $\\\\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained by querying every pair of points. Allowing for subset queries of unbounded size, $O(n)$ queries is possible with an adaptive scheme.  However, the realm of non-adaptive algorithms remains completely unknown. Is it possible to attain algorithms that are non-adaptive while still making a near-linear number of queries?\\n\\nIn this paper, we give the first non-adaptive algorithms for clustering with subset queries. We provide, (i) a non-adaptive algorithm making $O(n \\\\log^2 n \\\\log k)$ queries which improves to $O(n \\\\log k)$ when the cluster sizes are within any constant factor of each other, (ii) for constant $k$, a non-adaptive algorithm making $O(n \\\\log{\\\\log{n}})$ queries. In addition to non-adaptivity, we take into account other practical considerations, such as enforcing a bound on query size.  For constant $k$, we give an algorithm making $\\\\smash{\\\\widetilde{O}(n^2/s^2)}$ queries on subsets of size at most $s \\\\leq \\\\sqrt{n}$, which is optimal among all non-adaptive algorithms within a $\\\\log n$-factor. For arbitrary $k$, the dependence varies as $\\\\tilde{O}(n^2/s)$.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We provide efficient non-adaptive algorithms that query a near-linear (and near-optimal) number of subset-queries to infer clustering of a set of elements exactly.'}, 'pdf': {'value': '/pdf/61ca56abf9e2fcb4a96d5c3908c1d3617a81cb55.pdf'}, '_bibtex': {'value': '@inproceedings{\\nblack2024clustering,\\ntitle={Clustering with Non-adaptive Subset Queries},\\nauthor={Hadley Black and Euiwoong Lee and Arya Mazumdar and Barna Saha},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lgtsXxk4dF}\\n}'}, 'paperhash': {'value': 'black|clustering_with_nonadaptive_subset_queries'}},forum = 'lgtsXxk4dF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12294/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12294/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12294/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12294/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lfxIASyLxB',number = 5547,cdate = 1715548098929,pdate = 1727287787414,odate = 1730873885900,mdate = 1730873885917,tcdate = 1715548098929,tmdate = 1730873885917,ddate = None,content = {'title': {'value': 'In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness'}, 'authors': {'value': ['Liam Collins', 'Advait U Parulekar', 'Aryan Mokhtari', 'sujay sanghavi', 'Sanjay Shakkottai']}, 'authorids': {'value': ['~Liam_Collins1', '~Advait_U_Parulekar1', '~Aryan_Mokhtari3', '~sujay_sanghavi1', '~Sanjay_Shakkottai1']}, 'keywords': {'value': ['Transformers', 'self-attention', 'in-context learning']}, 'abstract': {'value': 'A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such, that learner must adapt to the context without additional training. We explore the role of *softmax* attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activation and thus cannot be replicated by the linear activation often studied in prior theoretical analyses.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4d0a9ce81958e13f8f0b42e37c7730d17dd79463.pdf'}, 'supplementary_material': {'value': '/attachment/0f62b7dca18a67a31c823e4d897b5d40bb3923c2.zip'}, '_bibtex': {'value': '@inproceedings{\\ncollins2024incontext,\\ntitle={In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness},\\nauthor={Liam Collins and Advait U Parulekar and Aryan Mokhtari and sujay sanghavi and Sanjay Shakkottai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lfxIASyLxB}\\n}'}, 'paperhash': {'value': 'collins|incontext_learning_with_transformers_softmax_attention_adapts_to_function_lipschitzness'}},forum = 'lfxIASyLxB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5547/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5547/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5547/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5547/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lflwtGE6Vf',number = 17426,cdate = 1715779452110,pdate = 1727288153089,odate = 1730873984424,mdate = 1730873984438,tcdate = 1715779452110,tmdate = 1730873984438,ddate = None,content = {'title': {'value': '(FL)$^2$: Overcoming Few Labels in Federated Semi-Supervised Learning'}, 'authors': {'value': ['Seungjoo Lee', 'Thanh-Long V. Le', 'Jaemin Shin', 'Sung-Ju Lee']}, 'authorids': {'value': ['~Seungjoo_Lee1', '~Thanh-Long_V._Le1', '~Jaemin_Shin1', '~Sung-Ju_Lee1']}, 'keywords': {'value': ['Federated Learning', 'Semi-Supervised Learning', 'Federated Semi-Supervised Learning']}, 'abstract': {'value': \"Federated Learning (FL) is a distributed machine learning framework that trains accurate global models while preserving clients' privacy-sensitive data. However, most FL approaches assume that clients possess labeled data, which is often not the case in practice. Federated Semi-Supervised Learning (FSSL) addresses this label deficiency problem, targeting situations where only the server has a small amount of labeled data while clients do not. However, a significant performance gap exists between Centralized Semi-Supervised Learning (SSL) and FSSL. This gap arises from confirmation bias, which is more pronounced in FSSL due to multiple local training epochs and the separation of labeled and unlabeled data. We propose $(FL)^2$, a robust training method for unlabeled clients using sharpness-aware consistency regularization. We show that regularizing the original pseudo-labeling loss is suboptimal, and hence we carefully select unlabeled samples for regularization. We further introduce client-specific adaptive thresholding and learning status-aware aggregation to adjust the training process based on the learning progress of each client. Our experiments on three benchmark datasets demonstrate that our approach significantly improves performance and bridges the gap with SSL, particularly in scenarios with scarce labeled data.\"}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Overcoming Few Labels in Federated Semi-Supervised Learning'}, 'pdf': {'value': '/pdf/1195fb6b6e8091390a490dd0e54953dc117b25df.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024fl,\\ntitle={({FL})\\\\${\\\\textasciicircum}2\\\\$: Overcoming Few Labels in Federated Semi-Supervised Learning},\\nauthor={Seungjoo Lee and Thanh-Long V. Le and Jaemin Shin and Sung-Ju Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lflwtGE6Vf}\\n}'}, 'paperhash': {'value': 'lee|fl^2_overcoming_few_labels_in_federated_semisupervised_learning'}},forum = 'lflwtGE6Vf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17426/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17426/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17426/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17426/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'lfY0SUT3m9',number = 7560,cdate = 1715628015380,pdate = 1727287851415,odate = 1730873904091,mdate = 1730873904164,tcdate = 1715628015380,tmdate = 1730873904164,ddate = None,content = {'title': {'value': 'Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization'}, 'authors': {'value': ['Quoc Tran-Dinh', 'Trang H. Tran', 'Lam M. Nguyen']}, 'authorids': {'value': ['~Quoc_Tran-Dinh2', '~Trang_H._Tran1', '~Lam_M._Nguyen1']}, 'keywords': {'value': ['Shuffling gradient method; nonconvex-concave minimax problem; oracle complexity; sample without replacement']}, 'TLDR': {'value': 'This paper develops two novel shuffling-based algorithms to solve two classes of nonconvex-concave minimax problems that have provable convergence guarantees.'}, 'abstract': {'value': \"This paper aims at developing novel shuffling gradient-based methods for tackling two classes of minimax problems: nonconvex-linear and nonconvex-strongly concave settings. The first algorithm addresses the nonconvex-linear minimax model and achieves the state-of-the-art oracle complexity typically observed in nonconvex optimization.  It also employs a new shuffling estimator for the ``hyper-gradient'', departing from standard shuffling techniques in optimization. The second method consists of two variants: semi-shuffling and full-shuffling schemes.  These variants tackle the nonconvex-strongly concave minimax setting.  We establish their oracle complexity bounds under standard assumptions, which, to our best knowledge, are the best-known for this specific setting. Numerical examples demonstrate the performance of our algorithms and compare them with two other methods.  Our results show that the new methods achieve comparable performance with SGD, supporting the potential of incorporating shuffling strategies into minimax algorithms.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/396caa232403726b82ad02411f633f45bd3bc3e6.pdf'}, 'supplementary_material': {'value': '/attachment/376adae174f55b456ad423e90e80dbd440a9d085.zip'}, '_bibtex': {'value': '@inproceedings{\\ntran-dinh2024shuffling,\\ntitle={Shuffling Gradient-Based Methods for Nonconvex-Concave Minimax Optimization},\\nauthor={Quoc Tran-Dinh and Trang H. Tran and Lam M. Nguyen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lfY0SUT3m9}\\n}'}, 'paperhash': {'value': 'trandinh|shuffling_gradientbased_methods_for_nonconvexconcave_minimax_optimization'}},forum = 'lfY0SUT3m9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7560/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7560/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7560/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7560/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'leqD3bJ4Ly',number = 16672,cdate = 1715773131374,pdate = 1727288133043,odate = 1730873980580,mdate = 1730873980598,tcdate = 1715773131374,tmdate = 1730873980598,ddate = None,content = {'title': {'value': 'OPEL: Optimal Transport Guided ProcedurE Learning'}, 'authors': {'value': ['Sayeed Shafayet Chowdhury', 'Soumyadeep Chandra', 'Kaushik Roy']}, 'authorids': {'value': ['~Sayeed_Shafayet_Chowdhury3', '~Soumyadeep_Chandra1', '~Kaushik_Roy1']}, 'keywords': {'value': ['Procedure learning', 'Egocentric vision', 'EgoProceL', 'Optimal Transport']}, 'TLDR': {'value': 'An unsupervised optimal transport guided method for procedure learning, which achieves state-of-the-art results'}, 'abstract': {'value': \"Procedure learning refers to the task of identifying the key-steps and determining their logical order, given several videos of the same task. For both third-person and first-person (egocentric) videos, state-of-the-art (SOTA) methods aim at finding correspondences across videos in time to accomplish procedure learning. However, to establish temporal relationships within the sequences, these methods often rely on frame-to-frame mapping, or assume monotonic alignment of video pairs, leading to sub-optimal results. To this end, we propose to treat the video frames as  samples from an unknown distribution, enabling us to frame their distance calculation as an optimal transport (OT) problem. Notably, the OT-based formulation allows us to relax the previously mentioned assumptions. To further improve performance, we enhance the OT formulation by introducing two regularization terms. The first,  inverse difference moment regularization, promotes transportation between instances that are homogeneous in the embedding space as well as being temporally closer. The second, regularization based on the KL-divergence with an exponentially decaying prior smooths the alignment while enforcing conformity to the optimality (alignment obtained from vanilla OT optimization) and temporal priors. The resultant optimal transport guided procedure learning framework (`OPEL') significantly outperforms the SOTA on benchmark datasets. Specifically, we achieve 22.4\\\\% (IoU) and 26.9\\\\% (F1) average improvement compared to the current SOTA on large scale egocentric benchmark, EgoProceL. Furthermore, for the third person benchmarks (ProCeL and CrossTask), the proposed approach obtains 46.2\\\\% (F1) average enhancement over SOTA.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5806637acc87823d00dc8ff1945b86c1c0462612.pdf'}, 'supplementary_material': {'value': '/attachment/0d4561ad68409abed8bd5e48af9d86b66788ef34.zip'}, '_bibtex': {'value': '@inproceedings{\\nchowdhury2024opel,\\ntitle={{OPEL}: Optimal Transport Guided ProcedurE Learning},\\nauthor={Sayeed Shafayet Chowdhury and Soumyadeep Chandra and Kaushik Roy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=leqD3bJ4Ly}\\n}'}, 'paperhash': {'value': 'chowdhury|opel_optimal_transport_guided_procedure_learning'}},forum = 'leqD3bJ4Ly',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16672/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16672/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16672/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16672/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'leeosk2RAM',number = 6274,cdate = 1715588838720,pdate = 1727287809621,odate = 1730873891641,mdate = 1736225623530,tcdate = 1715588838720,tmdate = 1736225623530,ddate = None,content = {'title': {'value': 'SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge'}, 'authors': {'value': ['Chuanhao Li', 'Zhen Li', 'Chenchen Jing', 'Shuo Liu', 'Wenqi Shao', 'Yuwei Wu', 'Ping Luo', 'Yu Qiao', 'Kaipeng Zhang']}, 'authorids': {'value': ['~Chuanhao_Li2', '~Zhen_Li15', '~Chenchen_Jing2', '~Shuo_Liu5', '~Wenqi_Shao2', '~Yuwei_Wu1', '~Ping_Luo2', '~Yu_Qiao1', '~Kaipeng_Zhang1']}, 'keywords': {'value': ['Internet-Augmented Generation', 'Large Vision-Language Models']}, 'abstract': {'value': \"Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a LVLM was released on January 2024, and it wouldn't know the singer of the theme song for the new Detective Conan movie, which wasn't released until April 2024. To solve the problem, a promising solution motivated by retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date knowledge via internet search during inference, i.e., internet-augmented generation (IAG), which is already integrated in some closed-source commercial LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain a mystery. In this paper, we propose a plug-and-play framework, for augmenting existing LVLMs in handling visual question answering (VQA) about up-to-date knowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to effectively and efficiently find the most helpful content from the websites returned by a search engine to prompt LVLMs with up-to-date knowledge. To train the model and evaluate our framework's performance, we propose a pipeline to automatically generate news-related VQA samples to construct a dataset, dubbed UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness of website/content for VQA samples to construct the training set. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by $\\\\sim$30\\\\% in accuracy.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0f27f7569896eff9d72e5134b77687131406bf9c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024searchlvlms,\\ntitle={Search{LVLM}s: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge},\\nauthor={Chuanhao Li and Zhen Li and Chenchen Jing and Shuo Liu and Wenqi Shao and Yuwei Wu and Ping Luo and Yu Qiao and Kaipeng Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=leeosk2RAM}\\n}'}, 'paperhash': {'value': 'li|searchlvlms_a_plugandplay_framework_for_augmenting_large_visionlanguage_models_by_searching_uptodate_internet_knowledge'}},forum = 'leeosk2RAM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6274/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6274/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6274/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6274/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ldvfaYzG35',number = 9343,cdate = 1715678550224,pdate = 1727287907846,odate = 1730873920072,mdate = 1734594866137,tcdate = 1715678550224,tmdate = 1734594866137,ddate = None,content = {'title': {'value': 'Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective'}, 'authors': {'value': ['MeiJun Wang', 'Yu Meng', 'Zhongwei Qiu', 'Chao Zheng', 'Yan Xu', 'Pengxiaorui', 'Jian Gao']}, 'authorids': {'value': ['~MeiJun_Wang1', '~Yu_Meng6', '~Zhongwei_Qiu1', '~Chao_Zheng7', '~Yan_Xu14', '~Pengxiaorui1', '~Jian_Gao11']}, 'keywords': {'value': ['Pedestrian Pre-collision pose', 'Human pose and shape estimation', 'Dashcam Perspective', 'Pedestrian-Vehicle Collision Pose dataset']}, 'TLDR': {'value': 'We construct the first Pedestrian-Vehicle Collision Pose (PVCP) dataset from the perspective of dashcam, and propose a pedestrian Pre-collision Pose and Shape Estimation network (PPSENet).'}, 'abstract': {'value': 'Pedestrian pre-collision pose is one of the key factors to determine the degree of pedestrian-vehicle injury in collision. Human pose estimation algorithm is an effective method to estimate pedestrian emergency pose from accident video. However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes. In this paper, we collect pedestrian-vehicle collision pose from the dashcam perspective of dashcam and construct the first Pedestrian-Vehicle Collision Pose dataset (PVCP) in a semi-automatic way, including 40k+ accident frames and 20K+ pedestrian pre-collision pose annotation (2D, 3D, Mesh). Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos. The PPSENet first estimates the 2D pose from the image (Image to Pose, ITP) and then lifts the 2D pose to 3D mesh (Pose to Mesh, PTM). Due to the small size of the dataset, we introduce a pre-training model that learns the human pose prior on a large number of pose datasets, and use iterative regression to estimate the pre-collision pose and shape of pedestrians. Further, we classify the pre-collision pose sequence and introduce pose class loss, which achieves the best accuracy compared with the existing relevant \\\\textit{state-of-the-art} methods. Code and data are available for research at https://github.com/wmj142326/PVCP.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b9fd46cc0bc56a82bb56b9555fd659ec25d1019f.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nwang2024pedestriancentric,\\ntitle={Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective},\\nauthor={MeiJun Wang and Yu Meng and Zhongwei Qiu and Chao Zheng and Yan Xu and Pengxiaorui and Jian Gao},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ldvfaYzG35}\\n}'}, 'paperhash': {'value': 'wang|pedestriancentric_3d_precollision_pose_and_shape_estimation_from_dashcam_perspective'}},forum = 'ldvfaYzG35',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9343/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9343/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9343/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9343/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'ldXyNSvXEr',number = 6771,cdate = 1715604526766,pdate = 1727287825195,odate = 1730873896144,mdate = 1730873896164,tcdate = 1715604526766,tmdate = 1730873896164,ddate = None,content = {'title': {'value': 'Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift'}, 'authors': {'value': ['Jiawei Ge', 'Debarghya Mukherjee', 'Jianqing Fan']}, 'authorids': {'value': ['~Jiawei_Ge3', '~Debarghya_Mukherjee1', '~Jianqing_Fan1']}, 'keywords': {'value': ['prediction intervals aggregation', 'unsupervised domain shift', 'minimal width', 'adequate coverage', 'distribution shift']}, 'abstract': {'value': \"As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts.\\nA distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model's performance. \\nThe prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. \\nIn this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain.\\nOur analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation.\\nOur proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through real-world datasets, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4b5e7856a214e261abbca6b3022232ac3b3f4ac3.pdf'}, 'supplementary_material': {'value': '/attachment/8c78f83257f9fcda0a0d8e6f7cc878c3b8dad560.zip'}, '_bibtex': {'value': '@inproceedings{\\nge2024optimal,\\ntitle={Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift},\\nauthor={Jiawei Ge and Debarghya Mukherjee and Jianqing Fan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ldXyNSvXEr}\\n}'}, 'paperhash': {'value': 'ge|optimal_aggregation_of_prediction_intervals_under_unsupervised_domain_shift'}},forum = 'ldXyNSvXEr',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6771/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6771/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6771/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6771/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lckAdnVzsT',number = 3792,cdate = 1715325967498,pdate = 1727287730915,odate = 1730873869522,mdate = 1730873869542,tcdate = 1715325967498,tmdate = 1730873869542,ddate = None,content = {'title': {'value': 'Coherent 3D Scene Diffusion From a Single RGB Image'}, 'authors': {'value': ['Manuel Dahnert', 'Angela Dai', 'Norman Müller', 'Matthias Nießner']}, 'authorids': {'value': ['~Manuel_Dahnert1', '~Angela_Dai1', '~Norman_Müller1', '~Matthias_Nießner2']}, 'keywords': {'value': ['Single RGB Image 3D Scene Reconstruction', 'Diffusion Models', 'Scene Understanding', '3D Scene Prior']}, 'abstract': {'value': 'We present a novel diffusion-based approach for coherent 3D scene reconstruction from a single RGB image. \\nOur method utilizes an image-conditioned 3D scene diffusion model to simultaneously denoise the 3D poses and geometries of all objects within the scene.\\n\\nMotivated by the ill-posed nature of the task and to obtain consistent scene reconstruction results, we learn a generative scene prior by conditioning on all scene objects simultaneously to capture scene context and by allowing the model to learn inter-object relationships throughout the diffusion process.\\n\\nWe further propose an efficient surface alignment loss to facilitate training even in the absence of full ground-truth annotation, which is common in publicly available datasets. This loss leverages an expressive shape representation, which enables direct point sampling from intermediate shape predictions.\\n\\nBy framing the task of single RGB image 3D scene reconstruction as a conditional diffusion process, our approach surpasses current state-of-the-art methods, achieving a 12.04\\\\% improvement in AP3D on SUN RGB-D and a 13.43\\\\% increase in F-Score on Pix3D.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'A novel diffusion-based 3D scene diffusion model for 3D scene reconstruction from a single RGB image'}, 'pdf': {'value': '/pdf/14518c4c229813583cef4952da32a8fbf3c7b5c4.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndahnert2024coherent,\\ntitle={Coherent 3D Scene Diffusion From a Single {RGB} Image},\\nauthor={Manuel Dahnert and Angela Dai and Norman M{\\\\\"u}ller and Matthias Nie{\\\\ss}ner},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lckAdnVzsT}\\n}'}, 'paperhash': {'value': 'dahnert|coherent_3d_scene_diffusion_from_a_single_rgb_image'}},forum = 'lckAdnVzsT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3792/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3792/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3792/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3792/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lcALCNF2qe',number = 6328,cdate = 1715590345449,pdate = 1727287811324,odate = 1730873892360,mdate = 1736734468762,tcdate = 1715590345449,tmdate = 1736734468762,ddate = None,content = {'title': {'value': 'Towards Universal Mesh Movement Networks'}, 'authors': {'value': ['Mingrui Zhang', 'Chunyang Wang', 'Stephan C. Kramer', 'Joseph Gregory Wallwork', 'Siyi Li', 'Jiancheng Liu', 'Xiang Chen', 'Matthew D Piggott']}, 'authorids': {'value': ['~Mingrui_Zhang4', '~Chunyang_Wang1', '~Stephan_C._Kramer1', '~Joseph_Gregory_Wallwork1', '~Siyi_Li6', '~Jiancheng_Liu2', '~Xiang_Chen8', '~Matthew_D_Piggott1']}, 'keywords': {'value': ['PDE', 'Physical Simulation', 'Mesh Adaptation', 'Physical Science']}, 'TLDR': {'value': 'We propose Universal Mesh Movement Networks (UM2N), which once trained, can be applied in a non-intrusive, zero-shot manner to move meshes with different sizes and structure, for solvers applicable to different PDE types and boundary geometries.'}, 'abstract': {'value': 'Solving complex Partial Differential Equations (PDEs) accurately and efficiently is an essential and challenging problem in all scientific and engineering disciplines. Mesh movement methods provide the capability to improve the accuracy of the numerical solution without increasing the overall mesh degree of freedom count. Conventional sophisticated mesh movement methods are extremely expensive and struggle to handle scenarios with complex boundary geometries. However, existing learning-based methods require re-training from scratch given a different PDE type or boundary geometry, which limits their applicability, and also often suffer from robustness issues in the form of inverted elements. In this paper, we introduce the Universal Mesh Movement Network (UM2N), which -- once trained -- can be applied in a non-intrusive, zero-shot manner to move meshes with different size distributions and structures, for solvers applicable to different PDE types and boundary geometries. UM2N consists of a Graph Transformer (GT) encoder for extracting features and a Graph Attention Network (GAT) based decoder for moving the mesh. We evaluate our method on advection and Navier-Stokes based examples, as well as a real-world tsunami simulation case. Our method out-performs existing learning-based mesh movement methods in terms of the benchmarks described above. In comparison to the conventional sophisticated Monge-Ampère PDE-solver based method, our approach not only significantly accelerates mesh movement, but also proves effective in scenarios where the conventional method fails. Our project page can be found at https://erizmr.github.io/UM2N/.'}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f76c7735ce5ebac8981b5ac2377795bf7fb37598.pdf'}, 'supplementary_material': {'value': '/attachment/eb2db1c1e9dd888a1deefbf1ce455329dd87338a.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024towards,\\ntitle={Towards Universal Mesh Movement Networks},\\nauthor={Mingrui Zhang and Chunyang Wang and Stephan C. Kramer and Joseph Gregory Wallwork and Siyi Li and Jiancheng Liu and Xiang Chen and Matthew D Piggott},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lcALCNF2qe}\\n}'}, 'paperhash': {'value': 'zhang|towards_universal_mesh_movement_networks'}},forum = 'lcALCNF2qe',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6328/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6328/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6328/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6328/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lbSI1j8m6p',number = 11468,cdate = 1715705312162,pdate = 1727287972250,odate = 1730873938906,mdate = 1730873938927,tcdate = 1715705312162,tmdate = 1730873938927,ddate = None,content = {'title': {'value': 'Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records'}, 'authors': {'value': ['Suhan Cui', 'Prasenjit Mitra']}, 'authorids': {'value': ['~Suhan_Cui1', '~Prasenjit_Mitra1']}, 'keywords': {'value': ['Electronic Health Records; Multi-Task Learning; Automated Machine Learning']}, 'abstract': {'value': \"In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients' future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing task combinations and architectures, we employ surrogate model-based optimization, enabling us to efficiently discover the optimal solution. Experimental results on real-world EHR data demonstrate the efficacy of the proposed AutoDP framework. It achieves significant performance improvements over both hand-crafted and automated state-of-the-art methods, also maintains a feasible search cost at the same time.\"}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3e53a3f862bb0e5f4bc8cba56ba94762f362d6c2.pdf'}, 'supplementary_material': {'value': '/attachment/01ffa1d846e770181915f518521c3440cee3fa74.zip'}, '_bibtex': {'value': '@inproceedings{\\ncui2024automated,\\ntitle={Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records},\\nauthor={Suhan Cui and Prasenjit Mitra},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lbSI1j8m6p}\\n}'}, 'paperhash': {'value': 'cui|automated_multitask_learning_for_joint_disease_prediction_on_electronic_health_records'}},forum = 'lbSI1j8m6p',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11468/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11468/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11468/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11468/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lbLC5OV9GY',number = 10566,cdate = 1715695404965,pdate = 1727287944949,odate = 1730873930420,mdate = 1730873930440,tcdate = 1715695404965,tmdate = 1730873930440,ddate = None,content = {'title': {'value': 'VISA: Variational Inference with Sequential Sample-Average Approximations'}, 'authors': {'value': ['Heiko Zimmermann', 'Christian A. Naesseth', 'Jan-Willem van de Meent']}, 'authorids': {'value': ['~Heiko_Zimmermann1', '~Christian_A._Naesseth1', '~Jan-Willem_van_de_Meent1']}, 'keywords': {'value': ['Variational Inference', 'Sample Average Approximations', 'Importance Sampling']}, 'abstract': {'value': 'We present variational inference with sequential sample-average approximations (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Forward-KL Variational Inference with Sequential Sample-Average Approximations'}, 'pdf': {'value': '/pdf/e71b3bcd7e5e54a9dcfe8b02dd70dbe0cfe4e6d2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzimmermann2024visa,\\ntitle={{VISA}: Variational Inference with Sequential Sample-Average Approximations},\\nauthor={Heiko Zimmermann and Christian A. Naesseth and Jan-Willem van de Meent},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lbLC5OV9GY}\\n}'}, 'paperhash': {'value': 'zimmermann|visa_variational_inference_with_sequential_sampleaverage_approximations'}},forum = 'lbLC5OV9GY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10566/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10566/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10566/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10566/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lZY9u0ijP7',number = 13907,cdate = 1715744065603,pdate = 1727288054361,odate = 1730873961907,mdate = 1730873961918,tcdate = 1715744065603,tmdate = 1730873961918,ddate = None,content = {'title': {'value': 'Cascade Speculative Drafting for Even Faster LLM Inference'}, 'authors': {'value': ['Ziyi Chen', 'Xiaocong Yang', 'Jiacheng Lin', 'Chenkai Sun', 'Kevin Chang', 'Jie Huang']}, 'authorids': {'value': ['~Ziyi_Chen8', '~Xiaocong_Yang3', '~Jiacheng_Lin3', '~Chenkai_Sun1', '~Kevin_Chang1', '~Jie_Huang3']}, 'keywords': {'value': ['Speculative Decoding', 'Large Language Models', 'Efficient NLP']}, 'abstract': {'value': 'Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The *Vertical Cascade* eliminates autoregressive generation from neural models, while the *Horizontal Cascade* optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model. Our code is publicly available at https://github.com/lfsszd/CS-Drafting.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7d6cf3bf7fac4f5e70a8ef98098a47f541169c34.pdf'}, 'supplementary_material': {'value': '/attachment/c530e526cc568eb45379de7175a1c813758c01d4.zip'}, '_bibtex': {'value': '@inproceedings{\\nchen2024cascade,\\ntitle={Cascade Speculative Drafting for Even Faster {LLM} Inference},\\nauthor={Ziyi Chen and Xiaocong Yang and Jiacheng Lin and Chenkai Sun and Kevin Chang and Jie Huang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lZY9u0ijP7}\\n}'}, 'paperhash': {'value': 'chen|cascade_speculative_drafting_for_even_faster_llm_inference'}},forum = 'lZY9u0ijP7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13907/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13907/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13907/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13907/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lZJ0WYI5YC',number = 1669,cdate = 1714740386768,pdate = 1727287668009,odate = 1730873851022,mdate = 1730873851042,tcdate = 1714740386768,tmdate = 1730873851042,ddate = None,content = {'title': {'value': 'Deep Learning in Medical Image Registration: Magic or Mirage?'}, 'authors': {'value': ['Rohit Jena', 'Deeksha Sethi', 'Pratik Chaudhari', 'James Gee']}, 'authorids': {'value': ['~Rohit_Jena1', '~Deeksha_Sethi1', '~Pratik_Chaudhari1', '~James_Gee1']}, 'keywords': {'value': ['image registration', 'image alignment', 'medical image registration', 'T1-weighed MRI', 'image alignment', 'deformable image registration', 'diffeomorphism', 'optimization', 'fairness', 'evaluation']}, 'TLDR': {'value': 'This paper establishes the assumptions and conditions under which either classical and deep-learning image registration algorithms surpass each other'}, 'abstract': {'value': 'Classical optimization and learning-based methods are the two reigning paradigms in deformable image registration. While optimization-based methods boast generalizability across modalities and robust performance, learning-based methods promise peak performance, incorporating weak supervision and amortized optimization. However, the exact conditions for either paradigm to perform well over the other are shrouded and not explicitly outlined in the existing literature. In this paper, we make an explicit correspondence between the mutual information of the distribution of per-pixel intensity and labels, and the performance of classical registration methods. This strong correlation hints to the fact that architectural designs in learning-based methods is unlikely to affect this correlation, and therefore, the performance of learning-based methods. This hypothesis is thoroughly validated with state-of-the-art classical and learning-based methods. However, learning-based methods with weak supervision can perform high-fidelity intensity and label registration, which is not possible with classical methods. Next, we show that this high-fidelity feature learning does not translate to invariance to domain shift, and learning-based methods are sensitive to such changes in the data distribution. We reassess and recalibrate performance expectations from classical and DLIR methods under access to label supervision, training time, and its generalization capabilities under minor domain shifts.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ecd9c365bfff00db86dd67633c7b20371b0a1963.pdf'}, 'supplementary_material': {'value': '/attachment/6cdfdae0ea7153dd31505b0c8cea3efa8b4b810e.zip'}, '_bibtex': {'value': '@inproceedings{\\njena2024deep,\\ntitle={Deep Learning in Medical Image Registration: Magic or Mirage?},\\nauthor={Rohit Jena and Deeksha Sethi and Pratik Chaudhari and James Gee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lZJ0WYI5YC}\\n}'}, 'paperhash': {'value': 'jena|deep_learning_in_medical_image_registration_magic_or_mirage'}},forum = 'lZJ0WYI5YC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1669/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1669/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1669/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1669/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lYdjzx3DYu',number = 4299,cdate = 1715398771487,pdate = 1727287746907,odate = 1730873874752,mdate = 1730873874769,tcdate = 1715398771487,tmdate = 1730873874769,ddate = None,content = {'title': {'value': 'EMR-Merging: Tuning-Free High-Performance Model Merging'}, 'authors': {'value': ['Chenyu Huang', 'Peng Ye', 'Tao Chen', 'Tong He', 'Xiangyu Yue', 'Wanli Ouyang']}, 'authorids': {'value': ['~Chenyu_Huang2', '~Peng_Ye4', '~Tao_Chen6', '~Tong_He2', '~Xiangyu_Yue1', '~Wanli_Ouyang1']}, 'keywords': {'value': ['Model Merging', 'Model Compression', 'Multi-task Learning', 'Supervised Finetuning']}, 'abstract': {'value': \"The success of pretrain-finetune paradigm brings about the release of numerous model weights. In this case, merging models finetuned on different tasks to enable a single model with multi-task capabilities is gaining increasing attention for its practicability. Existing model merging methods usually suffer from (1) significant performance degradation or (2) requiring tuning by additional data or training. In this paper, we rethink and analyze the existing model merging paradigm. We discover that using a single model's weights can hardly simulate all the models' performance. To tackle this issue, we propose Elect, Mask & Rescale-Merging (EMR-Merging). We first (a) elect a unified model from all the model weights and then (b) generate extremely lightweight task-specific modulators, including masks and rescalers, to align the direction and magnitude between the unified model and each specific model, respectively. EMR-Merging is tuning-free, thus requiring no data availability or any additional training while showing impressive performance. We find that EMR-Merging shows outstanding performance compared to existing merging methods under different classical and newly-established settings, including merging different numbers of vision models (up to 30), NLP models, PEFT models, and multi-modal models.\"}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Existing model merging methods usually suffer from significant performance degradation or requring additional tuning. We realize tuning-free model merging, which shows impressive performance under various experimental settings.'}, 'pdf': {'value': '/pdf/a38766f5219099b521172d338543f2873c94fc89.pdf'}, 'supplementary_material': {'value': '/attachment/f53c2db76033b5b3daebf83830c57f8abd6d0c84.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024emrmerging,\\ntitle={{EMR}-Merging: Tuning-Free High-Performance Model Merging},\\nauthor={Chenyu Huang and Peng Ye and Tao Chen and Tong He and Xiangyu Yue and Wanli Ouyang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lYdjzx3DYu}\\n}'}, 'paperhash': {'value': 'huang|emrmerging_tuningfree_highperformance_model_merging'}},forum = 'lYdjzx3DYu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4299/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4299/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4299/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4299/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lYPAYmfQqm',number = 20787,cdate = 1715798332997,pdate = 1727288241840,odate = 1730874003385,mdate = 1730874003405,tcdate = 1715798332997,tmdate = 1730874003405,ddate = None,content = {'title': {'value': 'Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond'}, 'authors': {'value': ['Yingcong Li', 'Ankit Singh Rawat', 'Samet Oymak']}, 'authorids': {'value': ['~Yingcong_Li1', '~Ankit_Singh_Rawat1', '~Samet_Oymak2']}, 'keywords': {'value': ['In-context learning', 'linear attention', 'state-space model', 'optimization', 'RAG', 'LoRA']}, 'abstract': {'value': 'Recent research has shown that Transformers with linear attention are capable of in-context learning (ICL) by implementing a linear estimator through gradient descent steps. However, the existing results on the optimization landscape apply under stylized settings where task and feature vectors are assumed to be IID and the attention weights are fully parameterized. In this work, we develop a stronger characterization of the optimization and generalization landscape of ICL through contributions on architectures, low-rank parameterization, and correlated designs: (1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-space model. Under a suitable correlated design assumption, we prove that both implement 1-step preconditioned gradient descent. We show that thanks to its native convolution filters, H3 also has the advantage of implementing sample weighting and outperforming linear attention in suitable settings. (2) By studying correlated designs, we provide new risk bounds for retrieval augmented generation (RAG) and task-feature alignment which reveal how ICL sample complexity benefits from distributional alignment. (3) We derive the optimal risk for low-rank parameterized attention weights in terms of covariance spectrum. Through this, we also shed light on how LoRA can adapt to a new distribution by capturing the shift between task covariances. Experimental results corroborate our theoretical findings. Overall, this work explores the optimization and risk landscape of ICL in practically meaningful settings and contributes to a more thorough understanding of its mechanics.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5cd41663646fb659e25aa417f82e583aa7f5c2bd.pdf'}, 'TLDR': {'value': 'We study the loss landscape of in-context learning for single-layer linear attention and state-space models under general linear tasks model while delineating the effect of distributional alignments (e.g., RAG), low-rank constraints, and LoRA.'}, '_bibtex': {'value': '@inproceedings{\\nli2024finegrained,\\ntitle={Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond},\\nauthor={Yingcong Li and Ankit Singh Rawat and Samet Oymak},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lYPAYmfQqm}\\n}'}, 'paperhash': {'value': 'li|finegrained_analysis_of_incontext_linear_estimation_data_architecture_and_beyond'}},forum = 'lYPAYmfQqm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20787/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20787/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20787/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20787/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lWYwZklSvg',number = 2076,cdate = 1714967221074,pdate = 1727287680102,odate = 1730873854725,mdate = 1730873854746,tcdate = 1714967221074,tmdate = 1730873854746,ddate = None,content = {'title': {'value': 'Full-Distance Evasion of Pedestrian Detectors in the Physical World'}, 'authors': {'value': ['Zhi Cheng', 'Zhanhao Hu', 'Yuqiu Liu', 'Jianmin Li', 'Hang Su', 'Xiaolin Hu']}, 'authorids': {'value': ['~Zhi_Cheng2', '~Zhanhao_Hu1', '~Yuqiu_Liu2', '~Jianmin_Li1', '~Hang_Su3', '~Xiaolin_Hu1']}, 'keywords': {'value': ['Adversarial Attacks', 'Pedestrian Detection']}, 'abstract': {'value': 'Many studies have proposed attack methods to generate adversarial patterns for evading pedestrian detection, alarming the computer vision community about the need for more attention to the robustness of detectors. However, adversarial patterns optimized by these methods commonly have limited performance at medium to long distances in the physical world. To overcome this limitation, we identify two main challenges. First, in existing methods, there is commonly an appearance gap between simulated distant adversarial patterns and their physical world counterparts, leading to incorrect optimization. Second, there exists a conflict between adversarial losses at different distances, which causes difficulties in optimization. To overcome these challenges, we introduce a Full Distance Attack (FDA) method. Our physical world experiments demonstrate the effectiveness of our FDA patterns across various detection models like YOLOv5, Deformable-DETR, and Mask RCNN. Codes available at https://github.com/zhicheng2T0/Full-Distance-Attack.git'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/36174efaee6f05139e43a80929070e81990026b0.pdf'}, 'supplementary_material': {'value': '/attachment/0a5dfd787fc79b1e6009ad5c36fb5faa38b4e424.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ncheng2024fulldistance,\\ntitle={Full-Distance Evasion of Pedestrian Detectors in the Physical World},\\nauthor={Zhi Cheng and Zhanhao Hu and Yuqiu Liu and Jianmin Li and Hang Su and Xiaolin Hu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lWYwZklSvg}\\n}'}, 'paperhash': {'value': 'cheng|fulldistance_evasion_of_pedestrian_detectors_in_the_physical_world'}},forum = 'lWYwZklSvg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2076/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2076/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2076/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission2076/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'lWHe7pmk7C',number = 2762,cdate = 1715157650805,pdate = 1727287699597,odate = 1730873860731,mdate = 1730873860758,tcdate = 1715157650805,tmdate = 1730873860758,ddate = None,content = {'title': {'value': 'From Chaos to Clarity: 3DGS in the Dark'}, 'authors': {'value': ['Zhihao Li', 'Yufei Wang', 'Alex Kot', 'Bihan Wen']}, 'authorids': {'value': ['~Zhihao_Li14', '~Yufei_Wang5', '~Alex_Kot1', '~Bihan_Wen2']}, 'keywords': {'value': ['Novel view synthesis; Raw images; 3D Gaussian Splatting (3DGS); Denosing; Self-supervised learning']}, 'abstract': {'value': 'Novel view synthesis from raw images provides superior high dynamic range (HDR) information compared to reconstructions from low dynamic range RGB images. However, the inherent noise in unprocessed raw images compromises the accuracy of 3D scene representation. Our study reveals that 3D Gaussian Splatting (3DGS) is particularly susceptible to this noise, leading to numerous elongated Gaussian shapes that overfit the noise, thereby significantly degrading reconstruction quality and reducing inference speed, especially in scenarios with limited views. To address these issues, we introduce a novel self-supervised learning framework designed to reconstruct HDR 3DGS from a limited number of noisy raw images. This framework enhances 3DGS by integrating a noise extractor and employing a noise-robust reconstruction loss that leverages a noise distribution prior. Experimental results show that our method outperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised and supervised pre-trained models in both reconstruction quality and inference speed on the RawNeRF dataset across a broad range of training views. We will release the code upon paper acceptance.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We enhance 3D Gaussian Splatting for HDR reconstruction from noisy raw images, outperforming prior models on the RawNeRF dataset.'}, 'pdf': {'value': '/pdf/4f75f8a55048a791aa6665df8a0ee829c5890bb3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024from,\\ntitle={From Chaos to Clarity: 3{DGS} in the Dark},\\nauthor={Zhihao Li and Yufei Wang and Alex Kot and Bihan Wen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lWHe7pmk7C}\\n}'}, 'paperhash': {'value': 'li|from_chaos_to_clarity_3dgs_in_the_dark'}},forum = 'lWHe7pmk7C',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2762/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2762/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2762/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2762/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'lW2zYQm0ox',number = 14694,cdate = 1715752681171,pdate = 1727288076910,odate = 1730873967965,mdate = 1735452069092,tcdate = 1715752681171,tmdate = 1735452069092,ddate = None,content = {'title': {'value': 'Accelerated Regularized Learning in Finite N-Person Games'}, 'authors': {'value': ['Kyriakos Lotidis', 'Angeliki Giannou', 'Panayotis Mertikopoulos', 'Nicholas Bambos']}, 'authorids': {'value': ['~Kyriakos_Lotidis1', '~Angeliki_Giannou1', '~Panayotis_Mertikopoulos1', '~Nicholas_Bambos1']}, 'keywords': {'value': ['game theory', 'Nash equilibrium', \"Nesterov's acceleration\", 'regularized learning']}, 'abstract': {'value': \"Motivated by the success of Nesterov's accelerated gradient algorithm for convex minimization problems, we examine whether it is possible to achieve similar performance gains in the context of online learning in games.\\nTo that end, we introduce a family of accelerated learning methods, which we call “follow the accelerated leader” (FTXL), and which incorporates the use of momentum within the general framework of regularized learning - and, in particular, the exponential / multiplicative weights algorithm and its variants.\\nDrawing inspiration and techniques from the continuous-time analysis of Nesterov's algorithm, we show that FTXL converges locally to strict Nash equilibria at a superlinear rate, achieving in this way an exponential speed-up over vanilla regularized learning methods (which, by comparison, converge to strict equilibria at a geometric, linear rate).\\nImportantly, the FTXL maintains its superlinear convergence rate in a broad range of feedback structures, from deterministic, full information models to stochastic, realization-based ones, and even bandit, payoff-based information, where players are only able to observe their individual realized payoffs.\"}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/cb955a5cd8536ddb4a5190a9ad887724c8b23c77.zip'}, 'pdf': {'value': '/pdf/d17054706562b8bf4554d2543453a9ad718d974d.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlotidis2024accelerated,\\ntitle={Accelerated Regularized Learning in Finite N-Person Games},\\nauthor={Kyriakos Lotidis and Angeliki Giannou and Panayotis Mertikopoulos and Nicholas Bambos},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lW2zYQm0ox}\\n}'}, 'paperhash': {'value': 'lotidis|accelerated_regularized_learning_in_finite_nperson_games'}},forum = 'lW2zYQm0ox',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14694/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14694/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14694/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14694/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lV4kTHTgpJ',number = 13260,cdate = 1715737546471,pdate = 1727288033552,odate = 1730873956491,mdate = 1735273384248,tcdate = 1715737546471,tmdate = 1735273384248,ddate = None,content = {'title': {'value': 'Model Fusion through Bayesian Optimization in Language Model Fine-Tuning'}, 'authors': {'value': ['Chaeyun Jang', 'Hyungi Lee', 'Jungtaek Kim', 'Juho Lee']}, 'authorids': {'value': ['~Chaeyun_Jang1', '~Hyungi_Lee1', '~Jungtaek_Kim1', '~Juho_Lee2']}, 'keywords': {'value': ['fine-tuning', 'language model', 'bayesian optimization']}, 'TLDR': {'value': 'This paper introduces a novel Bayesian Optimization based model fusion method, referred to as BOMF, designed specifically for the fine-tuning scenario involving Pre-trained Language Models.'}, 'abstract': {'value': 'Fine-tuning pre-trained models for downstream tasks is a widely adopted technique known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several troublesome engineering choices, such as selecting hyperparameters and determining checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model, one effective solution is model fusion, which combines multiple models in a parameter space. However, we observe a large discrepancy between loss and metric landscapes during the fine-tuning of pre-trained language models. Building on this observation, we introduce a novel model fusion technique that optimizes both the desired metric and loss through multi-objective Bayesian optimization. In addition, to effectively select hyperparameters, we establish a two-stage procedure by integrating Bayesian optimization processes into our framework. Experiments across various downstream tasks show considerable performance improvements using our Bayesian optimization-guided method.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a0c73d1c1962cefda48e9b0e93288b862aad428a.pdf'}, '_bibtex': {'value': '@inproceedings{\\njang2024model,\\ntitle={Model Fusion through Bayesian Optimization in Language Model Fine-Tuning},\\nauthor={Chaeyun Jang and Hyungi Lee and Jungtaek Kim and Juho Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lV4kTHTgpJ}\\n}'}, 'paperhash': {'value': 'jang|model_fusion_through_bayesian_optimization_in_language_model_finetuning'}},forum = 'lV4kTHTgpJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13260/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13260/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13260/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13260/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lV1wGHKd5x',number = 10433,cdate = 1715694006053,pdate = 1727287940669,odate = 1730873929036,mdate = 1736795477332,tcdate = 1715694006053,tmdate = 1736795477332,ddate = None,content = {'title': {'value': 'Listenable Maps for Zero-Shot Audio Classifiers'}, 'authors': {'value': ['Francesco Paissan', 'Luca Della Libera', 'Mirco Ravanelli', 'Cem Subakan']}, 'authorids': {'value': ['~Francesco_Paissan1', '~Luca_Della_Libera1', '~Mirco_Ravanelli1', '~Cem_Subakan1']}, 'keywords': {'value': ['Zero shot audio classifiers', 'Posthoc explanations']}, 'TLDR': {'value': 'We propose a posthoc explanation method for zero shot audio classifiers.'}, 'abstract': {'value': 'Interpreting the decisions of deep learning models, including audio classifiers, is crucial for ensuring the transparency and trustworthiness of this technology. In this paper, we introduce LMAC-ZS (Listenable Maps for Zero-Shot Audio Classifiers), which, to the best of our knowledge, is the first decoder-based post-hoc explanation method for explaining the decisions of zero-shot audio classifiers. The proposed method utilizes a novel loss function that aims to closely reproduce the original similarity patterns between text-and-audio pairs in the generated explanations. We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context. Moreover, we qualitatively show that our method produces meaningful explanations that correlate well with different text prompts.'}, 'pdf': {'value': '/pdf/40c76bf3adcc85890de672ea7134879aaad5d2cc.pdf'}, 'primary_area': {'value': 'interpretability_and_explainability'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\npaissan2024listenable,\\ntitle={Listenable Maps for Zero-Shot Audio Classifiers},\\nauthor={Francesco Paissan and Luca Della Libera and Mirco Ravanelli and Cem Subakan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lV1wGHKd5x}\\n}'}, 'paperhash': {'value': 'paissan|listenable_maps_for_zeroshot_audio_classifiers'}},forum = 'lV1wGHKd5x',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10433/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10433/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10433/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10433/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lT3oc04mDp',number = 2124,cdate = 1714980134865,pdate = 1727287681693,odate = 1730873855447,mdate = 1730873855461,tcdate = 1714980134865,tmdate = 1730873855461,ddate = None,content = {'title': {'value': 'Kangaroo: Lossless Self-Speculative Decoding for Accelerating LLMs via Double Early Exiting'}, 'authors': {'value': ['Fangcheng Liu', 'Yehui Tang', 'Zhenhua Liu', 'Yunsheng Ni', 'Duyu Tang', 'Kai Han', 'Yunhe Wang']}, 'authorids': {'value': ['~Fangcheng_Liu1', '~Yehui_Tang1', '~Zhenhua_Liu2', '~Yunsheng_Ni1', '~Duyu_Tang3', '~Kai_Han2', '~Yunhe_Wang1']}, 'keywords': {'value': ['speculative decoding', 'LLMs', 'self-drafting', 'early exiting']}, 'abstract': {'value': \"Speculative decoding has demonstrated its effectiveness in accelerating the inference of large language models (LLMs) while maintaining an identical sampling distribution. However, the conventional approach of training separate draft model to achieve a satisfactory token acceptance rate can be costly and impractical. In this paper, we propose a novel self-speculative decoding framework \\\\emph{Kangaroo} with \\\\emph{double} early exiting strategy, which leverages the shallow sub-network and the \\\\texttt{LM Head} of the well-trained target LLM to construct a self-drafting model. Then, the self-verification stage only requires computing the remaining layers over the \\\\emph{early-exited} hidden states in parallel. To bridge the representation gap between the sub-network and the full model, we train a lightweight and efficient adapter module on top of the sub-network. One significant challenge that comes with the proposed method is that the inference latency of the self-draft model may no longer be negligible compared to the big model. To boost the token acceptance rate while minimizing the latency of the self-drafting model, we introduce an additional \\\\emph{early exiting} mechanism for both single-sequence and the tree decoding scenarios. Specifically, we dynamically halt the small model's subsequent prediction during the drafting phase once the confidence level for the current step falls below a certain threshold. This approach reduces unnecessary computations and improves overall efficiency. Extensive experiments on multiple benchmarks demonstrate our effectiveness, where Kangaroo achieves walltime speedups up to 2.04$\\\\times$, outperforming Medusa-1 with 88.7\\\\% fewer additional parameters. The code for Kangaroo is available at https://github.com/Equationliu/Kangaroo.\"}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b8852e1ddfa243d2aa74ab89a7bf9adfcfd6cbf2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024kangaroo,\\ntitle={Kangaroo: Lossless Self-Speculative Decoding for Accelerating {LLM}s via Double Early Exiting},\\nauthor={Fangcheng Liu and Yehui Tang and Zhenhua Liu and Yunsheng Ni and Duyu Tang and Kai Han and Yunhe Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lT3oc04mDp}\\n}'}, 'paperhash': {'value': 'liu|kangaroo_lossless_selfspeculative_decoding_for_accelerating_llms_via_double_early_exiting'}},forum = 'lT3oc04mDp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2124/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2124/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2124/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2124/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'lS9e36lkxG',number = 5140,cdate = 1715506517096,pdate = 1727287774871,odate = 1730873882478,mdate = 1730873882492,tcdate = 1715506517096,tmdate = 1730873882492,ddate = None,content = {'title': {'value': 'D2R2: Diffusion-based Representation with Random Distance Matching for Tabular Few-shot Learning'}, 'authors': {'value': ['Ruoxue Liu', 'Linjiajie Fang', 'Wenjia Wang', 'Bingyi Jing']}, 'authorids': {'value': ['~Ruoxue_Liu1', '~Linjiajie_Fang1', '~Wenjia_Wang2', '~Bingyi_Jing1']}, 'keywords': {'value': ['Classification', 'Few-shot learning', 'Tabular data', 'Representation learning', 'Diffusion models', 'Self-supervised learning']}, 'TLDR': {'value': 'We propose a novel framework for tabular few-shot learning, comprising a diffusion-based model with random distance matching for representation learning, and an instance-wise iterative prototype scheme for few-shot classification.'}, 'abstract': {'value': 'Tabular data is widely utilized in a wide range of real-world applications. The challenge of few-shot learning with tabular data stands as a crucial problem in both industry and academia, due to the high cost or even impossibility of annotating additional samples. However, the inherent heterogeneity of tabular features, combined with the scarcity of labeled data, presents a significant challenge in tabular few-shot classification. In this paper, we propose a novel approach named Diffusion-based Representation with Random Distance matching (D2R2) for tabular few-shot learning. D2R2 leverages the powerful expression ability of diffusion models to extract essential semantic knowledge crucial for denoising process. This semantic knowledge proves beneficial in few-shot downstream tasks. During the training process of our designed diffusion model, we introduce a random distance matching to preserve distance information in the embeddings, thereby improving effectiveness for classification. During the classification stage, we introduce an instance-wise iterative prototype scheme to improve performance by accommodating the multimodality of embeddings and increasing clustering robustness. Our experiments reveal the significant efficacy of D2R2 across various tabular few-shot learning benchmarks, demonstrating its state-of-the-art performance in this field.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/41a59ee8015cb8195a7efc2d4ad6ee9223b8efcc.pdf'}, 'supplementary_material': {'value': '/attachment/cc9f2272a1cdaff658dc3a9c1dff800f2d4521c2.zip'}, '_bibtex': {'value': '@inproceedings{\\nliu2024dr,\\ntitle={D2R2: Diffusion-based Representation with Random Distance Matching for Tabular Few-shot Learning},\\nauthor={Ruoxue Liu and Linjiajie Fang and Wenjia Wang and Bingyi Jing},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lS9e36lkxG}\\n}'}, 'paperhash': {'value': 'liu|d2r2_diffusionbased_representation_with_random_distance_matching_for_tabular_fewshot_learning'}},forum = 'lS9e36lkxG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5140/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5140/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5140/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5140/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'lQ45aR8L7D',number = 1858,cdate = 1714853288360,pdate = 1727287673788,odate = 1730873852880,mdate = 1735788086277,tcdate = 1714853288360,tmdate = 1735788086277,ddate = None,content = {'title': {'value': 'Order-Independence Without Fine Tuning'}, 'authors': {'value': ['Reid McIlroy-Young', 'Katrina Brown', 'Conlan Olson', 'Linjun Zhang', 'Cynthia Dwork']}, 'authorids': {'value': ['~Reid_McIlroy-Young1', '~Katrina_Brown1', '~Conlan_Olson1', '~Linjun_Zhang1', '~Cynthia_Dwork2']}, 'keywords': {'value': ['LLMs', 'Multiple Choice Questions', 'Transformers', 'Positional Encodings', 'Modified Attention Mask']}, 'TLDR': {'value': \"We present the Set-Based Prompting method which *guarantees* any LLM's outputs will be unaffected by reordering.\"}, 'abstract': {'value': \"The development of generative language models that can create long and coherent textual outputs via autoregression has lead to a proliferation of uses and a corresponding sweep of analyses as researches work to determine the limitations of this new paradigm. Unlike humans, these '*Large Language Models*' (LLMs) are highly sensitive to small changes in their inputs, leading to unwanted inconsistency in their behavior. One problematic inconsistency when LLMs are used to answer multiple-choice questions or analyze multiple inputs is *order dependency*: the output of an LLM can (and often does) change significantly when sub-sequences are swapped, despite both orderings being semantically identical. In this paper we present , a technique that *guarantees* the output of an LLM will not have order dependence on a specified set of sub-sequences. We show that this method *provably* eliminates order dependency, and that it can be applied to *any* transformer-based LLM to enable text generation that is unaffected by re-orderings. Delving into the implications of our method, we show that, despite our inputs being out of distribution, the impact on expected accuracy is small, where the expectation is over the order of uniformly chosen shuffling of the candidate responses, and usually significantly less in practice. Thus, can be used as a '*dropped-in*' method on fully trained models. Finally, we discuss how our method's success suggests that other strong guarantees can be obtained on LLM performance via modifying the input representations.\\n\\nCode is available at [github.com/reidmcy/set-based-prompting](https://github.com/reidmcy/set-based-prompting.).\"}, 'pdf': {'value': '/pdf/70ccc1cf119b3e0022271b0391a23ec802dae939.pdf'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/25d494ed09b901ee73225f77c01cd36ed85a5fea.zip'}, '_bibtex': {'value': '@inproceedings{\\nmcilroy-young2024orderindependence,\\ntitle={Order-Independence Without Fine Tuning},\\nauthor={Reid McIlroy-Young and Katrina Brown and Conlan Olson and Linjun Zhang and Cynthia Dwork},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lQ45aR8L7D}\\n}'}, 'paperhash': {'value': 'mcilroyyoung|orderindependence_without_fine_tuning'}},forum = 'lQ45aR8L7D',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1858/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1858/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1858/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1858/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lPTWdyIY4O',number = 19376,cdate = 1715790598954,pdate = 1727288208370,odate = 1730873995230,mdate = 1730873995245,tcdate = 1715790598954,tmdate = 1730873995245,ddate = None,content = {'title': {'value': 'The Selective $G$-Bispectrum and its Inversion: Applications to $G$-Invariant Networks'}, 'authors': {'value': ['Simon Mataigne', 'Johan Mathe', 'Sophia Sanborn', 'Christopher Hillar', 'Nina Miolane']}, 'authorids': {'value': ['~Simon_Mataigne1', '~Johan_Mathe1', '~Sophia_Sanborn1', '~Christopher_Hillar3', '~Nina_Miolane2']}, 'keywords': {'value': ['CNN', 'group invariance', 'bispectrum', 'Neural Network', 'AI']}, 'TLDR': {'value': 'We propose a new layer in the architecture of Group-Equivariant Neural Networks to achieve invariance to group action on the input.'}, 'abstract': {'value': 'An important problem in signal processing and deep learning is to achieve *invariance* to nuisance factors not relevant for the task. Since many of these factors are describable as the action of a group $G$ (e.g. rotations, translations, scalings), we want methods to be $G$-invariant. The $G$-Bispectrum extracts every characteristic of a given signal up to group action: for example, the shape of an object in an image, but not its orientation. Consequently, the $G$-Bispectrum has been incorporated into deep neural network architectures as a computational primitive for $G$-invariance\\\\textemdash akin to a pooling mechanism, but with greater selectivity and robustness. However, the computational cost of the $G$-Bispectrum ($\\\\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has limited its widespread adoption. Here, we show that the $G$-Bispectrum computation contains redundancies that can be reduced into a *selective $G$-Bispectrum* with $\\\\mathcal{O}(|G|)$ complexity. We prove desirable mathematical properties of the selective $G$-Bispectrum and demonstrate how its integration in neural networks enhances accuracy and robustness compared to traditional approaches, while enjoying considerable speeds-up compared to the full $G$-Bispectrum.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/104a5cbe7e6ac2adae6ddaf5be03e3746c89b66f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmataigne2024the,\\ntitle={The Selective \\\\$G\\\\$-Bispectrum and its Inversion: Applications to \\\\$G\\\\$-Invariant Networks},\\nauthor={Simon Mataigne and Johan Mathe and Sophia Sanborn and Christopher Hillar and Nina Miolane},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lPTWdyIY4O}\\n}'}, 'paperhash': {'value': 'mataigne|the_selective_gbispectrum_and_its_inversion_applications_to_ginvariant_networks'}},forum = 'lPTWdyIY4O',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19376/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19376/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19376/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19376/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lPDxPVS6ix',number = 18070,cdate = 1715783548788,pdate = 1727288168795,odate = 1730873987633,mdate = 1730873987707,tcdate = 1715783548788,tmdate = 1730873987707,ddate = None,content = {'title': {'value': 'SPEAR: Exact Gradient Inversion of Batches in Federated Learning'}, 'authors': {'value': ['Dimitar Iliev Dimitrov', 'Maximilian Baader', 'Mark Niklas Mueller', 'Martin Vechev']}, 'authorids': {'value': ['~Dimitar_Iliev_Dimitrov2', '~Maximilian_Baader1', '~Mark_Niklas_Mueller2', '~Martin_Vechev1']}, 'keywords': {'value': ['Federated Learning', 'Exact Gradient Inversion', 'Gradient Leakage', 'Privacy', 'Attack']}, 'TLDR': {'value': 'We present the first algorithm for exact gradient inversion on batch sizes $>1$ to reconstruct inputs in the honest-but-curious federated learning setting.'}, 'abstract': {'value': 'Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$, with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, *the first algorithm reconstructing whole batches with $b >1$ exactly*. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b \\\\lesssim 25$ exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/425190129086719899d40db794d7ff0e4ecdaec9.zip'}, 'pdf': {'value': '/pdf/61205805e58614e9dd0c5acf4f4f9f6a0ca0d7c1.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndimitrov2024spear,\\ntitle={{SPEAR}: Exact Gradient Inversion of Batches in Federated Learning},\\nauthor={Dimitar Iliev Dimitrov and Maximilian Baader and Mark Niklas Mueller and Martin Vechev},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lPDxPVS6ix}\\n}'}, 'paperhash': {'value': 'dimitrov|spear_exact_gradient_inversion_of_batches_in_federated_learning'}},forum = 'lPDxPVS6ix',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18070/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18070/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18070/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18070/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lOdBHkqzRH',number = 11637,cdate = 1715707973173,pdate = 1727287978007,odate = 1730873940684,mdate = 1730873940705,tcdate = 1715707973173,tmdate = 1730873940705,ddate = None,content = {'title': {'value': 'Contextual Linear Optimization with Bandit Feedback'}, 'authors': {'value': ['Yichun Hu', 'Nathan Kallus', 'Xiaojie Mao', 'Yanchen Wu']}, 'authorids': {'value': ['~Yichun_Hu1', '~Nathan_Kallus1', '~Xiaojie_Mao1', '~Yanchen_Wu1']}, 'keywords': {'value': ['Contextual stochastic optimization', 'Personalized decision making', 'Prescriptive analytics']}, 'abstract': {'value': 'Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ad86d25f249f89cccc7115f67824dcf6d0c13831.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhu2024contextual,\\ntitle={Contextual Linear Optimization with Bandit Feedback},\\nauthor={Yichun Hu and Nathan Kallus and Xiaojie Mao and Yanchen Wu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lOdBHkqzRH}\\n}'}, 'paperhash': {'value': 'hu|contextual_linear_optimization_with_bandit_feedback'}},forum = 'lOdBHkqzRH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11637/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11637/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11637/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11637/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lOV9kSX3Uo',number = 9826,cdate = 1715686347968,pdate = 1727287921931,odate = 1730873923866,mdate = 1730873923885,tcdate = 1715686347968,tmdate = 1730873923885,ddate = None,content = {'title': {'value': 'Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition'}, 'authors': {'value': ['Shihong Ding', 'Long Yang', 'Luo Luo', 'Cong Fang']}, 'authorids': {'value': ['~Shihong_Ding1', '~Long_Yang4', '~Luo_Luo1', '~Cong_Fang1']}, 'keywords': {'value': ['Generalized Quasar-Convexity', 'Generalized Quasar-Convexity-Concavity']}, 'abstract': {'value': \"We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem,  we propose a new structural condition/landscape description named  generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity \\\\citep{hinder2020near}, GQC allows an individual quasar-convex parameter $\\\\gamma_i$ for each variable block $i$ and the smaller of $\\\\gamma_i$ implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive $\\\\tilde{\\\\mathcal{O}}((\\\\sum_{i=1}^d1/\\\\gamma_i)\\\\epsilon^{-1})$ iteration complexity to find an $\\\\varepsilon$-suboptimal global solution without pre-known the exact values of $\\\\gamma_i$ when the objective admits ``polynomial-like'' structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster $(\\\\sum_{i=1}^d 1/\\\\gamma_i \\\\text{ v.s. } d\\\\max_{i\\\\in[1:d]} 1/\\\\gamma_i)$ than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7a8a0213db6bb1ad89cd679a12e0083e3643481d.pdf'}, 'supplementary_material': {'value': '/attachment/715738c83f90988002d48e3c28dce727f6e4e390.zip'}, '_bibtex': {'value': '@inproceedings{\\nding2024optimizing,\\ntitle={Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition},\\nauthor={Shihong Ding and Long Yang and Luo Luo and Cong Fang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lOV9kSX3Uo}\\n}'}, 'paperhash': {'value': 'ding|optimizing_over_multiple_distributions_under_generalized_quasarconvexity_condition'}},forum = 'lOV9kSX3Uo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9826/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9826/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9826/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9826/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lOMHt16T8R',number = 8804,cdate = 1715669940969,pdate = 1727287891337,odate = 1730873914901,mdate = 1736277411602,tcdate = 1715669940969,tmdate = 1736277411602,ddate = None,content = {'title': {'value': 'PaCE: Parsimonious Concept Engineering for Large Language Models'}, 'authors': {'value': ['Jinqi Luo', 'Tianjiao Ding', 'Kwan Ho Ryan Chan', 'Darshan Thaker', 'Aditya Chattopadhyay', 'Chris Callison-Burch', 'Rene Vidal']}, 'authorids': {'value': ['~Jinqi_Luo1', '~Tianjiao_Ding1', '~Kwan_Ho_Ryan_Chan1', '~Darshan_Thaker1', '~Aditya_Chattopadhyay1', '~Chris_Callison-Burch1', '~Rene_Vidal1']}, 'keywords': {'value': ['Large Language Model', 'Sparse Coding', 'Trustworthy Machine Learning']}, 'TLDR': {'value': \"Parsimonious Concept Engineering (PaCE) uses sparse coding on a large-scale concept dictionary to precisely control and modify a language model's neural activations, effectively improving the model trustworthiness.\"}, 'abstract': {'value': 'Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Then, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activations as linear combinations of benign and undesirable components. By removing the latter ones from the activations, we reorient the behavior of the LLM towards the alignment goal. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0d56400d745f8d9a97b66ace937cb8f59ab08390.pdf'}, 'supplementary_material': {'value': '/attachment/540e3153649547b718e80b9890a73f21d73d0393.zip'}, '_bibtex': {'value': '@inproceedings{\\nluo2024pace,\\ntitle={Pa{CE}: Parsimonious Concept Engineering for Large Language Models},\\nauthor={Jinqi Luo and Tianjiao Ding and Kwan Ho Ryan Chan and Darshan Thaker and Aditya Chattopadhyay and Chris Callison-Burch and Rene Vidal},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lOMHt16T8R}\\n}'}, 'paperhash': {'value': 'luo|pace_parsimonious_concept_engineering_for_large_language_models'}},forum = 'lOMHt16T8R',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8804/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8804/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8804/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8804/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lNCsyA5uS1',number = 12234,cdate = 1715718859335,pdate = 1727287999498,odate = 1730873947104,mdate = 1730873947366,tcdate = 1715718859335,tmdate = 1730873947366,ddate = None,content = {'title': {'value': 'Thought of Search: Planning with Language Models Through The Lens of Efficiency'}, 'authors': {'value': ['Michael Katz', 'Harsha Kokel', 'Kavitha Srinivas', 'Shirin Sohrabi']}, 'authorids': {'value': ['~Michael_Katz1', '~Harsha_Kokel1', '~Kavitha_Srinivas2', '~Shirin_Sohrabi1']}, 'keywords': {'value': ['planning', 'large language models', 'search']}, 'TLDR': {'value': 'We find that recent trends in using LLMs for planning are profoundly uneconomical, unsound and incomplete. We propose a significantly more efficient approach that is sound and complete and argue for a responsible use of compute resources.'}, 'abstract': {'value': 'Among the most important properties of algorithms investigated in computer science are soundness, completeness, and complexity. These properties, however, are rarely analyzed for the vast collection of recently proposed methods for planning with large language models. In this work, we alleviate this gap. We analyse these properties of using LLMs for planning and highlight that recent trends abandon both soundness and completeness for the sake of inefficiency. We propose a significantly more efficient approach that can, at the same time, maintain both soundness and completeness. We exemplify on four representative search problems, comparing to the LLM-based solutions from the literature that attempt to solve these problems. We show that by using LLMs to produce the code for the search components we can solve the entire datasets with 100% accuracy with only a few calls to the LLM. In contrast, the compared approaches require hundreds of thousands of calls and achieve significantly lower accuracy. We argue for a responsible use of compute resources; urging research community to investigate sound and complete LLM-based approaches that uphold efficiency.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c3b9f6dac697975151973b9512513649e4a3cf31.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkatz2024thought,\\ntitle={Thought of Search: Planning with Language Models Through The Lens of Efficiency},\\nauthor={Michael Katz and Harsha Kokel and Kavitha Srinivas and Shirin Sohrabi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lNCsyA5uS1}\\n}'}, 'paperhash': {'value': 'katz|thought_of_search_planning_with_language_models_through_the_lens_of_efficiency'}},forum = 'lNCsyA5uS1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12234/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12234/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12234/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12234/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'lKnl4CLhhS',number = 12354,cdate = 1715720810065,pdate = 1727288003728,odate = 1730873948491,mdate = 1730873948519,tcdate = 1715720810065,tmdate = 1730873948519,ddate = None,content = {'title': {'value': 'Efficient and Private Marginal Reconstruction with Local Non-Negativity'}, 'authors': {'value': ['Brett Mullins', 'Miguel Fuentes', 'Yingtai Xiao', 'Daniel Kifer', 'Cameron N Musco', 'Daniel Sheldon']}, 'authorids': {'value': ['~Brett_Mullins1', '~Miguel_Fuentes1', '~Yingtai_Xiao1', '~Daniel_Kifer1', '~Cameron_N_Musco1', '~Daniel_Sheldon1']}, 'keywords': {'value': ['differential privacy', 'query release', 'synthetic data']}, 'abstract': {'value': 'Differential privacy is the dominant standard for formal and quantifiable privacy and has been used in major deployments that impact millions of people. Many differentially private algorithms for query release and synthetic data contain steps that reconstruct answers to queries from answers to other queries that have been measured privately.  Reconstruction is an important subproblem for such mechanisms to economize the privacy budget, minimize error on reconstructed answers, and allow for scalability to high-dimensional datasets. In this paper, we introduce a principled and efficient postprocessing method ReM (Residuals-to-Marginals) for reconstructing answers to marginal queries. Our method builds on recent work on efficient mechanisms for marginal query release, based on making measurements using a residual query basis that admits efficient pseudoinversion, which is an important primitive used in reconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with Local Non-negativity) reconstructs marginals under Gaussian noise satisfying consistency and non-negativity, which often reduces error on reconstructed answers.  We demonstrate the utility of ReM and GReM-LNN by applying them to improve existing private query answering mechanisms.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/74ef2a254d1aef2663edcdb2e0ac71b90a95897e.pdf'}, 'supplementary_material': {'value': '/attachment/9e2c09ea2eb19b4053d2fd887d6f2df2eecf549e.zip'}, 'TLDR': {'value': 'We propose a novel, scalable method for reconstructing answers to marginal queries. It makes existing mechanisms more scalable, accurate, or both.'}, '_bibtex': {'value': '@inproceedings{\\nmullins2024efficient,\\ntitle={Efficient and Private Marginal Reconstruction with Local Non-Negativity},\\nauthor={Brett Mullins and Miguel Fuentes and Yingtai Xiao and Daniel Kifer and Cameron N Musco and Daniel Sheldon},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lKnl4CLhhS}\\n}'}, 'paperhash': {'value': 'mullins|efficient_and_private_marginal_reconstruction_with_local_nonnegativity'}},forum = 'lKnl4CLhhS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12354/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12354/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12354/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12354/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lJuQxkDbDo',number = 3048,cdate = 1715218424840,pdate = 1727287708899,odate = 1730873863283,mdate = 1730873863301,tcdate = 1715218424840,tmdate = 1730873863301,ddate = None,content = {'title': {'value': 'DisenGCD: A Meta Multigraph-assisted Disentangled Graph Learning Framework for  Cognitive Diagnosis'}, 'authors': {'value': ['Shangshang Yang', 'Mingyang Chen', 'Ziwen Wang', 'Xiaoshan Yu', 'Panpan Zhang', 'Haiping Ma', 'Xingyi Zhang']}, 'authorids': {'value': ['~Shangshang_Yang1', '~Mingyang_Chen7', '~Ziwen_Wang3', '~Xiaoshan_Yu2', '~Panpan_Zhang4', '~Haiping_Ma1', '~Xingyi_Zhang2']}, 'keywords': {'value': ['Cognitive Diagnosis', 'Intelligent Education', 'Disentanglement Learning', 'Robustness', 'Graph Learning']}, 'abstract': {'value': \"Existing graph learning-based cognitive diagnosis (CD) methods have made relatively good results, but their student, exercise, and concept representations are learned and exchanged in an implicit unified graph, which makes the interaction-agnostic exercise and concept representations be learned poorly, failing to provide high robustness against noise in students' interactions. Besides,  lower-order exercise latent representations obtained in shallow layers are not well explored when learning the student representation. \\nTo tackle the issues, this paper suggests a meta multigraph-assisted disentangled graph learning framework for CD (DisenGCD), which learns three types of representations on three disentangled graphs: student-exercise-concept interaction,  exercise-concept relation, and concept dependency graphs, respectively. \\nSpecifically,  the latter two graphs are first disentangled from the interaction graph. \\nThen, the student representation is learned from the interaction graph by a devised meta multigraph learning module; multiple learnable propagation paths in this module  enable current student latent representation to access  lower-order exercise latent representations,\\nwhich can lead to  more effective nad robust student representations learned; \\nthe exercise and concept representations are learned on the relation and dependency graphs by graph attention modules. \\nFinally, a novel diagnostic function is devised to handle three disentangled representations for prediction.  Experiments show better performance and robustness of DisenGCD than state-of-the-art CD methods and demonstrate the effectiveness of the disentangled learning framework and meta multigraph module.The source code is available at https://github.com/BIMK/Intelligent-Education/tree/main/DisenGCD.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e51af5d4e5dbe15572e497a2ca8aadbb75729501.pdf'}, 'supplementary_material': {'value': '/attachment/6b4182aca74183a6ff68f78313ea081482a5546c.zip'}, '_bibtex': {'value': '@inproceedings{\\nyang2024disengcd,\\ntitle={Disen{GCD}: A Meta Multigraph-assisted Disentangled Graph Learning Framework for  Cognitive Diagnosis},\\nauthor={Shangshang Yang and Mingyang Chen and Ziwen Wang and Xiaoshan Yu and Panpan Zhang and Haiping Ma and Xingyi Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lJuQxkDbDo}\\n}'}, 'paperhash': {'value': 'yang|disengcd_a_meta_multigraphassisted_disentangled_graph_learning_framework_for_cognitive_diagnosis'}},forum = 'lJuQxkDbDo',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3048/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3048/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3048/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3048/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lIH6oCdppg',number = 12136,cdate = 1715716958518,pdate = 1727287995690,odate = 1730873945770,mdate = 1730873945797,tcdate = 1715716958518,tmdate = 1730873945797,ddate = None,content = {'title': {'value': 'On the Role of Attention Masks and LayerNorm in Transformers'}, 'authors': {'value': ['Xinyi Wu', 'Amir Ajorlou', 'Yifei Wang', 'Stefanie Jegelka', 'Ali Jadbabaie']}, 'authorids': {'value': ['~Xinyi_Wu3', '~Amir_Ajorlou1', '~Yifei_Wang1', '~Stefanie_Jegelka3', '~Ali_Jadbabaie1']}, 'keywords': {'value': ['attention mechanism', 'transformers', 'layer normalization', 'deep learning theory', 'dynamical systems']}, 'abstract': {'value': 'Self-attention is the key mechanism of transformers, which are the essential building blocks of modern foundation models. Recent studies have shown that pure self-attention suffers from an increasing degree of rank collapse as depth increases, limiting model expressivity and further utilization of model depth. The existing literature on rank collapse, however, has mostly overlooked other critical components in transformers that may alleviate the rank collapse issue. In this paper, we provide a general analysis of rank collapse under self-attention, taking into account the effects of attention masks and layer normalization (LayerNorm). In particular, we find that although pure masked attention still suffers from exponential collapse to a rank one subspace, sparse or local masked attention can provably slow down the collapse rate. In the case of self-attention with LayerNorm, we first show that for certain classes of value matrices, collapse to a rank one subspace still happens exponentially. However, through construction of nontrivial counterexamples, we then establish that with proper choice of value matrices, a general class of sequences may not converge to a rank one subspace, and the self-attention dynamics with LayerNorm can simultaneously possess a rich set of equilibria with any possible rank between one and full. Our result refutes the previous hypothesis that LayerNorm plays no role in the rank collapse of self-attention and suggests that self-attention with LayerNorm constitutes a much more expressive, versatile nonlinear dynamical system than what was originally thought.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e3f16b5718fda36c132a629e2933050ff3bab9e7.pdf'}, 'TLDR': {'value': 'We rigorously show that sparse or local masked attention can mitigate rank collapse of tokens, while LayerNorm can prevent it in transformers, both enhancing model expressivity.'}, '_bibtex': {'value': '@inproceedings{\\nwu2024on,\\ntitle={On the Role of Attention Masks and LayerNorm in Transformers},\\nauthor={Xinyi Wu and Amir Ajorlou and Yifei Wang and Stefanie Jegelka and Ali Jadbabaie},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lIH6oCdppg}\\n}'}, 'paperhash': {'value': 'wu|on_the_role_of_attention_masks_and_layernorm_in_transformers'}},forum = 'lIH6oCdppg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12136/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12136/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12136/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12136/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lHcvjsQFQq',number = 16302,cdate = 1715769028439,pdate = 1727288122880,odate = 1730873978453,mdate = 1736329431444,tcdate = 1715769028439,tmdate = 1736329431444,ddate = None,content = {'title': {'value': 'Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction'}, 'authors': {'value': ['Seokin Seo', 'Byung-Jun Lee', 'Jongmin Lee', 'HyeongJoo Hwang', 'Hongseok Yang', 'Kee-Eung Kim']}, 'authorids': {'value': ['~Seokin_Seo1', '~Byung-Jun_Lee1', '~Jongmin_Lee1', '~HyeongJoo_Hwang1', '~Hongseok_Yang2', '~Kee-Eung_Kim2']}, 'keywords': {'value': ['Imitation Learning', 'Behavioral Cloning', 'Robust Learning']}, 'abstract': {'value': 'We consider offline imitation learning (IL), which aims to train an agent to imitate from the dataset of expert demonstrations without online interaction with the environment. Behavioral Cloning (BC) has been a simple yet effective approach to offline IL, but it is also well-known to be vulnerable to the covariate shift resulting from the mismatch between the state distributions induced by the learned policy and the expert policy. Moreover, as often occurs in practice, when expert datasets are collected from an arbitrary state distribution instead of a stationary one, these shifts become more pronounced, potentially leading to substantial failures in existing IL methods. Specifically, we focus on covariate shift resulting from arbitrary state data distributions, such as biased data collection or incomplete trajectories, rather than shifts induced by changes in dynamics or noisy expert actions. In this paper, to mitigate the effect of the covariate shifts in BC, we propose DrilDICE, which utilizes a distributionally robust BC objective by employing a stationary distribution correction ratio estimation (DICE) to derive a feasible solution. We evaluate the effectiveness of our method through an extensive set of experiments covering diverse covariate shift scenarios. The results demonstrate the efficacy of the proposed approach in improving the robustness against the shifts, outperforming existing offline IL methods in such scenarios.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/02649fa4c12a342d0f11364b5bddb2bebeee7252.pdf'}, '_bibtex': {'value': '@inproceedings{\\nseo2024mitigating,\\ntitle={Mitigating Covariate Shift in Behavioral Cloning via Robust Stationary Distribution Correction},\\nauthor={Seokin Seo and Byung-Jun Lee and Jongmin Lee and HyeongJoo Hwang and Hongseok Yang and Kee-Eung Kim},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lHcvjsQFQq}\\n}'}, 'TLDR': {'value': 'We propose DrilDICE, a method that leverages a distributionally robust objective with using DICE technique, to mitigate the covariate shift in offline imitation learning with arbitrary data state distributions.'}, 'paperhash': {'value': 'seo|mitigating_covariate_shift_in_behavioral_cloning_via_robust_stationary_distribution_correction'}},forum = 'lHcvjsQFQq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16302/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16302/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16302/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16302/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lG1VEQJvUH',number = 13004,cdate = 1715733329269,pdate = 1727288025330,odate = 1730873954264,mdate = 1734613623186,tcdate = 1715733329269,tmdate = 1734613623186,ddate = None,content = {'title': {'value': 'Unitary Convolutions for Learning on Graphs and Groups'}, 'authors': {'value': ['Bobak Kiani', 'Lukas Fesser', 'Melanie Weber']}, 'authorids': {'value': ['~Bobak_Kiani1', '~Lukas_Fesser1', '~Melanie_Weber1']}, 'keywords': {'value': ['graph neural networks', 'geometric deep learning', 'learning stability', 'unitary', 'orthogonal']}, 'abstract': {'value': 'Data with geometric structure is ubiquitous in machine learning often arising from fundamental symmetries in a domain, such as permutation-invariance in graphs and translation-invariance in images. Group-convolutional architectures, which encode symmetries as inductive bias, have shown great success in applications, but can suffer from instabilities as their depth increases and often struggle to learn long range dependencies in data. For instance, graph neural networks experience instability due to the convergence of node representations (over-smoothing), which can occur after only a few iterations of message-passing, reducing their effectiveness in downstream tasks. Here, we propose and study unitary group convolutions, which allow for deeper networks that are more stable during training. The main focus of the paper are graph neural networks, where we show that unitary graph convolutions provably avoid over-smoothing. Our experimental results confirm that unitary graph convolutional networks achieve competitive performance on benchmark datasets compared to state-of-the-art graph neural networks. We complement our analysis of the graph domain with the study of general unitary convolutions and analyze their role in enhancing stability in general group convolutional architectures.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/de7d409d57d061cb57fc4fec5f73651167e8f042.pdf'}, '_bibtex': {'value': '@inproceedings{\\nkiani2024unitary,\\ntitle={Unitary Convolutions for Learning on Graphs and Groups},\\nauthor={Bobak Kiani and Lukas Fesser and Melanie Weber},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lG1VEQJvUH}\\n}'}, 'paperhash': {'value': 'kiani|unitary_convolutions_for_learning_on_graphs_and_groups'}},forum = 'lG1VEQJvUH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13004/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13004/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13004/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13004/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lEUle8S4xQ',number = 13213,cdate = 1715736710884,pdate = 1727288031806,odate = 1730873955921,mdate = 1737118255600,tcdate = 1715736710884,tmdate = 1737118255600,ddate = None,content = {'title': {'value': 'S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity'}, 'authors': {'value': ['Xinyu Yang', 'Jixuan Leng', 'Geyang Guo', 'Jiawei Zhao', 'Ryumei Nakada', 'Linjun Zhang', 'Huaxiu Yao', 'Beidi Chen']}, 'authorids': {'value': ['~Xinyu_Yang4', '~Jixuan_Leng1', '~Geyang_Guo2', '~Jiawei_Zhao2', '~Ryumei_Nakada1', '~Linjun_Zhang1', '~Huaxiu_Yao1', '~Beidi_Chen1']}, 'keywords': {'value': ['PEFT; LLM Efficiency; LLM Post-training']}, 'abstract': {'value': 'Current PEFT methods for LLMs can achieve high quality, efficient training, or scalable serving, but not all three simultaneously.  \\nTo address this limitation, we investigate sparse fine-tuning and observe a remarkable improvement in generalization ability. \\nUtilizing this key insight, we propose a family of Structured Sparse Fine-Tuning (S${^2}$FT) methods for LLMs, which concurrently achieve state-of-the-art fine-tuning performance, training efficiency, and inference scalability. S${^2}$FT accomplishes this by \"selecting sparsely and computing densely\". Based on the coupled structures in LLMs, \\\\model selects a few attention heads and channels in the MHA and FFN modules for each Transformer block, respectively. Next, it co-permutes the weight matrices on both sides of all coupled structures to connect the selected subsets in each layer into a dense submatrix. Finally, S${^2}$FT performs in-place gradient updates on all selected submatrices.\\nThrough theoretical analyses and empirical results, our method prevents forgetting while simplifying optimization, delivers SOTA performance on both commonsense and arithmetic reasoning with 4.6% and 1.3% average improvements compared to LoRA, and surpasses full FT by 11.5% when generalizing to various domains after instruction tuning. \\nUsing our partial back-propagation algorithm, S${^2}$FT saves training memory up to 3$\\\\times$ and improves latency by 1.5-2.7$\\\\times$ compared to full FT, while achieving an average 10\\\\% improvement over LoRA on both metrics. We further demonstrate that the weight updates in S${^2}$FT can be decoupled into adapters, enabling effective fusion, fast switch, and efficient parallelism when serving multiple fine-tuned models.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/763f00c00a16031639982f402b7a01b159cb473e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nyang2024sft,\\ntitle={S\\\\${\\\\textasciicircum}\\\\{2\\\\}\\\\${FT}: Efficient, Scalable and Generalizable {LLM} Fine-tuning by Structured Sparsity},\\nauthor={Xinyu Yang and Jixuan Leng and Geyang Guo and Jiawei Zhao and Ryumei Nakada and Linjun Zhang and Huaxiu Yao and Beidi Chen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lEUle8S4xQ}\\n}'}, 'paperhash': {'value': 'yang|s^2ft_efficient_scalable_and_generalizable_llm_finetuning_by_structured_sparsity'}},forum = 'lEUle8S4xQ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13213/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13213/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13213/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13213/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lDtABI541U',number = 19653,cdate = 1715792169231,pdate = 1727288214755,odate = 1730873996882,mdate = 1730873996901,tcdate = 1715792169231,tmdate = 1730873996901,ddate = None,content = {'title': {'value': 'Quadratic Quantum Variational Monte Carlo'}, 'authors': {'value': ['Baiyu Su', 'qiang liu']}, 'authorids': {'value': ['~Baiyu_Su1', '~qiang_liu4']}, 'keywords': {'value': ['AI for science', 'Machine learning for physics', 'Machine learning for chemistry', 'Quantum physics', 'Variational Monte Carlo', 'MCMC', 'Transformers']}, 'abstract': {'value': \"This paper introduces the Quadratic Quantum Variational Monte Carlo (Q$^2$VMC) algorithm, an innovative algorithm in quantum chemistry that significantly enhances the efficiency and accuracy of solving the Schrödinger equation. Inspired by the discretization of imaginary-time Schrödinger evolution, Q$^2$VMC employs a novel quadratic update mechanism that integrates seamlessly with neural network-based ansatzes. Our extensive experiments showcase Q$^2$VMC's superior performance, achieving faster convergence and lower ground state energies in wavefunction optimization across various molecular systems, without additional computational cost. This study not only advances the field of computational quantum chemistry but also highlights the important role of discretized evolution in variational quantum algorithms, offering a scalable and robust framework for future quantum research.\"}, 'primary_area': {'value': 'machine_learning_for_physical_sciences'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We proposed Q^2VMC, a method to enhance efficiency and accuracy of quantum variational Monte Carlo methods.'}, 'pdf': {'value': '/pdf/527d486e263097c96afd6c6da02cec07ff157b22.pdf'}, 'supplementary_material': {'value': '/attachment/40c42f218498b9a391b198f109c40a4b2bcef9e6.zip'}, '_bibtex': {'value': '@inproceedings{\\nsu2024quadratic,\\ntitle={Quadratic Quantum Variational Monte Carlo},\\nauthor={Baiyu Su and qiang liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lDtABI541U}\\n}'}, 'paperhash': {'value': 'su|quadratic_quantum_variational_monte_carlo'}},forum = 'lDtABI541U',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19653/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19653/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19653/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19653/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lD7ziaMHbf',number = 7002,cdate = 1715610443791,pdate = 1727287831717,odate = 1730873898355,mdate = 1730873898373,tcdate = 1715610443791,tmdate = 1730873898373,ddate = None,content = {'title': {'value': 'Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices'}, 'authors': {'value': ['Zhihao Shu', 'Xiaowei Yu', 'Zihao Wu', 'Wenqi Jia', 'Yinchen Shi', 'Miao Yin', 'Tianming Liu', 'Dajiang Zhu', 'Wei Niu']}, 'authorids': {'value': ['~Zhihao_Shu1', '~Xiaowei_Yu1', '~Zihao_Wu1', '~Wenqi_Jia2', '~Yinchen_Shi1', '~Miao_Yin1', '~Tianming_Liu3', '~Dajiang_Zhu1', '~Wei_Niu3']}, 'keywords': {'value': ['Mobile DNN Acceleration', 'Model Pruning', 'ViT']}, 'abstract': {'value': 'Mobile devices have become essential enablers for AI applications, particularly in scenarios that require real-time performance. Vision Transformer (ViT) has become a fundamental cornerstone in this regard due to its high accuracy. Recent efforts have been dedicated to developing various transformer architectures that offer im- proved accuracy while reducing the computational requirements. However, existing research primarily focuses on reducing the theoretical computational complexity through methods such as local attention and model pruning, rather than considering realistic performance on mobile hardware. Although these optimizations reduce computational demands, they either introduce additional overheads related to data transformation (e.g., Reshape and Transpose) or irregular computation/data-access patterns. These result in significant overhead on mobile devices due to their limited bandwidth, which even makes the latency worse than vanilla ViT on mobile. In this paper, we present ECP-ViT, a real-time framework that employs the core-periphery principle inspired by the brain functional networks to guide self-attention in ViTs and enable the deployment of ViT models on smartphones. We identify the main bottleneck in transformer structures caused by data transformation and propose a hardware-friendly core-periphery guided self-attention to decrease computation demands. Additionally, we design the system optimizations for intensive data transformation in pruned models. ECP-ViT, with the proposed algorithm-system co-optimizations, achieves a speedup of 4.6× to 26.9× on mobile GPUs across four datasets: STL-10, CIFAR100, TinyImageNet, and ImageNet.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/bcb3daacd691d78e3a034dacbc9da8c718a545ff.pdf'}, '_bibtex': {'value': '@inproceedings{\\nshu2024realtime,\\ntitle={Real-time Core-Periphery Guided ViT with Smart Data Layout Selection on Mobile Devices},\\nauthor={Zhihao Shu and Xiaowei Yu and Zihao Wu and Wenqi Jia and Yinchen Shi and Miao Yin and Tianming Liu and Dajiang Zhu and Wei Niu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lD7ziaMHbf}\\n}'}, 'paperhash': {'value': 'shu|realtime_coreperiphery_guided_vit_with_smart_data_layout_selection_on_mobile_devices'}},forum = 'lD7ziaMHbf',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7002/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7002/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7002/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7002/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lCj0Rvr4D6',number = 20453,cdate = 1715796514106,pdate = 1727288233903,odate = 1730874001317,mdate = 1730874001328,tcdate = 1715796514106,tmdate = 1730874001328,ddate = None,content = {'title': {'value': 'John Ellipsoids via Lazy Updates'}, 'authors': {'value': ['David Woodruff', 'Taisuke Yasuda']}, 'authorids': {'value': ['~David_Woodruff1', '~Taisuke_Yasuda1']}, 'keywords': {'value': ['John ellipsoid', 'sketching', 'sampling', 'fast matrix multiplication']}, 'TLDR': {'value': 'Efficient algorithms for John ellipsoids by lazily updating the weights'}, 'abstract': {'value': 'We give a faster algorithm for computing an approximate John ellipsoid around $n$ points in $d$ dimensions. The best known prior algorithms are based on repeatedly computing the leverage scores of the points and reweighting them by these scores (Cohen et al., 2019). We show that this algorithm can be substantially sped up by delaying the computation of high accuracy leverage scores by using sampling, and then later computing multiple batches of high accuracy leverage scores via fast rectangular matrix multiplication. We also give low-space streaming algorithms for John ellipsoids using similar ideas.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/387c461aca4700294414362d200e7093395878a9.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwoodruff2024john,\\ntitle={John Ellipsoids via Lazy Updates},\\nauthor={David Woodruff and Taisuke Yasuda},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lCj0Rvr4D6}\\n}'}, 'paperhash': {'value': 'woodruff|john_ellipsoids_via_lazy_updates'}},forum = 'lCj0Rvr4D6',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20453/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20453/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20453/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20453/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lCiqPxcyC0',number = 7886,cdate = 1715644730649,pdate = 1727287862133,odate = 1730873907402,mdate = 1730873907423,tcdate = 1715644730649,tmdate = 1730873907423,ddate = None,content = {'title': {'value': 'Replicable Uniformity Testing'}, 'authors': {'value': ['Sihan Liu', 'Christopher Ye']}, 'authorids': {'value': ['~Sihan_Liu2', '~Christopher_Ye1']}, 'keywords': {'value': ['Replicability', 'Uniformity Testing']}, 'TLDR': {'value': 'We give the best known algorithm for replicable uniformity testing, and a matching lower bound for the natural class of symmetric algorithms.'}, 'abstract': {'value': 'Uniformity testing is arguably one of the most fundamental distribution testing problems. Given sample access to an unknown distribution $\\\\mathbf{p}$ on $[n]$, one must decide if $\\\\mathbf{p}$ is uniform or $\\\\varepsilon$-far from uniform (in total variation distance). A long line of work established that uniformity testing has sample complexity $\\\\Theta(\\\\sqrt{n}\\\\varepsilon^{-2})$. However, when the input distribution is neither uniform nor far from uniform, known algorithms may have highly non-replicable behavior. \\nConsequently, if these algorithms are applied in scientific studies, they may lead to contradictory results that erode public trust in science.\\n\\nIn this work, we revisit uniformity testing under the framework of algorithmic replicability [STOC \\'22], requiring the algorithm to be replicable under arbitrary distributions. While replicability typically incurs a $\\\\rho^{-2}$ factor overhead in sample complexity, we obtain a replicable uniformity tester using only $\\\\tilde{O}(\\\\sqrt{n} \\\\varepsilon^{-2} \\\\rho^{-1})$ samples. To our knowledge, this is the first replicable learning algorithm with (nearly) linear dependence on $\\\\rho$.\\n\\nLastly, we consider a class of ``symmetric\" algorithms [FOCS \\'00] whose outputs are invariant under relabeling of the domain $[n]$, which includes all existing uniformity testers (including ours). For this natural class of algorithms, we prove a nearly matching sample complexity lower bound for replicable uniformity testing.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/758d6cce9c70c9995acb03f75db29817882bc7e5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nliu2024replicable,\\ntitle={Replicable Uniformity Testing},\\nauthor={Sihan Liu and Christopher Ye},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lCiqPxcyC0}\\n}'}, 'paperhash': {'value': 'liu|replicable_uniformity_testing'}},forum = 'lCiqPxcyC0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7886/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7886/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7886/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7886/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'lBp2cda7sp',number = 1300,cdate = 1714550304989,pdate = 1727287657013,odate = 1730873847352,mdate = 1730873847373,tcdate = 1714550304989,tmdate = 1730873847373,ddate = None,content = {'title': {'value': 'RMLR: Extending Multinomial Logistic Regression into General Geometries'}, 'authors': {'value': ['Ziheng Chen', 'Yue Song', 'Rui Wang', 'Xiaojun Wu', 'Nicu Sebe']}, 'authorids': {'value': ['~Ziheng_Chen2', '~Yue_Song1', '~Rui_Wang39', '~Xiaojun_Wu2', '~Nicu_Sebe1']}, 'keywords': {'value': ['Riemannian neural networks', 'Matrix manifolds', 'SPD manifolds', 'Special orthogonal groups']}, 'TLDR': {'value': 'We propose a general framework of building intrinsic Riemannian classifiers for general geometries , and showcase our framework on the SPD manifold and special orthogonal group.'}, 'abstract': {'value': 'Riemannian neural networks, which extend deep learning techniques to Riemannian spaces, have gained significant attention in machine learning. To better classify the manifold-valued features, researchers have started extending Euclidean multinomial logistic regression (MLR) into Riemannian manifolds. However, existing approaches suffer from limited applicability due to their strong reliance on specific geometric properties. This paper proposes a framework for designing Riemannian MLR over general geometries, referred to as RMLR. Our framework only requires minimal geometric properties, thus exhibiting broad applicability and enabling its use with a wide range of geometries. Specifically, we showcase our framework on the Symmetric Positive Definite (SPD) manifold and special orthogonal group, i.e., the set of rotation matrices. On the SPD manifold, we develop five families of SPD MLRs under five types of power-deformed metrics. On rotation matrices we propose Lie MLR based on the popular bi-invariant metric. Extensive experiments on different Riemannian backbone networks validate the effectiveness of our framework.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4e9e7232245f21a8c71f9672e837dafd27701168.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024rmlr,\\ntitle={{RMLR}: Extending Multinomial Logistic Regression into General Geometries},\\nauthor={Ziheng Chen and Yue Song and Rui Wang and Xiaojun Wu and Nicu Sebe},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lBp2cda7sp}\\n}'}, 'paperhash': {'value': 'chen|rmlr_extending_multinomial_logistic_regression_into_general_geometries'}},forum = 'lBp2cda7sp',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1300/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1300/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1300/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission1300/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'lBh5kuuY1L',number = 14344,cdate = 1715748851917,pdate = 1727288066809,odate = 1730873964940,mdate = 1737456992388,tcdate = 1715748851917,tmdate = 1737456992388,ddate = None,content = {'title': {'value': 'TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models'}, 'authors': {'value': ['Kiwoong Yoo', 'Owen Oertell', 'Junhyun Lee', 'Sanghoon Lee', 'Jaewoo Kang']}, 'authorids': {'value': ['~Kiwoong_Yoo1', '~Owen_Oertell1', '~Junhyun_Lee1', '~Sanghoon_Lee5', '~Jaewoo_Kang1']}, 'keywords': {'value': ['Scaffold Hopping', 'Consistency Models', 'Diffusion Models', '3D Structure-Based Drug Design', 'Reinforcement Learning', 'Drug Discovery', 'Generative Models']}, 'TLDR': {'value': 'Fast, and efficient E(3)-equivariant scaffold-hopping model utilizing consistency models for rapid generation additionally powered by RL'}, 'abstract': {'value': 'Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates. Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising. Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products. However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds. To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models. This synergy not only enhances efficiency but also significantly boosts generation speeds, achieving up to 30 times faster inference speed as well as superior generation quality compared to existing diffusion-based models, establishing TurboHopp as a powerful tool in drug discovery. Supported by faster inference speed, we further optimize our model, using Reinforcement Learning for Consistency Models (RLCM), to output desirable molecules. We demonstrate the broad applicability of TurboHopp across multiple drug discovery scenarios, underscoring its potential in diverse molecular settings.The code is provided at https://github.com/orgw/TurboHopp'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5cf03f33e3737bfbe2d52e41d80d1b3be59c431f.pdf'}, 'supplementary_material': {'value': '/attachment/03b8b017a135c929c3affa4c62c4efe71aa0603f.zip'}, '_bibtex': {'value': '@inproceedings{\\nyoo2024turbohopp,\\ntitle={TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models},\\nauthor={Kiwoong Yoo and Owen Oertell and Junhyun Lee and Sanghoon Lee and Jaewoo Kang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lBh5kuuY1L}\\n}'}, 'paperhash': {'value': 'yoo|turbohopp_accelerated_molecule_scaffold_hopping_with_consistency_models'}},forum = 'lBh5kuuY1L',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14344/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14344/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14344/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14344/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'lA48H7pW3q',number = 5373,cdate = 1715526394462,pdate = 1727287781753,odate = 1730873884015,mdate = 1730873884030,tcdate = 1715526394462,tmdate = 1730873884030,ddate = None,content = {'title': {'value': 'QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization'}, 'authors': {'value': ['Qi Song', 'Tianxiang Gong', 'Shiqi Gao', 'Haoyi Zhou', 'Jianxin Li']}, 'authorids': {'value': ['~Qi_Song9', '~Tianxiang_Gong1', '~Shiqi_Gao2', '~Haoyi_Zhou1', '~Jianxin_Li3']}, 'keywords': {'value': ['Contrastive Learning', 'Multi-View Learning', 'Multimodal Learning', 'Vision-Language Representation Degeneration']}, 'abstract': {'value': \"Multimodal contrastive learning (MCL) has recently demonstrated significant success across various tasks. However, the existing MCL treats all negative samples equally and ignores the potential semantic association with positive samples, which limits the model's ability to achieve fine-grained alignment. In multi-view scenarios, MCL tends to prioritize shared information while neglecting modality-specific unique information across different views, leading to feature suppression and suboptimal performance in downstream tasks. To address these limitations, we propose a novel contrastive framework name *QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization*. In the QUEST framework, we propose quaternion contrastive objectives and orthogonal constraints to extract sufficient unique information. Meanwhile, a shared information-guided penalization is introduced to ensure that shared information does not excessively influence the optimization of unique information. Our method leverages quaternion vector spaces to simultaneously optimize shared and unique information. Experiments on multiple datasets show that our method achieves superior performance in multimodal contrastive learning benchmarks. On public benchmark, our approach achieves state-of-the-art performance, and on synthetic shortcut datasets, we outperform existing baseline methods by an average of 97.95\\\\% on the CLIP model.\"}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We propose QUEST, a novel framework with quaternion objectives and constraints to both capture shared and unique information.'}, 'pdf': {'value': '/pdf/84107019c16a3341189d0a7d6a78e026b2f05c9c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsong2024quest,\\ntitle={{QUEST}: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization},\\nauthor={Qi Song and Tianxiang Gong and Shiqi Gao and Haoyi Zhou and Jianxin Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=lA48H7pW3q}\\n}'}, 'supplementary_material': {'value': '/attachment/8d3204bdc96772a07a1531ee3879d0acdc6f5668.zip'}, 'paperhash': {'value': 'song|quest_quadruple_multimodal_contrastive_learning_with_constraints_and_selfpenalization'}},forum = 'lA48H7pW3q',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5373/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5373/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5373/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5373/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l8XnqbQYBK',number = 6318,cdate = 1715590096423,pdate = 1727287810901,odate = 1730873892173,mdate = 1736120539923,tcdate = 1715590096423,tmdate = 1736120539923,ddate = None,content = {'title': {'value': 'Toward Conditional Distribution Calibration in Survival Prediction'}, 'authors': {'value': ['Shi-ang Qi', 'Yakun Yu', 'Russell Greiner']}, 'authorids': {'value': ['~Shi-ang_Qi1', '~Yakun_Yu1', '~Russell_Greiner2']}, 'keywords': {'value': ['survival analysis; calibration; conformal prediction; censorship; discrimination']}, 'TLDR': {'value': 'This paper underscores the critical role of marginal and conditional calibration in survival analysis and introduces a method that enhances both marginal and conditional calibration without sacrificing discrimination performance in survival models.'}, 'abstract': {'value': 'Survival prediction often involves estimating the time-to-event distribution from censored datasets. Previous approaches have focused on enhancing discrimination and marginal calibration. In this paper, we highlight the significance of *conditional calibration* for real-world applications – especially its role in individual decision-making. We propose a method based on conformal prediction that uses the model’s predicted individual survival probability at that instance’s observed time. This method effectively improves the model’s marginal and conditional calibration, without compromising discrimination. We provide asymptotic theoretical guarantees for both marginal and conditional calibration and test it extensively across 15 diverse real-world datasets, demonstrating the method’s practical effectiveness and\\nversatility in various settings.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a88c44af42915ac04bb6600e67e8fe783142c008.pdf'}, 'supplementary_material': {'value': '/attachment/94dd887e4b1d9cde90ef738657ea1c65e44f6979.zip'}, '_bibtex': {'value': '@inproceedings{\\nqi2024toward,\\ntitle={Toward Conditional Distribution Calibration in Survival Prediction},\\nauthor={Shi-ang Qi and Yakun Yu and Russell Greiner},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l8XnqbQYBK}\\n}'}, 'paperhash': {'value': 'qi|toward_conditional_distribution_calibration_in_survival_prediction'}},forum = 'l8XnqbQYBK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6318/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6318/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6318/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6318/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l6xVqzm72i',number = 8743,cdate = 1715668736079,pdate = 1727287889632,odate = 1730873914389,mdate = 1730873914409,tcdate = 1715668736079,tmdate = 1730873914409,ddate = None,content = {'title': {'value': 'MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space'}, 'authors': {'value': ['Jiangwei Weng', 'Zhiqiang Yan', 'Ying Tai', 'Jianjun Qian', 'Jian Yang', 'Jun Li']}, 'authorids': {'value': ['~Jiangwei_Weng1', '~Zhiqiang_Yan1', '~Ying_Tai1', '~Jianjun_Qian2', '~Jian_Yang1', '~Jun_Li16']}, 'keywords': {'value': ['Low Light Enhancement', 'State Space Models', 'Feature Control']}, 'abstract': {'value': 'Recent advances in low light image enhancement have been dominated by Retinex-based learning framework, leveraging convolutional neural networks (CNNs) and Transformers. However, the vanilla Retinex theory primarily addresses global illumination degradation and neglects local issues such as noise and blur in dark conditions. Moreover, CNNs and Transformers struggle to capture global degradation due to their limited receptive fields. While state space models (SSMs) have shown promise in the long-sequence modeling, they face challenges in combining local invariants and global context in visual data. In this paper, we introduce MambaLLIE, an implicit Retinex-aware low light enhancer featuring a global-then-local state space design. We first propose a Local-Enhanced State Space Module (LESSM) that incorporates an augmented local bias within a 2D selective scan mechanism, enhancing the original SSMs by preserving local 2D dependency. Additionally, an Implicit Retinex-aware Selective Kernel module (IRSK) dynamically selects features using spatially-varying operations, adapting to varying inputs through an adaptive kernel selection process. Our Global-then-Local State Space Block (GLSSB) integrates LESSM and IRSK with layer normalization (LN) as its core. This design enables MambaLLIE to achieve comprehensive global long-range modeling and flexible local feature aggregation. Extensive experiments demonstrate that MambaLLIE significantly outperforms state-of-the-art CNN and Transformer-based methods. Our code is available at https://github.com/wengjiangwei/MambaLLIE.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/90cb332aa165948530e0ea9a1b5949966821bef1.pdf'}, 'supplementary_material': {'value': '/attachment/7614fe2adeff99c6083bb41f73ca3e8b76498b64.zip'}, '_bibtex': {'value': '@inproceedings{\\nweng2024mamballie,\\ntitle={Mamba{LLIE}: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space},\\nauthor={Jiangwei Weng and Zhiqiang Yan and Ying Tai and Jianjun Qian and Jian Yang and Jun Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l6xVqzm72i}\\n}'}, 'paperhash': {'value': 'weng|mamballie_implicit_retinexaware_low_light_enhancement_with_globalthenlocal_state_space'}},forum = 'l6xVqzm72i',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8743/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8743/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8743/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8743/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l6iICoILGB',number = 19692,cdate = 1715792331173,pdate = 1727288215595,odate = 1730873997211,mdate = 1730873997237,tcdate = 1715792331173,tmdate = 1730873997237,ddate = None,content = {'title': {'value': 'Practical $0.385$-Approximation for Submodular Maximization Subject to a Cardinality Constraint'}, 'authors': {'value': ['Murad Tukan', 'Loay Mualem', 'Moran Feldman']}, 'authorids': {'value': ['~Murad_Tukan1', '~Loay_Mualem2', '~Moran_Feldman1']}, 'keywords': {'value': ['Submodular maximization', 'Discrete optimization', 'Machine learning']}, 'TLDR': {'value': 'In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of $0.385$-approximation with a low and practical query complexity of $O(n+k^2)$.'}, 'abstract': {'value': \"Non-monotone constrained submodular maximization plays a crucial role in various machine learning applications. However, existing algorithms often struggle with a trade-off between approximation guarantees and practical efficiency. The current state-of-the-art is a recent $0.401$-approximation algorithm, but its computational complexity makes it highly impractical. The best practical algorithms for the problem only guarantee $1/e$-approximation. In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of $0.385$-approximation with a low and practical query complexity of $O(n+k^2)$. Furthermore, we evaluate our algorithm's performance through extensive machine learning applications, including Movie Recommendation, Image Summarization, and more. These evaluations demonstrate the efficacy of our approach.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/6395d4f3a2a5cddad2cfc9c42ab1c7e957620c36.pdf'}, '_bibtex': {'value': '@inproceedings{\\ntukan2024practical,\\ntitle={Practical \\\\$0.385\\\\$-Approximation for Submodular Maximization Subject to a Cardinality Constraint},\\nauthor={Murad Tukan and Loay Mualem and Moran Feldman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l6iICoILGB}\\n}'}, 'paperhash': {'value': 'tukan|practical_0385approximation_for_submodular_maximization_subject_to_a_cardinality_constraint'}},forum = 'l6iICoILGB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19692/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19692/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19692/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19692/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l5wEQPcDab',number = 7256,cdate = 1715616412390,pdate = 1727287840926,odate = 1730873900965,mdate = 1737020800576,tcdate = 1715616412390,tmdate = 1737020800576,ddate = None,content = {'title': {'value': 'Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning'}, 'authors': {'value': ['Andreas Schlaginhaufen', 'Maryam Kamgarpour']}, 'authorids': {'value': ['~Andreas_Schlaginhaufen1', '~Maryam_Kamgarpour1']}, 'keywords': {'value': ['Inverse reinforcement learning', 'Transferability', 'Identifiability', 'Robustness', 'Alignment']}, 'abstract': {'value': \"Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [Ng et al., 2000]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [Rolland et al., 2022]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a82c47f7aae6394135e038dcd7150a74ec3fc145.pdf'}, 'supplementary_material': {'value': '/attachment/3d0f3744530a5a023386fa8947d2547a0d17133c.zip'}, '_bibtex': {'value': '@inproceedings{\\nschlaginhaufen2024towards,\\ntitle={Towards the Transferability of Rewards Recovered via Regularized Inverse Reinforcement Learning},\\nauthor={Andreas Schlaginhaufen and Maryam Kamgarpour},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l5wEQPcDab}\\n}'}, 'paperhash': {'value': 'schlaginhaufen|towards_the_transferability_of_rewards_recovered_via_regularized_inverse_reinforcement_learning'}},forum = 'l5wEQPcDab',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7256/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7256/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7256/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7256/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'l5SbrtvSRS',number = 10779,cdate = 1715697639237,pdate = 1727287951017,odate = 1730873932103,mdate = 1730873932114,tcdate = 1715697639237,tmdate = 1730873932114,ddate = None,content = {'title': {'value': 'Parameter Competition Balancing for Model Merging'}, 'authors': {'value': ['Guodong DU', 'Junlin Lee', 'Jing Li', 'Runhua Jiang', 'Yifei Guo', 'Shuyang Yu', 'Hanting Liu', 'Sim Kuan Goh', 'Ho-Kin Tang', 'Daojing He', 'Min Zhang']}, 'authorids': {'value': ['~Guodong_DU2', '~Junlin_Lee1', '~Jing_Li19', '~Runhua_Jiang2', '~Yifei_Guo2', '~Shuyang_Yu2', '~Hanting_Liu1', '~Sim_Kuan_Goh2', '~Ho-Kin_Tang1', '~Daojing_He1', '~Min_Zhang9']}, 'keywords': {'value': ['Model Merging', 'Knowledge Fusion', 'Model Editing', 'Task Arithmetic', 'Robust Fine-tuning']}, 'TLDR': {'value': 'This paper introduces Parameter Competition Balancing Merging (PCB-Merging), a lightweight and training-free technique for adjusting the coefficient of each parameter for model merging.'}, 'abstract': {'value': 'While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named **PCB-Merging** (Parameter Competition Balancing), a *lightweight* and *training-free* technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods.'}, 'pdf': {'value': '/pdf/3ae464a0e9569a90e72beea9c144f50bef1f0f03.pdf'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/ea14e3ed37db8b74c9bb0d015d397727110dc79c.zip'}, '_bibtex': {'value': '@inproceedings{\\ndu2024parameter,\\ntitle={Parameter Competition Balancing for Model Merging},\\nauthor={Guodong DU and Junlin Lee and Jing Li and Runhua Jiang and Yifei Guo and Shuyang Yu and Hanting Liu and Sim Kuan Goh and Ho-Kin Tang and Daojing He and Min Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l5SbrtvSRS}\\n}'}, 'paperhash': {'value': 'du|parameter_competition_balancing_for_model_merging'}},forum = 'l5SbrtvSRS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10779/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10779/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10779/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10779/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l2yvtrz3On',number = 10108,cdate = 1715690382987,pdate = 1727287929981,odate = 1730873926163,mdate = 1730873926192,tcdate = 1715690382987,tmdate = 1730873926192,ddate = None,content = {'title': {'value': 'Improved Sample Complexity for Multiclass PAC Learning'}, 'authors': {'value': ['Steve Hanneke', 'Shay Moran', 'Qian Zhang']}, 'authorids': {'value': ['~Steve_Hanneke1', '~Shay_Moran1', '~Qian_Zhang10']}, 'keywords': {'value': ['Multiclass learning', 'PAC learning', 'Statistical learning', 'List learning']}, 'abstract': {'value': 'We aim to understand the optimal PAC sample complexity in multiclass learning. While finiteness of the Daniely-Shalev-Shwartz (DS) dimension has been shown to characterize the PAC learnability of a concept class [Brukhim, Carmon, Dinur, Moran, and Yehudayoff, 2022], there exist polylog factor gaps in the leading term of the sample complexity. In this paper, we reduce the gap in terms of the dependence on the error parameter to a single log factor and also propose two possible routes towards completely resolving the optimal sample complexity, each based on a key open question we formulate: one concerning list learning with bounded list size, the other concerning a new type of shifting for multiclass concept classes. We prove that a positive answer to either of the two questions would completely resolve the optimal sample complexity up to log factors of the DS dimension.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We improve the upper bound of the sample complexity of multiclass PAC learning.'}, 'pdf': {'value': '/pdf/fb5f5aaf84f1e16cc00ed0d7bb029ee322fa4259.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhanneke2024improved,\\ntitle={Improved Sample Complexity for Multiclass {PAC} Learning},\\nauthor={Steve Hanneke and Shay Moran and Qian Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l2yvtrz3On}\\n}'}, 'paperhash': {'value': 'hanneke|improved_sample_complexity_for_multiclass_pac_learning'}},forum = 'l2yvtrz3On',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10108/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10108/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10108/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10108/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'l0c1j4QvTq',number = 5884,cdate = 1715573913623,pdate = 1727287797845,odate = 1730873888876,mdate = 1734572701416,tcdate = 1715573913623,tmdate = 1734572701416,ddate = None,content = {'title': {'value': 'Diffusion Actor-Critic with Entropy Regulator'}, 'authors': {'value': ['Yinuo Wang', 'Likun Wang', 'Yuxuan Jiang', 'Wenjun Zou', 'Tong Liu', 'Xujie Song', 'Wenxuan Wang', 'Liming Xiao', 'Jiang WU', 'Jingliang Duan', 'Shengbo Eben Li']}, 'authorids': {'value': ['~Yinuo_Wang5', '~Likun_Wang1', '~Yuxuan_Jiang1', '~Wenjun_Zou1', '~Tong_Liu11', '~Xujie_Song1', '~Wenxuan_Wang5', '~Liming_Xiao1', '~Jiang_WU9', '~Jingliang_Duan1', '~Shengbo_Eben_Li2']}, 'keywords': {'value': ['Diffusion model', 'online reinforcement learning', 'Gaussian mixture model']}, 'abstract': {'value': 'Reinforcement learning (RL) has proven highly effective in addressing complex decision-making and control tasks. However, in most traditional RL algorithms, the policy is typically parameterized as a diagonal Gaussian distribution with learned mean and variance, which constrains their capability to acquire complex policies. In response to this problem, we propose an online RL algorithm termed diffusion actor-critic with entropy regulator (DACER). This algorithm conceptualizes the reverse process of the diffusion model as a novel policy function and leverages the capability of the diffusion model to fit multimodal distributions, thereby enhancing the representational capacity of the policy. Since the distribution of the diffusion policy lacks an analytical expression, its entropy cannot be determined analytically. To mitigate this, we propose a method to estimate the entropy of the diffusion policy utilizing Gaussian mixture model. Building on the estimated entropy, we can learn a parameter $\\\\alpha$ that modulates the degree of exploration and exploitation. Parameter $\\\\alpha$ will be employed to adaptively regulate the variance of the added noise, which is applied to the action output by the diffusion model. Experimental trials on MuJoCo benchmarks and a multimodal task demonstrate that the DACER algorithm achieves state-of-the-art (SOTA) performance in most MuJoCo control tasks while exhibiting a stronger representational capacity of the diffusion policy.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0bcd516c1659debb7a9519921f2284981132e3b0.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024diffusion,\\ntitle={Diffusion Actor-Critic with Entropy Regulator},\\nauthor={Yinuo Wang and Likun Wang and Yuxuan Jiang and Wenjun Zou and Tong Liu and Xujie Song and Wenxuan Wang and Liming Xiao and Jiang WU and Jingliang Duan and Shengbo Eben Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l0c1j4QvTq}\\n}'}, 'paperhash': {'value': 'wang|diffusion_actorcritic_with_entropy_regulator'}},forum = 'l0c1j4QvTq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5884/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5884/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5884/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5884/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'l04i6dPMxK',number = 9104,cdate = 1715674649232,pdate = 1727287901122,odate = 1730873917432,mdate = 1730873917450,tcdate = 1715674649232,tmdate = 1730873917450,ddate = None,content = {'title': {'value': 'Bandits with Abstention under Expert Advice'}, 'authors': {'value': ['Stephen Pasteris', 'Alberto Rumi', 'Maximilian Thiessen', 'Shota Saito', 'Atsushi Miyauchi', 'Fabio Vitale', 'Mark Herbster']}, 'authorids': {'value': ['~Stephen_Pasteris1', '~Alberto_Rumi1', '~Maximilian_Thiessen1', '~Shota_Saito2', '~Atsushi_Miyauchi1', '~Fabio_Vitale1', '~Mark_Herbster1']}, 'keywords': {'value': ['Multi-armed bandits', 'Expert advice', 'Abstention', 'Contextual bandits']}, 'TLDR': {'value': 'We study bandits with expert advice when given an option to abstain from making any action, and achieve novel reward bounds for confidence rated predictors.'}, 'abstract': {'value': \"We study the classic problem of prediction with expert advice under bandit feedback.  Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA (Confidence-rated Bandits with Abstentions) algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. Our problem can be construed as the aggregation of confidence-rated predictors, with the learner having the option to abstain from play. We are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists, we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). We discuss how CBA can be applied to the problem of adversarial contextual bandits with the option of abstaining from selecting any action. We are able to leverage a wide range of inductive biases, outperforming previous approaches both theoretically and in preliminary experimental analysis. Additionally, we achieve a reduction in runtime from quadratic to almost linear in the number of contexts for the specific case of metric space contexts.\"}, 'primary_area': {'value': 'bandits'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2e3c2588e8326b867dbf1d68a87379ed9a71cffd.pdf'}, 'supplementary_material': {'value': '/attachment/6ea72c832be95585af9cd99066d1152ca9685eaa.zip'}, '_bibtex': {'value': '@inproceedings{\\npasteris2024bandits,\\ntitle={Bandits with Abstention under Expert Advice},\\nauthor={Stephen Pasteris and Alberto Rumi and Maximilian Thiessen and Shota Saito and Atsushi Miyauchi and Fabio Vitale and Mark Herbster},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=l04i6dPMxK}\\n}'}, 'paperhash': {'value': 'pasteris|bandits_with_abstention_under_expert_advice'}},forum = 'l04i6dPMxK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9104/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9104/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9104/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9104/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'kzJ9P7VPnS',number = 18928,cdate = 1715788177917,pdate = 1727288197204,odate = 1730873992623,mdate = 1730873992645,tcdate = 1715788177917,tmdate = 1730873992645,ddate = None,content = {'title': {'value': 'LP-3DGS: Learning to Prune 3D Gaussian Splatting'}, 'authors': {'value': ['Zhaoliang Zhang', 'Tianchen Song', 'Yongjae Lee', 'Li Yang', 'Cheng Peng', 'Rama Chellappa', 'Deliang Fan']}, 'authorids': {'value': ['~Zhaoliang_Zhang1', '~Tianchen_Song1', '~Yongjae_Lee4', '~Li_Yang6', '~Cheng_Peng2', '~Rama_Chellappa1', '~Deliang_Fan1']}, 'keywords': {'value': ['Novel view synthesis', 'Gaussian splatting', 'Learn to prune']}, 'TLDR': {'value': 'We propose a learning method to prune the points in 3D Gaussian Splatting by applying trainable mask to the importance score of points and minimize the model size with only one-time training while maintaining the rendering quality.'}, 'abstract': {'value': 'Recently, 3D Gaussian Splatting (3DGS) has become one of the mainstream methodologies for novel view synthesis (NVS) due to its high quality and fast rendering speed. However, as a point-based scene representation, 3DGS potentially generates a large number of Gaussians to fit the scene, leading to high memory usage. Improvements that have been proposed require either an empirical pre-set pruning ratio or importance score threshold to prune the point cloud. Such hyperparameters require multiple rounds of training to optimize and achieve the maximum pruning ratio while maintaining the rendering quality for each scene. In this work, we propose learning-to-prune 3DGS (LP-3DGS), where a trainable binary mask is applied to the importance score to automatically find a favorable pruning ratio. Instead of using the traditional straight-through estimator (STE) method to approximate the binary mask gradient, we redesign the masking function to leverage the Gumbel-Sigmoid method, making it differentiable and compatible with the existing training process of 3DGS. Extensive experiments have shown that LP-3DGS consistently achieves a good balance between efficiency and high quality.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a0617a04ee1110a70438b1ec2b77aa4464ec2eb2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024lpdgs,\\ntitle={{LP}-3{DGS}: Learning to Prune 3D Gaussian Splatting},\\nauthor={Zhaoliang Zhang and Tianchen Song and Yongjae Lee and Li Yang and Cheng Peng and Rama Chellappa and Deliang Fan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kzJ9P7VPnS}\\n}'}, 'paperhash': {'value': 'zhang|lp3dgs_learning_to_prune_3d_gaussian_splatting'}},forum = 'kzJ9P7VPnS',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18928/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18928/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18928/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18928/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kxBsNEWB42',number = 15,cdate = 1713816220485,pdate = 1727287628484,odate = 1730873837456,mdate = 1730873837475,tcdate = 1713816220485,tmdate = 1730873837475,ddate = None,content = {'title': {'value': 'Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values'}, 'authors': {'value': ['Aleksandr Lobanov', 'Alexander Gasnikov', 'Andrey Krasnov']}, 'authorids': {'value': ['~Aleksandr_Lobanov1', '~Alexander_Gasnikov1', '~Andrey_Krasnov1']}, 'keywords': {'value': ['Black Box Optimization', 'Order Oracle', 'Accelerated Algorithms', 'Asymptotic Convergence']}, 'abstract': {'value': 'Frequently, the burgeoning field of black-box optimization encounters challenges due to a limited understanding of the mechanisms of the objective function. To address such problems, in this work we focus on the deterministic concept of Order Oracle, which only utilizes order access between function values (possibly with some bounded noise), but without assuming access to their values. As theoretical results, we propose a new approach to create non-accelerated optimization algorithms (obtained by integrating Order Oracle into existing optimization “tools”) in non-convex, convex, and strongly convex settings that are as good as both SOTA coordinate algorithms with first-order oracle and SOTA algorithms with Order Oracle up to logarithm factor. Moreover, using the proposed approach, _we provide the first accelerated optimization algorithm using the Order Oracle_. And also, using an already different approach we provide the asymptotic convergence of _the first algorithm with the stochastic Order Oracle concept_. Finally, our theoretical results demonstrate effectiveness of proposed algorithms through numerical experiments.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/799c77c17baccd8920d1b8e54eecfe9fe4e2ea10.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlobanov2024acceleration,\\ntitle={Acceleration Exists! Optimization Problems When Oracle Can Only Compare Objective Function Values},\\nauthor={Aleksandr Lobanov and Alexander Gasnikov and Andrey Krasnov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kxBsNEWB42}\\n}'}, 'paperhash': {'value': 'lobanov|acceleration_exists_optimization_problems_when_oracle_can_only_compare_objective_function_values'}},forum = 'kxBsNEWB42',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15/-/Revision', 'NeurIPS.cc/2024/Conference/Submission15/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kuCY0mW4Q3',number = 12172,cdate = 1715717636800,pdate = 1727287997252,odate = 1730873946122,mdate = 1730873946140,tcdate = 1715717636800,tmdate = 1730873946140,ddate = None,content = {'title': {'value': 'VB-LoRA: Extreme Parameter Efficient Fine-Tuning with Vector Banks'}, 'authors': {'value': ['Yang Li', 'Shaobo Han', 'Shihao Ji']}, 'authorids': {'value': ['~Yang_Li101', '~Shaobo_Han1', '~Shihao_Ji1']}, 'keywords': {'value': ['Parameter-efficient fine-tuning', 'Low-rank adaptation', 'Transfer learning', 'Transformer', 'Top-k admixture module']}, 'abstract': {'value': 'As the adoption of large language models increases and the need for per-user or per-task model customization grows, the parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, incur substantial storage and transmission costs. To further reduce stored parameters, we introduce a \"divide-and-share\" paradigm that breaks the barriers of low-rank decomposition across matrix dimensions, modules, and layers by sharing  parameters globally via a vector bank. As an instantiation of the paradigm to LoRA, our proposed VB-LoRA composites all the low-rank matrices of LoRA from a shared vector bank with a differentiable top-$k$ admixture module. VB-LoRA achieves extreme parameter efficiency while maintaining comparable or better performance compared to state-of-the-art PEFT methods. Extensive experiments demonstrate the effectiveness of VB-LoRA on natural language understanding, natural language generation, instruction tuning, and mathematical reasoning tasks. When fine-tuning the Llama2-13B model, VB-LoRA only uses 0.4% of LoRA\\'s stored parameters, yet achieves superior results. Our source code is available at https://github.com/leo-yangli/VB-LoRA. This method has been merged into the Hugging Face PEFT package.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d0ded460792ec7f1fc26f1bf560cf85baa3118db.pdf'}, 'TLDR': {'value': 'An extreme parameter-efficient fine-tuning method based on vector banks'}, 'supplementary_material': {'value': '/attachment/410881688a57341c02da0a1aacc882d5c423b383.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024vblora,\\ntitle={{VB}-Lo{RA}: Extreme Parameter Efficient Fine-Tuning with Vector Banks},\\nauthor={Yang Li and Shaobo Han and Shihao Ji},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kuCY0mW4Q3}\\n}'}, 'paperhash': {'value': 'li|vblora_extreme_parameter_efficient_finetuning_with_vector_banks'}},forum = 'kuCY0mW4Q3',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12172/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12172/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12172/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12172/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'ktpG37Dzh5',number = 3321,cdate = 1715259999944,pdate = 1727287716390,odate = 1730873865121,mdate = 1730873865139,tcdate = 1715259999944,tmdate = 1730873865139,ddate = None,content = {'title': {'value': 'BMRS: Bayesian Model Reduction for Structured Pruning'}, 'authors': {'value': ['Dustin Wright', 'Christian Igel', 'Raghavendra Selvan']}, 'authorids': {'value': ['~Dustin_Wright2', '~Christian_Igel1', '~Raghavendra_Selvan1']}, 'keywords': {'value': ['Bayesian model reduction', 'structured pruning', 'variational inference', 'efficient machine learning', 'deep learning']}, 'abstract': {'value': 'Modern neural networks are often massively overparameterized leading to high compute costs during training and at inference. One effective method to improve both the compute and energy efficiency of neural networks while maintaining good performance is structured pruning, where full network structures (e.g. neurons or convolutional filters) that have limited impact on the model output are removed. In this work, we propose Bayesian Model Reduction for Structured pruning (BMRS), a fully end-to-end Bayesian method of structured pruning. BMRS is based on two recent methods: Bayesian structured pruning with multiplicative noise, and Bayesian model reduction (BMR), a method which allows efficient comparison of Bayesian models under a change in prior. We present two realizations of BMRS derived from different priors which yield different structured pruning characteristics:  1) BMRS_N with the truncated log-normal prior, which offers reliable compression rates and accuracy without the need for tuning any thresholds and 2) BMRS_U with the truncated log-uniform prior that can achieve more aggressive compression based on the boundaries of truncation. Overall, we find that BMRS offers a theoretically grounded approach to structured pruning of neural networks yielding both high compression rates and accuracy. Experiments on multiple datasets and neural networks of varying complexity showed that the two BMRS methods offer a competitive performance-efficiency trade-off compared to other pruning methods.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4d318f245a9a2a2e0fd394f7f7b3c91bdb084724.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwright2024bmrs,\\ntitle={{BMRS}: Bayesian Model Reduction for Structured Pruning},\\nauthor={Dustin Wright and Christian Igel and Raghavendra Selvan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=ktpG37Dzh5}\\n}'}, 'paperhash': {'value': 'wright|bmrs_bayesian_model_reduction_for_structured_pruning'}},forum = 'ktpG37Dzh5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3321/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3321/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3321/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3321/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'kr7eN85mIT',number = 1915,cdate = 1714887441413,pdate = 1727287675445,odate = 1730873853355,mdate = 1734651433432,tcdate = 1714887441413,tmdate = 1734651433432,ddate = None,content = {'title': {'value': 'Tell What You Hear From What You See - Video to Audio Generation Through Text'}, 'authors': {'value': ['Xiulong Liu', 'Kun Su', 'Eli Shlizerman']}, 'authorids': {'value': ['~Xiulong_Liu1', '~Kun_Su1', '~Eli_Shlizerman1']}, 'keywords': {'value': ['multi-modal learning', 'audio-visual learning', 'multi-modal large-language-model', 'text-guided video-to-audio generation', 'video-to-audio captioning']}, 'TLDR': {'value': 'A novel multi-modal generation framework for text guided video-to-audio generation and video-to-audio captioning.'}, 'abstract': {'value': 'The content of visual and audio scenes is multi-faceted such that a video stream can\\nbe paired with various audio streams and vice-versa. Thereby, in video-to-audio\\ngeneration task, it is imperative to introduce steering approaches for controlling the\\ngenerated audio. While Video-to-Audio generation is a well-established generative\\ntask, existing methods lack such controllability. In this work, we propose VATT, a\\nmulti-modal generative framework that takes a video and an optional text prompt\\nas input, and generates audio and optional textual description (caption) of the\\naudio. Such a framework has two unique advantages: i) Video-to-Audio generation\\nprocess can be refined and controlled via text which complements the context\\nof the visual information, and ii) The model can suggest what audio to generate\\nfor the video by generating audio captions. VATT consists of two key modules:\\nVATT Converter, which is an LLM that has been fine-tuned for instructions and\\nincludes a projection layer that maps video features to the LLM vector space, and\\nVATT Audio, a bi-directional transformer that generates audio tokens from visual\\nframes and from optional text prompt using iterative parallel decoding. The audio\\ntokens and the text prompt are used by a pretrained neural codec to convert them\\ninto a waveform. Our experiments show that when VATT is compared to existing\\nvideo-to-audio generation methods in objective metrics, such as VGGSound audiovisual dataset, it achieves competitive performance when the audio caption is\\nnot provided. When the audio caption is provided as a prompt, VATT achieves\\neven more refined performance (with lowest KLD score of 1.41). Furthermore,\\nsubjective studies asking participants to choose the most compatible generated\\naudio for a given silent video, show that VATT Audio has been chosen on average\\nas a preferred generated audio than the audio generated by existing methods. VATT\\nenables controllable video-to-audio generation through text as well as suggesting\\ntext prompts for videos through audio captions, unlocking novel applications such\\nas text-guided video-to-audio generation and video-to-audio captioning.'}, 'primary_area': {'value': 'speech_and_audio'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/7f048a863a36075771abbaa28445e76c2233bd97.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nliu2024tell,\\ntitle={Tell What You Hear From What You See - Video to Audio Generation Through Text},\\nauthor={Xiulong Liu and Kun Su and Eli Shlizerman},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kr7eN85mIT}\\n}'}, 'paperhash': {'value': 'liu|tell_what_you_hear_from_what_you_see_video_to_audio_generation_through_text'}},forum = 'kr7eN85mIT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1915/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1915/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1915/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission1915/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kqmucDKVcU',number = 10229,cdate = 1715691970445,pdate = 1727287933196,odate = 1730873926947,mdate = 1730873926957,tcdate = 1715691970445,tmdate = 1730873926957,ddate = None,content = {'title': {'value': 'Optimal Flow Matching: Learning Straight Trajectories in Just One Step'}, 'authors': {'value': ['Nikita Maksimovich Kornilov', 'Petr Mokrov', 'Alexander Gasnikov', 'Alexander Korotin']}, 'authorids': {'value': ['~Nikita_Maksimovich_Kornilov1', '~Petr_Mokrov1', '~Alexander_Gasnikov1', '~Alexander_Korotin2']}, 'keywords': {'value': ['Flow Matching', 'Optimal Transport', 'Rectified Flow', 'straight trajectories']}, 'abstract': {'value': \"Over the several recent years, there has been a boom in development of Flow Matching (FM) methods for generative modeling. One intriguing property pursued by the community is the ability to learn flows with straight trajectories which realize the Optimal Transport (OT) displacements. Straightness is crucial for the fast integration (inference) of the learned flow's paths. Unfortunately, most existing flow straightening methods are based on non-trivial iterative FM procedures which accumulate the error during training or exploit heuristics based on minibatch OT. To address these issues, we develop and theoretically justify the novel Optimal Flow Matching approach which allows recovering the straight OT displacement for the quadratic transport in just one FM step. The main idea of our approach is the employment of vector field for FM which are parameterized by convex functions. The code of our OFM implementation and the conducted experiments is available at https://github.com/Jhomanik/Optimal-Flow-Matching\"}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/05aa3a9c9c8809ebd4d4d2eedbec48e8773f322c.pdf'}, 'supplementary_material': {'value': '/attachment/9c77a7619ed0f9aedd65011b02c018c185b973bb.zip'}, '_bibtex': {'value': '@inproceedings{\\nkornilov2024optimal,\\ntitle={Optimal Flow Matching: Learning Straight Trajectories in Just One Step},\\nauthor={Nikita Maksimovich Kornilov and Petr Mokrov and Alexander Gasnikov and Alexander Korotin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kqmucDKVcU}\\n}'}, 'paperhash': {'value': 'kornilov|optimal_flow_matching_learning_straight_trajectories_in_just_one_step'}},forum = 'kqmucDKVcU',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10229/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10229/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10229/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10229/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kq166jACVP',number = 16191,cdate = 1715767606194,pdate = 1727288119293,odate = 1730873977656,mdate = 1737126458522,tcdate = 1715767606194,tmdate = 1737126458522,ddate = None,content = {'title': {'value': 'Aligner: Efficient Alignment by Learning to Correct'}, 'authors': {'value': ['Jiaming Ji', 'Boyuan Chen', 'Hantao Lou', 'Donghai Hong', 'Borong Zhang', 'Xuehai Pan', 'Tianyi Qiu', 'Juntao Dai', 'Yaodong Yang']}, 'authorids': {'value': ['~Jiaming_Ji2', '~Boyuan_Chen4', '~Hantao_Lou1', '~Donghai_Hong1', '~Borong_Zhang1', '~Xuehai_Pan1', '~Tianyi_Qiu1', '~Juntao_Dai1', '~Yaodong_Yang1']}, 'keywords': {'value': ['Large Language Models', 'Alignment', 'Reinforcement Learning from Human Feedback']}, 'TLDR': {'value': 'a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers'}, 'abstract': {'value': \"With the rapid development of large language models (LLMs) and ever-evolving practical requirements, finding an efficient and effective alignment method has never been more critical. However, the tension between the complexity of current alignment methods and the need for rapid iteration in deployment scenarios necessitates the development of a model-agnostic alignment approach that can operate under these constraints. In this paper, we introduce Aligner, a novel and simple alignment paradigm that learns the correctional residuals between preferred and dispreferred answers using a small model. Designed as a model-agnostic, plug-and-play module, Aligner can be directly applied to various open-source and API-based models with only one-off training, making it suitable for rapid iteration. Notably, Aligner can be applied to any powerful, large-scale upstream models. Moreover, it can even iteratively bootstrap the upstream models using corrected responses as synthetic human preference data, breaking through the model's performance ceiling. Our experiments demonstrate performance improvements by deploying the same Aligner model across 11 different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9% in helpfulness and 22.8% in harmlessness across the tested LLMs while also effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%, surpassing GPT-4 Omni's 57.5% Win Rate (community report).\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 oral'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8413cbc690f0263fff27f69ca2e9ae16dcdb584d.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nji2024aligner,\\ntitle={Aligner: Efficient Alignment by Learning to Correct},\\nauthor={Jiaming Ji and Boyuan Chen and Hantao Lou and Donghai Hong and Borong Zhang and Xuehai Pan and Tianyi Qiu and Juntao Dai and Yaodong Yang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kq166jACVP}\\n}'}, 'paperhash': {'value': 'ji|aligner_efficient_alignment_by_learning_to_correct'}},forum = 'kq166jACVP',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16191/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16191/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16191/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16191/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'kpo6ZCgVZH',number = 6116,cdate = 1715584110269,pdate = 1727287805564,odate = 1730873890679,mdate = 1736409466040,tcdate = 1715584110269,tmdate = 1736409466040,ddate = None,content = {'title': {'value': 'Functional Gradient Flows for Constrained Sampling'}, 'authors': {'value': ['Shiyue Zhang', 'Longlin Yu', 'Ziheng Cheng', 'Cheng Zhang']}, 'authorids': {'value': ['~Shiyue_Zhang3', '~Longlin_Yu1', '~Ziheng_Cheng4', '~Cheng_Zhang3']}, 'keywords': {'value': ['particle-based variational inference', 'constrained sampling', 'functional gradient flow', 'boundary integral']}, 'TLDR': {'value': 'A new functional gradient particle-based variational inference method for sampling on constrained domains.'}, 'abstract': {'value': 'Recently, through a unified gradient flow perspective of Markov chain Monte Carlo (MCMC) and variational inference (VI), particle-based variational inference methods (ParVIs) have been proposed that tend to combine the best of both worlds. While typical ParVIs such as Stein Variational Gradient Descent (SVGD) approximate the gradient flow within a reproducing kernel Hilbert space (RKHS), many attempts have been made recently to replace RKHS with more expressive function spaces, such as neural networks. While successful, these methods are mainly designed for sampling from unconstrained domains. In this paper, we offer a general solution to constrained sampling by introducing a boundary condition for the gradient flow which would confine the particles within the specific domain. This allows us to propose a new functional gradient ParVI method for constrained sampling, called *constrained functional gradient flow* (CFG), with provable continuous-time convergence in total variation (TV). We also present novel numerical strategies to handle the boundary integral term arising from the domain constraints. Our theory and experiments demonstrate the effectiveness of the proposed framework.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/51c557b2278d9e80a9145a83530964feffa18f5b.pdf'}, 'supplementary_material': {'value': '/attachment/305109022a9222fb2d5d17ea07f1da1e210e2a51.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024functional,\\ntitle={Functional Gradient Flows for Constrained Sampling},\\nauthor={Shiyue Zhang and Longlin Yu and Ziheng Cheng and Cheng Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kpo6ZCgVZH}\\n}'}, 'paperhash': {'value': 'zhang|functional_gradient_flows_for_constrained_sampling'}},forum = 'kpo6ZCgVZH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6116/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6116/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6116/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6116/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kngLs5H6l1',number = 8220,cdate = 1715655807798,pdate = 1727287872691,odate = 1730873910289,mdate = 1730873910309,tcdate = 1715655807798,tmdate = 1730873910309,ddate = None,content = {'title': {'value': 'Normal-GS: 3D Gaussian Splatting with Normal-Involved Rendering'}, 'authors': {'value': ['Meng Wei', 'Qianyi Wu', 'Jianmin Zheng', 'Hamid Rezatofighi', 'Jianfei Cai']}, 'authorids': {'value': ['~Meng_Wei10', '~Qianyi_Wu2', '~Jianmin_Zheng1', '~Hamid_Rezatofighi1', '~Jianfei_Cai1']}, 'keywords': {'value': ['neural rendering', '3D Gaussian Splatting', 'neural radiance field', 'computer vision', 'computer graphics']}, 'TLDR': {'value': 'We propose a normal-invovled rendering strategy for 3DGS, termed Normal-GS, which help enhance both the rendering quality and the normal estimation accuracy.'}, 'abstract': {'value': 'Rendering and reconstruction are long-standing topics in computer vision and graphics. Achieving both high rendering quality and accurate geometry is a challenge. Recent advancements in 3D Gaussian Splatting (3DGS) have enabled high-fidelity novel view synthesis at real-time speeds. However, the noisy and discrete nature of 3D Gaussian primitives hinders accurate surface estimation. Previous attempts to regularize 3D Gaussian normals often degrade rendering quality due to the fundamental disconnect between normal vectors and the rendering pipeline in 3DGS-based methods. Therefore, we introduce Normal-GS, a novel approach that integrates normal vectors into the 3DGS rendering pipeline. The core idea is to model the interaction between normals and incident lighting using the physically-based rendering equation. Our approach re-parameterizes surface colors as the product of normals and a designed Integrated Directional Illumination Vector (IDIV). To optimize memory usage and simplify optimization, we employ an anchor-based 3DGS to implicitly encode locally-shared IDIVs. Additionally, Normal-GS leverages optimized normals and Integrated Directional Encoding (IDE) to accurately model specular effects, enhancing both rendering quality and surface normal precision. Extensive experiments demonstrate that Normal-GS achieves near state-of-the-art visual quality while obtaining accurate surface normals and preserving real-time rendering performance.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/281b0739653518bc1782310574db76aca0e8652c.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwei2024normalgs,\\ntitle={Normal-{GS}: 3D Gaussian Splatting with Normal-Involved Rendering},\\nauthor={Meng Wei and Qianyi Wu and Jianmin Zheng and Hamid Rezatofighi and Jianfei Cai},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kngLs5H6l1}\\n}'}, 'paperhash': {'value': 'wei|normalgs_3d_gaussian_splatting_with_normalinvolved_rendering'}},forum = 'kngLs5H6l1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8220/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8220/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8220/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8220/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'klsyhjLlX5',number = 19047,cdate = 1715788874901,pdate = 1727288200424,odate = 1730873993569,mdate = 1730873993584,tcdate = 1715788874901,tmdate = 1730873993584,ddate = None,content = {'title': {'value': 'Group-wise oracle-efficient algorithms for online multi-group learning'}, 'authors': {'value': ['Samuel Deng', 'Jingwen Liu', 'Daniel Hsu']}, 'authorids': {'value': ['~Samuel_Deng1', '~Jingwen_Liu1', '~Daniel_Hsu1']}, 'keywords': {'value': ['multi-group learning', 'online learning', 'oracle-efficient']}, 'TLDR': {'value': 'We develop algorithms for achieving sublinear regret in online multi-group learning when the collection of groups is exponentially large or infinite.'}, 'abstract': {'value': 'We study the problem of online multi-group learning, a learning model in which an online learner must simultaneously achieve small prediction regret on a large collection of (possibly overlapping) subsequences corresponding to a family of groups. Groups are subsets of the context space, and in fairness applications, they may correspond to subpopulations defined by expressive functions of demographic attributes. In this paper, we design such oracle-efficient algorithms with sublinear regret under a variety of settings, including: (i) the i.i.d. setting, (ii) the adversarial setting with smoothed context distributions, and (iii) the adversarial transductive setting.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a546a85c17266d91e06a1b57959c2263f21c6295.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndeng2024groupwise,\\ntitle={Group-wise oracle-efficient algorithms for online multi-group learning},\\nauthor={Samuel Deng and Jingwen Liu and Daniel Hsu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=klsyhjLlX5}\\n}'}, 'paperhash': {'value': 'deng|groupwise_oracleefficient_algorithms_for_online_multigroup_learning'}},forum = 'klsyhjLlX5',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19047/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19047/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19047/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19047/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'klqhrq7fvB',number = 12003,cdate = 1715714639333,pdate = 1727287991119,odate = 1730873944339,mdate = 1730873944355,tcdate = 1715714639333,tmdate = 1730873944355,ddate = None,content = {'title': {'value': 'On the Scalability of GNNs for Molecular Graphs'}, 'authors': {'value': ['Maciej Sypetkowski', 'Frederik Wenkel', 'Farimah Poursafaei', 'Nia Dickson', 'Karush Suri', 'Philip Fradkin', 'Dominique Beaini']}, 'authorids': {'value': ['~Maciej_Sypetkowski1', '~Frederik_Wenkel1', '~Farimah_Poursafaei1', '~Nia_Dickson1', '~Karush_Suri1', '~Philip_Fradkin1', '~Dominique_Beaini1']}, 'keywords': {'value': ['Molecular Biology', 'Graph Neural Networks', 'Graph Transformers', 'Scaling Laws', 'Pretraining', 'Finetuning']}, 'TLDR': {'value': 'We study scaling laws of Graph Neural Networks on 2D molecular graphs during pretraining as well as finetuning.'}, 'abstract': {'value': 'Scaling deep learning models has been at the heart of recent revolutions in language modelling and image generation. Practitioners have observed a strong relationship between model size, dataset size, and performance. However, structure-based architectures such as Graph Neural Networks (GNNs) are yet to show the benefits of scale mainly due to lower efficiency of sparse operations, large data requirements, and lack of clarity about the effectiveness of various architectures. We address this drawback of GNNs by studying their scaling behavior. Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs for supervised pretraining. For the first time, we observe that GNNs benefit tremendously from the increasing scale of depth, width, number of molecules and associated labels. A major factor is the diversity of the pretraining data that comprises thousands of labels per molecule derived from bio-assays, quantum simulations, transcriptomics and phenomic imaging. We further demonstrate strong finetuning scaling behavior on 38 highly competitive downstream tasks, outclassing previous large models. This gives rise to MolGPS, a new graph foundation model that allows to navigate the chemical space, outperforming the previous state-of-the-arts on 26 out the 38 downstream tasks. We hope that our work paves the way for an era where foundational GNNs drive pharmaceutical drug discovery.'}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/10d89ae98e5eb08d3571feb3470901a034427b55.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsypetkowski2024on,\\ntitle={On the Scalability of {GNN}s for Molecular Graphs},\\nauthor={Maciej Sypetkowski and Frederik Wenkel and Farimah Poursafaei and Nia Dickson and Karush Suri and Philip Fradkin and Dominique Beaini},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=klqhrq7fvB}\\n}'}, 'paperhash': {'value': 'sypetkowski|on_the_scalability_of_gnns_for_molecular_graphs'}},forum = 'klqhrq7fvB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12003/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12003/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12003/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12003/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'kkmPe0rzY1',number = 12110,cdate = 1715716606642,pdate = 1727287994805,odate = 1730873945628,mdate = 1736436755680,tcdate = 1715716606642,tmdate = 1736436755680,ddate = None,content = {'title': {'value': 'Robust Conformal Prediction Using Privileged Information'}, 'authors': {'value': ['Shai Feldman', 'Yaniv Romano']}, 'authorids': {'value': ['~Shai_Feldman1', '~Yaniv_Romano1']}, 'keywords': {'value': ['Conformal Prediction', 'Uncertainty Quantification', 'Distribution Shift', 'Corrupted Data', 'Privileged Information']}, 'abstract': {'value': 'We develop a method to generate prediction sets with a guaranteed coverage rate that is robust to corruptions in the training data, such as missing or noisy variables. \\nOur approach builds on conformal prediction, a powerful framework to construct prediction sets that are valid under the i.i.d assumption. Importantly, naively applying conformal prediction does not provide reliable predictions in this setting, due to the distribution shift induced by the corruptions. \\nTo account for the distribution shift, we assume access to privileged information (PI). The PI is formulated as additional features that explain the distribution shift, however, they are only available during training and absent at test time.\\nWe approach this problem by introducing a novel generalization of weighted conformal prediction and support our method with theoretical coverage guarantees. \\nEmpirical experiments on both real and synthetic datasets indicate that our approach achieves a valid coverage rate and constructs more informative predictions compared to existing methods, which are not supported by theoretical guarantees.'}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/94a8cd10f96199c5cbd95fa62606eb31e618d6d0.zip'}, 'pdf': {'value': '/pdf/b1f57081fd9a0607d9e9969f0b916f0c1795d449.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfeldman2024robust,\\ntitle={Robust Conformal Prediction Using Privileged Information},\\nauthor={Shai Feldman and Yaniv Romano},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kkmPe0rzY1}\\n}'}, 'paperhash': {'value': 'feldman|robust_conformal_prediction_using_privileged_information'}},forum = 'kkmPe0rzY1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12110/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12110/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12110/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12110/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'kk0Eaunc58',number = 18490,cdate = 1715785831239,pdate = 1727288185492,odate = 1730873990200,mdate = 1736784814840,tcdate = 1715785831239,tmdate = 1736784814840,ddate = None,content = {'title': {'value': 'HydraViT: Stacking Heads for a Scalable ViT'}, 'authors': {'value': ['Janek Haberer', 'Ali Hojjat', 'Olaf Landsiedel']}, 'authorids': {'value': ['~Janek_Haberer1', '~Ali_Hojjat1', '~Olaf_Landsiedel1']}, 'keywords': {'value': ['Deep Learning', 'Transformers', 'Vision Transformers', 'Scalable Transformers']}, 'abstract': {'value': 'The architecture of Vision Transformers (ViTs), particularly the Multi-head Attention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs on devices with varying constraints, such as mobile phones, requires multiple models of different sizes. However, this approach has limitations, such as training and storing each required model separately. This paper introduces HydraViT, a novel approach that addresses these limitations by stacking attention heads to achieve a scalable ViT. By repeatedly changing the size of the embedded dimensions throughout each layer and their corresponding number of attention heads in MHA during training, HydraViT induces multiple subnetworks. Thereby, HydraViT achieves adaptability across a wide spectrum of hardware environments while maintaining performance. Our experimental results demonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10 subnetworks, covering a wide range of resource constraints. HydraViT achieves up to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time. The source code is available at https://github.com/ds-kiel/HydraViT.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0a4798ce4a42e7560241b118ad05d3c00fbabec3.pdf'}, 'supplementary_material': {'value': '/attachment/3ab7715caebceea6466b92c61f3aa38afe9b9d78.zip'}, '_bibtex': {'value': '@inproceedings{\\nhaberer2024hydravit,\\ntitle={HydraViT: Stacking Heads for a Scalable ViT},\\nauthor={Janek Haberer and Ali Hojjat and Olaf Landsiedel},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kk0Eaunc58}\\n}'}, 'TLDR': {'value': 'By sorting attention heads during training, we enable flexible inference that adapts to diverse hardware constraints by dropping the least important heads.'}, 'paperhash': {'value': 'haberer|hydravit_stacking_heads_for_a_scalable_vit'}},forum = 'kk0Eaunc58',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission18490/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission18490/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission18490/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission18490/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'kfdEXQu6MC',number = 20594,cdate = 1715797322107,pdate = 1727288237475,odate = 1730874002202,mdate = 1736851290678,tcdate = 1715797322107,tmdate = 1736851290678,ddate = None,content = {'title': {'value': 'A generalized neural tangent kernel for surrogate gradient learning'}, 'authors': {'value': ['Luke Eilers', 'Raoul-Martin Memmesheimer', 'Sven Goedeke']}, 'authorids': {'value': ['~Luke_Eilers1', '~Raoul-Martin_Memmesheimer1', '~Sven_Goedeke1']}, 'keywords': {'value': ['Neural Tangent Kernel', 'Surrogate Gradient Descent', 'Binary Neural Networks', 'Infinite Width']}, 'TLDR': {'value': 'We derive a generalized neural tangent kernel that describes surrogate gradient learning.'}, 'abstract': {'value': \"State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation.\\n\\nThe neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.\"}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a3dec7e2507a4c6b97bd5406e21049ad0ea78ef0.pdf'}, 'supplementary_material': {'value': '/attachment/d8c7546b617ea1918b78a3ceb36fa344c44a23b9.zip'}, '_bibtex': {'value': '@inproceedings{\\neilers2024a,\\ntitle={A generalized neural tangent kernel for surrogate gradient learning},\\nauthor={Luke Eilers and Raoul-Martin Memmesheimer and Sven Goedeke},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kfdEXQu6MC}\\n}'}, 'paperhash': {'value': 'eilers|a_generalized_neural_tangent_kernel_for_surrogate_gradient_learning'}},forum = 'kfdEXQu6MC',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20594/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20594/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20594/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20594/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kf80ZS3fVy',number = 2278,cdate = 1715015307839,pdate = 1727287685743,odate = 1730873857056,mdate = 1730873857077,tcdate = 1715015307839,tmdate = 1730873857077,ddate = None,content = {'title': {'value': 'Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration'}, 'authors': {'value': ['Kaihang Pan', 'Zhaoyu Fan', 'Juncheng Li', 'Qifan Yu', 'Hao Fei', 'Siliang Tang', 'Richang Hong', 'Hanwang Zhang', 'Qianru Sun']}, 'authorids': {'value': ['~Kaihang_Pan1', '~Zhaoyu_Fan1', '~Juncheng_Li3', '~Qifan_Yu1', '~Hao_Fei1', '~Siliang_Tang1', '~Richang_Hong1', '~Hanwang_Zhang3', '~Qianru_Sun2']}, 'keywords': {'value': ['vision and language', 'multimodal understanding']}, 'abstract': {'value': 'The swift advancement in Multimodal LLMs (MLLMs) also presents significant challenges for effective knowledge editing. Current methods, including intrinsic knowledge editing and external knowledge resorting, each possess strengths and weaknesses, struggling to balance the desired properties of reliability, generality, and locality when applied to MLLMs. In this paper, we propose \\\\textbf{UniKE}, a novel multimodal editing method that establishes a unified perspective and paradigm for intrinsic knowledge editing and external knowledge resorting. Both types of knowledge are conceptualized as vectorized key-value memories, with the corresponding editing processes resembling the assimilation and accommodation phases of human cognition, conducted at the same semantic levels.  Within such a unified framework, we further promote knowledge collaboration by disentangling the knowledge representations into the semantic and truthfulness spaces. Extensive experiments validate the effectiveness of our method, which ensures that the post-edit MLLM simultaneously maintains excellent reliability, generality, and locality. The code for UniKE is available at https://github.com/beepkh/UniKE.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/97d46003efdb73c62ba270d8ea1a2c70c2afea04.pdf'}, '_bibtex': {'value': '@inproceedings{\\npan2024towards,\\ntitle={Towards Unified Multimodal Editing with Enhanced Knowledge Collaboration},\\nauthor={Kaihang Pan and Zhaoyu Fan and Juncheng Li and Qifan Yu and Hao Fei and Siliang Tang and Richang Hong and Hanwang Zhang and Qianru Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kf80ZS3fVy}\\n}'}, 'paperhash': {'value': 'pan|towards_unified_multimodal_editing_with_enhanced_knowledge_collaboration'}},forum = 'kf80ZS3fVy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2278/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2278/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2278/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2278/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kcQKIzQPZj',number = 9041,cdate = 1715673617378,pdate = 1727287899189,odate = 1730873917056,mdate = 1730873917067,tcdate = 1715673617378,tmdate = 1730873917067,ddate = None,content = {'title': {'value': 'Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention Reasoner'}, 'authors': {'value': ['Xing Cui', 'Pei Pei Li', 'Zekun Li', 'Xuannan Liu', 'Yueying Zou', 'Zhaofeng He']}, 'authorids': {'value': ['~Xing_Cui1', '~Pei_Pei_Li2', '~Zekun_Li2', '~Xuannan_Liu1', '~Yueying_Zou1', '~Zhaofeng_He1']}, 'keywords': {'value': ['Image editing', 'Diffusion model', 'Large Language Model']}, 'TLDR': {'value': 'We introduce a methodology for semantic-aware image dragging with high image fidelity.'}, 'abstract': {'value': 'Flexible and accurate drag-based editing is a challenging task that has recently garnered significant attention. Current methods typically model this problem as automatically learning \"how to drag\" through point dragging and often produce one deterministic estimation, which presents two key limitations: 1) Overlooking the inherently ill-posed nature of drag-based editing, where multiple results may correspond to a given input, as illustrated in Fig.1; 2) Ignoring the constraint of image quality, which may lead to unexpected distortion.\\nTo alleviate this, we propose LucidDrag, which shifts the focus from \"how to drag\" to \"what-then-how\" paradigm.  LucidDrag comprises an intention reasoner and a collaborative guidance sampling mechanism. The former infers several optimal editing strategies, identifying what content and what semantic direction to be edited. Based on the former, the latter addresses \"how to drag\" by collaboratively integrating existing editing guidance with the newly proposed semantic guidance and quality guidance.\\nSpecifically, semantic guidance is derived by establishing a semantic editing direction based on reasoned intentions, while quality guidance is achieved through classifier guidance using an image fidelity discriminator.\\nBoth qualitative and quantitative comparisons demonstrate the superiority of LucidDrag over previous methods.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b0da9fc404710a3b44c4e77d1232f7812394483e.pdf'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\ncui2024localize,\\ntitle={Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention Reasoner},\\nauthor={Xing Cui and Pei Pei Li and Zekun Li and Xuannan Liu and Yueying Zou and Zhaofeng He},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kcQKIzQPZj}\\n}'}, 'paperhash': {'value': 'cui|localize_understand_collaborate_semanticaware_dragging_via_intention_reasoner'}},forum = 'kcQKIzQPZj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9041/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9041/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9041/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/Submission9041/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'kbBjVMcJ7G',number = 20052,cdate = 1715794053430,pdate = 1727288224308,odate = 1730873998986,mdate = 1740735280527,tcdate = 1715794053430,tmdate = 1740735280527,ddate = None,content = {'title': {'value': 'Operator World Models for Reinforcement Learning'}, 'authors': {'value': ['Pietro Novelli', 'Marco Pratticò', 'Massimiliano Pontil', 'Carlo Ciliberto']}, 'authorids': {'value': ['~Pietro_Novelli1', '~Marco_Pratticò1', '~Massimiliano_Pontil4', '~Carlo_Ciliberto1']}, 'keywords': {'value': ['Reinforcement Learning', 'Transfer Operators', 'World Models', 'Policy Gradient', 'Conditional Mean Embeddings', 'Mirror Descent']}, 'abstract': {'value': 'Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/210c4da4f5377b49a1d75a7cbf62a6257152db59.pdf'}, '_bibtex': {'value': '@inproceedings{\\nnovelli2024operator,\\ntitle={Operator World Models for Reinforcement Learning},\\nauthor={Pietro Novelli and Marco Prattic{\\\\`o} and Massimiliano Pontil and Carlo Ciliberto},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kbBjVMcJ7G}\\n}'}, 'paperhash': {'value': 'novelli|operator_world_models_for_reinforcement_learning'}},forum = 'kbBjVMcJ7G',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20052/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20052/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20052/-/Revision', 'NeurIPS.cc/2024/Conference/-/Desk_Rejected_Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20052/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kamAXSJxGV',number = 4161,cdate = 1715369564579,pdate = 1727287742555,odate = 1730873873180,mdate = 1730873873198,tcdate = 1715369564579,tmdate = 1730873873198,ddate = None,content = {'title': {'value': 'Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy'}, 'authors': {'value': ['Zeki Kazan', 'Jerome Reiter']}, 'authorids': {'value': ['~Zeki_Kazan1', '~Jerome_Reiter1']}, 'keywords': {'value': ['confidentiality', 'disclosure', 'risk', 'semantics', 'utility']}, 'TLDR': {'value': 'We propose a framework for setting epsilon for data releases satisfying differential privacy.'}, 'abstract': {'value': \"When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects' confidentiality. For releases satisfying differential privacy, this balance is reflected by the privacy budget, $\\\\varepsilon$. We provide a framework for setting $\\\\varepsilon$ based on its relationship with Bayesian posterior probabilities of disclosure. The agency responsible for the data release decides how much posterior risk it is willing to accept at various levels of prior risk, which implies a unique $\\\\varepsilon$. Agencies can evaluate different risk profiles to determine one that leads to an acceptable trade-off in risk and utility.\"}, 'pdf': {'value': '/pdf/ad3ee055c2553f84b6b18d6557012e1793d1c17a.pdf'}, 'supplementary_material': {'value': '/attachment/8517c8fcd53056ebd33c9625649cc973ca97481a.zip'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, '_bibtex': {'value': '@inproceedings{\\nkazan2024prioritizing,\\ntitle={Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy},\\nauthor={Zeki Kazan and Jerome Reiter},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kamAXSJxGV}\\n}'}, 'paperhash': {'value': 'kazan|prioritizing_privacy_a_bayesian_approach_to_setting_the_privacy_budget_in_differential_privacy'}},forum = 'kamAXSJxGV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4161/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4161/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4161/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4161/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kZpNDbZrzy',number = 11455,cdate = 1715705144598,pdate = 1727287971690,odate = 1730873938622,mdate = 1730873938642,tcdate = 1715705144598,tmdate = 1730873938642,ddate = None,content = {'title': {'value': 'GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning'}, 'authors': {'value': ['Jaewoo Lee', 'Sujin Yun', 'Taeyoung Yun', 'Jinkyoo Park']}, 'authorids': {'value': ['~Jaewoo_Lee3', '~Sujin_Yun1', '~Taeyoung_Yun1', '~Jinkyoo_Park1']}, 'keywords': {'value': ['Offline Reinforcement Learning', 'Data Augmentation', 'Diffusion Models.']}, 'TLDR': {'value': 'Novel data augmentation method for offline RL using conditional diffusion model.'}, 'abstract': {'value': 'Offline Reinforcement Learning (Offline RL) presents challenges of learning effective decision-making policies from static datasets without any online interactions. Data augmentation techniques, such as noise injection and data synthesizing, aim to improve Q-function approximation by smoothing the learned state-action region. However, these methods often fall short of directly improving the quality of offline datasets, leading to suboptimal results. In response, we introduce GTA, Generative Trajectory Augmentation, a novel generative data augmentation approach designed to enrich offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA applies a diffusion model within the data augmentation framework. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. Our results show that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms across various tasks with unique challenges. Furthermore, we conduct a quality analysis of data augmented by GTA and demonstrate that GTA improves the quality of the data. Our code is available at https://github.com/Jaewoopudding/GTA'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/0654ab64b73938184f454a5419d4545766a9f5c3.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024gta,\\ntitle={{GTA}: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning},\\nauthor={Jaewoo Lee and Sujin Yun and Taeyoung Yun and Jinkyoo Park},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kZpNDbZrzy}\\n}'}, 'paperhash': {'value': 'lee|gta_generative_trajectory_augmentation_with_guidance_for_offline_reinforcement_learning'}},forum = 'kZpNDbZrzy',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11455/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11455/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11455/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11455/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kZfxICBXd1',number = 9111,cdate = 1715674779276,pdate = 1727287901254,odate = 1730873917532,mdate = 1736241198272,tcdate = 1715674779276,tmdate = 1736241198272,ddate = None,content = {'title': {'value': 'Multi-Winner Reconfiguration'}, 'authors': {'value': ['Jiehua Chen', 'Christian Hatschka', 'Sofia Simola']}, 'authorids': {'value': ['~Jiehua_Chen1', '~Christian_Hatschka1', '~Sofia_Simola1']}, 'keywords': {'value': ['computational social choice', 'computational complexity', 'algorithmic design']}, 'abstract': {'value': 'We introduce a multi-winner reconfiguration model to examine how to transition between subsets of alternatives (aka. committees) through a sequence of minor yet impactful modifications, called reconfiguration path. We analyze this model under four approval-based voting rules: Chamberlin-Courant (CC), Proportional Approval Voting (PAV), Approval Voting (AV), and Satisfaction Approval Voting (SAV). The problem exhibits computational intractability for CC and PAV, and polynomial solvability for AV and SAV. We provide a detailed multivariate complexity analysis for CC and PAV, demonstrating that although the problem remains challenging in many scenarios, there are specific cases that allow for efficient parameterized algorithms.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'supplementary_material': {'value': '/attachment/f072147f62f0c10914e1d111458b0468b8f69e39.zip'}, 'pdf': {'value': '/pdf/49d0c000bcf1a313f5be08ebcaa26e07a244ad0f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchen2024multiwinner,\\ntitle={Multi-Winner Reconfiguration},\\nauthor={Jiehua Chen and Christian Hatschka and Sofia Simola},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kZfxICBXd1}\\n}'}, 'paperhash': {'value': 'chen|multiwinner_reconfiguration'}},forum = 'kZfxICBXd1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9111/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9111/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9111/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9111/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'kZ4Kc5GhGB',number = 17641,cdate = 1715781020728,pdate = 1727288158550,odate = 1730873985368,mdate = 1730873985388,tcdate = 1715781020728,tmdate = 1730873985388,ddate = None,content = {'title': {'value': 'Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity'}, 'authors': {'value': ['Guhao Feng', 'Han Zhong']}, 'authorids': {'value': ['~Guhao_Feng1', '~Han_Zhong1']}, 'keywords': {'value': ['RL theory', 'representation complexity', 'model-based RL', 'policy-based RL', 'value-based RL']}, 'TLDR': {'value': 'We uncover a potential representation complexity hierarchy among different RL paradigms, including model-based RL, policy-based RL, and value-based RL.'}, 'abstract': {'value': 'Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity among these RL paradigms. By utilizing computational complexity measures, including time complexity and circuit complexity, we theoretically unveil a potential representation complexity hierarchy within RL. We find that representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge. Additionally, we reaffirm this hierarchy from the perspective of the expressiveness of Multi-Layer Perceptrons (MLPs), which align more closely with practical deep RL and contribute to a completely new perspective in theoretical studying representation complexity in RL. Finally, we conduct deep RL experiments to validate our theoretical findings.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8cbd951349284ebb7f4cb65d9d776f14f42fbe70.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfeng2024rethinking,\\ntitle={Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity},\\nauthor={Guhao Feng and Han Zhong},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kZ4Kc5GhGB}\\n}'}, 'paperhash': {'value': 'feng|rethinking_modelbased_policybased_and_valuebased_reinforcement_learning_via_the_lens_of_representation_complexity'}},forum = 'kZ4Kc5GhGB',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission17641/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission17641/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission17641/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission17641/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kYio3xH6eb',number = 7983,cdate = 1715649570440,pdate = 1727287864786,odate = 1730873908114,mdate = 1730873908132,tcdate = 1715649570440,tmdate = 1730873908132,ddate = None,content = {'title': {'value': 'Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation'}, 'authors': {'value': ['Xiaoying Zhang', 'Jean-Francois Ton', 'Wei Shen', 'Hongning Wang', 'Yang Liu']}, 'authorids': {'value': ['~Xiaoying_Zhang3', '~Jean-Francois_Ton2', '~Wei_Shen12', '~Hongning_Wang1', '~Yang_Liu3']}, 'keywords': {'value': ['Overoptimization in RLHF', 'Lightweight Uncertainty Estimation', 'Adversarial Policy Optimization']}, 'TLDR': {'value': 'We present a novel solution to the prevalent problem of reward overoptimization in RLHF through adversarial policy optimization with lightweight uncertainty estimation.'}, 'abstract': {'value': 'Reinforcement Learning from Human Feedback (RLHF) has been pivotal in aligning Large Language Models with human values but often suffers from overoptimization due to its reliance on a proxy reward model. To mitigate this limitation, we first propose a lightweight uncertainty quantification method that assesses the reliability of the proxy reward using only the last layer embeddings of the reward model. Enabled by this efficient uncertainty quantification method, we formulate AdvPO, a distributionally robust optimization procedure to tackle the reward overoptimization problem in RLHF. Through extensive experiments on the Anthropic HH and TL;DR summarization datasets, we verify the effectiveness of AdvPO in mitigating the overoptimization problem, resulting in enhanced RLHF performance as evaluated through human-assisted evaluation.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d5204c40ecb830dd2c23620408620ca203e3e897.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024mitigating,\\ntitle={Mitigating Reward Overoptimization via Lightweight Uncertainty Estimation},\\nauthor={Xiaoying Zhang and Jean-Francois Ton and Wei Shen and Hongning Wang and Yang Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kYio3xH6eb}\\n}'}, 'paperhash': {'value': 'zhang|mitigating_reward_overoptimization_via_lightweight_uncertainty_estimation'}},forum = 'kYio3xH6eb',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7983/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7983/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7983/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7983/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kXKrLsR4aJ',number = 20062,cdate = 1715794113991,pdate = 1727288224600,odate = 1730873999037,mdate = 1734873129885,tcdate = 1715794113991,tmdate = 1734873129885,ddate = None,content = {'title': {'value': 'Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space'}, 'authors': {'value': ['Maximilian Stölzle', 'Cosimo Della Santina']}, 'authorids': {'value': ['~Maximilian_Stölzle1', '~Cosimo_Della_Santina1']}, 'keywords': {'value': ['Dynamical Systems', 'Control Theory', 'Robotics', 'Decision and Control', 'Deep Autoencoders']}, 'abstract': {'value': 'Even though a variety of methods have been proposed in the literature, efficient and effective latent-space control (i.e., control in a learned low-dimensional space) of physical systems remains an open challenge.\\nWe argue that a promising avenue is to leverage powerful and well-understood closed-form strategies from control theory literature in combination with learned dynamics, such as potential-energy shaping.\\nWe identify three fundamental shortcomings in existing latent-space models that have so far prevented this powerful combination: (i) they lack the mathematical structure of a physical system, (ii) they do not inherently conserve the stability properties of the real systems, (iii) these methods do not have an invertible mapping between input and latent-space forcing.\\nThis work proposes a novel Coupled Oscillator Network (CON) model that simultaneously tackles all these issues. \\nMore specifically, (i) we show analytically that CON is a Lagrangian system - i.e., it possesses well-defined potential and kinetic energy terms. Then, (ii) we provide formal proof of global Input-to-State stability using Lyapunov arguments.\\nMoving to the experimental side, we demonstrate that CON reaches SoA performance when learning complex nonlinear dynamics of mechanical systems directly from images.\\nAn additional methodological innovation contributing to achieving this third goal is an approximated closed-form solution for efficient integration of network dynamics, which eases efficient training.\\nWe tackle (iii) by approximating the forcing-to-input mapping with a decoder that is trained to reconstruct the input based on the encoded latent space force.\\nFinally, we leverage these three properties and show that they enable latent-space control. We use an integral-saturated PID with potential force compensation and demonstrate high-quality performance on a soft robot using raw pixels as the only feedback information.'}, 'primary_area': {'value': 'robotics'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We leverage input-to-state stable coupled oscillator networks for conducting model-based control in latent space.'}, 'pdf': {'value': '/pdf/fac38d41d80be55ab5f6463f5ba220894dd5d996.pdf'}, '_bibtex': {'value': '@inproceedings{\\nst{\\\\\"o}lzle2024inputtostate,\\ntitle={Input-to-State Stable Coupled Oscillator Networks for Closed-form Model-based Control in Latent Space},\\nauthor={Maximilian St{\\\\\"o}lzle and Cosimo Della Santina},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kXKrLsR4aJ}\\n}'}, 'paperhash': {'value': 'stölzle|inputtostate_stable_coupled_oscillator_networks_for_closedform_modelbased_control_in_latent_space'}},forum = 'kXKrLsR4aJ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20062/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20062/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20062/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20062/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kXErlJSZ84',number = 15983,cdate = 1715765477837,pdate = 1727288113007,odate = 1730873976249,mdate = 1737122893353,tcdate = 1715765477837,tmdate = 1737122893353,ddate = None,content = {'title': {'value': 'General Detection-based Text Line Recognition'}, 'authors': {'value': ['Raphael Baena', 'syrine kalleli', 'Mathieu Aubry']}, 'authorids': {'value': ['~Raphael_Baena1', '~syrine_kalleli1', '~Mathieu_Aubry3']}, 'keywords': {'value': ['Text Recognition', 'Handwritten Text Recognition', 'Optical Character Recognition', 'Transformer']}, 'TLDR': {'value': 'A transformer-based OCR/HTR approach that performs explicit character detection and we demonstrate to be very general.'}, 'abstract': {'value': 'We introduce a general detection-based approach to text line recognition, be it printed (OCR) or handwritten text (HTR), with latin, chinese or ciphered characters. Detection-based approaches have until now largely been discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive. We overcome these challenges thanks to three main insights: (i) synthetic pre-training with diverse enough data to learn reasonable character localization in any script; (ii) modern transformer-based detectors can jointly detect a large number of instances and, if trained with an adequate masking strategy, leverage consistency between the different detections; (iii) once a pre-trained detection model with approximate character localization is available, it is possible to fine-tune it with line-level annotation on real data, even with a different alphabet.  Our approach thus builds on a completely different paradigm than most state-of-the-art methods, which rely on autoregressive decoding, predicting character values one by one, while we treat a complete line in parallel. Remarkably, our method demonstrates good performance on range of scripts, usually tackled with specialized approaches: latin script, chinese script, and ciphers, for which we significantly improve state-of-the-art performances. \\nOur code and models are available at [https://github.com/raphael-baena/DTLR](https://github.com/raphael-baena/DTLR).'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b8d5175bef8ceea82145cbd46834f32f5fa9df51.pdf'}, '_bibtex': {'value': '@inproceedings{\\nbaena2024general,\\ntitle={General Detection-based Text Line Recognition},\\nauthor={Raphael Baena and syrine kalleli and Mathieu Aubry},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kXErlJSZ84}\\n}'}, 'paperhash': {'value': 'baena|general_detectionbased_text_line_recognition'}},forum = 'kXErlJSZ84',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission15983/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission15983/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission15983/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission15983/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kWMVzIdCEn',number = 10830,cdate = 1715698071257,pdate = 1727287952346,odate = 1730873932572,mdate = 1730873932589,tcdate = 1715698071257,tmdate = 1730873932589,ddate = None,content = {'title': {'value': 'Multi-Scale Representation Learning for Protein Fitness Prediction'}, 'authors': {'value': ['Zuobai Zhang', 'Pascal Notin', 'Yining Huang', 'Aurelie Lozano', 'Vijil Chenthamarakshan', 'Debora Susan Marks', 'Payel Das', 'Jian Tang']}, 'authorids': {'value': ['~Zuobai_Zhang1', '~Pascal_Notin1', '~Yining_Huang1', '~Aurelie_Lozano1', '~Vijil_Chenthamarakshan1', '~Debora_Susan_Marks1', '~Payel_Das1', '~Jian_Tang1']}, 'keywords': {'value': ['Protein representation learning; protein fitness prediction; self-supervised pre-training']}, 'abstract': {'value': 'Designing novel functional proteins crucially depends on accurately modeling their fitness landscape. Given the limited availability of functional annotations from wet-lab experiments, previous methods have primarily relied on self-supervised models trained on vast, unlabeled protein sequence or structure datasets. While initial protein representation learning studies solely focused on either sequence or structural features, recent hybrid architectures have sought to merge these modalities to harness their respective strengths. However, these sequence-structure models have so far achieved only incremental improvements when compared to the leading sequence-only approaches, highlighting unresolved challenges effectively leveraging these modalities together. Moreover, the function of certain proteins is highly dependent on the granular aspects of their surface topology, which have been overlooked by prior models.\\nTo address these limitations, we introduce the Sequence-Structure-Surface Fitness (**S3F**) model — a novel multimodal representation learning framework that integrates protein features across several scales. Our approach combines sequence representations from a protein language model with Geometric Vector Perceptron networks encoding protein backbone and detailed surface topology. The proposed method achieves state-of-the-art fitness prediction on the ProteinGym benchmark encompassing 217 substitution deep mutational scanning assays, and provides insights into the determinants of protein function.\\nOur code is at https://github.com/DeepGraphLearning/S3F.'}, 'primary_area': {'value': 'machine_learning_for_healthcare'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/42a9efb3a54368d5d17852708b09e753ae1f9ef0.pdf'}, 'supplementary_material': {'value': '/attachment/229c096c8d5f97f10a461f44a15e6a3715d40575.zip'}, 'flagged_for_ethics_review': {'value': True}, '_bibtex': {'value': '@inproceedings{\\nzhang2024multiscale,\\ntitle={Multi-Scale Representation Learning for Protein Fitness Prediction},\\nauthor={Zuobai Zhang and Pascal Notin and Yining Huang and Aurelie Lozano and Vijil Chenthamarakshan and Debora Susan Marks and Payel Das and Jian Tang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kWMVzIdCEn}\\n}'}, 'paperhash': {'value': 'zhang|multiscale_representation_learning_for_protein_fitness_prediction'}},forum = 'kWMVzIdCEn',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission10830/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission10830/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission10830/-/Revision', 'NeurIPS.cc/2024/Conference/-/Ethics_Review_Flag', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission10830/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kW30LbNwdV',number = 3800,cdate = 1715326927091,pdate = 1727287731297,odate = 1730873869668,mdate = 1730873869680,tcdate = 1715326927091,tmdate = 1730873869680,ddate = None,content = {'title': {'value': 'Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation'}, 'authors': {'value': ['Shiji Zhao', 'Ranjie Duan', 'xizhewang', 'Xingxing Wei']}, 'authorids': {'value': ['~Shiji_Zhao1', '~Ranjie_Duan1', '~xizhewang1', '~Xingxing_Wei1']}, 'keywords': {'value': ['Adversarial Robustness', 'Robust Fairness', 'Knowledge Distillation']}, 'abstract': {'value': \"Adversarial Training (AT) has been widely proved to be an effective method to improve the adversarial robustness against adversarial examples for Deep Neural Networks (DNNs). As a variant of AT, Adversarial Robustness Distillation (ARD) has demonstrated its superior performance in improving the robustness of small student models with the guidance of large teacher models. However, both AT and ARD encounter the robust fairness problem: these models exhibit strong robustness when facing part of classes (easy class), but weak robustness when facing others (hard class). In this paper, we give an in-depth analysis of the potential factors and argue that the smoothness degree of samples' soft labels for different classes (i.e., hard class or easy class) will affect the robust fairness of DNNs from both empirical observation and theoretical analysis. Based on the above finding, we propose an Anti-Bias Soft Label Distillation (ABSLD) method to mitigate the adversarial robust fairness problem within the framework of Knowledge Distillation (KD). Specifically, ABSLD adaptively reduces the student's error risk gap between different classes to achieve fairness by adjusting the class-wise smoothness degree of samples' soft labels during the training process, and the smoothness degree of soft labels is controlled by assigning different temperatures in KD to different classes. Extensive experiments demonstrate that ABSLD outperforms state-of-the-art AT, ARD, and robust fairness methods in the comprehensive metric (Normalized Standard Deviation) of robustness and fairness.\"}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/4778b077085c3a1b78c8e8796dd0c9ad4b693d64.pdf'}, 'supplementary_material': {'value': '/attachment/25db253ede48e8e9d957ed7118a00214544864fc.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024improving,\\ntitle={Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation},\\nauthor={Shiji Zhao and Ranjie Duan and xizhewang and Xingxing Wei},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kW30LbNwdV}\\n}'}, 'paperhash': {'value': 'zhao|improving_adversarial_robust_fairness_via_antibias_soft_label_distillation'}},forum = 'kW30LbNwdV',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3800/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3800/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3800/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3800/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kVuw8vzsqZ',number = 12066,cdate = 1715715799589,pdate = 1727287993438,odate = 1730873945108,mdate = 1730873945123,tcdate = 1715715799589,tmdate = 1730873945123,ddate = None,content = {'title': {'value': 'SkipPredict: When to Invest in Predictions for Scheduling'}, 'authors': {'value': ['Rana Shahout', 'Michael Mitzenmacher']}, 'authorids': {'value': ['~Rana_Shahout1', '~Michael_Mitzenmacher1']}, 'keywords': {'value': ['Algorithms with predictions; scheduling']}, 'abstract': {'value': 'Expanding on recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions in queueing systems, removing the assumption in prior research that predictions are external to the system’s resources and/or cost-free. Additionally, we introduce a novel approach to utilizing predictions, SkipPredict, designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose a tailored approach that categorizes jobs to improve the effectiveness of prediction on performance. To achieve this, we employ one-bit “cheap predictions” to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs over long jobs, and for the long jobs, SkipPredict applies a second round of more detailed “expensive predictions” to approximate Shortest Remaining Processing Time for these jobs. Importantly, our analyses take into account the cost of prediction. We derive closed-form formulas that calculate the mean response time of jobs with size predictions accounting for the prediction cost. We examine the effect of this cost for two distinct models in real-world and synthetic datasets. In the external cost model, predictions are generated by external method without impacting job service times but incur a cost. In the server time cost model, predictions themselves require server processing time and are scheduled on the same server as the jobs.'}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5162380d9c32a0fd2737b61b998c536a3eec2ceb.pdf'}, 'supplementary_material': {'value': '/attachment/c818a54f5e0e172cf0f465f090c3bbe2114a3c44.zip'}, '_bibtex': {'value': '@inproceedings{\\nshahout2024skippredict,\\ntitle={SkipPredict: When to Invest in Predictions for Scheduling},\\nauthor={Rana Shahout and Michael Mitzenmacher},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kVuw8vzsqZ}\\n}'}, 'paperhash': {'value': 'shahout|skippredict_when_to_invest_in_predictions_for_scheduling'}},forum = 'kVuw8vzsqZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12066/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12066/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12066/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12066/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kVr3L73pNH',number = 1056,cdate = 1714363227254,pdate = 1727287650929,odate = 1730873845211,mdate = 1730873845222,tcdate = 1714363227254,tmdate = 1730873845222,ddate = None,content = {'title': {'value': 'Data Attribution for Text-to-Image Models by Unlearning Synthesized Images'}, 'authors': {'value': ['Sheng-Yu Wang', 'Aaron Hertzmann', 'Alexei A Efros', 'Jun-Yan Zhu', 'Richard Zhang']}, 'authorids': {'value': ['~Sheng-Yu_Wang1', '~Aaron_Hertzmann1', '~Alexei_A_Efros1', '~Jun-Yan_Zhu1', '~Richard_Zhang1']}, 'keywords': {'value': ['Data Attribution', 'Influence Estimation', 'Text-to-Image models', 'Machine Unlearning']}, 'abstract': {'value': 'The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method\\'s advantages over previous methods.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/2e32e842f544c74dfff23ed357c51ab664e326cb.pdf'}, 'TLDR': {'value': 'We identify influential training images that make a synthesized image possible, using our proposed attribution method by unlearning a synthesized image.'}, '_bibtex': {'value': '@inproceedings{\\nwang2024data,\\ntitle={Data Attribution for Text-to-Image Models by Unlearning Synthesized Images},\\nauthor={Sheng-Yu Wang and Aaron Hertzmann and Alexei A Efros and Jun-Yan Zhu and Richard Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kVr3L73pNH}\\n}'}, 'paperhash': {'value': 'wang|data_attribution_for_texttoimage_models_by_unlearning_synthesized_images'}},forum = 'kVr3L73pNH',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1056/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1056/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1056/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1056/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'kVL5rvkqGG',number = 12177,cdate = 1715717721389,pdate = 1727287997460,odate = 1730873946275,mdate = 1730873946296,tcdate = 1715717721389,tmdate = 1730873946296,ddate = None,content = {'title': {'value': 'Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe'}, 'authors': {'value': ['Albert Q. Jiang', 'Alicja Ziarko', 'Bartosz Piotrowski', 'Wenda Li', 'Mateja Jamnik', 'Piotr Miłoś']}, 'authorids': {'value': ['~Albert_Q._Jiang1', '~Alicja_Ziarko1', '~Bartosz_Piotrowski1', '~Wenda_Li1', '~Mateja_Jamnik1', '~Piotr_Miłoś1']}, 'keywords': {'value': ['text embedding', 'embedding models', 'scaling laws']}, 'abstract': {'value': 'Text embeddings are essential for tasks such as document retrieval, clustering, and semantic similarity assessment. In this paper, we study how to contrastively train text embedding models in a compute-optimal fashion, given a suite of pretrained decoder-only language models. Our innovation is an algorithm that produces optimal configurations of model sizes, data quantities, and fine-tuning methods for text-embedding models at different computational budget levels. The resulting recipe, which we obtain through extensive experiments, can be used by practitioners to make informed design choices for their embedding models. Specifically, our findings suggest that full fine-tuning and Low-Rank Adaptation fine-tuning produce optimal models at lower and higher computational budgets respectively.'}, 'primary_area': {'value': 'natural_language_processing'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/eefb08b5efe5e72f4d802bf9c78722becbd5a0f9.pdf'}, 'supplementary_material': {'value': '/attachment/ad32bcb230e70841be3aaada050827e3ffe367e7.tgz'}, 'TLDR': {'value': 'Providing a compute-optimal recipe for efficient contrastive fine-tuning of pretrained language models into embedding models.'}, '_bibtex': {'value': \"@inproceedings{\\njiang2024repurposing,\\ntitle={Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe},\\nauthor={Albert Q. Jiang and Alicja Ziarko and Bartosz Piotrowski and Wenda Li and Mateja Jamnik and Piotr Mi{\\\\l}o{\\\\'s}},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kVL5rvkqGG}\\n}\"}, 'paperhash': {'value': 'jiang|repurposing_language_models_into_embedding_models_finding_the_computeoptimal_recipe'}},forum = 'kVL5rvkqGG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12177/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12177/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12177/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12177/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kV80nC1afE',number = 5007,cdate = 1715495768561,pdate = 1727287768486,odate = 1730873881083,mdate = 1730873881101,tcdate = 1715495768561,tmdate = 1730873881101,ddate = None,content = {'title': {'value': 'Adaptive Passive-Aggressive Framework for Online Regression with Side Information'}, 'authors': {'value': ['Runhao Shi', 'Jiaxi Ying', 'Daniel P. Palomar']}, 'authorids': {'value': ['~Runhao_Shi1', '~Jiaxi_Ying1', '~Daniel_P._Palomar1']}, 'keywords': {'value': ['passive-aggressive', 'online learning', 'adaptive method', 'online regression problem', 'side information', 'financial engineering']}, 'abstract': {'value': 'The Passive-Aggressive (PA) method is widely used in online regression problems for handling large-scale streaming data, typically updating model parameters in a passive-aggressive manner based on whether the error exceeds a predefined threshold. However, this approach struggles with determining optimal thresholds and adapting to complex scenarios with side information, where tracking accuracy is not the sole metric in the regression model. To address these challenges, we introduce a novel adaptive framework that allows finer adjustments to the weight vector in PA using side information. This framework adaptively selects the threshold parameter in PA, theoretically ensuring convergence to the optimal setting. Additionally, we present an efficient implementation of our algorithm that significantly reduces computational complexity. Numerical experiments show that our model achieves outstanding performance associated with the side information while maintaining low tracking error, demonstrating marked improvements over traditional PA methods across various scenarios.'}, 'primary_area': {'value': 'online_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5012fa5c9454cc7c9e5f86f0f644c2d92dc2ef9e.pdf'}, 'TLDR': {'value': 'We introduce an adaptive Passive-Aggressive (PA) framework that leverages side information for optimal threshold selection, ensuring low tracking error and improved performance in online regression tasks.'}, '_bibtex': {'value': '@inproceedings{\\nshi2024adaptive,\\ntitle={Adaptive Passive-Aggressive Framework for Online Regression with Side Information},\\nauthor={Runhao Shi and Jiaxi Ying and Daniel P. Palomar},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kV80nC1afE}\\n}'}, 'paperhash': {'value': 'shi|adaptive_passiveaggressive_framework_for_online_regression_with_side_information'}},forum = 'kV80nC1afE',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5007/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5007/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5007/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5007/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'kTtK65vKvD',number = 5244,cdate = 1715518000597,pdate = 1727287777972,odate = 1730873883169,mdate = 1734592252670,tcdate = 1715518000597,tmdate = 1734592252670,ddate = None,content = {'title': {'value': 'ODGEN: Domain-specific Object Detection Data Generation with Diffusion Models'}, 'authors': {'value': ['JingYuan Zhu', 'Shiyu Li', 'Yuxuan Liu', 'Jian Yuan', 'Ping Huang', 'Jiulong Shan', 'Huimin Ma']}, 'authorids': {'value': ['~JingYuan_Zhu1', '~Shiyu_Li2', '~Yuxuan_Liu11', '~Jian_Yuan1', '~Ping_Huang1', '~Jiulong_Shan2', '~Huimin_Ma1']}, 'keywords': {'value': ['Object Detection Dataset Generation', 'Complex Scene Synthesis', 'Domain-Specific', 'Diffusion Models']}, 'TLDR': {'value': 'We propose a novel method to control diffusion models with bounding box labels, exhibiting robustness in handling complex scenes and specific domains and enabling detector enhancement with synthetic data.'}, 'abstract': {'value': 'Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multi-class objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domain-specific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. In addition, we design an evaluation protocol based on COCO-2014 to validate ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods.'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/43caf5b26317143d806ba6e2ea8d28c2724fe228.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024odgen,\\ntitle={{ODGEN}: Domain-specific Object Detection Data Generation with Diffusion Models},\\nauthor={JingYuan Zhu and Shiyu Li and Yuxuan Liu and Jian Yuan and Ping Huang and Jiulong Shan and Huimin Ma},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kTtK65vKvD}\\n}'}, 'paperhash': {'value': 'zhu|odgen_domainspecific_object_detection_data_generation_with_diffusion_models'}},forum = 'kTtK65vKvD',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5244/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5244/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5244/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5244/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kS9dciADtY',number = 6910,cdate = 1715607753085,pdate = 1727287828540,odate = 1730873897469,mdate = 1730873897480,tcdate = 1715607753085,tmdate = 1730873897480,ddate = None,content = {'title': {'value': 'Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection'}, 'authors': {'value': ['Yearang Lee', 'Ho-Joong Kim', 'Seong-Whan Lee']}, 'authorids': {'value': ['~Yearang_Lee1', '~Ho-Joong_Kim1', '~Seong-Whan_Lee3']}, 'keywords': {'value': ['Temporal Action Detection', 'Vision and Language', 'Zero-Shot Learning']}, 'abstract': {'value': 'Zero-Shot Temporal Action Detection (ZSTAD) aims to classify and localize action segments in untrimmed videos for unseen action categories. Most existing ZSTAD methods utilize a foreground-based approach, limiting the integration of text and visual features due to their reliance on pre-extracted proposals. In this paper, we introduce a cross-modal ZSTAD baseline with mutual cross-attention, integrating both text and visual information throughout the detection process. Our simple approach results in superior performance compared to previous methods. Despite this improvement, we further identify a common-action bias issue that the cross-modal baseline over-focus on common sub-actions due to a lack of ability to discriminate text-related visual parts. To address this issue, we propose Text-infused attention and Foreground-aware Action Detection (Ti-FAD), which enhances the ability to focus on text-related sub-actions and distinguish relevant action segments from the background. Our extensive experiments demonstrate that Ti-FAD outperforms the state-of-the-art methods on ZSTAD benchmarks by a large margin:  41.2\\\\% (+ 11.0\\\\%) on THUMOS14 and 32.0\\\\% (+ 5.4\\\\%) on ActivityNet v1.3. Code is available at: https://github.com/YearangLee/Ti-FAD.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5fe47f61eee7ec17c41c636e0cd550c708fb2e99.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlee2024textinfused,\\ntitle={Text-Infused Attention and Foreground-Aware Modeling for Zero-Shot Temporal Action Detection},\\nauthor={Yearang Lee and Ho-Joong Kim and Seong-Whan Lee},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kS9dciADtY}\\n}'}, 'paperhash': {'value': 'lee|textinfused_attention_and_foregroundaware_modeling_for_zeroshot_temporal_action_detection'}},forum = 'kS9dciADtY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6910/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6910/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6910/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6910/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kRwQCAIA7z',number = 13437,cdate = 1715739643962,pdate = 1727288039613,odate = 1730873957959,mdate = 1735893813910,tcdate = 1715739643962,tmdate = 1735893813910,ddate = None,content = {'title': {'value': 'Dimension-free Private Mean Estimation for Anisotropic Distributions'}, 'authors': {'value': ['Yuval Dagan', 'Michael Jordan', 'Xuelin Yang', 'Lydia Zakynthinou', 'Nikita Zhivotovskiy']}, 'authorids': {'value': ['~Yuval_Dagan1', '~Michael_Jordan1', '~Xuelin_Yang1', '~Lydia_Zakynthinou1', '~Nikita_Zhivotovskiy1']}, 'keywords': {'value': ['differential privacy', 'mean estimation', 'anisotropic', 'covariance-adaptive error']}, 'abstract': {'value': 'We present differentially private algorithms for high-dimensional mean estimation. Previous private estimators on distributions over $\\\\mathbb{R}^d$ suffer from a curse of dimensionality, as they require $\\\\Omega(d^{1/2})$ samples to achieve non-trivial error, even in cases where $O(1)$ samples suffice without privacy. This rate is  unavoidable when the distribution is isotropic, namely, when the covariance is a multiple of the identity matrix. Yet, real-world data is often highly anisotropic, with signals concentrated on a small number of principal components. We develop estimators that are appropriate for such signals---our estimators are $(\\\\varepsilon,\\\\delta)$-differentially private and have sample complexity that is dimension-independent for anisotropic subgaussian distributions.  Given $n$ samples from a distribution with known covariance-proxy $\\\\Sigma$ and unknown mean $\\\\mu$, we present an estimator $\\\\hat{\\\\mu}$ that achieves error, $\\\\|\\\\hat{\\\\mu}-\\\\mu\\\\|_2\\\\leq \\\\alpha$, as long as $n\\\\gtrsim \\\\text{tr}(\\\\Sigma)/\\\\alpha^2+ \\\\text{tr}(\\\\Sigma^{1/2})/(\\\\alpha\\\\varepsilon)$. We show that this is the optimal sample complexity for this task up to logarithmic factors. Moreover, for the case of unknown covariance, we present an algorithm whose sample complexity has improved dependence on the dimension, from $d^{1/2}$ to $d^{1/4}$.'}, 'primary_area': {'value': 'privacy'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present private mean estimators for anisotropic distributions with dimension-free sample complexity, which we prove is optimal. We also give an estimator under unknown covariance, with a dimension-dependence that is milder than in prior work.'}, 'pdf': {'value': '/pdf/90919b8b60143f0e171dd5310efcb65e20de7354.pdf'}, '_bibtex': {'value': '@inproceedings{\\ndagan2024dimensionfree,\\ntitle={Dimension-free Private Mean Estimation for Anisotropic Distributions},\\nauthor={Yuval Dagan and Michael Jordan and Xuelin Yang and Lydia Zakynthinou and Nikita Zhivotovskiy},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kRwQCAIA7z}\\n}'}, 'paperhash': {'value': 'dagan|dimensionfree_private_mean_estimation_for_anisotropic_distributions'}},forum = 'kRwQCAIA7z',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13437/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13437/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13437/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13437/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kREpCQtHdN',number = 12027,cdate = 1715714978905,pdate = 1727287992062,odate = 1730873944679,mdate = 1737028369178,tcdate = 1715714978905,tmdate = 1737028369178,ddate = None,content = {'title': {'value': 'Identifying Latent State-Transition Processes for Individualized Reinforcement Learning'}, 'authors': {'value': ['Yuewen Sun', 'Biwei Huang', 'Yu Yao', 'Donghuo Zeng', 'Xinshuai Dong', 'Songyao Jin', 'Boyang Sun', 'Roberto Legaspi', 'Kazushi Ikeda', 'Peter Spirtes', 'Kun Zhang']}, 'authorids': {'value': ['~Yuewen_Sun1', '~Biwei_Huang1', '~Yu_Yao3', '~Donghuo_Zeng1', '~Xinshuai_Dong1', '~Songyao_Jin1', '~Boyang_Sun1', '~Roberto_Legaspi1', '~Kazushi_Ikeda2', '~Peter_Spirtes1', '~Kun_Zhang1']}, 'keywords': {'value': ['Reinforcement Learning', 'Identifiability', 'Individualization']}, 'abstract': {'value': 'The application of reinforcement learning (RL) involving interactions with individuals has grown significantly in recent years. These interactions, influenced by factors such as personal preferences and physiological differences, causally influence state transitions, ranging from health conditions in healthcare to learning progress in education. As a result, different individuals may exhibit different state-transition processes. Understanding individualized state-transition processes is essential for optimizing individualized policies. In practice, however, identifying these state-transition processes is challenging, as individual-specific factors often remain latent. In this paper, we establish the identifiability of these latent factors and introduce a practical method that effectively learns these processes from observed state-action trajectories. Experiments on various datasets show that the proposed method can effectively identify latent state-transition processes and facilitate the learning of individualized RL policies.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c647ba82ceef44f9b60644936da1ec2daade0bd7.pdf'}, '_bibtex': {'value': '@inproceedings{\\nsun2024identifying,\\ntitle={Identifying Latent State-Transition Processes for Individualized Reinforcement Learning},\\nauthor={Yuewen Sun and Biwei Huang and Yu Yao and Donghuo Zeng and Xinshuai Dong and Songyao Jin and Boyang Sun and Roberto Legaspi and Kazushi Ikeda and Peter Spirtes and Kun Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kREpCQtHdN}\\n}'}, 'paperhash': {'value': 'sun|identifying_latent_statetransition_processes_for_individualized_reinforcement_learning'}},forum = 'kREpCQtHdN',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission12027/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission12027/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission12027/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission12027/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kQPzFiwVIu',number = 20674,cdate = 1715797741126,pdate = 1727288239139,odate = 1730874002632,mdate = 1737594530693,tcdate = 1715797741126,tmdate = 1737594530693,ddate = None,content = {'title': {'value': 'Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages'}, 'authors': {'value': ['Federico Mora', 'Justin Wong', 'Haley Lepe', 'Sahil Bhatia', 'Karim Elmaaroufi', 'George Varghese', 'Joseph E. Gonzalez', 'Elizabeth Polgreen', 'Sanjit A. Seshia']}, 'authorids': {'value': ['~Federico_Mora1', '~Justin_Wong1', '~Haley_Lepe1', '~Sahil_Bhatia3', '~Karim_Elmaaroufi1', '~George_Varghese1', '~Joseph_E._Gonzalez1', '~Elizabeth_Polgreen2', '~Sanjit_A._Seshia1']}, 'keywords': {'value': ['Text-to-Code', 'Low-Resource Programming Languages', 'MAX-SAT', 'Parsing', 'Program Repair']}, 'TLDR': {'value': 'Design an intermediate language and use a MAX-SAT solver to improve LLM-based text-to-code for very low resource programming langauges.'}, 'abstract': {'value': \"Recent advances in large language models (LLMs) for code applications have demonstrated remarkable zero-shot fluency and instruction following on challenging code related tasks ranging from test case generation to self-repair. Unsurprisingly, however, models struggle to compose syntactically valid programs in programming languages unrepresented in pre-training, referred to as very low-resource Programming Languages (VLPLs). VLPLs appear in crucial settings, including domain-specific languages for internal tools, tool-chains for legacy languages, and formal verification frameworks. Inspired by a technique called natural programming elicitation, we propose designing an intermediate language that LLMs ``naturally'' know how to use and which can be automatically compiled to a target VLPL. When LLMs generate code that lies outside of this intermediate language, we use compiler techniques to repair the code into programs in the intermediate language. Overall, we introduce _synthetic programming elicitation and compilation_ (SPEAC), an approach that enables LLMs to generate syntactically valid code even for VLPLs. We empirically evaluate the performance of SPEAC in a case study for the UCLID5 formal verification language and find that, compared to existing retrieval and fine-tuning baselines, SPEAC produces syntactically correct programs more frequently and without sacrificing semantic correctness.\"}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/fb296b40f1d3c4fd79e6e3ea90f07eef6954f0ea.pdf'}, 'supplementary_material': {'value': '/attachment/4ee03c6902936a232b37e15b803ce88c0751d69c.zip'}, '_bibtex': {'value': '@inproceedings{\\nmora2024synthetic,\\ntitle={Synthetic Programming Elicitation for Text-to-Code in Very Low-Resource Programming and Formal Languages},\\nauthor={Federico Mora and Justin Wong and Haley Lepe and Sahil Bhatia and Karim Elmaaroufi and George Varghese and Joseph E. Gonzalez and Elizabeth Polgreen and Sanjit A. Seshia},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kQPzFiwVIu}\\n}'}, 'paperhash': {'value': 'mora|synthetic_programming_elicitation_for_texttocode_in_very_lowresource_programming_and_formal_languages'}},forum = 'kQPzFiwVIu',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20674/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20674/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20674/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20674/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kQMyiDWbOG',number = 6081,cdate = 1715582916779,pdate = 1727287804480,odate = 1730873890444,mdate = 1730873890461,tcdate = 1715582916779,tmdate = 1730873890461,ddate = None,content = {'title': {'value': 'Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators'}, 'authors': {'value': ['Changze Lv', 'Dongqi Han', 'Yansen Wang', 'Xiaoqing Zheng', 'Xuanjing Huang', 'Dongsheng Li']}, 'authorids': {'value': ['~Changze_Lv1', '~Dongqi_Han1', '~Yansen_Wang2', '~Xiaoqing_Zheng2', '~Xuanjing_Huang1', '~Dongsheng_Li2']}, 'keywords': {'value': ['Spiking Neural Networks', 'Central Pattern Generators', 'Positional Encoding']}, 'TLDR': {'value': 'Inspired by central pattern generators, we propose a novel positional encoding technique tailored for spiking neural networks.'}, 'abstract': {'value': 'Spiking neural networks (SNNs) represent a promising approach to developing artificial neural networks that are both energy-efficient and biologically plausible.\\nHowever, applying SNNs to sequential tasks, such as text classification and time-series forecasting, has been hindered by the challenge of creating an effective and hardware-friendly spike-form positional encoding (PE) strategy.\\nDrawing inspiration from the central pattern generators (CPGs) in the human brain,  which produce rhythmic patterned outputs without requiring rhythmic inputs, we propose a novel PE technique for SNNs, termed CPG-PE.\\nWe demonstrate that the commonly used sinusoidal PE is mathematically a specific solution to the membrane potential dynamics of a particular CPG.\\nMoreover, extensive experiments across various domains, including time-series forecasting, natural language processing, and image classification, show that SNNs with CPG-PE outperform their conventional counterparts.\\nAdditionally, we perform analysis experiments to elucidate the mechanism through which SNNs encode positional information and to explore the function of CPGs in the human brain.\\nThis investigation may offer valuable insights into the fundamental principles of neural computation.'}, 'primary_area': {'value': 'neuroscience_and_cognitive_science'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/493bc45a8f861549109d3a613e0bba96314e96a9.pdf'}, 'supplementary_material': {'value': '/attachment/f7c9aa40426d0a22968f925000e80da246c8f0f8.zip'}, '_bibtex': {'value': '@inproceedings{\\nlv2024advancing,\\ntitle={Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators},\\nauthor={Changze Lv and Dongqi Han and Yansen Wang and Xiaoqing Zheng and Xuanjing Huang and Dongsheng Li},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kQMyiDWbOG}\\n}'}, 'paperhash': {'value': 'lv|advancing_spiking_neural_networks_for_sequential_modeling_with_central_pattern_generators'}},forum = 'kQMyiDWbOG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission6081/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission6081/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission6081/-/Revision', 'NeurIPS.cc/2024/Conference/Submission6081/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kQ9LgM2JQT',number = 11200,cdate = 1715702112183,pdate = 1727287963725,odate = 1730873935707,mdate = 1730873935729,tcdate = 1715702112183,tmdate = 1730873935729,ddate = None,content = {'title': {'value': 'QGFN: Controllable Greediness with Action Values'}, 'authors': {'value': ['Elaine Lau', 'Stephen Zhewen Lu', 'Ling Pan', 'Doina Precup', 'Emmanuel Bengio']}, 'authorids': {'value': ['~Elaine_Lau1', '~Stephen_Zhewen_Lu1', '~Ling_Pan1', '~Doina_Precup1', '~Emmanuel_Bengio1']}, 'keywords': {'value': ['GFlowNets', 'generative models', 'molecule design']}, 'TLDR': {'value': 'We combine a GFlowNet policy and an action-value estimate, $Q$ into mixture policies that get better rewards without losing diversity'}, 'abstract': {'value': 'Generative Flow Networks (GFlowNets; GFNs) are a family of energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, consistently biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.'}, 'primary_area': {'value': 'generative_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/12247b57958af9d2378bc7d2a6b85d40a7150ccb.pdf'}, 'supplementary_material': {'value': '/attachment/c53465f098891b8062244cda47c3e3f40cef60e5.zip'}, '_bibtex': {'value': '@inproceedings{\\nlau2024qgfn,\\ntitle={{QGFN}: Controllable Greediness with Action Values},\\nauthor={Elaine Lau and Stephen Zhewen Lu and Ling Pan and Doina Precup and Emmanuel Bengio},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kQ9LgM2JQT}\\n}'}, 'paperhash': {'value': 'lau|qgfn_controllable_greediness_with_action_values'}},forum = 'kQ9LgM2JQT',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11200/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11200/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11200/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11200/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kPmSfhCM5s',number = 1758,cdate = 1714790838176,pdate = 1727287670487,odate = 1730873851617,mdate = 1734584162854,tcdate = 1714790838176,tmdate = 1734584162854,ddate = None,content = {'title': {'value': 'Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing'}, 'authors': {'value': ['Hao Fei', 'Shengqiong Wu', 'Hanwang Zhang', 'Tat-Seng Chua', 'Shuicheng YAN']}, 'authorids': {'value': ['~Hao_Fei1', '~Shengqiong_Wu2', '~Hanwang_Zhang3', '~Tat-Seng_Chua2', '~Shuicheng_YAN3']}, 'keywords': {'value': ['Multimodal Large Language Model', 'Unified Large Language Model']}, 'abstract': {'value': 'Recent developments of vision large language models (LLMs) have seen remarkable progress, yet still encounter challenges towards multimodal generalists, such as coarse-grained instance-level understanding, lack of unified support for both images and videos, and insufficient coverage across various vision tasks. In this paper we present Vitron, a universal pixel-level vision LLM designed for comprehensive understanding, generating, segmenting, and editing of both static images and dynamic videos. Building on top of an LLM backbone, Vitron incorporates encoders for images, videos, and pixel-level regional visuals within its frontend modules, while employing state-of-the-art visual specialists as its backend, via which Vitron supports a spectrum of vision end tasks, spanning visual comprehension to visual generation, from low level to high level. To ensure an effective and precise message passing from LLM to backend modules for function invocation, we propose a novel hybrid method by simultaneously integrating discrete textual instructions and continuous signal embeddings. Further, we design various pixel-level spatiotemporal vision-language alignment learning for Vitron to reach the best fine-grained visual capability. Finally, a cross-task synergy module is advised to learn to maximize the task-invariant fine-grained visual features, enhancing the synergy between different visual tasks. Demonstrated over 12 visual tasks and evaluated across 22 datasets, Vitron showcases its extensive capabilities in the four main vision task clusters. Overall, this work illuminates the great potential of developing a more unified multimodal generalist.'}, 'primary_area': {'value': 'human-AI_interaction'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'We present a universal pixel-level vision LLM designed for comprehensive understanding, generating, segmenting, and editing of both static images and dynamic videos.'}, 'pdf': {'value': '/pdf/ece3852405e10f753140e63de9458bfcd4ffc29a.pdf'}, '_bibtex': {'value': '@inproceedings{\\nfei2024vitron,\\ntitle={Vitron: A Unified Pixel-level Vision {LLM} for Understanding, Generating, Segmenting, Editing},\\nauthor={Hao Fei and Shengqiong Wu and Hanwang Zhang and Tat-Seng Chua and Shuicheng YAN},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kPmSfhCM5s}\\n}'}, 'paperhash': {'value': 'fei|vitron_a_unified_pixellevel_vision_llm_for_understanding_generating_segmenting_editing'}},forum = 'kPmSfhCM5s',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1758/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1758/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1758/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1758/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-ND 4.0'),\n",
       " Note(id = 'kPGNE4CrTq',number = 14441,cdate = 1715750017453,pdate = 1727288069550,odate = 1730873965827,mdate = 1730873965861,tcdate = 1715750017453,tmdate = 1730873965861,ddate = None,content = {'title': {'value': 'Solving Sparse \\\\& High-Dimensional-Output Regression via Compression'}, 'authors': {'value': ['Renyuan Li', 'Zhehui Chen', 'Guanyi Wang']}, 'authorids': {'value': ['~Renyuan_Li1', '~Zhehui_Chen1', '~Guanyi_Wang1']}, 'keywords': {'value': ['multi-output regression', 'sparsity', 'compression', 'non-convex optimization']}, 'abstract': {'value': 'Multi-Output Regression (MOR) has been widely used in scientific data analysis for decision-making. Unlike traditional regression models, MOR aims to simultaneously predict multiple real-valued outputs given an input. However, the increasing dimensionality of the outputs poses significant challenges regarding interpretability and computational scalability for modern MOR applications. As a first step to address these challenges, this paper proposes a Sparse \\\\& High-dimensional-Output REgression (SHORE) model by incorporating additional sparsity requirements to resolve the output interpretability, and then designs a computationally efficient two-stage optimization framework capable of solving SHORE with provable accuracy via compression on outputs. Theoretically, we show that the proposed framework is computationally scalable while maintaining the same order of training loss and prediction loss before-and-after compression under arbitrary or relatively weak sample set conditions. Empirically, numerical results further validate the theoretical findings, showcasing the efficiency and accuracy of the proposed framework.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/e15821cac0372ce18b41ca66bf708a472e59ec0d.pdf'}, 'supplementary_material': {'value': '/attachment/16bcb72acc677a16093f604b8b666e0bc4f7f043.zip'}, '_bibtex': {'value': '@inproceedings{\\nli2024solving,\\ntitle={Solving Sparse {\\\\textbackslash}\\\\& High-Dimensional-Output Regression via Compression},\\nauthor={Renyuan Li and Zhehui Chen and Guanyi Wang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kPGNE4CrTq}\\n}'}, 'paperhash': {'value': 'li|solving_sparse_\\\\_highdimensionaloutput_regression_via_compression'}},forum = 'kPGNE4CrTq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14441/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14441/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14441/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14441/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kPBEAZU5Nm',number = 13012,cdate = 1715733452389,pdate = 1727288025603,odate = 1730873954340,mdate = 1736992626161,tcdate = 1715733452389,tmdate = 1736992626161,ddate = None,content = {'title': {'value': 'Chain of Thoughtlessness? An Analysis of CoT in Planning'}, 'authors': {'value': ['Kaya Stechly', 'Karthik Valmeekam', 'Subbarao Kambhampati']}, 'authorids': {'value': ['~Kaya_Stechly1', '~Karthik_Valmeekam1', '~Subbarao_Kambhampati1']}, 'keywords': {'value': ['LLMs', 'Planning', 'Reasoning', 'Chain of Thought']}, 'TLDR': {'value': 'We carefully examined the performance of Chain of Thought techniques on classical planning problems and found that, contrary to previous claims, they do not lead to generalizable improvement..'}, 'abstract': {'value': \"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting--a method of demonstrating solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.\\nThis paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.\\nWe also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.\\nOur results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.\"}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a82cd0364297987adf2884886b3878f92a9e4f0f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nstechly2024chain,\\ntitle={Chain of Thoughtlessness? An Analysis of CoT in Planning},\\nauthor={Kaya Stechly and Karthik Valmeekam and Subbarao Kambhampati},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kPBEAZU5Nm}\\n}'}, 'paperhash': {'value': 'stechly|chain_of_thoughtlessness_an_analysis_of_cot_in_planning'}},forum = 'kPBEAZU5Nm',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13012/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13012/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13012/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13012/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kOMrm4ZJ3m',number = 19856,cdate = 1715793123305,pdate = 1727288219574,odate = 1730873998010,mdate = 1730873998028,tcdate = 1715793123305,tmdate = 1730873998028,ddate = None,content = {'title': {'value': 'Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers'}, 'authors': {'value': ['Alberto Alfarano', 'Francois Charton', 'Amaury Hayat']}, 'authorids': {'value': ['~Alberto_Alfarano1', '~Francois_Charton1', '~Amaury_Hayat1']}, 'keywords': {'value': ['mathematics', 'Lyapunov', 'transformers', 'control', 'AI for science', 'AI for maths', 'reasoning']}, 'TLDR': {'value': 'Transformers can be trained from synthetic data to find Lyapunov functions, a long-standing open problem in mathematics'}, 'abstract': {'value': 'Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics.\\nWe consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems.\\nWe propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.'}, 'primary_area': {'value': 'machine_learning_for_other_sciences_and_fields'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c03afce8667bfd67eac3dfb7917b2dcdef937e21.pdf'}, '_bibtex': {'value': '@inproceedings{\\nalfarano2024global,\\ntitle={Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers},\\nauthor={Alberto Alfarano and Francois Charton and Amaury Hayat},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kOMrm4ZJ3m}\\n}'}, 'paperhash': {'value': 'alfarano|global_lyapunov_functions_a_longstanding_open_problem_in_mathematics_with_symbolic_transformers'}},forum = 'kOMrm4ZJ3m',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19856/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19856/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19856/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19856/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC 4.0'),\n",
       " Note(id = 'kN7GTUss0l',number = 9144,cdate = 1715675368952,pdate = 1727287902053,odate = 1730873918061,mdate = 1730873918080,tcdate = 1715675368952,tmdate = 1730873918080,ddate = None,content = {'title': {'value': 'This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization'}, 'authors': {'value': ['Anthony Bardou', 'Patrick Thiran', 'Giovanni Ranieri']}, 'authorids': {'value': ['~Anthony_Bardou1', '~Patrick_Thiran1', '~Giovanni_Ranieri1']}, 'keywords': {'value': ['Dynamic Bayesian Optimization', 'Black-Box Optimization', 'Zeroth-Order Optimization']}, 'abstract': {'value': 'Bayesian Optimization (BO) has proven to be very successful at optimizing a static, noisy, costly-to-evaluate black-box function $f : \\\\mathcal{S} \\\\to \\\\mathbb{R}$. However, optimizing a black-box which is also a function of time (*i.e.*, a *dynamic* function) $f : \\\\mathcal{S} \\\\times \\\\mathcal{T} \\\\to \\\\mathbb{R}$ remains a challenge, since a dynamic Bayesian Optimization (DBO) algorithm has to keep track of the optimum over time. This changes the nature of the optimization problem in at least three aspects: (i) querying an arbitrary point in $\\\\mathcal{S} \\\\times \\\\mathcal{T}$ is impossible, (ii) past observations become less and less relevant for keeping track of the optimum as time goes by and (iii) the DBO algorithm must have a high sampling frequency so it can collect enough relevant observations to keep track of the optimum through time. In this paper, we design a Wasserstein distance-based criterion able to quantify the relevancy of an observation with respect to future predictions. Then, we leverage this criterion to build W-DBO, a DBO algorithm able to remove irrelevant observations from its dataset on the fly, thus maintaining simultaneously a good predictive performance and a high sampling frequency, even in continuous-time optimization tasks with unknown horizon. Numerical experiments establish the superiority of W-DBO, which outperforms state-of-the-art methods by a comfortable margin.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1c922085c278e2badc969eaf12a8be24ef8316a2.pdf'}, 'supplementary_material': {'value': '/attachment/f61db0f508e21c0cfe72b8d3153a626e0e1a9856.zip'}, '_bibtex': {'value': '@inproceedings{\\nbardou2024this,\\ntitle={This Too Shall Pass: Removing Stale Observations in Dynamic Bayesian Optimization},\\nauthor={Anthony Bardou and Patrick Thiran and Giovanni Ranieri},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kN7GTUss0l}\\n}'}, 'paperhash': {'value': 'bardou|this_too_shall_pass_removing_stale_observations_in_dynamic_bayesian_optimization'}},forum = 'kN7GTUss0l',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9144/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9144/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9144/-/Revision', 'NeurIPS.cc/2024/Conference/Submission9144/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kMxdV4Blhn',number = 1528,cdate = 1714664332289,pdate = 1727287663478,odate = 1730873849549,mdate = 1730873849560,tcdate = 1714664332289,tmdate = 1730873849560,ddate = None,content = {'title': {'value': 'Rethinking 3D Convolution in $\\\\ell_p$-norm Space'}, 'authors': {'value': ['Li Zhang', 'Yan Zhong', 'Jianan Wang', 'Zhe Min', 'RujingWang', 'Liu Liu']}, 'authorids': {'value': ['~Li_Zhang25', '~Yan_Zhong2', '~Jianan_Wang2', '~Zhe_Min2', '~RujingWang1', '~Liu_Liu13']}, 'keywords': {'value': ['$\\\\ell_p$-norm; 3D Convolution']}, 'abstract': {'value': 'Convolution is a fundamental operation in the 3D backbone. However, under certain conditions, the feature extraction ability of traditional convolution methods may be weakened. In this paper, we introduce a new convolution method based on  $\\\\ell_p$-norm. \\nFor theoretical support, we prove the universal approximation theorem for $\\\\ell_p$-norm based convolution, and analyze the robustness and feasibility of  $\\\\ell_p$-norms in 3D point cloud tasks. Concretely, $\\\\ell_{\\\\infty}$-norm based convolution is prone to feature loss. $\\\\ell_2$-norm based convolution is essentially a linear transformation of the traditional convolution.  $\\\\ell_1$-norm based convolution is an economical and effective feature extractor. We propose customized optimization strategies to accelerate the training process of $\\\\ell_1$-norm based Nets and enhance the performance. Besides, a theoretical guarantee is given for the convergence by \\\\textit{regret} argument. We apply our methods to classic networks and conduct related experiments. Experimental results indicate that our approach exhibits competitive performance with traditional CNNs, with lower energy consumption and instruction latency.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/78929bbc3eea171f200078a0b1108595b20ad678.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024rethinking,\\ntitle={Rethinking 3D Convolution in \\\\${\\\\textbackslash}ell\\\\_p\\\\$-norm Space},\\nauthor={Li Zhang and Yan Zhong and Jianan Wang and Zhe Min and RujingWang and Liu Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kMxdV4Blhn}\\n}'}, 'paperhash': {'value': 'zhang|rethinking_3d_convolution_in_\\\\ell_pnorm_space'}},forum = 'kMxdV4Blhn',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1528/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1528/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1528/-/Revision', 'NeurIPS.cc/2024/Conference/-/PC_Revision', 'NeurIPS.cc/2024/Conference/Submission1528/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kMnoh7CXrq',number = 5685,cdate = 1715565272448,pdate = 1727287791866,odate = 1730873887314,mdate = 1730873887326,tcdate = 1715565272448,tmdate = 1730873887326,ddate = None,content = {'title': {'value': 'DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging'}, 'authors': {'value': ['Matteo Pagliardini', 'Amirkeivan Mohtashami', 'François Fleuret', 'Martin Jaggi']}, 'authorids': {'value': ['~Matteo_Pagliardini1', '~Amirkeivan_Mohtashami1', '~François_Fleuret2', '~Martin_Jaggi1']}, 'keywords': {'value': ['Transformer architecture', 'Large language models']}, 'TLDR': {'value': 'We propose a novel architecture called DenseFormer which outperforms the classical Transformer in many ways'}, 'abstract': {'value': 'The transformer architecture by Vaswani et al. (2017) is now ubiquitous across application domains, from natural language processing to speech processing and image understanding. We propose DenseFormer, a simple modification to the standard architecture that improves the perplexity of the model without increasing its size---adding a few thousand parameters for large-scale models in the 100B parameters range. Our approach relies on an additional averaging step after each transformer block, which computes a weighted average of current and past representations---we refer to this operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit coherent patterns of information flow, revealing the strong and structured reuse of activations from distant layers. Experiments demonstrate that DenseFormer is more data efficient, reaching the same perplexity of much deeper transformer models, and that for the same perplexity, these new models outperform transformer baselines in terms of memory efficiency and inference time.'}, 'primary_area': {'value': 'deep_learning_architectures'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/03cba71ba6f566405b4789c98f7c477405d8231d.pdf'}, '_bibtex': {'value': '@inproceedings{\\npagliardini2024denseformer,\\ntitle={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},\\nauthor={Matteo Pagliardini and Amirkeivan Mohtashami and Fran{\\\\c{c}}ois Fleuret and Martin Jaggi},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kMnoh7CXrq}\\n}'}, 'paperhash': {'value': 'pagliardini|denseformer_enhancing_information_flow_in_transformers_via_depth_weighted_averaging'}},forum = 'kMnoh7CXrq',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5685/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5685/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5685/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5685/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kMAXN7HF6d',number = 11135,cdate = 1715701321561,pdate = 1727287961597,odate = 1730873935244,mdate = 1730873935264,tcdate = 1715701321561,tmdate = 1730873935264,ddate = None,content = {'title': {'value': 'Fairness and Efficiency in Online Class Matching'}, 'authors': {'value': ['MohammadTaghi Hajiaghayi', 'Shayan Chashm Jahan', 'Mohammad Sharifi', 'Suho Shin', 'Max Springer']}, 'authorids': {'value': ['~MohammadTaghi_Hajiaghayi1', '~Shayan_Chashm_Jahan1', '~Mohammad_Sharifi1', '~Suho_Shin1', '~Max_Springer1']}, 'keywords': {'value': ['online algorithm', 'matching', 'envy-free', 'fair allocation', 'fairness']}, 'abstract': {'value': 'The online bipartite matching problem, extensively studied in the literature, deals with the allocation of online arriving vertices (items) to a predetermined set of offline vertices (agents). However, little attention has been given to the concept of class fairness, where agents are categorized into different classes, and the matching algorithm must ensure equitable distribution across these classes.\\n\\nWe here focus on randomized algorithms for the fair matching of indivisible items, subject to various definitions of fairness. Our main contribution is the first (randomized) non-wasteful algorithm that simultaneously achieves a $1/2$ approximation to class envy-freeness (CEF) while simultaneously ensuring an equivalent approximation to the class proportionality (CPROP) and utilitarian social welfare (USW) objectives. We supplement this result by demonstrating that no non-wasteful algorithm can achieve an $\\\\alpha$-CEF guarantee for $\\\\alpha > 0.761$. In a similar vein, we provide a novel input instance for deterministic divisible matching that demonstrates a nearly tight CEF approximation.\\n\\nLastly, we define the ``price of fairness,\" which represents the trade-off between optimal and fair matching. We demonstrate that increasing the level of fairness in the approximation of the solution leads to a decrease in the objective of maximizing USW, following an inverse proportionality relationship.'}, 'primary_area': {'value': 'algorithmic_game_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/47d12f3d34deef31f170d34a952125b9823ca7e2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nhajiaghayi2024fairness,\\ntitle={Fairness and Efficiency in Online Class Matching},\\nauthor={MohammadTaghi Hajiaghayi and Shayan Chashm Jahan and Mohammad Sharifi and Suho Shin and Max Springer},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kMAXN7HF6d}\\n}'}, 'paperhash': {'value': 'hajiaghayi|fairness_and_efficiency_in_online_class_matching'}},forum = 'kMAXN7HF6d',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11135/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11135/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11135/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11135/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kLiWXUdCEw',number = 2760,cdate = 1715157577821,pdate = 1727287699476,odate = 1730873860681,mdate = 1736754638078,tcdate = 1715157577821,tmdate = 1736754638078,ddate = None,content = {'title': {'value': 'An Analysis of Elo Rating Systems via Markov Chains'}, 'authors': {'value': ['Sam Olesker-Taylor', 'Luca Zanetti']}, 'authorids': {'value': ['~Sam_Olesker-Taylor1', '~Luca_Zanetti1']}, 'keywords': {'value': ['Elo ratings', 'Bradley–Terry–Luce model', 'tournament design', 'concentration']}, 'TLDR': {'value': 'We present an analysis of Elo rating systems under the Bradley–Terry–Luce model'}, 'abstract': {'value': 'We present a theoretical analysis of the Elo rating system, a popular method for ranking skills of players in an online setting. In particular, we study Elo under the Bradley-Terry-Luce model and, using techniques from Markov chain theory, show that Elo learns the model parameters at a rate competitive with the state-of-the-art. We apply our results to the problem of efficient tournament design and discuss a connection with the fastest-mixing Markov chain problem.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d223385188df584cf3838cc3ee9ba1f0eb510f9e.pdf'}, 'supplementary_material': {'value': '/attachment/df2c1946551193c0450aec01235eedd4c20cc536.zip'}, '_bibtex': {'value': '@inproceedings{\\nolesker-taylor2024an,\\ntitle={An Analysis of Elo Rating Systems via Markov Chains},\\nauthor={Sam Olesker-Taylor and Luca Zanetti},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kLiWXUdCEw}\\n}'}, 'paperhash': {'value': 'oleskertaylor|an_analysis_of_elo_rating_systems_via_markov_chains'}},forum = 'kLiWXUdCEw',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission2760/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission2760/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission2760/-/Revision', 'NeurIPS.cc/2024/Conference/Submission2760/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kLen1XyW6P',number = 19440,cdate = 1715790962519,pdate = 1727288209779,odate = 1730873995543,mdate = 1734551891102,tcdate = 1715790962519,tmdate = 1734551891102,ddate = None,content = {'title': {'value': 'On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models'}, 'authors': {'value': ['Aditya Bhaskara', 'Agastya Vibhuti Jha', 'Michael Kapralov', 'Naren Sarayu Manoj', 'Davide Mazzali', 'Weronika Wrzos-Kaminska']}, 'authorids': {'value': ['~Aditya_Bhaskara1', '~Agastya_Vibhuti_Jha1', '~Michael_Kapralov1', '~Naren_Sarayu_Manoj1', '~Davide_Mazzali1', '~Weronika_Wrzos-Kaminska1']}, 'keywords': {'value': ['stochastic block model', 'clustering', 'spectral algorithms', 'random matrices', 'semirandom model', 'monotone adversary']}, 'TLDR': {'value': 'We investigate the robustness of spectral clustering algorithms against various semirandom adversaries.'}, 'abstract': {'value': \"In a graph bisection problem, we are given a graph $G$ with two equally-sized unlabeled communities, and the goal is to recover the vertices in these communities. A popular heuristic, known as spectral clustering, is to output an estimated community assignment based on the eigenvector corresponding to the second-smallest eigenvalue of the Laplacian of $G$. Spectral algorithms can be shown to provably recover the cluster structure for graphs generated from probabilistic models, such as the Stochastic Block Model (SBM). However, spectral clustering is known to be non-robust to model mis-specification. Techniques based on semidefinite programming have been shown to be more robust, but they incur significant computational overheads. \\n\\nIn this work, we study the robustness of spectral algorithms against semirandom adversaries. Informally, a semirandom adversary is allowed to ``helpfully'' change the specification of the model in a way that is consistent with the ground-truth solution. Our semirandom adversaries in particular are allowed to add edges inside clusters or increase the probability that an edge appears inside a cluster. Semirandom adversaries are a useful tool to determine the extent to which an algorithm has overfit to statistical assumptions on the input. \\n\\nOn the positive side, we identify a wide range of semirandom adversaries under which spectral bisection using the _unnormalized_ Laplacian is strongly consistent, i.e., it exactly recovers the planted partitioning. On the negative side, we show that in many of these settings, _normalized_ spectral bisection outputs a partitioning that makes a classification mistake on a constant fraction of the vertices. Finally, we demonstrate numerical experiments that complement our theoretical findings.\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/5a9d1312a2385c283f86a5a81131f54ed15f0605.pdf'}, 'supplementary_material': {'value': '/attachment/0399ab625dad65f487cf574fec96aad101c9c293.zip'}, '_bibtex': {'value': '@inproceedings{\\nbhaskara2024on,\\ntitle={On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models},\\nauthor={Aditya Bhaskara and Agastya Vibhuti Jha and Michael Kapralov and Naren Sarayu Manoj and Davide Mazzali and Weronika Wrzos-Kaminska},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kLen1XyW6P}\\n}'}, 'paperhash': {'value': 'bhaskara|on_the_robustness_of_spectral_algorithms_for_semirandom_stochastic_block_models'}},forum = 'kLen1XyW6P',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19440/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19440/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19440/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19440/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kK23oMGe9g',number = 20882,cdate = 1715798913270,pdate = 1727288243965,odate = 1730874003709,mdate = 1730874003727,tcdate = 1715798913270,tmdate = 1730874003727,ddate = None,content = {'title': {'value': 'Immiscible Diffusion: Accelerating Diffusion Training with Noise Assignment'}, 'authors': {'value': ['Yiheng Li', 'Heyang Jiang', 'Akio Kodaira', 'Masayoshi Tomizuka', 'Kurt Keutzer', 'Chenfeng Xu']}, 'authorids': {'value': ['~Yiheng_Li2', '~Heyang_Jiang2', '~Akio_Kodaira1', '~Masayoshi_Tomizuka2', '~Kurt_Keutzer1', '~Chenfeng_Xu1']}, 'keywords': {'value': ['Diffusion Model', 'Training Efficiency']}, 'TLDR': {'value': 'We propose Immiscible Diffusion to increase the training efficiency up to 3x with only one line of code by noise assignment.'}, 'abstract': {'value': 'In this paper, we point out that suboptimal noise-data mapping leads to slow training of diffusion models. During diffusion training, current methods diffuse each image across the entire noise space, resulting in a mixture of all images at every point in the noise layer. We emphasize that this random mixture of noise-data mapping complicates the optimization of the denoising function in diffusion models. Drawing inspiration from the immiscibility phenomenon in physics, we propose *Immiscible Diffusion*, a simple and effective method to improve the random mixture of noise-data mapping. In physics, miscibility can vary according to various intermolecular forces. Thus, immiscibility means that the mixing of molecular sources is distinguishable. Inspired by this concept, we propose an assignment-then-diffusion training strategy to achieve *Immiscible Diffusion*. As one example, prior to diffusing the image data into noise, we assign diffusion target noise for the image data by minimizing the total image-noise pair distance in a mini-batch. The assignment functions analogously to external forces to expel the diffuse-able areas of images, thus mitigating the inherent difficulties in diffusion training. Our approach is remarkably simple, requiring only *one line of code* to restrict the diffuse-able area for each image while preserving the Gaussian distribution of noise. In this way, each image is preferably projected to nearby noise. To address the high complexity of the assignment algorithm, we employ a quantized assignment strategy, which significantly reduces the computational overhead to a negligible level (e.g. 22.8ms for a large batch size of 1024 on an A6000). Experiments demonstrate that our method can achieve up to 3x faster training for unconditional Consistency Models on the CIFAR dataset, as well as for DDIM and Stable Diffusion on CelebA and ImageNet dataset, and in class-conditional training and fine-tuning. In addition, we conducted a thorough analysis that sheds light on how it improves diffusion training speed while improving fidelity. The code is available at https://yhli123.github.io/immiscible-diffusion'}, 'primary_area': {'value': 'diffusion_based_models'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/8093e21d1d0b2604a4b881a70ce04e78aa527088.pdf'}, '_bibtex': {'value': '@inproceedings{\\nli2024immiscible,\\ntitle={Immiscible Diffusion: Accelerating Diffusion Training with Noise Assignment},\\nauthor={Yiheng Li and Heyang Jiang and Akio Kodaira and Masayoshi Tomizuka and Kurt Keutzer and Chenfeng Xu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kK23oMGe9g}\\n}'}, 'paperhash': {'value': 'li|immiscible_diffusion_accelerating_diffusion_training_with_noise_assignment'}},forum = 'kK23oMGe9g',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20882/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20882/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20882/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20882/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kJzecLYsRi',number = 8440,cdate = 1715660527276,pdate = 1727287879627,odate = 1730873912217,mdate = 1730873912228,tcdate = 1715660527276,tmdate = 1730873912228,ddate = None,content = {'title': {'value': 'On the Saturation Effects of Spectral Algorithms in Large Dimensions'}, 'authors': {'value': ['Weihao Lu', 'Haobo Zhang', 'Yicheng Li', 'Qian Lin']}, 'authorids': {'value': ['~Weihao_Lu2', '~Haobo_Zhang2', '~Yicheng_Li2', '~Qian_Lin2']}, 'keywords': {'value': ['reproducing kernel Hilbert space', 'spectral algorithm', 'high-dimensional statistics', 'minimax rates']}, 'TLDR': {'value': 'We fully depict the saturation effects of spectral algorithms and reveal a new phenomenon in large dimensional settings.'}, 'abstract': {'value': \"The saturation effects, which originally refer to the fact that kernel ridge regression (KRR) fails to achieve the information-theoretical lower bound when the regression function is over-smooth, have been observed for almost 20 years and were rigorously proved recently for kernel ridge regression and some other spectral algorithms over a fixed dimensional domain. The main focus of this paper is to explore the saturation effects for a large class of spectral algorithms (including the KRR, gradient descent, etc.) in large dimensional settings where $n \\\\asymp d^{\\\\gamma}$. More precisely, we first propose an improved minimax lower bound for the kernel regression problem in large dimensional settings and show that the gradient flow with early stopping strategy will result in an estimator achieving this lower bound (up to a logarithmic factor). Similar to the results in KRR, we can further determine the exact convergence rates (both upper and lower bounds) of a large class of (optimal tuned) spectral algorithms with different qualification $\\\\tau$'s. In particular, we find that these exact rate curves (varying along $\\\\gamma$) exhibit the periodic plateau behavior and the polynomial approximation barrier. Consequently, we can fully depict the saturation effects of the spectral algorithms and reveal a new phenomenon in large dimensional settings (i.e.,  the saturation effect occurs in large dimensional setting as long as the source condition $s>\\\\tau$ while it occurs in fixed dimensional setting as long as $s>2\\\\tau$).\"}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3d74bcf2f80ed19d7bee6b2e745383d8139847da.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlu2024on,\\ntitle={On the Saturation Effects of Spectral Algorithms in Large Dimensions},\\nauthor={Weihao Lu and Haobo Zhang and Yicheng Li and Qian Lin},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kJzecLYsRi}\\n}'}, 'paperhash': {'value': 'lu|on_the_saturation_effects_of_spectral_algorithms_in_large_dimensions'}},forum = 'kJzecLYsRi',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission8440/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission8440/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission8440/-/Revision', 'NeurIPS.cc/2024/Conference/Submission8440/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kJkp2ECJT7',number = 3507,cdate = 1715294023687,pdate = 1727287723101,odate = 1730873867106,mdate = 1730873867118,tcdate = 1715294023687,tmdate = 1730873867118,ddate = None,content = {'title': {'value': 'Towards Flexible Visual Relationship Segmentation'}, 'authors': {'value': ['Fangrui Zhu', 'Jianwei Yang', 'Huaizu Jiang']}, 'authorids': {'value': ['~Fangrui_Zhu1', '~Jianwei_Yang1', '~Huaizu_Jiang1']}, 'keywords': {'value': ['Visual relationship segmentation', 'relationship understanding', 'human-object interaction', 'scene graph generation.']}, 'abstract': {'value': 'Visual relationship understanding has been studied separately in human-object interaction(HOI) detection, scene graph generation(SGG),  and referring relationships(RR) tasks. \\nGiven the complexity and interconnectedness of these tasks, it is crucial to have a flexible framework that can effectively address these tasks in a cohesive manner.\\nIn this work, we propose FleVRS, a single model that seamlessly integrates the above three aspects in standard and promptable visual relationship segmentation, and further possesses the capability for open-vocabulary segmentation to adapt to novel scenarios. \\nFleVRS leverages the synergy between text and image modalities, \\nto ground various types of relationships from images and use textual features from vision-language models to visual conceptual understanding.\\nEmpirical validation across various datasets demonstrates that our framework outperforms existing models in standard, promptable, and open-vocabulary tasks, e.g., +1.9 $mAP$ on HICO-DET, +11.4 $Acc$ on VRD,  +4.7 $mAP$ on unseen HICO-DET.\\nOur FleVRS represents a significant step towards a more intuitive, comprehensive, and scalable understanding of visual relationships.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/b7afe6940b58cb99a3ef4a9c95a315a93cb6550f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhu2024towards,\\ntitle={Towards Flexible Visual Relationship Segmentation},\\nauthor={Fangrui Zhu and Jianwei Yang and Huaizu Jiang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kJkp2ECJT7}\\n}'}, 'paperhash': {'value': 'zhu|towards_flexible_visual_relationship_segmentation'}},forum = 'kJkp2ECJT7',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3507/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3507/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3507/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3507/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kHXUb494SY',number = 7727,cdate = 1715635362795,pdate = 1727287856601,odate = 1730873905637,mdate = 1730873905654,tcdate = 1715635362795,tmdate = 1730873905654,ddate = None,content = {'title': {'value': 'Nesterov acceleration despite very noisy gradients'}, 'authors': {'value': ['Kanan Gupta', 'Jonathan W. Siegel', 'Stephan Wojtowytsch']}, 'authorids': {'value': ['~Kanan_Gupta1', '~Jonathan_W._Siegel1', '~Stephan_Wojtowytsch1']}, 'keywords': {'value': ['Stochastic optimization', 'stochastic acceleration', 'smooth convex optimization', 'deep learning', 'accelerated gradient descent']}, 'abstract': {'value': \"We present a generalization of Nesterov's accelerated gradient descent algorithm. Our algorithm (AGNES) provably achieves acceleration for smooth convex and strongly convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to the magnitude of the gradient at every point. Nesterov's method converges at an accelerated rate if the constant of proportionality is below 1, while AGNES accommodates any signal-to-noise ratio. The noise model is motivated by applications in overparametrized machine learning. AGNES requires only two parameters in convex and three in strongly convex minimization tasks, improving on existing methods. We further provide clear geometric interpretations and heuristics for the choice of parameters.\"}, 'primary_area': {'value': 'optimization'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/ca4c88f389c842b4ad3fdfef8bc336b23ad0e69a.pdf'}, 'supplementary_material': {'value': '/attachment/f49fa8b536ef571a70e687b220d38f52b5925d56.zip'}, '_bibtex': {'value': '@inproceedings{\\ngupta2024nesterov,\\ntitle={Nesterov acceleration despite very noisy gradients},\\nauthor={Kanan Gupta and Jonathan W. Siegel and Stephan Wojtowytsch},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kHXUb494SY}\\n}'}, 'paperhash': {'value': 'gupta|nesterov_acceleration_despite_very_noisy_gradients'}},forum = 'kHXUb494SY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission7727/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission7727/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission7727/-/Revision', 'NeurIPS.cc/2024/Conference/Submission7727/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'kEQFjKqiqM',number = 13553,cdate = 1715740709615,pdate = 1727288042842,odate = 1730873958646,mdate = 1730873958665,tcdate = 1715740709615,tmdate = 1730873958665,ddate = None,content = {'title': {'value': 'Distributed-Order Fractional Graph Operating Network'}, 'authors': {'value': ['Kai Zhao', 'Xuhao Li', 'Qiyu Kang', 'Feng Ji', 'Qinxu Ding', 'Yanan Zhao', 'Wenfei Liang', 'Wee Peng Tay']}, 'authorids': {'value': ['~Kai_Zhao7', '~Xuhao_Li2', '~Qiyu_Kang2', '~Feng_Ji2', '~Qinxu_Ding1', '~Yanan_Zhao1', '~Wenfei_Liang1', '~Wee_Peng_Tay1']}, 'keywords': {'value': ['Graph Neural Networks']}, 'abstract': {'value': \"We introduce the Distributed-order fRActional Graph Operating Network (DRAGON), a novel continuous Graph Neural Network (GNN) framework that incorporates distributed-order fractional calculus. \\nUnlike traditional continuous GNNs that utilize integer-order or single fractional-order differential equations, DRAGON uses a learnable probability distribution over a range of real numbers for the derivative orders. \\nBy allowing a flexible and learnable superposition of multiple derivative orders, our framework captures complex graph feature updating dynamics beyond the reach of conventional models.\\nWe provide a comprehensive interpretation of our framework's capability to capture intricate dynamics through the lens of a non-Markovian graph random walk with node feature updating driven by an anomalous diffusion process over the graph. \\nFurthermore, to highlight the versatility of the DRAGON framework, we conduct empirical evaluations across a range of graph learning tasks. The results consistently demonstrate superior performance when compared to traditional continuous GNN models. The implementation code is available at \\\\url{https://github.com/zknus/NeurIPS-2024-DRAGON}.\"}, 'primary_area': {'value': 'graph_neural_networks'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c82624477e33c3427be1e4c8b9093f4e5714ff96.pdf'}, 'supplementary_material': {'value': '/attachment/8e577907221cc2158198adc675c35a2bdd03147e.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024distributedorder,\\ntitle={Distributed-Order Fractional Graph Operating Network},\\nauthor={Kai Zhao and Xuhao Li and Qiyu Kang and Feng Ji and Qinxu Ding and Yanan Zhao and Wenfei Liang and Wee Peng Tay},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kEQFjKqiqM}\\n}'}, 'paperhash': {'value': 'zhao|distributedorder_fractional_graph_operating_network'}},forum = 'kEQFjKqiqM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission13553/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission13553/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission13553/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission13553/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kEPpD7yETM',number = 9835,cdate = 1715686462605,pdate = 1727287922233,odate = 1730873924002,mdate = 1735898868926,tcdate = 1715686462605,tmdate = 1735898868926,ddate = None,content = {'title': {'value': 'Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach'}, 'authors': {'value': ['Weiyu Ma', 'Qirui Mi', 'Yongcheng Zeng', 'Xue Yan', 'Runji Lin', 'Yuqiao Wu', 'Jun Wang', 'Haifeng Zhang']}, 'authorids': {'value': ['~Weiyu_Ma1', '~Qirui_Mi1', '~Yongcheng_Zeng1', '~Xue_Yan2', '~Runji_Lin1', '~Yuqiao_Wu1', '~Jun_Wang2', '~Haifeng_Zhang3']}, 'keywords': {'value': ['LLM Agent', 'StarCraft2']}, 'TLDR': {'value': 'large language models play starcraft2'}, 'abstract': {'value': \"With the continued advancement of Large Language Models (LLMs) Agents in reasoning, planning, and decision-making, benchmarks have become crucial in evaluating these skills. However, there is a notable gap in benchmarks for real-time strategic decision-making. StarCraft II (SC2), with its complex and dynamic nature, serves as an ideal setting for such evaluations. To this end, we have developed TextStarCraft II, a specialized environment for assessing LLMs in real-time strategic scenarios within SC2. Addressing the limitations of traditional Chain of Thought (CoT) methods, we introduce the Chain of Summarization (CoS) method, enhancing LLMs' capabilities in rapid and effective decision-making. Our key experiments included:\\n1. LLM Evaluation: Tested 10 LLMs in TextStarCraft II, most of them defeating LV5 build-in AI, showcasing effective strategy skills.\\n2. Commercial Model Knowledge: Evaluated four commercial  models on SC2 knowledge; GPT-4 ranked highest by Grandmaster-level experts.\\n3. Human-AI Matches: Experimental results showed that fine-tuned LLMs performed on par with Gold-level players in real-time matches, demonstrating comparable strategic abilities.\\n\\nAll code and data from this\\nstudy have been made pulicly available at https://github.com/histmeisah/Large-Language-Models-play-StarCraftII\"}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/f00682bc7e1756fbaf5b9deed1c49567ba4f89a8.pdf'}, 'supplementary_material': {'value': '/attachment/89cde7ab347ffb6e79d01f016701ab029bc42f44.zip'}, '_bibtex': {'value': '@inproceedings{\\nma2024large,\\ntitle={Large Language Models Play StarCraft {II}:Benchmarks and A Chain of Summarization Approach},\\nauthor={Weiyu Ma and Qirui Mi and Yongcheng Zeng and Xue Yan and Runji Lin and Yuqiao Wu and Jun Wang and Haifeng Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kEPpD7yETM}\\n}'}, 'paperhash': {'value': 'ma|large_language_models_play_starcraft_iibenchmarks_and_a_chain_of_summarization_approach'}},forum = 'kEPpD7yETM',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission9835/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission9835/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission9835/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission9835/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'kCabCEhQWv',number = 3013,cdate = 1715206566931,pdate = 1727287707849,odate = 1730873862990,mdate = 1730873863009,tcdate = 1715206566931,tmdate = 1730873863009,ddate = None,content = {'title': {'value': 'Neural Isometries: Taming Transformations for Equivariant ML'}, 'authors': {'value': ['Thomas Mitchel', 'Michael Taylor', 'Vincent Sitzmann']}, 'authorids': {'value': ['~Thomas_Mitchel1', '~Michael_Taylor5', '~Vincent_Sitzmann1']}, 'keywords': {'value': ['Equivariance', 'Geometric Deep Learning', 'Representation Learning']}, 'abstract': {'value': 'Real-world geometry and 3D vision tasks are replete with challenging symmetries that defy tractable analytical expression. In this paper, we introduce Neural Isometries, an autoencoder framework which learns to map the observation space to a general-purpose latent space wherein encodings are related by isometries whenever their corresponding observations are geometrically related in world space. Specifically, we regularize the latent space such that maps between encodings preserve a learned inner product and commute with a learned functional operator, in the same manner as rigid-body transformations commute with the Laplacian. This approach forms an effective backbone for self-supervised representation learning, and we demonstrate that a simple off-the-shelf equivariant network operating in the pre-trained latent space can achieve results on par with meticulously-engineered, handcrafted networks designed to handle complex, nonlinear symmetries. Furthermore, isometric maps capture information about the respective transformations in world space, and we show that this allows us to regress camera poses directly from the coefficients of the maps between encodings of adjacent views of a scene.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'Neural Isometries find latent spaces where complicated transformations become tractable for downstream tasks.'}, 'pdf': {'value': '/pdf/8c568b63a9a2879a066eefc227b8733dc6a6a1f2.pdf'}, '_bibtex': {'value': '@inproceedings{\\nmitchel2024neural,\\ntitle={Neural Isometries: Taming Transformations for Equivariant {ML}},\\nauthor={Thomas Mitchel and Michael Taylor and Vincent Sitzmann},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=kCabCEhQWv}\\n}'}, 'paperhash': {'value': 'mitchel|neural_isometries_taming_transformations_for_equivariant_ml'}},forum = 'kCabCEhQWv',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission3013/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission3013/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission3013/-/Revision', 'NeurIPS.cc/2024/Conference/Submission3013/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k9uZfaeerK',number = 4023,cdate = 1715350807246,pdate = 1727287737819,odate = 1730873871543,mdate = 1730873871559,tcdate = 1715350807246,tmdate = 1730873871559,ddate = None,content = {'title': {'value': 'UQ-Guided Hyperparameter Optimization for Iterative Learners'}, 'authors': {'value': ['Jiesong Liu', 'Feng Zhang', 'Jiawei Guan', 'Xipeng Shen']}, 'authorids': {'value': ['~Jiesong_Liu1', '~Feng_Zhang10', '~Jiawei_Guan1', '~Xipeng_Shen1']}, 'keywords': {'value': ['Uncertainty quantification; Hyperparameter Optimization; iterative learners']}, 'abstract': {'value': 'Hyperparameter Optimization (HPO) plays a pivotal role in unleashing the potential of iterative machine learning models. This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training. The paper introduces the concept of uncertainty-aware HPO and presents a novel approach called the UQ-guided scheme for quantifying uncertainty. This scheme offers a principled and versatile method to empower HPO techniques in handling model uncertainty during their exploration of the candidate space.\\nBy constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters. It achieves a notable performance improvement of over 50\\\\% in terms of accuracy regret and exploration time.'}, 'primary_area': {'value': 'optimization_for_deep_networks'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/c8864263288112957724326a0561151ae2fa6186.pdf'}, 'supplementary_material': {'value': '/attachment/cfb4339619a9df8a4603a78d9ae286cc7525f52c.zip'}, '_bibtex': {'value': '@inproceedings{\\nliu2024uqguided,\\ntitle={{UQ}-Guided Hyperparameter Optimization for Iterative Learners},\\nauthor={Jiesong Liu and Feng Zhang and Jiawei Guan and Xipeng Shen},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k9uZfaeerK}\\n}'}, 'paperhash': {'value': 'liu|uqguided_hyperparameter_optimization_for_iterative_learners'}},forum = 'k9uZfaeerK',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4023/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4023/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4023/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4023/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k9SH68MvJs',number = 1667,cdate = 1714739566350,pdate = 1727287667861,odate = 1730873850972,mdate = 1730873850990,tcdate = 1714739566350,tmdate = 1730873850990,ddate = None,content = {'title': {'value': 'Diffusion-Reward Adversarial Imitation Learning'}, 'authors': {'value': ['Chun-Mao Lai', 'Hsiang-Chun Wang', 'Ping-Chun Hsieh', 'Yu-Chiang Frank Wang', 'Min-Hung Chen', 'Shao-Hua Sun']}, 'authorids': {'value': ['~Chun-Mao_Lai1', '~Hsiang-Chun_Wang1', '~Ping-Chun_Hsieh1', '~Yu-Chiang_Frank_Wang2', '~Min-Hung_Chen2', '~Shao-Hua_Sun1']}, 'keywords': {'value': ['Imitation Learning', 'Adversarial Imitation Learning', 'Diffusion Model']}, 'TLDR': {'value': 'This work proposes a novel adversarial imitation learning framework that integrates a diffusion model into generative adversarial imitation learning.'}, 'abstract': {'value': 'Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, we propose Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more robust and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator, and design diffusion rewards based on the classifier’s output for policy learning. Extensive experiments are conducted in navigation, manipulation, and locomotion, verifying DRAIL’s effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more robust and smoother rewards. Project page: https://nturobotlearninglab.github.io/DRAIL/'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/066237eeddc2c1e8b6468563b612bea809593220.pdf'}, '_bibtex': {'value': '@inproceedings{\\nlai2024diffusionreward,\\ntitle={Diffusion-Reward Adversarial Imitation Learning},\\nauthor={Chun-Mao Lai and Hsiang-Chun Wang and Ping-Chun Hsieh and Yu-Chiang Frank Wang and Min-Hung Chen and Shao-Hua Sun},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k9SH68MvJs}\\n}'}, 'paperhash': {'value': 'lai|diffusionreward_adversarial_imitation_learning'}},forum = 'k9SH68MvJs',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1667/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1667/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1667/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1667/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k9PXsryuWG',number = 1720,cdate = 1714761058722,pdate = 1727287669354,odate = 1730873851395,mdate = 1730873851406,tcdate = 1714761058722,tmdate = 1730873851406,ddate = None,content = {'title': {'value': 'Metric Transforms and Low Rank Representations of Kernels for Fast Attention'}, 'authors': {'value': ['Timothy Zer-An Chu', 'Josh Alman', 'Gary Miller', 'Shyam Narayanan', 'Mark Sellke', 'Zhao Song']}, 'authorids': {'value': ['~Timothy_Zer-An_Chu1', '~Josh_Alman1', '~Gary_Miller1', '~Shyam_Narayanan1', '~Mark_Sellke1', '~Zhao_Song3']}, 'keywords': {'value': ['hardness', 'impossibility', 'low rank transform', 'kernel method', 'LLM', 'attention']}, 'abstract': {'value': 'We introduce a new linear-algebraic tool based on group representation theory, and use it to address three key problems in machine learning.\\n\\n1. Past researchers have proposed fast attention algorithms for LLMs by approximating or replace softmax attention with other functions, such as low-degree polynomials. The key property of these functions is that, when applied entry-wise to the matrix $QK^{\\\\top}$, the result is a low rank matrix when $Q$ and $K$ are $n \\\\times d$ matrices and $n \\\\gg d$. This suggests a natural question: what are all functions $f$ with this property? If other $f$ exist and are quickly computable, they can be used in place of softmax for fast subquadratic attention algorithms. It was previously known that low-degree polynomials have this property. We prove that low-degree polynomials are the only piecewise continuous functions with this property. This suggests that the low-rank fast attention only works for functions approximable by polynomials. Our work gives a converse to the polynomial method in algorithm design.\\n\\n2. We prove the first full classification of all positive definite kernels that are functions of Manhattan or $\\\\ell_1$ distance. Our work generalizes an existing theorem at the heart of all kernel methods in machine learning: the classification of all positive definite kernels that are functions of Euclidean distance. \\n\\n3. The key problem in metric transforms, a mathematical theory used in geometry and machine learning, asks what functions transform pairwise distances in semi-metric space $M$ to semi-metric space $N$ for specified $M$ and $N$. We provide the first full classification of functions that transform Manhattan distances to Manhattan distances. Our work generalizes the foundational work of Schoenberg, which fully classifies functions that transform Euclidean to Euclidean distances.\\n \\nWe additionally prove results about stable-rank preserving functions that are potentially useful in algorithmic design, and more. Our core new tool is called the representation theory of the hyperrectangle.'}, 'primary_area': {'value': 'learning_theory'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'TLDR': {'value': 'This paper studies the possibility and impossibility for kernel methods and metric transform'}, 'pdf': {'value': '/pdf/f786c80be037eb55165d1a67bfb4db0a35bff5f5.pdf'}, '_bibtex': {'value': '@inproceedings{\\nchu2024metric,\\ntitle={Metric Transforms and Low Rank Representations of Kernels for Fast Attention},\\nauthor={Timothy Zer-An Chu and Josh Alman and Gary Miller and Shyam Narayanan and Mark Sellke and Zhao Song},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k9PXsryuWG}\\n}'}, 'paperhash': {'value': 'chu|metric_transforms_and_low_rank_representations_of_kernels_for_fast_attention'}},forum = 'k9PXsryuWG',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1720/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1720/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1720/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1720/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY-NC-SA 4.0'),\n",
       " Note(id = 'k8AYft5ED1',number = 5740,cdate = 1715567849773,pdate = 1727287793424,odate = 1730873887713,mdate = 1730873887725,tcdate = 1715567849773,tmdate = 1730873887725,ddate = None,content = {'title': {'value': 'Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation'}, 'authors': {'value': ['Kaike Zhang', 'Qi Cao', 'Yunfan Wu', 'Fei Sun', 'Huawei Shen', 'Xueqi Cheng']}, 'authorids': {'value': ['~Kaike_Zhang1', '~Qi_Cao1', '~Yunfan_Wu1', '~Fei_Sun3', '~Huawei_Shen1', '~Xueqi_Cheng1']}, 'keywords': {'value': ['Adversarial Collaborative Filtering', 'Robust Recommender System', 'Poisoning Attacks']}, 'TLDR': {'value': 'We theoretically analyze Adversarial Collaborative Filtering (ACF) and further propose a method to improve ACF for robust recommendations.'}, 'abstract': {'value': \"Adversarial Collaborative Filtering (ACF), which typically applies adversarial perturbations at user and item embeddings through adversarial training, is widely recognized as an effective strategy for enhancing the robustness of Collaborative Filtering (CF) recommender systems against poisoning attacks. Besides, numerous studies have empirically shown that ACF can also improve recommendation performance compared to traditional CF. Despite these empirical successes, the theoretical understanding of ACF's effectiveness in terms of both performance and robustness remains unclear. To bridge this gap, in this paper, we first theoretically show that ACF can achieve a lower recommendation error compared to traditional CF with the same training epochs in both clean and poisoned data contexts. Furthermore, by establishing bounds for reductions in recommendation error during ACF's optimization process, we find that applying personalized magnitudes of perturbation for different users based on their embedding scales can further improve ACF's effectiveness. Building on these theoretical understandings, we propose Personalized Magnitude Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate that PamaCF effectively defends against various types of poisoning attacks while significantly enhancing recommendation performance.\"}, 'primary_area': {'value': 'safety_in_machine_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/9ff2572b003febc56e01190bdc05435dc777c13c.pdf'}, 'supplementary_material': {'value': '/attachment/be37d397e929c4fada1a4c6fda6f8b2ebcb22655.zip'}, '_bibtex': {'value': '@inproceedings{\\nzhang2024understanding,\\ntitle={Understanding and Improving Adversarial Collaborative Filtering for Robust Recommendation},\\nauthor={Kaike Zhang and Qi Cao and Yunfan Wu and Fei Sun and Huawei Shen and Xueqi Cheng},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k8AYft5ED1}\\n}'}, 'paperhash': {'value': 'zhang|understanding_and_improving_adversarial_collaborative_filtering_for_robust_recommendation'}},forum = 'k8AYft5ED1',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission5740/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission5740/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission5740/-/Revision', 'NeurIPS.cc/2024/Conference/Submission5740/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k6m3y6qnSj',number = 1566,cdate = 1714690398638,pdate = 1727287664881,odate = 1730873849947,mdate = 1730873849959,tcdate = 1714690398638,tmdate = 1730873849959,ddate = None,content = {'title': {'value': 'IllumiNeRF: 3D Relighting Without Inverse Rendering'}, 'authors': {'value': ['Xiaoming Zhao', 'Pratul P. Srinivasan', 'Dor Verbin', 'Keunhong Park', 'Ricardo Martin Brualla', 'Philipp Henzler']}, 'authorids': {'value': ['~Xiaoming_Zhao1', '~Pratul_P._Srinivasan1', '~Dor_Verbin1', '~Keunhong_Park1', '~Ricardo_Martin_Brualla1', '~Philipp_Henzler1']}, 'keywords': {'value': ['3D Relighting; NeRF; Diffusion Model']}, 'abstract': {'value': 'Existing methods for relightable view synthesis --- using a set of images of an object under unknown lighting to recover a 3D representation that can be rendered from novel viewpoints under a target illumination --- are based on inverse rendering, and attempt to disentangle the object geometry, materials, and lighting that explain the input images. Furthermore, this typically involves optimization through differentiable Monte Carlo rendering, which is brittle and computationally-expensive. In this work, we propose a simpler approach: we first relight each input image using an image diffusion model conditioned on target environment lighting and estimated object geometry. We then reconstruct a Neural Radiance Field (NeRF) with these relit images, from which we render novel views under the target lighting. We demonstrate that this strategy is surprisingly competitive and achieves state-of-the-art results on multiple relighting benchmarks. Please see our project page at [illuminerf.github.io](illuminerf.github.io).'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/d3516590217a76116dc05204c27c8be9474d896e.pdf'}, '_bibtex': {'value': '@inproceedings{\\nzhao2024illuminerf,\\ntitle={IllumiNe{RF}: 3D Relighting Without Inverse Rendering},\\nauthor={Xiaoming Zhao and Pratul P. Srinivasan and Dor Verbin and Keunhong Park and Ricardo Martin Brualla and Philipp Henzler},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k6m3y6qnSj}\\n}'}, 'paperhash': {'value': 'zhao|illuminerf_3d_relighting_without_inverse_rendering'}},forum = 'k6m3y6qnSj',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission1566/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission1566/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission1566/-/Revision', 'NeurIPS.cc/2024/Conference/Submission1566/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k6iyUfwdI9',number = 20115,cdate = 1715794473977,pdate = 1727288225849,odate = 1730873999374,mdate = 1736939870091,tcdate = 1715794473977,tmdate = 1736939870091,ddate = None,content = {'title': {'value': 'To Believe or Not to Believe Your LLM: Iterative Prompting for Estimating Epistemic Uncertainty'}, 'authors': {'value': ['Yasin Abbasi-Yadkori', 'Ilja Kuzborskij', 'András György', 'Csaba Szepesvari']}, 'authorids': {'value': ['~Yasin_Abbasi-Yadkori1', '~Ilja_Kuzborskij1', '~András_György2', '~Csaba_Szepesvari1']}, 'keywords': {'value': ['uncertainty quantification', 'large language models']}, 'abstract': {'value': 'We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.'}, 'primary_area': {'value': 'probabilistic_methods'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/57904b52970ba62ecbbe68e1d3c8e28b90f4d449.pdf'}, '_bibtex': {'value': '@inproceedings{\\nabbasi-yadkori2024to,\\ntitle={To Believe or Not to Believe Your {LLM}: IterativePrompting for Estimating Epistemic Uncertainty},\\nauthor={Yasin Abbasi-Yadkori and Ilja Kuzborskij and Andr{\\\\\\'a}s Gy{\\\\\"o}rgy and Csaba Szepesvari},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k6iyUfwdI9}\\n}'}, 'paperhash': {'value': 'abbasiyadkori|to_believe_or_not_to_believe_your_llm_iterative_prompting_for_estimating_epistemic_uncertainty'}},forum = 'k6iyUfwdI9',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission20115/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission20115/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission20115/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission20115/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k6ZHvF1vkg',number = 19942,cdate = 1715793576716,pdate = 1727288221786,odate = 1730873998483,mdate = 1730873998494,tcdate = 1715793576716,tmdate = 1730873998494,ddate = None,content = {'title': {'value': 'Beyond Optimism: Exploration With Partially Observable Rewards'}, 'authors': {'value': ['Simone Parisi', 'Alireza Kazemipour', 'Michael Bowling']}, 'authorids': {'value': ['~Simone_Parisi1', '~Alireza_Kazemipour1', '~Michael_Bowling1']}, 'keywords': {'value': ['reinforcement learning', 'partial observability', 'exploration', 'successor representations']}, 'TLDR': {'value': 'Directed exploration with the successor representation for MDPs with partially observable rewards'}, 'abstract': {'value': 'Exploration in reinforcement learning (RL) remains an open challenge.\\nRL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. \\nTo improve exploration and reward discovery, popular algorithms rely on optimism. \\nBut what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? \\nIn this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty.\\nWith this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. \\nWe further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/88408786e6ac812794e794e43e6e371ebf63675f.pdf'}, '_bibtex': {'value': '@inproceedings{\\nparisi2024beyond,\\ntitle={Beyond Optimism: Exploration With Partially Observable Rewards},\\nauthor={Simone Parisi and Alireza Kazemipour and Michael Bowling},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k6ZHvF1vkg}\\n}'}, 'paperhash': {'value': 'parisi|beyond_optimism_exploration_with_partially_observable_rewards'}},forum = 'k6ZHvF1vkg',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19942/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19942/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19942/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19942/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k4EP46Q9X2',number = 4584,cdate = 1715430537873,pdate = 1727287754680,odate = 1730873877278,mdate = 1736774340954,tcdate = 1715430537873,tmdate = 1736774340954,ddate = None,content = {'title': {'value': 'Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators'}, 'authors': {'value': ['Yiyan HUANG', 'Cheuk Hang LEUNG', 'WANG Siyi', 'YIJUN LI', 'Qi WU']}, 'authorids': {'value': ['~Yiyan_HUANG2', '~Cheuk_Hang_LEUNG2', '~WANG_Siyi1', '~YIJUN_LI6', '~Qi_WU5']}, 'keywords': {'value': ['Causal inference', 'Treatment effect', 'CATE estimator selection', 'Model Validation', 'Robustness']}, 'abstract': {'value': 'The growing demand for personalized decision-making has led to a surge of interest in estimating the Conditional Average Treatment Effect (CATE). Various types of CATE estimators have been developed with advancements in machine learning and causal inference. However, selecting the desirable CATE estimator through a conventional model validation procedure remains impractical due to the absence of counterfactual outcomes in observational data. Existing approaches for CATE estimator selection, such as plug-in and pseudo-outcome metrics, face two challenges. First, they must determine the metric form and the underlying machine learning models for fitting nuisance parameters (e.g., outcome function, propensity function, and plug-in learner). Second, they lack a specific focus on selecting a robust CATE estimator. To address these challenges, this paper introduces a Distributionally Robust Metric (DRM) for CATE estimator selection. The proposed DRM is nuisance-free, eliminating the need to fit models for nuisance parameters, and it effectively prioritizes the selection of a distributionally robust CATE estimator. The experimental results validate the effectiveness of the DRM method in selecting CATE estimators that are robust to the distribution shift incurred by covariate shift and hidden confounders.'}, 'primary_area': {'value': 'causal_inference'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/34a923b41726d5d7aac047dffdbe97ab5ebb4968.pdf'}, 'supplementary_material': {'value': '/attachment/0c6112a4515039f4e353318941196115867a1891.zip'}, '_bibtex': {'value': '@inproceedings{\\nhuang2024unveiling,\\ntitle={Unveiling the Potential of Robustness in Selecting Conditional Average Treatment Effect Estimators},\\nauthor={Yiyan HUANG and Cheuk Hang LEUNG and WANG Siyi and YIJUN LI and Qi WU},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k4EP46Q9X2}\\n}'}, 'paperhash': {'value': 'huang|unveiling_the_potential_of_robustness_in_selecting_conditional_average_treatment_effect_estimators'}},forum = 'k4EP46Q9X2',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission4584/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission4584/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission4584/-/Revision', 'NeurIPS.cc/2024/Conference/Submission4584/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k2hS5Rt1N0',number = 19286,cdate = 1715790144391,pdate = 1727288205954,odate = 1730873994782,mdate = 1730873994800,tcdate = 1715790144391,tmdate = 1730873994800,ddate = None,content = {'title': {'value': 'Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation'}, 'authors': {'value': ['Yihong Guo', 'Yixuan Wang', 'Yuanyuan Shi', 'Pan Xu', 'Anqi Liu']}, 'authorids': {'value': ['~Yihong_Guo4', '~Yixuan_Wang7', '~Yuanyuan_Shi1', '~Pan_Xu1', '~Anqi_Liu2']}, 'keywords': {'value': ['off-dynamics reinforcement learning', 'domain adaptation', 'imitation learning', 'dynamics mismatch']}, 'abstract': {'value': 'Training a policy in a source domain for deployment in the target domain under a dynamics shift can be challenging, often resulting in performance degradation. Previous work tackles this challenge by training on the source domain with modified rewards derived by matching distributions between the source and the target optimal trajectories. However, pure modified rewards only ensure the behavior of the learned policy in the source domain resembles trajectories produced by the target optimal policies, which does not guarantee optimal performance when the learned policy is actually deployed to the target domain. In this work, we propose to utilize imitation learning to transfer the policy learned from the reward modification to the target domain so that the new policy can generate the same trajectories in the target domain. Our approach, Domain Adaptation and Reward Augmented Imitation Learning (DARAIL), utilizes the reward modification for domain adaptation and follows the general framework of generative adversarial imitation learning from observation (GAIfO) by applying a reward augmented estimator for the policy optimization step. Theoretically, we present an error bound for our method under a mild assumption regarding the dynamics shift to justify the motivation of our method. Empirically, our method outperforms the pure modified reward method without imitation learning and also outperforms other baselines in benchmark off-dynamics environments.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/1c7da2697e981d7dc99700ecf2763a88cb4e7628.pdf'}, '_bibtex': {'value': '@inproceedings{\\nguo2024offdynamics,\\ntitle={Off-Dynamics Reinforcement Learning via Domain Adaptation and Reward Augmented Imitation},\\nauthor={Yihong Guo and Yixuan Wang and Yuanyuan Shi and Pan Xu and Anqi Liu},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k2hS5Rt1N0}\\n}'}, 'paperhash': {'value': 'guo|offdynamics_reinforcement_learning_via_domain_adaptation_and_reward_augmented_imitation'}},forum = 'k2hS5Rt1N0',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission19286/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission19286/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission19286/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission19286/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k29Iv0XrBF',number = 11647,cdate = 1715708172246,pdate = 1727287978325,odate = 1730873940776,mdate = 1735672611980,tcdate = 1715708172246,tmdate = 1735672611980,ddate = None,content = {'title': {'value': 'Physically Compatible 3D Object Modeling from a Single Image'}, 'authors': {'value': ['Minghao Guo', 'Bohan Wang', 'Pingchuan Ma', 'Tianyuan Zhang', 'Crystal Elaine Owens', 'Chuang Gan', 'Joshua B. Tenenbaum', 'Kaiming He', 'Wojciech Matusik']}, 'authorids': {'value': ['~Minghao_Guo1', '~Bohan_Wang9', '~Pingchuan_Ma3', '~Tianyuan_Zhang2', '~Crystal_Elaine_Owens1', '~Chuang_Gan1', '~Joshua_B._Tenenbaum1', '~Kaiming_He2', '~Wojciech_Matusik2']}, 'keywords': {'value': ['single-image modeling', 'physical compatibility']}, 'abstract': {'value': 'We present a computational framework that transforms single images into 3D physical objects. The visual geometry of a physical object in an image is determined by three orthogonal attributes: mechanical properties, external forces, and rest-shape geometry. Existing single-view 3D reconstruction methods often overlook this underlying composition, presuming rigidity or neglecting external forces. Consequently, the reconstructed objects fail to withstand real-world physical forces, resulting in instability or undesirable deformation -- diverging from their intended designs as depicted in the image. Our optimization framework addresses this by embedding physical compatibility into the reconstruction process. We explicitly decompose the three physical attributes and link them through static equilibrium, which serves as a hard constraint, ensuring that the optimized physical shapes exhibit desired physical behaviors. Evaluations on a dataset collected from Objaverse demonstrate that our framework consistently enhances the physical realism of 3D models over existing methods. The utility of our framework extends to practical applications in dynamic simulations and 3D printing, where adherence to physical compatibility is paramount.'}, 'primary_area': {'value': 'machine_vision'}, 'venue': {'value': 'NeurIPS 2024 spotlight'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/3e82eada922f6bd39360615136f2df132ecffbf1.pdf'}, 'supplementary_material': {'value': '/attachment/322115a3d0d994fc79dbb38fc527c4c6761c7eea.zip'}, 'TLDR': {'value': 'We present a physical compatibility optimization framework that transforms single images into 3D physical objects, significantly enhancing stability and structural integrity under real-world physics.'}, '_bibtex': {'value': '@inproceedings{\\nguo2024physically,\\ntitle={Physically Compatible 3D Object Modeling from a Single Image},\\nauthor={Minghao Guo and Bohan Wang and Pingchuan Ma and Tianyuan Zhang and Crystal Elaine Owens and Chuang Gan and Joshua B. Tenenbaum and Kaiming He and Wojciech Matusik},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k29Iv0XrBF}\\n}'}, 'paperhash': {'value': 'guo|physically_compatible_3d_object_modeling_from_a_single_image'}},forum = 'k29Iv0XrBF',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission11647/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission11647/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission11647/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission11647/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'k1VrxRS6WZ',number = 16743,cdate = 1715773943150,pdate = 1727288135115,odate = 1730873981023,mdate = 1730873981043,tcdate = 1715773943150,tmdate = 1730873981043,ddate = None,content = {'title': {'value': 'Multi-Label Open Set Recognition'}, 'authors': {'value': ['Yibo Wang', 'Jun-Yi Hang', 'Min-Ling Zhang']}, 'authorids': {'value': ['~Yibo_Wang3', '~Jun-Yi_Hang1', '~Min-Ling_Zhang2']}, 'keywords': {'value': ['machine learning', 'multi-label learning', 'open set recognition']}, 'abstract': {'value': 'In multi-label learning, each training instance is associated with multiple labels simultaneously. Traditional multi-label learning studies primarily focus on closed set scenario, i.e. the class label set of test data is identical to those used in training phase. Nevertheless, in numerous real-world scenarios, the environment is open and dynamic where unknown labels may emerge gradually during testing. In this paper, the problem of multi-label open set recognition (MLOSR) is investigated, which poses significant challenges in classifying and recognizing instances with unknown labels in multi-label setting. To enable open set multi-label prediction, a novel approach named SLAN is proposed by leveraging sub-labeling information enriched by structural information in the feature space. Accordingly, unknown labels are recognized by differentiating the sub-labeling information from holistic supervision. Experimental results on various datasets validate the effectiveness of the proposed approach in dealing with the MLOSR problem.'}, 'primary_area': {'value': 'other'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/a51e206c67ffabf616dd9c4287e10c082a65f554.pdf'}, '_bibtex': {'value': '@inproceedings{\\nwang2024multilabel,\\ntitle={Multi-Label Open Set Recognition},\\nauthor={Yibo Wang and Jun-Yi Hang and Min-Ling Zhang},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=k1VrxRS6WZ}\\n}'}, 'paperhash': {'value': 'wang|multilabel_open_set_recognition'}},forum = 'k1VrxRS6WZ',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission16743/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission16743/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission16743/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission16743/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " Note(id = 'jzngdJQ2lY',number = 14948,cdate = 1715755495252,pdate = 1727288083624,odate = 1730873969431,mdate = 1730873969450,tcdate = 1715755495252,tmdate = 1730873969450,ddate = None,content = {'title': {'value': 'Solving Minimum-Cost Reach Avoid using Reinforcement Learning'}, 'authors': {'value': ['Oswin So', 'Cheng Ge', 'Chuchu Fan']}, 'authorids': {'value': ['~Oswin_So1', '~Cheng_Ge4', '~Chuchu_Fan2']}, 'keywords': {'value': ['Reinforcement Learning', 'Optimal Control', 'Reachability Analysis']}, 'abstract': {'value': 'Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.'}, 'primary_area': {'value': 'reinforcement_learning'}, 'venue': {'value': 'NeurIPS 2024 poster'}, 'venueid': {'value': 'NeurIPS.cc/2024/Conference'}, 'pdf': {'value': '/pdf/15e73716d9dac9b995ad791b1c3647813a8020f1.pdf'}, 'supplementary_material': {'value': '/attachment/c131b5a4d8faabfd95b6491e9d829f51f5403cca.zip'}, 'TLDR': {'value': 'We propose a new RL method for solving the minimum-cost reach-avoid problem, inspired by reachability analysis.'}, '_bibtex': {'value': '@inproceedings{\\nso2024solving,\\ntitle={Solving Minimum-Cost Reach Avoid using Reinforcement Learning},\\nauthor={Oswin So and Cheng Ge and Chuchu Fan},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=jzngdJQ2lY}\\n}'}, 'paperhash': {'value': 'so|solving_minimumcost_reach_avoid_using_reinforcement_learning'}},forum = 'jzngdJQ2lY',replyto = None,readers = ['everyone'],nonreaders = None,signatures = ['NeurIPS.cc/2024/Conference/Submission14948/Authors'],writers = ['NeurIPS.cc/2024/Conference', 'NeurIPS.cc/2024/Conference/Submission14948/Authors'],details = None,invitations = ['NeurIPS.cc/2024/Conference/-/Submission', 'NeurIPS.cc/2024/Conference/-/Post_Submission', 'NeurIPS.cc/2024/Conference/Submission14948/-/Revision', 'NeurIPS.cc/2024/Conference/-/Edit', 'NeurIPS.cc/2024/Conference/Submission14948/-/Camera_Ready_Revision'],domain = 'NeurIPS.cc/2024/Conference',license = 'CC BY 4.0'),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions = client.get_all_notes(content={'venueid': VENUE_ID})\n",
    "submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active_learning',\n",
       " 'algorithmic_game_theory',\n",
       " 'bandits',\n",
       " 'causal_inference',\n",
       " 'deep_learning_architectures',\n",
       " 'diffusion_based_models',\n",
       " 'evaluation',\n",
       " 'fairness',\n",
       " 'generative_models',\n",
       " 'graph_neural_networks',\n",
       " 'human-AI_interaction',\n",
       " 'infrastructure',\n",
       " 'interpretability_and_explainability',\n",
       " 'learning_theory',\n",
       " 'machine_learning_for_healthcare',\n",
       " 'machine_learning_for_other_sciences_and_fields',\n",
       " 'machine_learning_for_physical_sciences',\n",
       " 'machine_learning_for_social_sciences',\n",
       " 'machine_vision',\n",
       " 'natural_language_processing',\n",
       " 'neuroscience_and_cognitive_science',\n",
       " 'online_learning',\n",
       " 'optimization',\n",
       " 'optimization_for_deep_networks',\n",
       " 'other',\n",
       " 'privacy',\n",
       " 'probabilistic_methods',\n",
       " 'reinforcement_learning',\n",
       " 'robotics',\n",
       " 'safety_in_machine_learning',\n",
       " 'speech_and_audio'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# primary areas\n",
    "areas = set()\n",
    "\n",
    "for submission in submissions:\n",
    "    areas.add(submission.content['primary_area']['value'])\n",
    "\n",
    "areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['NeurIPS 2024 poster', 'NeurIPS 2024 spotlight', 'NeurIPS 2024 oral'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "allpaper = {}\n",
    "\n",
    "for submission in submissions:\n",
    "    venue = submission.content['venue']['value']\n",
    "    area = submission.content['primary_area']['value']\n",
    "\n",
    "    if venue not in allpaper:\n",
    "        allpaper[venue] = defaultdict(dict)\n",
    "\n",
    "    if area not in allpaper[venue]:\n",
    "        allpaper[venue][area] = []\n",
    "\n",
    "    allpaper[venue][area].append(submission)\n",
    "\n",
    "allpaper.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export .bib & .md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@inproceedings{\\nhe2024learning,\\ntitle={Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},\\nauthor={Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov},\\nbooktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},\\nyear={2024},\\nurl={https://openreview.net/forum?id=aVh9KRZdRk}\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_paper.content\n",
    "sample_paper.content['venue']['value']\n",
    "sample_paper.content['primary_area']['value']\n",
    "sample_paper.content['_bibtex']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'inproceedings',\n",
       " 'id': 'TianyuHe2024NeurIPS',\n",
       " 'title': 'Learning to grok{:} Emergence of in-context learning and skill composition in modular arithmetic tasks',\n",
       " 'author': 'Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov',\n",
       " 'booktitle': 'The Thirty-eighth Annual Conference on Neural Information Processing Systems',\n",
       " 'year': '2024',\n",
       " 'url': 'https://openreview.net/forum?id=aVh9KRZdRk'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_bitex(bibtex_data, gen_id=False, id_surfix='', lower_case_type=True):\n",
    "    entry_regex = re.compile(r'@([a-zA-Z]+){([^,]+),(.*)}', re.DOTALL)\n",
    "    match = entry_regex.search(bibtex_data)\n",
    "\n",
    "    if not match:\n",
    "        raise ValueError(\"Invalid BibTeX data\")\n",
    "\n",
    "    fields = {\n",
    "        'type': match.group(1),\n",
    "        'id': match.group(2)\n",
    "    }\n",
    "    fields_str = match.group(3)\n",
    "\n",
    "    mode = 'key'\n",
    "    store = ''\n",
    "    max_layer = 0\n",
    "    stack = []\n",
    "    keys = []\n",
    "    values = []\n",
    "\n",
    "    for idx, char in enumerate(fields_str):\n",
    "        if mode == 'key':\n",
    "            if char == '=':\n",
    "                keys.append(store.strip())\n",
    "                store = ''\n",
    "                mode = 'value'\n",
    "            else:\n",
    "                store += char\n",
    "        elif mode == 'value':\n",
    "            store += char\n",
    "\n",
    "            if char == '{':\n",
    "                stack.append(char)\n",
    "                max_layer += 1\n",
    "                if max_layer == 1:\n",
    "                    store = ''\n",
    "            elif char == '}':\n",
    "                stack.pop()\n",
    "                if not stack:\n",
    "                    store = store[:-1]\n",
    "\n",
    "            if (max_layer > 0 and not stack) or (max_layer == 0 and (char in ',}' or idx == len(fields_str) - 1)):\n",
    "                value = store.strip().replace(\": \", \"{:} \")\n",
    "                if value.startswith('{') or value.endswith('}'):\n",
    "                    value = f'\"{value}\"'\n",
    "                values.append(value)\n",
    "                store = ''\n",
    "                max_layer = 0\n",
    "                mode = 'key'\n",
    "\n",
    "    for key, value in zip(keys, values):\n",
    "        fields[key.lower().replace('\\n', '').replace(',', '')] = value\n",
    "\n",
    "    if gen_id and 'author' in fields:\n",
    "        authors = fields['author'].split(' and ')\n",
    "        first_author = authors[0]\n",
    "        if ',' in first_author:\n",
    "            last_name, first_name = map(str.strip, first_author.split(','))\n",
    "        else:\n",
    "            name_parts = list(map(str.strip, first_author.split()))\n",
    "            last_name = name_parts.pop()\n",
    "            first_name = ' '.join(name_parts)\n",
    "\n",
    "        fields['id'] = re.sub(r'[^a-zA-Z0-9]', '', f\"{first_name}{last_name}{fields.get('year', '')}\") + id_surfix\n",
    "\n",
    "    if lower_case_type:\n",
    "        fields['type'] = fields['type'].lower()\n",
    "\n",
    "    return fields\n",
    "\n",
    "bib = sample_paper.content['_bibtex']['value']\n",
    "fields = parse_bitex(bib, gen_id=True, id_surfix='NeurIPS')\n",
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@inproceedings{TianyuHe2024NeurIPS,\n",
      "  title = {Learning to grok{:} Emergence of in-context learning and skill composition in modular arithmetic tasks},\n",
      "  author = {Tianyu He and Darshil Doshi and Aritra Das and Andrey Gromov},\n",
      "  booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},\n",
      "  year = {2024},\n",
      "  url = {https://openreview.net/forum?id=aVh9KRZdRk},\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_bibtex(fields):\n",
    "    # generate bibtex from a dict\n",
    "    bibtex = '@' + fields['type'] + '{' + fields['id'] + ',\\n'\n",
    "\n",
    "    for key, value in fields.items():\n",
    "        if key not in ['type', 'id']:\n",
    "            bibtex += '  ' + key + ' = ' + '{' + value + '},\\n'\n",
    "    bibtex += \"}\\n\"\n",
    "    return bibtex\n",
    "\n",
    "print(make_bibtex(fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeurIPS 2024 poster\n",
      "safety_in_machine_learning 169\n",
      "machine_vision 530\n",
      "generative_models 184\n",
      "learning_theory 219\n",
      "natural_language_processing 270\n",
      "infrastructure 25\n",
      "machine_learning_for_healthcare 69\n",
      "graph_neural_networks 110\n",
      "deep_learning_architectures 176\n",
      "causal_inference 72\n",
      "neuroscience_and_cognitive_science 89\n",
      "reinforcement_learning 251\n",
      "diffusion_based_models 200\n",
      "interpretability_and_explainability 111\n",
      "other 132\n",
      "probabilistic_methods 107\n",
      "privacy 77\n",
      "machine_learning_for_physical_sciences 77\n",
      "optimization_for_deep_networks 113\n",
      "optimization 173\n",
      "speech_and_audio 27\n",
      "evaluation 40\n",
      "fairness 46\n",
      "algorithmic_game_theory 43\n",
      "online_learning 56\n",
      "active_learning 23\n",
      "machine_learning_for_other_sciences_and_fields 127\n",
      "robotics 38\n",
      "human-AI_interaction 18\n",
      "bandits 57\n",
      "machine_learning_for_social_sciences 19\n",
      "NeurIPS 2024 spotlight\n",
      "machine_learning_for_healthcare 5\n",
      "graph_neural_networks 7\n",
      "natural_language_processing 21\n",
      "machine_vision 43\n",
      "reinforcement_learning 17\n",
      "generative_models 15\n",
      "machine_learning_for_other_sciences_and_fields 16\n",
      "interpretability_and_explainability 9\n",
      "other 15\n",
      "machine_learning_for_physical_sciences 7\n",
      "learning_theory 29\n",
      "optimization 15\n",
      "probabilistic_methods 18\n",
      "online_learning 6\n",
      "infrastructure 1\n",
      "active_learning 2\n",
      "diffusion_based_models 16\n",
      "speech_and_audio 2\n",
      "safety_in_machine_learning 9\n",
      "neuroscience_and_cognitive_science 16\n",
      "algorithmic_game_theory 11\n",
      "evaluation 6\n",
      "optimization_for_deep_networks 9\n",
      "robotics 5\n",
      "privacy 7\n",
      "causal_inference 5\n",
      "human-AI_interaction 2\n",
      "deep_learning_architectures 8\n",
      "bandits 3\n",
      "fairness 1\n",
      "NeurIPS 2024 oral\n",
      "optimization 1\n",
      "machine_vision 6\n",
      "human-AI_interaction 2\n",
      "machine_learning_for_other_sciences_and_fields 2\n",
      "diffusion_based_models 5\n",
      "learning_theory 3\n",
      "reinforcement_learning 8\n",
      "generative_models 5\n",
      "natural_language_processing 9\n",
      "safety_in_machine_learning 2\n",
      "machine_learning_for_physical_sciences 3\n",
      "interpretability_and_explainability 1\n",
      "bandits 1\n",
      "deep_learning_architectures 2\n",
      "causal_inference 2\n",
      "neuroscience_and_cognitive_science 2\n",
      "graph_neural_networks 4\n",
      "infrastructure 1\n",
      "machine_learning_for_healthcare 1\n",
      "optimization_for_deep_networks 1\n"
     ]
    }
   ],
   "source": [
    "for name, value in allpaper.items():\n",
    "    print(name)\n",
    "    for name, value in value.items():\n",
    "        print(name, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib = []\n",
    "mdbib = []\n",
    "bibkey = set()\n",
    "\n",
    "for name, venue in allpaper.items():\n",
    "    bib.append('% ---------------------------')\n",
    "    bib.append(f'% {name}')\n",
    "    bib.append('% ---------------------------\\n')\n",
    "\n",
    "    mdbib.append(f'## {name}\\n')\n",
    "\n",
    "    for name, area in venue.items():\n",
    "        bib.append(f'% {name}\\n')\n",
    "        mdbib.append(f'### {name}\\n')\n",
    "        mdbib.append(f'```bibtex')\n",
    "\n",
    "        for paper in area:\n",
    "            bibtex = paper.content['_bibtex']['value']\n",
    "            fields = parse_bitex(bibtex, gen_id=True, id_surfix='NeurIPS')\n",
    "            \n",
    "            # resolve duplicated bibkey\n",
    "            while(fields['id'] in bibkey):\n",
    "                fields['id'] += '+'\n",
    "                \n",
    "            bibkey.add(fields['id'])\n",
    "\n",
    "            # add abstract\n",
    "            abstract = paper.content['abstract']['value'].replace('\\n', '')\n",
    "            fields['abstract'] = abstract\n",
    "\n",
    "            # add tags\n",
    "            tags = [paper.content['venue']['value'], paper.content['primary_area']['value']]\n",
    "            fields['tags'] = ', '.join(tags)\n",
    "\n",
    "            # add to bib list (w/o abstract)\n",
    "            bib.append(make_bibtex({\n",
    "                key: fields[key]\n",
    "                for key in fields.keys() if key != 'abstract'\n",
    "            }))\n",
    "\n",
    "            # add to markdown list\n",
    "            mdbib.append(make_bibtex(fields))\n",
    "\n",
    "        mdbib.append(f'```\\n')\n",
    "\n",
    "with open('output/neurips2024.bib', 'w') as f:\n",
    "    f.write('\\n'.join(bib))\n",
    "\n",
    "with open('output/neurips2024.md', 'w') as f:\n",
    "    f.write('\\n'.join(mdbib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top-conf-scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
